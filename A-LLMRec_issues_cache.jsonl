[{"url": "https://github.com/ghdtjr/A-LLMRec/issues/11", "number": 11, "title": "How to Enable hit@3 and hit@5 Accuracy Metrics in the Model", "state": "OPEN", "createdAt": "2024-10-31T11:18:55Z", "updatedAt": "2024-10-31T11:18:55Z", "author": {"login": "haorongchen1015"}, "labels": {"nodes": []}, "comments": {"nodes": []}, "reactions": {"totalCount": 0}}, {"url": "https://github.com/ghdtjr/A-LLMRec/issues/10", "number": 10, "title": "Inconsistent result in Amazon_Beauty.", "state": "OPEN", "createdAt": "2024-10-18T07:22:42Z", "updatedAt": "2024-10-28T13:07:15Z", "author": {"login": "joyjiuyi"}, "labels": {"nodes": []}, "comments": {"nodes": [{"author": {"login": "betoobusy"}, "bodyText": "I have tested on Amazon_Beauty dataset, the result is 55.90 which is inconsistent with that reported in the paper, could you give me some instructions? Thank you!\nHello, my result is even lower, 53.63. Can we communicate"}, {"author": {"login": "ghdtjr"}, "bodyText": "I appreciate your interest in my work.\nThere might be several parameters for the performance inconsistency.\nDid you change the dataset preprocessing code for the Beauty dataset?\nThe uploaded code is a preprocessing for the Movies dataset and the Beauty dataset follows slightly different settings to make it smaller.\nSorry for the late response."}, {"author": {"login": "joyjiuyi"}, "bodyText": "Thank you very much for your reply, your work is very inspiring to me. As you said, I am using the preprocessing code you uploaded, can you please give me more guidance about beauty data preprocessing? Currently I am getting user and item numbers of 11690 and 6534 respectively, which does not match with 9930 and 6141 in the article. Thanks again for your answer and looking forward to your reply!"}, {"author": {"login": "ghdtjr"}, "bodyText": "I described the details of the pre-processing methods in our paper.\nYou can easily follow the protocol with some adjustments from the uploaded code."}, {"author": {"login": "joyjiuyi"}, "bodyText": "Thank you very much for your patience. But I still have a little problem regarding the data processing, even if I process the Movies_and_TV dataset according to the code you uploaded, the user and item numbers are still not the same as reported in your article. With the threshold set to 5, I get numbers 3111143 and 86678 which are different from 297,498 and 59,944 in the article, can you give me more guidance? Much appreciated!"}]}, "reactions": {"totalCount": 0}}, {"url": "https://github.com/ghdtjr/A-LLMRec/issues/9", "number": 9, "title": "Issue with Input Length Error during Inference", "state": "CLOSED", "createdAt": "2024-09-09T12:36:17Z", "updatedAt": "2024-10-22T03:02:29Z", "author": {"login": "haorongchen1015"}, "labels": {"nodes": []}, "comments": {"nodes": [{"author": {"login": "haorongchen1015"}, "bodyText": "data set movies and tvs also has this issue"}, {"author": {"login": "betoobusy"}, "bodyText": "me too\uff01"}, {"author": {"login": "joyjiuyi"}, "bodyText": "Maybe you can solve this problem refer to boheumd/MA-LMM#16."}]}, "reactions": {"totalCount": 0}}, {"url": "https://github.com/ghdtjr/A-LLMRec/issues/8", "number": 8, "title": "RuntimeError: expected mat1 and mat2 to have the same dtype, but got: c10::Half != float", "state": "CLOSED", "createdAt": "2024-09-07T06:34:52Z", "updatedAt": "2024-09-12T08:45:24Z", "author": {"login": "97z"}, "labels": {"nodes": []}, "comments": {"nodes": [{"author": {"login": "ghdtjr"}, "bodyText": "I often faced that issue due to the quantizing of the LLM model. From below code in the llm4rec.py, try removing the load_in_8bit=True option.\nself.llm_model = OPTForCausalLM.from_pretrained(\"facebook/opt-6.7b\", torch_dtype=torch.float16, load_in_8bit=True, device_map=self.device)"}, {"author": {"login": "97z"}, "bodyText": "I often faced that issue due to the quantizing of the LLM model. From below code in the llm4rec.py, try removing the load_in_8bit=True option.\nself.llm_model = OPTForCausalLM.from_pretrained(\"facebook/opt-6.7b\", torch_dtype=torch.float16, load_in_8bit=True, device_map=self.device)\n\nThank you! I have done"}]}, "reactions": {"totalCount": 0}}, {"url": "https://github.com/ghdtjr/A-LLMRec/issues/7", "number": 7, "title": "Early stop", "state": "CLOSED", "createdAt": "2024-09-06T07:38:18Z", "updatedAt": "2024-09-08T03:02:30Z", "author": {"login": "97z"}, "labels": {"nodes": []}, "comments": {"nodes": [{"author": {"login": "ghdtjr"}, "bodyText": "We did not implement the early stopping. If you are dealing with a huge dataset such as Movies, that might be correct. If your purpose is just to reproduce our code, I suggest you to run with the smaller dataset. Or you can add early stopping."}, {"author": {"login": "97z"}, "bodyText": "Thank you for your reply !"}]}, "reactions": {"totalCount": 0}}, {"url": "https://github.com/ghdtjr/A-LLMRec/issues/6", "number": 6, "title": "processing dataset errors", "state": "OPEN", "createdAt": "2024-08-27T03:17:32Z", "updatedAt": "2024-09-10T07:04:04Z", "author": {"login": "xingjinshuo"}, "labels": {"nodes": []}, "comments": {"nodes": [{"author": {"login": "97z"}, "bodyText": "I have the same errors. I use Amazon review 2018, and download the review data and meta data. The video domain can catch the paper's data, but the movie domain's number is inconsistent. 311143 86678 is not the same as the paper \"297,498 59,944\""}, {"author": {"login": "ghdtjr"}, "bodyText": "@xingjinshuo We used the Luxury_Beauty dataset not All_Beauty.\nThe thresholds for the preprocessing are varying for each dataset. I guess, I did not implement the automatically to set the threshold based on the dataset. You should check the value of the threshold which is the number of minimum interactions."}, {"author": {"login": "xingjinshuo"}, "bodyText": "@ghdtjr I understand,Thanks for your answer. Is \"Toys\" the \"Toys and Games\" in the amazon review dataset?"}, {"author": {"login": "ghdtjr"}, "bodyText": "@xingjinshuo You're right. The \"Toys\" dataset means the \"Toys and Games\"."}, {"author": {"login": "97z"}, "bodyText": "@xingjinshuo You're right. The \"Toys\" dataset means the \"Toys and Games\".\n\nHello, your paper has mentioned you select 30K(your paper wrote 3K maybe is wrong)\uff0cBut the whole data after filter 4-cores is larger than 30K. Do you select the user randomly\uff1for something else\uff1f Thank you for your reply!"}]}, "reactions": {"totalCount": 0}}, {"url": "https://github.com/ghdtjr/A-LLMRec/issues/4", "number": 4, "title": "The recommendation loss calculation seems different from the original paper?", "state": "CLOSED", "createdAt": "2024-08-07T20:05:47Z", "updatedAt": "2024-08-13T09:43:50Z", "author": {"login": "ptkenny"}, "labels": {"nodes": []}, "comments": {"nodes": [{"author": {"login": "ptkenny"}, "bodyText": "Also, is the dot product missing here or is it done somewhere and I'm missing it?"}, {"author": {"login": "ghdtjr"}, "bodyText": "We followed the implementation details with the SASRec. It seems that the difference between the 'sum' and 'mean' is only about the scale. You can implement one of them.\nIf you are interested in the detailed implementation please reference the below links.\nhttps://github.com/pmixer/SASRec.pytorch\nhttps://arxiv.org/abs/1808.09781"}]}, "reactions": {"totalCount": 0}}, {"url": "https://github.com/ghdtjr/A-LLMRec/issues/3", "number": 3, "title": "cuda out of memory", "state": "CLOSED", "createdAt": "2024-07-27T12:25:02Z", "updatedAt": "2024-08-08T14:17:04Z", "author": {"login": "Ansa-Saeed"}, "labels": {"nodes": []}, "comments": {"nodes": [{"author": {"login": "haorongchen1015"}, "bodyText": "Hi Ansa,\nI hope this message finds you well. I\u2019m reaching out because I encountered an issue while setting up the environment this project. Specifically, I\u2019m seeing the following error in the requirements.txt file:\n\"\"\"\nERROR: Invalid requirement: '_libgcc_mutex=0.1=main' (from line 4 of requirements.txt)\nHint: = is not a valid operator. Did you mean == ?\n\"\"\"\nHave you encountered this issue before? If so, could you please share how you resolved it? Any advice or guidance would be greatly appreciated.\nThank you in advance for your help!\nBest regards,\nHaorong"}, {"author": {"login": "Ansa-Saeed"}, "bodyText": "Try using _libgcc_mutex==0.1, but in my case, it didn't work. I am installing only specific libraries separately, like:\ntransformers==4.32.1\nsentencetransformers==2.2.2\naccelerate==0.25.0\npytz\ntorch==2.1.2\n\nPlease see the\nrequirements.txt\nfile. Not all libraries were installed, so I am installing only the specific ones and running the code on Jupyter Notebook."}, {"author": {"login": "haorongchen1015"}, "bodyText": "I see.\nMany thanks"}, {"author": {"login": "ghdtjr"}, "bodyText": "Which dataset are you planning to use for learning the model?\nThe length of each item title directly impacts memory usage. Therefore, based on the specific dataset you might decrease the batch size or limit the item title length."}, {"author": {"login": "Ansa-Saeed"}, "bodyText": "Can you please tell me about num of user and no of item which version of amazon dataset you used *?\n\nIn your paper you used 2014 version of amazon dataset i run the code on beauty dataset but when i run the preprocess file so i got user num and item num 82331 70286  with threshold 4 [cid:fd827280-02dd-4bff-8836-24f331ce1d99]\nBut in your paper beauty result other  how its possible kindly guide me Thanks ![cid:193f6983-306e-4575-87f7-f7edb8aa7e65]\n\u2026\n________________________________\nFrom: Khs0311 ***@***.***>\nSent: Sunday, July 28, 2024 05:10\nTo: ghdtjr/A-LLMRec ***@***.***>\nCc: Ansa Saeed ***@***.***>; Author ***@***.***>\nSubject: Re: [ghdtjr/A-LLMRec] cuda out of memory (Issue #3)\n\n\nWhich dataset are you planning to use for learning the model?\n\nThe length of each item title directly impacts memory usage. Therefore, based on the specific dataset you might decrease the batch size or limit the item title length.\n\n\u2014\nReply to this email directly, view it on GitHub<#3 (comment)>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AV6YGYON6KEPCYL6YBNASLLZOTNTTAVCNFSM6AAAAABLR2ZIP6VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDENJUGQ4TMMRYGY>.\nYou are receiving this because you authored the thread."}]}, "reactions": {"totalCount": 0}}, {"url": "https://github.com/ghdtjr/A-LLMRec/issues/2", "number": 2, "title": "Invalid requirement in requirements.txt", "state": "CLOSED", "createdAt": "2024-07-17T12:44:23Z", "updatedAt": "2024-08-27T13:17:50Z", "author": {"login": "haorongchen1015"}, "labels": {"nodes": []}, "comments": {"nodes": [{"author": {"login": "ghdtjr"}, "bodyText": "Well, I have no idea about this issue.\nHave you tried this below command?\nI've created the requirements.txt file with conda.\nIf you are facing issues with the right command, it seems the versions of conda do not match.\n\"conda create --name [env] --file [this file]\""}, {"author": {"login": "haorongchen1015"}, "bodyText": "Thank you for your suggestion. I tried using the command you provided, and I'm pleased to inform you that it worked. The environment configuration was successful, and the issue with the conda versions has been resolved."}, {"author": {"login": "haorongchen1015"}, "bodyText": "Hi,\nThere is another question. When I using that \"conda create --name [env] --file requirements.txt\"\nIt says\nPackagesNotFoundError: The following packages are not available from current channels:\n\uff08The number of packages not found is far greater than listed below\uff09\n\nsympy==1.12=pypi_0\njupyter_client==7.3.4=pyhd8ed1ab_0\ntriton==2.1.0=pypi_0\ndecorator==5.1.1=pyhd8ed1ab_0\nmatplotlib-inline==0.1.6=pyhd8ed1ab_0\n...\nMany thanks"}, {"author": {"login": "ghdtjr"}, "bodyText": "It appears that the uploaded requirements.txt file contains some packages that are installed on my Linux server.\nI checked the other issue of [Anna-Saeed], I agree with Ansa-Saeed's suggestion: \"I am installing only specific libraries separately, like...\""}]}, "reactions": {"totalCount": 0}}, {"url": "https://github.com/ghdtjr/A-LLMRec/issues/1", "number": 1, "title": "Dataset", "state": "CLOSED", "createdAt": "2024-06-07T07:27:18Z", "updatedAt": "2024-06-18T08:20:25Z", "author": {"login": "Yeo-Jun-Choi"}, "labels": {"nodes": []}, "comments": {"nodes": [{"author": {"login": "ghdtjr"}, "bodyText": "I'm sorry for the late reply to your question.\nWe used the dataset of 2018 Amazon Review dataset for our experiment."}]}, "reactions": {"totalCount": 0}}]