identifier,type,docstring,function,start_point,end_point,class_identifier,f_path
cli,function,,"def cli():
    pass","Point(row=16, column=0)","Point(row=17, column=8)",,classic/cli.py
setup,function,"Installs dependencies needed for your system. Works with Linux, MacOS and Windows WSL.","def setup():
    """"""Installs dependencies needed for your system. Works with Linux, MacOS and Windows WSL.""""""
    import os
    import subprocess

    click.echo(
        click.style(
            """"""
       d8888          888             .d8888b.  8888888b. 88888888888 
      d88888          888            d88P  Y88b 888   Y88b    888     
     d88P888          888            888    888 888    888    888     
    d88P 888 888  888 888888 .d88b.  888        888   d88P    888     
   d88P  888 888  888 888   d88""""88b 888  88888 8888888P""     888     
  d88P   888 888  888 888   888  888 888    888 888           888     
 d8888888888 Y88b 888 Y88b. Y88..88P Y88b  d88P 888           888     
d88P     888  ""Y88888  ""Y888 ""Y88P""   ""Y8888P88 888           888     
                                                                                                                                       
"""""",
            fg=""green"",
        )
    )

    script_dir = os.path.dirname(os.path.realpath(__file__))
    setup_script = os.path.join(script_dir, ""setup.sh"")
    install_error = False
    if os.path.exists(setup_script):
        click.echo(click.style(""üöÄ Setup initiated...\n"", fg=""green""))
        try:
            subprocess.check_call([setup_script], cwd=script_dir)
        except subprocess.CalledProcessError:
            click.echo(
                click.style(""‚ùå There was an issue with the installation."", fg=""red"")
            )
            install_error = True
    else:
        click.echo(
            click.style(
                ""‚ùå Error: setup.sh does not exist in the current directory."", fg=""red""
            )
        )
        install_error = True

    if install_error:
        click.echo(
            click.style(
                ""\n\nüî¥ If you need help, please raise a ticket on GitHub at https://github.com/Significant-Gravitas/AutoGPT/issues\n\n"",
                fg=""magenta"",
                bold=True,
            )
        )
    else:
        click.echo(click.style(""üéâ Setup completed!\n"", fg=""green""))","Point(row=21, column=0)","Point(row=72, column=70)",,classic/cli.py
agent,function,"Commands to create, start and stop agents","def agent():
    """"""Commands to create, start and stop agents""""""
    pass","Point(row=76, column=0)","Point(row=78, column=8)",,classic/cli.py
create,function,Create's a new agent with the agent name provided,"def create(agent_name: str):
    """"""Create's a new agent with the agent name provided""""""
    import os
    import re
    import shutil

    if not re.match(r""\w*$"", agent_name):
        click.echo(
            click.style(
                f""üòû Agent name '{agent_name}' is not valid. It should not contain spaces or special characters other than -_"",
                fg=""red"",
            )
        )
        return
    try:
        new_agent_dir = f""./agents/{agent_name}""
        new_agent_name = f""{agent_name.lower()}.json""

        if not os.path.exists(new_agent_dir):
            shutil.copytree(""./forge"", new_agent_dir)
            click.echo(
                click.style(
                    f""üéâ New agent '{agent_name}' created. The code for your new agent is in: agents/{agent_name}"",
                    fg=""green"",
                )
            )
        else:
            click.echo(
                click.style(
                    f""üòû Agent '{agent_name}' already exists. Enter a different name for your agent, the name needs to be unique regardless of case"",
                    fg=""red"",
                )
            )
    except Exception as e:
        click.echo(click.style(f""üò¢ An error occurred: {e}"", fg=""red""))","Point(row=83, column=0)","Point(row=117, column=73)",,classic/cli.py
start,function,Start agent command,"def start(agent_name: str, no_setup: bool):
    """"""Start agent command""""""
    import os
    import subprocess

    script_dir = os.path.dirname(os.path.realpath(__file__))
    agent_dir = os.path.join(
        script_dir,
        f""agents/{agent_name}""
        if agent_name not in [""original_autogpt"", ""forge""]
        else agent_name,
    )
    run_command = os.path.join(agent_dir, ""run"")
    run_bench_command = os.path.join(agent_dir, ""run_benchmark"")
    if (
        os.path.exists(agent_dir)
        and os.path.isfile(run_command)
        and os.path.isfile(run_bench_command)
    ):
        os.chdir(agent_dir)
        if not no_setup:
            click.echo(f""‚åõ Running setup for agent '{agent_name}'..."")
            setup_process = subprocess.Popen([""./setup""], cwd=agent_dir)
            setup_process.wait()
            click.echo()

        # FIXME: Doesn't work: Command not found: agbenchmark
        # subprocess.Popen([""./run_benchmark"", ""serve""], cwd=agent_dir)
        # click.echo(""‚åõ (Re)starting benchmark server..."")
        # wait_until_conn_ready(8080)
        # click.echo()

        subprocess.Popen([""./run""], cwd=agent_dir)
        click.echo(f""‚åõ (Re)starting agent '{agent_name}'..."")
        wait_until_conn_ready(8000)
        click.echo(""‚úÖ Agent application started and available on port 8000"")
    elif not os.path.exists(agent_dir):
        click.echo(
            click.style(
                f""üòû Agent '{agent_name}' does not exist. Please create the agent first."",
                fg=""red"",
            )
        )
    else:
        click.echo(
            click.style(
                f""üòû Run command does not exist in the agent '{agent_name}' directory."",
                fg=""red"",
            )
        )","Point(row=127, column=0)","Point(row=176, column=9)",,classic/cli.py
stop,function,Stop agent command,"def stop():
    """"""Stop agent command""""""
    import os
    import signal
    import subprocess

    try:
        pids = subprocess.check_output([""lsof"", ""-t"", ""-i"", "":8000""]).split()
        if isinstance(pids, int):
            os.kill(int(pids), signal.SIGTERM)
        else:
            for pid in pids:
                os.kill(int(pid), signal.SIGTERM)
    except subprocess.CalledProcessError:
        click.echo(""No process is running on port 8000"")

    try:
        pids = int(subprocess.check_output([""lsof"", ""-t"", ""-i"", "":8080""]))
        if isinstance(pids, int):
            os.kill(int(pids), signal.SIGTERM)
        else:
            for pid in pids:
                os.kill(int(pid), signal.SIGTERM)
    except subprocess.CalledProcessError:
        click.echo(""No process is running on port 8080"")","Point(row=180, column=0)","Point(row=204, column=56)",,classic/cli.py
list,function,List agents command,"def list():
    """"""List agents command""""""
    import os

    try:
        agents_dir = ""./agents""
        agents_list = [
            d
            for d in os.listdir(agents_dir)
            if os.path.isdir(os.path.join(agents_dir, d))
        ]
        if os.path.isdir(""./original_autogpt""):
            agents_list.append(""original_autogpt"")
        if agents_list:
            click.echo(click.style(""Available agents: ü§ñ"", fg=""green""))
            for agent in agents_list:
                click.echo(click.style(f""\tüêô {agent}"", fg=""blue""))
        else:
            click.echo(click.style(""No agents found üòû"", fg=""red""))
    except FileNotFoundError:
        click.echo(click.style(""The agents directory does not exist üò¢"", fg=""red""))
    except Exception as e:
        click.echo(click.style(f""An error occurred: {e} üò¢"", fg=""red""))","Point(row=208, column=0)","Point(row=230, column=73)",,classic/cli.py
benchmark,function,Commands to start the benchmark and list tests and categories,"def benchmark():
    """"""Commands to start the benchmark and list tests and categories""""""
    pass","Point(row=234, column=0)","Point(row=236, column=8)",,classic/cli.py
start,function,Starts the benchmark command,"def start(agent_name, subprocess_args):
    """"""Starts the benchmark command""""""
    import os
    import subprocess

    script_dir = os.path.dirname(os.path.realpath(__file__))
    agent_dir = os.path.join(
        script_dir,
        f""agents/{agent_name}""
        if agent_name not in [""original_autogpt"", ""forge""]
        else agent_name,
    )
    benchmark_script = os.path.join(agent_dir, ""run_benchmark"")
    if os.path.exists(agent_dir) and os.path.isfile(benchmark_script):
        os.chdir(agent_dir)
        subprocess.Popen([benchmark_script, *subprocess_args], cwd=agent_dir)
        click.echo(
            click.style(
                f""üöÄ Running benchmark for '{agent_name}' with subprocess arguments: {' '.join(subprocess_args)}"",
                fg=""green"",
            )
        )
    else:
        click.echo(
            click.style(
                f""üòû Agent '{agent_name}' does not exist. Please create the agent first."",
                fg=""red"",
            )
        )","Point(row=246, column=0)","Point(row=274, column=9)",,classic/cli.py
benchmark_categories,function,Benchmark categories group command,"def benchmark_categories():
    """"""Benchmark categories group command""""""
    pass","Point(row=278, column=0)","Point(row=280, column=8)",,classic/cli.py
benchmark_categories_list,function,List benchmark categories command,"def benchmark_categories_list():
    """"""List benchmark categories command""""""
    import glob
    import json
    import os

    categories = set()

    # Get the directory of this file
    this_dir = os.path.dirname(os.path.abspath(__file__))

    glob_path = os.path.join(
        this_dir,
        ""./benchmark/agbenchmark/challenges/**/[!deprecated]*/data.json"",
    )
    # Use it as the base for the glob pattern, excluding 'deprecated' directory
    for data_file in glob.glob(glob_path, recursive=True):
        if ""deprecated"" not in data_file:
            with open(data_file, ""r"") as f:
                try:
                    data = json.load(f)
                    categories.update(data.get(""category"", []))
                except json.JSONDecodeError:
                    print(f""Error: {data_file} is not a valid JSON file."")
                    continue
                except IOError:
                    print(f""IOError: file could not be read: {data_file}"")
                    continue

    if categories:
        click.echo(click.style(""Available categories: üìö"", fg=""green""))
        for category in categories:
            click.echo(click.style(f""\tüìñ {category}"", fg=""blue""))
    else:
        click.echo(click.style(""No categories found üòû"", fg=""red""))","Point(row=284, column=0)","Point(row=318, column=69)",,classic/cli.py
benchmark_tests,function,Benchmark tests group command,"def benchmark_tests():
    """"""Benchmark tests group command""""""
    pass","Point(row=322, column=0)","Point(row=324, column=8)",,classic/cli.py
benchmark_tests_list,function,List benchmark tests command,"def benchmark_tests_list():
    """"""List benchmark tests command""""""
    import glob
    import json
    import os
    import re

    tests = {}

    # Get the directory of this file
    this_dir = os.path.dirname(os.path.abspath(__file__))

    glob_path = os.path.join(
        this_dir,
        ""./benchmark/agbenchmark/challenges/**/[!deprecated]*/data.json"",
    )
    # Use it as the base for the glob pattern, excluding 'deprecated' directory
    for data_file in glob.glob(glob_path, recursive=True):
        if ""deprecated"" not in data_file:
            with open(data_file, ""r"") as f:
                try:
                    data = json.load(f)
                    category = data.get(""category"", [])
                    test_name = data.get(""name"", """")
                    if category and test_name:
                        if category[0] not in tests:
                            tests[category[0]] = []
                        tests[category[0]].append(test_name)
                except json.JSONDecodeError:
                    print(f""Error: {data_file} is not a valid JSON file."")
                    continue
                except IOError:
                    print(f""IOError: file could not be read: {data_file}"")
                    continue

    if tests:
        click.echo(click.style(""Available tests: üìö"", fg=""green""))
        for category, test_list in tests.items():
            click.echo(click.style(f""\tüìñ {category}"", fg=""blue""))
            for test in sorted(test_list):
                test_name = (
                    "" "".join(word for word in re.split(""([A-Z][a-z]*)"", test) if word)
                    .replace(""_"", """")
                    .replace(""C L I"", ""CLI"")
                    .replace(""  "", "" "")
                )
                test_name_padded = f""{test_name:<40}""
                click.echo(click.style(f""\t\tüî¨ {test_name_padded} - {test}"", fg=""cyan""))
    else:
        click.echo(click.style(""No tests found üòû"", fg=""red""))","Point(row=328, column=0)","Point(row=377, column=64)",,classic/cli.py
benchmark_tests_details,function,Benchmark test details command,"def benchmark_tests_details(test_name):
    """"""Benchmark test details command""""""
    import glob
    import json
    import os

    # Get the directory of this file
    this_dir = os.path.dirname(os.path.abspath(__file__))

    glob_path = os.path.join(
        this_dir,
        ""./benchmark/agbenchmark/challenges/**/[!deprecated]*/data.json"",
    )
    # Use it as the base for the glob pattern, excluding 'deprecated' directory
    for data_file in glob.glob(glob_path, recursive=True):
        with open(data_file, ""r"") as f:
            try:
                data = json.load(f)
                if data.get(""name"") == test_name:
                    click.echo(
                        click.style(
                            f""\n{data.get('name')}\n{'-'*len(data.get('name'))}\n"",
                            fg=""blue"",
                        )
                    )
                    click.echo(
                        click.style(
                            f""\tCategory:  {', '.join(data.get('category'))}"",
                            fg=""green"",
                        )
                    )
                    click.echo(click.style(f""\tTask:  {data.get('task')}"", fg=""green""))
                    click.echo(
                        click.style(
                            f""\tDependencies:  {', '.join(data.get('dependencies')) if data.get('dependencies') else 'None'}"",
                            fg=""green"",
                        )
                    )
                    click.echo(
                        click.style(f""\tCutoff:  {data.get('cutoff')}\n"", fg=""green"")
                    )
                    click.echo(
                        click.style(""\tTest Conditions\n\t-------"", fg=""magenta"")
                    )
                    click.echo(
                        click.style(
                            f""\t\tAnswer: {data.get('ground').get('answer')}"",
                            fg=""magenta"",
                        )
                    )
                    click.echo(
                        click.style(
                            f""\t\tShould Contain: {', '.join(data.get('ground').get('should_contain'))}"",
                            fg=""magenta"",
                        )
                    )
                    click.echo(
                        click.style(
                            f""\t\tShould Not Contain: {', '.join(data.get('ground').get('should_not_contain'))}"",
                            fg=""magenta"",
                        )
                    )
                    click.echo(
                        click.style(
                            f""\t\tFiles: {', '.join(data.get('ground').get('files'))}"",
                            fg=""magenta"",
                        )
                    )
                    click.echo(
                        click.style(
                            f""\t\tEval: {data.get('ground').get('eval').get('type')}\n"",
                            fg=""magenta"",
                        )
                    )
                    click.echo(click.style(""\tInfo\n\t-------"", fg=""yellow""))
                    click.echo(
                        click.style(
                            f""\t\tDifficulty: {data.get('info').get('difficulty')}"",
                            fg=""yellow"",
                        )
                    )
                    click.echo(
                        click.style(
                            f""\t\tDescription: {data.get('info').get('description')}"",
                            fg=""yellow"",
                        )
                    )
                    click.echo(
                        click.style(
                            f""\t\tSide Effects: {', '.join(data.get('info').get('side_effects'))}"",
                            fg=""yellow"",
                        )
                    )
                    break

            except json.JSONDecodeError:
                print(f""Error: {data_file} is not a valid JSON file."")
                continue
            except IOError:
                print(f""IOError: file could not be read: {data_file}"")
                continue","Point(row=382, column=0)","Point(row=482, column=24)",,classic/cli.py
wait_until_conn_ready,function,"
    Polls localhost:{port} until it is available for connections

    Params:
        port: The port for which to wait until it opens
        timeout: Timeout in seconds; maximum amount of time to wait

    Raises:
        TimeoutError: If the timeout (seconds) expires before the port opens
","def wait_until_conn_ready(port: int = 8000, timeout: int = 30):
    """"""
    Polls localhost:{port} until it is available for connections

    Params:
        port: The port for which to wait until it opens
        timeout: Timeout in seconds; maximum amount of time to wait

    Raises:
        TimeoutError: If the timeout (seconds) expires before the port opens
    """"""
    import socket
    import time

    start = time.time()
    while True:
        time.sleep(0.5)
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            if s.connect_ex((""localhost"", port)) == 0:
                break
        if time.time() > start + timeout:
            raise TimeoutError(f""Port {port} did not open within {timeout} seconds"")","Point(row=485, column=0)","Point(row=506, column=84)",,classic/cli.py
run_api_agent,function,,"async def run_api_agent(
    task: str,
    config: AgentBenchmarkConfig,
    timeout: int,
    artifacts_location: Optional[Path] = None,
    *,
    mock: bool = False,
) -> AsyncIterator[Step]:
    configuration = Configuration(host=config.host)
    async with ApiClient(configuration) as api_client:
        api_instance = AgentApi(api_client)
        task_request_body = TaskRequestBody(input=task, additional_input=None)

        start_time = time.time()
        response = await api_instance.create_agent_task(
            task_request_body=task_request_body
        )
        task_id = response.task_id

        if artifacts_location:
            logger.debug(""Uploading task input artifacts to agent..."")
            await upload_artifacts(
                api_instance, artifacts_location, task_id, ""artifacts_in""
            )

        logger.debug(""Running agent until finished or timeout..."")
        while True:
            step = await api_instance.execute_agent_task_step(task_id=task_id)
            yield step

            if time.time() - start_time > timeout:
                raise TimeoutError(""Time limit exceeded"")
            if step and mock:
                step.is_last = True
            if not step or step.is_last:
                break

        if artifacts_location:
            # In ""mock"" mode, we cheat by giving the correct artifacts to pass the test
            if mock:
                logger.debug(""Uploading mock artifacts to agent..."")
                await upload_artifacts(
                    api_instance, artifacts_location, task_id, ""artifacts_out""
                )

            logger.debug(""Downloading agent artifacts..."")
            await download_agent_artifacts_into_folder(
                api_instance, task_id, config.temp_folder
            )","Point(row=19, column=0)","Point(row=67, column=13)",,classic/benchmark/agbenchmark/agent_api_interface.py
download_agent_artifacts_into_folder,function,,"async def download_agent_artifacts_into_folder(
    api_instance: AgentApi, task_id: str, folder: Path
):
    artifacts = await api_instance.list_agent_task_artifacts(task_id=task_id)

    for artifact in artifacts.artifacts:
        # current absolute path of the directory of the file
        if artifact.relative_path:
            path: str = (
                artifact.relative_path
                if not artifact.relative_path.startswith(""/"")
                else artifact.relative_path[1:]
            )
            folder = (folder / path).parent

        if not folder.exists():
            folder.mkdir(parents=True)

        file_path = folder / artifact.file_name
        logger.debug(f""Downloading agent artifact {artifact.file_name} to {folder}"")
        with open(file_path, ""wb"") as f:
            content = await api_instance.download_agent_task_artifact(
                task_id=task_id, artifact_id=artifact.artifact_id
            )

            f.write(content)","Point(row=70, column=0)","Point(row=95, column=28)",,classic/benchmark/agbenchmark/agent_api_interface.py
upload_artifacts,function,,"async def upload_artifacts(
    api_instance: AgentApi, artifacts_location: Path, task_id: str, type: str
) -> None:
    for file_path in get_list_of_file_paths(artifacts_location, type):
        relative_path: Optional[str] = ""/"".join(
            str(file_path).split(f""{type}/"", 1)[-1].split(""/"")[:-1]
        )
        if not relative_path:
            relative_path = None

        await api_instance.upload_agent_task_artifacts(
            task_id=task_id, file=str(file_path), relative_path=relative_path
        )","Point(row=98, column=0)","Point(row=110, column=9)",,classic/benchmark/agbenchmark/agent_api_interface.py
config,function,,"def config() -> AgentBenchmarkConfig:
    return agbenchmark_config","Point(row=35, column=0)","Point(row=36, column=29)",,classic/benchmark/agbenchmark/conftest.py
temp_folder,function,"
    Pytest fixture that sets up and tears down the temporary folder for each test.
    It is automatically used in every test due to the 'autouse=True' parameter.
","def temp_folder() -> Generator[Path, None, None]:
    """"""
    Pytest fixture that sets up and tears down the temporary folder for each test.
    It is automatically used in every test due to the 'autouse=True' parameter.
    """"""

    # create output directory if it doesn't exist
    if not os.path.exists(agbenchmark_config.temp_folder):
        os.makedirs(agbenchmark_config.temp_folder, exist_ok=True)

    yield agbenchmark_config.temp_folder
    # teardown after test function completes
    if not os.getenv(""KEEP_TEMP_FOLDER_FILES""):
        for filename in os.listdir(agbenchmark_config.temp_folder):
            file_path = os.path.join(agbenchmark_config.temp_folder, filename)
            try:
                if os.path.isfile(file_path) or os.path.islink(file_path):
                    os.unlink(file_path)
                elif os.path.isdir(file_path):
                    shutil.rmtree(file_path)
            except Exception as e:
                logger.warning(f""Failed to delete {file_path}. Reason: {e}"")","Point(row=40, column=0)","Point(row=61, column=76)",,classic/benchmark/agbenchmark/conftest.py
pytest_addoption,function,"
    Pytest hook that adds command-line options to the `pytest` command.
    The added options are specific to agbenchmark and control its behavior:
    * `--mock` is used to run the tests in mock mode.
    * `--host` is used to specify the host for the tests.
    * `--category` is used to run only tests of a specific category.
    * `--nc` is used to run the tests without caching.
    * `--cutoff` is used to specify a cutoff time for the tests.
    * `--improve` is used to run only the tests that are marked for improvement.
    * `--maintain` is used to run only the tests that are marked for maintenance.
    * `--explore` is used to run the tests in exploration mode.
    * `--test` is used to run a specific test.
    * `--no-dep` is used to run the tests without dependencies.
    * `--keep-answers` is used to keep the answers of the tests.

    Args:
        parser: The Pytest CLI parser to which the command-line options are added.
","def pytest_addoption(parser: pytest.Parser) -> None:
    """"""
    Pytest hook that adds command-line options to the `pytest` command.
    The added options are specific to agbenchmark and control its behavior:
    * `--mock` is used to run the tests in mock mode.
    * `--host` is used to specify the host for the tests.
    * `--category` is used to run only tests of a specific category.
    * `--nc` is used to run the tests without caching.
    * `--cutoff` is used to specify a cutoff time for the tests.
    * `--improve` is used to run only the tests that are marked for improvement.
    * `--maintain` is used to run only the tests that are marked for maintenance.
    * `--explore` is used to run the tests in exploration mode.
    * `--test` is used to run a specific test.
    * `--no-dep` is used to run the tests without dependencies.
    * `--keep-answers` is used to keep the answers of the tests.

    Args:
        parser: The Pytest CLI parser to which the command-line options are added.
    """"""
    parser.addoption(""-N"", ""--attempts"", action=""store"")
    parser.addoption(""--no-dep"", action=""store_true"")
    parser.addoption(""--mock"", action=""store_true"")
    parser.addoption(""--host"", default=None)
    parser.addoption(""--nc"", action=""store_true"")
    parser.addoption(""--cutoff"", action=""store"")
    parser.addoption(""--category"", action=""append"")
    parser.addoption(""--test"", action=""append"")
    parser.addoption(""--improve"", action=""store_true"")
    parser.addoption(""--maintain"", action=""store_true"")
    parser.addoption(""--explore"", action=""store_true"")
    parser.addoption(""--keep-answers"", action=""store_true"")","Point(row=64, column=0)","Point(row=94, column=59)",,classic/benchmark/agbenchmark/conftest.py
pytest_configure,function,,"def pytest_configure(config: pytest.Config) -> None:
    # Register category markers to prevent ""unknown marker"" warnings
    for category in Category:
        config.addinivalue_line(""markers"", f""{category.value}: {category}"")","Point(row=97, column=0)","Point(row=100, column=75)",,classic/benchmark/agbenchmark/conftest.py
check_regression,function,"
    Fixture that checks for every test if it should be treated as a regression test,
    and whether to skip it based on that.

    The test name is retrieved from the `request` object. Regression reports are loaded
    from the path specified in the benchmark configuration.

    Effect:
    * If the `--improve` option is used and the current test is considered a regression
      test, it is skipped.
    * If the `--maintain` option is used and the current test  is not considered a
      regression test, it is also skipped.

    Args:
        request: The request object from which the test name and the benchmark
            configuration are retrieved.
","def check_regression(request: pytest.FixtureRequest) -> None:
    """"""
    Fixture that checks for every test if it should be treated as a regression test,
    and whether to skip it based on that.

    The test name is retrieved from the `request` object. Regression reports are loaded
    from the path specified in the benchmark configuration.

    Effect:
    * If the `--improve` option is used and the current test is considered a regression
      test, it is skipped.
    * If the `--maintain` option is used and the current test  is not considered a
      regression test, it is also skipped.

    Args:
        request: The request object from which the test name and the benchmark
            configuration are retrieved.
    """"""
    with contextlib.suppress(FileNotFoundError):
        rt_tracker = RegressionTestsTracker(agbenchmark_config.regression_tests_file)

        assert isinstance(request.node, pytest.Function)
        assert isinstance(request.node.parent, pytest.Class)
        test_name = request.node.parent.name
        challenge_location = getattr(request.node.cls, ""CHALLENGE_LOCATION"", """")
        skip_string = f""Skipping {test_name} at {challenge_location}""

        # Check if the test name exists in the regression tests
        is_regression_test = rt_tracker.has_regression_test(test_name)
        if request.config.getoption(""--improve"") and is_regression_test:
            pytest.skip(f""{skip_string} because it's a regression test"")
        elif request.config.getoption(""--maintain"") and not is_regression_test:
            pytest.skip(f""{skip_string} because it's not a regression test"")","Point(row=104, column=0)","Point(row=136, column=76)",,classic/benchmark/agbenchmark/conftest.py
mock,function,"
    Pytest fixture that retrieves the value of the `--mock` command-line option.
    The `--mock` option is used to run the tests in mock mode.

    Args:
        request: The `pytest.FixtureRequest` from which the `--mock` option value
            is retrieved.

    Returns:
        bool: Whether `--mock` is set for this session.
","def mock(request: pytest.FixtureRequest) -> bool:
    """"""
    Pytest fixture that retrieves the value of the `--mock` command-line option.
    The `--mock` option is used to run the tests in mock mode.

    Args:
        request: The `pytest.FixtureRequest` from which the `--mock` option value
            is retrieved.

    Returns:
        bool: Whether `--mock` is set for this session.
    """"""
    mock = request.config.getoption(""--mock"")
    assert isinstance(mock, bool)
    return mock","Point(row=140, column=0)","Point(row=154, column=15)",,classic/benchmark/agbenchmark/conftest.py
pytest_runtest_makereport,function,"
    Pytest hook that is called when a test report is being generated.
    It is used to generate and finalize reports for each test.

    Args:
        item: The test item for which the report is being generated.
        call: The call object from which the test result is retrieved.
","def pytest_runtest_makereport(item: pytest.Item, call: pytest.CallInfo) -> None:
    """"""
    Pytest hook that is called when a test report is being generated.
    It is used to generate and finalize reports for each test.

    Args:
        item: The test item for which the report is being generated.
        call: The call object from which the test result is retrieved.
    """"""
    challenge: type[BaseChallenge] = item.cls  # type: ignore
    challenge_id = challenge.info.eval_id

    if challenge_id not in test_reports:
        test_reports[challenge_id] = make_empty_test_report(challenge.info)

    if call.when == ""setup"":
        test_name = item.nodeid.split(""::"")[1]
        item.user_properties.append((""test_name"", test_name))

    if call.when == ""call"":
        add_test_result_to_report(
            test_reports[challenge_id], item, call, agbenchmark_config
        )","Point(row=160, column=0)","Point(row=182, column=9)",,classic/benchmark/agbenchmark/conftest.py
timeout_monitor,function,"
    Function that limits the total execution time of the test suite.
    This function is supposed to be run in a separate thread and calls `pytest.exit`
    if the total execution time has exceeded the global timeout.

    Args:
        start_time (int): The start time of the test suite.
","def timeout_monitor(start_time: int) -> None:
    """"""
    Function that limits the total execution time of the test suite.
    This function is supposed to be run in a separate thread and calls `pytest.exit`
    if the total execution time has exceeded the global timeout.

    Args:
        start_time (int): The start time of the test suite.
    """"""
    while time.time() - start_time < GLOBAL_TIMEOUT:
        time.sleep(1)  # check every second

    pytest.exit(""Test suite exceeded the global timeout"", returncode=1)","Point(row=185, column=0)","Point(row=197, column=71)",,classic/benchmark/agbenchmark/conftest.py
pytest_sessionstart,function,"
    Pytest hook that is called at the start of a test session.

    Sets up and runs a `timeout_monitor` in a separate thread.
","def pytest_sessionstart(session: pytest.Session) -> None:
    """"""
    Pytest hook that is called at the start of a test session.

    Sets up and runs a `timeout_monitor` in a separate thread.
    """"""
    start_time = time.time()
    t = threading.Thread(target=timeout_monitor, args=(start_time,))
    t.daemon = True  # Daemon threads are abruptly stopped at shutdown
    t.start()","Point(row=200, column=0)","Point(row=209, column=13)",,classic/benchmark/agbenchmark/conftest.py
pytest_sessionfinish,function,"
    Pytest hook that is called at the end of a test session.

    Finalizes and saves the test reports.
","def pytest_sessionfinish(session: pytest.Session) -> None:
    """"""
    Pytest hook that is called at the end of a test session.

    Finalizes and saves the test reports.
    """"""
    session_finish(agbenchmark_config)","Point(row=212, column=0)","Point(row=218, column=38)",,classic/benchmark/agbenchmark/conftest.py
pytest_generate_tests,function,,"def pytest_generate_tests(metafunc: pytest.Metafunc):
    n = metafunc.config.getoption(""-N"")
    metafunc.parametrize(""i_attempt"", range(int(n)) if type(n) is str else [0])","Point(row=221, column=0)","Point(row=223, column=79)",,classic/benchmark/agbenchmark/conftest.py
pytest_collection_modifyitems,function,"
    Pytest hook that is called after initial test collection has been performed.
    Modifies the collected test items based on the agent benchmark configuration,
    adding the dependency marker and category markers.

    Args:
        items: The collected test items to be modified.
        config: The active pytest configuration.
","def pytest_collection_modifyitems(
    items: list[pytest.Function], config: pytest.Config
) -> None:
    """"""
    Pytest hook that is called after initial test collection has been performed.
    Modifies the collected test items based on the agent benchmark configuration,
    adding the dependency marker and category markers.

    Args:
        items: The collected test items to be modified.
        config: The active pytest configuration.
    """"""
    rt_tracker = RegressionTestsTracker(agbenchmark_config.regression_tests_file)

    try:
        challenges_beaten_in_the_past = json.loads(
            agbenchmark_config.challenges_already_beaten_file.read_bytes()
        )
    except FileNotFoundError:
        challenges_beaten_in_the_past = {}

    selected_tests: tuple[str] = config.getoption(""--test"")  # type: ignore
    selected_categories: tuple[str] = config.getoption(""--category"")  # type: ignore

    # Can't use a for-loop to remove items in-place
    i = 0
    while i < len(items):
        item = items[i]
        assert item.cls and issubclass(item.cls, BaseChallenge)
        challenge = item.cls
        challenge_name = challenge.info.name

        if not issubclass(challenge, BaseChallenge):
            item.warn(
                pytest.PytestCollectionWarning(
                    f""Non-challenge item collected: {challenge}""
                )
            )
            i += 1
            continue

        # --test: remove the test from the set if it's not specifically selected
        if selected_tests and challenge.info.name not in selected_tests:
            items.remove(item)
            continue

        # Filter challenges for --maintain, --improve, and --explore:
        # --maintain -> only challenges expected to be passed (= regression tests)
        # --improve -> only challenges that so far are not passed (reliably)
        # --explore -> only challenges that have never been passed
        is_regression_test = rt_tracker.has_regression_test(challenge.info.name)
        has_been_passed = challenges_beaten_in_the_past.get(challenge.info.name, False)
        if (
            (config.getoption(""--maintain"") and not is_regression_test)
            or (config.getoption(""--improve"") and is_regression_test)
            or (config.getoption(""--explore"") and has_been_passed)
        ):
            items.remove(item)
            continue

        dependencies = challenge.info.dependencies
        if (
            config.getoption(""--test"")
            or config.getoption(""--no-dep"")
            or config.getoption(""--maintain"")
        ):
            # Ignore dependencies:
            # --test -> user selected specific tests to run, don't care about deps
            # --no-dep -> ignore dependency relations regardless of test selection
            # --maintain -> all ""regression"" tests must pass, so run all of them
            dependencies = []
        elif config.getoption(""--improve""):
            # Filter dependencies, keep only deps that are not ""regression"" tests
            dependencies = [
                d for d in dependencies if not rt_tracker.has_regression_test(d)
            ]

        # Set category markers
        challenge_categories = set(c.value for c in challenge.info.category)
        for category in challenge_categories:
            item.add_marker(category)

        # Enforce category selection
        if selected_categories:
            if not challenge_categories.intersection(set(selected_categories)):
                items.remove(item)
                continue
            # # Filter dependencies, keep only deps from selected categories
            # dependencies = [
            #     d for d in dependencies
            #     if not set(d.categories).intersection(set(selected_categories))
            # ]

        # Skip items in optional categories that are not selected for the subject agent
        challenge_optional_categories = challenge_categories & set(OPTIONAL_CATEGORIES)
        if challenge_optional_categories and not (
            agbenchmark_config.categories
            and challenge_optional_categories.issubset(
                set(agbenchmark_config.categories)
            )
        ):
            logger.debug(
                f""Skipping {challenge_name}: ""
                f""category {' and '.join(challenge_optional_categories)} is optional, ""
                ""and not explicitly selected in the benchmark config.""
            )
            items.remove(item)
            continue

        # Add marker for the DependencyManager
        item.add_marker(pytest.mark.depends(on=dependencies, name=challenge_name))

        i += 1","Point(row=226, column=0)","Point(row=338, column=14)",,classic/benchmark/agbenchmark/conftest.py
_calculate_info_test_path,function,"
    Calculates the path to the directory where the test report will be saved.
","def _calculate_info_test_path(base_path: Path, benchmark_start_time: datetime) -> Path:
    """"""
    Calculates the path to the directory where the test report will be saved.
    """"""
    # Ensure the reports path exists
    base_path.mkdir(parents=True, exist_ok=True)

    # Get current UTC date-time stamp
    date_stamp = benchmark_start_time.strftime(""%Y%m%dT%H%M%S"")

    # Default run name
    run_name = ""full_run""

    # Map command-line arguments to their respective labels
    arg_labels = {
        ""--test"": None,
        ""--category"": None,
        ""--maintain"": ""maintain"",
        ""--improve"": ""improve"",
        ""--explore"": ""explore"",
    }

    # Identify the relevant command-line argument
    for arg, label in arg_labels.items():
        if arg in sys.argv:
            test_arg = sys.argv[sys.argv.index(arg) + 1] if label is None else None
            run_name = arg.strip(""--"")
            if test_arg:
                run_name = f""{run_name}_{test_arg}""
            break

    # Create the full new directory path with ISO standard UTC date-time stamp
    report_path = base_path / f""{date_stamp}_{run_name}""

    # Ensure the new directory is created
    # FIXME: this is not a desirable side-effect of loading the config
    report_path.mkdir(exist_ok=True)

    return report_path","Point(row=10, column=0)","Point(row=48, column=22)",,classic/benchmark/agbenchmark/config.py
AgentBenchmarkConfig,class,"
    Configuration model and loader for the AGBenchmark.

    Projects that want to use AGBenchmark should contain an agbenchmark_config folder
    with a config.json file that - at minimum - specifies the `host` at which the
    subject application exposes an Agent Protocol compliant API.
","class AgentBenchmarkConfig(BaseSettings, extra=""allow""):
    """"""
    Configuration model and loader for the AGBenchmark.

    Projects that want to use AGBenchmark should contain an agbenchmark_config folder
    with a config.json file that - at minimum - specifies the `host` at which the
    subject application exposes an Agent Protocol compliant API.
    """"""

    agbenchmark_config_dir: Path = Field(exclude=True)
    """"""Path to the agbenchmark_config folder of the subject agent application.""""""

    categories: list[str] | None = None
    """"""Categories to benchmark the agent for. If omitted, all categories are assumed.""""""

    host: str
    """"""Host (scheme://address:port) of the subject agent application.""""""

    reports_folder: Path = Field(None)
    """"""
    Path to the folder where new reports should be stored.
    Defaults to {agbenchmark_config_dir}/reports.
    """"""

    @classmethod
    def load(cls, config_dir: Optional[Path] = None) -> ""AgentBenchmarkConfig"":
        config_dir = config_dir or cls.find_config_folder()
        with (config_dir / ""config.json"").open(""r"") as f:
            return cls(
                agbenchmark_config_dir=config_dir,
                **json.load(f),
            )

    @staticmethod
    def find_config_folder(for_dir: Path = Path.cwd()) -> Path:
        """"""
        Find the closest ancestor folder containing an agbenchmark_config folder,
        and returns the path of that agbenchmark_config folder.
        """"""
        current_directory = for_dir
        while current_directory != Path(""/""):
            if (path := current_directory / ""agbenchmark_config"").exists():
                if (path / ""config.json"").is_file():
                    return path
            current_directory = current_directory.parent
        raise FileNotFoundError(
            ""No 'agbenchmark_config' directory found in the path hierarchy.""
        )

    @property
    def config_file(self) -> Path:
        return self.agbenchmark_config_dir / ""config.json""

    @field_validator(""reports_folder"", mode=""before"")
    def set_reports_folder(cls, value: Path, info: ValidationInfo):
        if not value:
            return info.data[""agbenchmark_config_dir""] / ""reports""
        return value

    def get_report_dir(self, benchmark_start_time: datetime) -> Path:
        return _calculate_info_test_path(self.reports_folder, benchmark_start_time)

    @property
    def regression_tests_file(self) -> Path:
        return self.reports_folder / ""regression_tests.json""

    @property
    def success_rate_file(self) -> Path:
        return self.reports_folder / ""success_rate.json""

    @property
    def challenges_already_beaten_file(self) -> Path:
        return self.agbenchmark_config_dir / ""challenges_already_beaten.json""

    @property
    def temp_folder(self) -> Path:
        return self.agbenchmark_config_dir / ""temp_folder""","Point(row=51, column=0)","Point(row=127, column=58)",,classic/benchmark/agbenchmark/config.py
AgentBenchmarkConfig.load,function,,"def load(cls, config_dir: Optional[Path] = None) -> ""AgentBenchmarkConfig"":
        config_dir = config_dir or cls.find_config_folder()
        with (config_dir / ""config.json"").open(""r"") as f:
            return cls(
                agbenchmark_config_dir=config_dir,
                **json.load(f),
            )","Point(row=76, column=4)","Point(row=82, column=13)",AgentBenchmarkConfig,classic/benchmark/agbenchmark/config.py
AgentBenchmarkConfig.find_config_folder,function,"
        Find the closest ancestor folder containing an agbenchmark_config folder,
        and returns the path of that agbenchmark_config folder.
","def find_config_folder(for_dir: Path = Path.cwd()) -> Path:
        """"""
        Find the closest ancestor folder containing an agbenchmark_config folder,
        and returns the path of that agbenchmark_config folder.
        """"""
        current_directory = for_dir
        while current_directory != Path(""/""):
            if (path := current_directory / ""agbenchmark_config"").exists():
                if (path / ""config.json"").is_file():
                    return path
            current_directory = current_directory.parent
        raise FileNotFoundError(
            ""No 'agbenchmark_config' directory found in the path hierarchy.""
        )","Point(row=85, column=4)","Point(row=98, column=9)",AgentBenchmarkConfig,classic/benchmark/agbenchmark/config.py
AgentBenchmarkConfig.config_file,function,,"def config_file(self) -> Path:
        return self.agbenchmark_config_dir / ""config.json""","Point(row=101, column=4)","Point(row=102, column=58)",AgentBenchmarkConfig,classic/benchmark/agbenchmark/config.py
AgentBenchmarkConfig.set_reports_folder,function,,"def set_reports_folder(cls, value: Path, info: ValidationInfo):
        if not value:
            return info.data[""agbenchmark_config_dir""] / ""reports""
        return value","Point(row=105, column=4)","Point(row=108, column=20)",AgentBenchmarkConfig,classic/benchmark/agbenchmark/config.py
AgentBenchmarkConfig.get_report_dir,function,,"def get_report_dir(self, benchmark_start_time: datetime) -> Path:
        return _calculate_info_test_path(self.reports_folder, benchmark_start_time)","Point(row=110, column=4)","Point(row=111, column=83)",AgentBenchmarkConfig,classic/benchmark/agbenchmark/config.py
AgentBenchmarkConfig.regression_tests_file,function,,"def regression_tests_file(self) -> Path:
        return self.reports_folder / ""regression_tests.json""","Point(row=114, column=4)","Point(row=115, column=60)",AgentBenchmarkConfig,classic/benchmark/agbenchmark/config.py
AgentBenchmarkConfig.success_rate_file,function,,"def success_rate_file(self) -> Path:
        return self.reports_folder / ""success_rate.json""","Point(row=118, column=4)","Point(row=119, column=56)",AgentBenchmarkConfig,classic/benchmark/agbenchmark/config.py
AgentBenchmarkConfig.challenges_already_beaten_file,function,,"def challenges_already_beaten_file(self) -> Path:
        return self.agbenchmark_config_dir / ""challenges_already_beaten.json""","Point(row=122, column=4)","Point(row=123, column=77)",AgentBenchmarkConfig,classic/benchmark/agbenchmark/config.py
AgentBenchmarkConfig.temp_folder,function,,"def temp_folder(self) -> Path:
        return self.agbenchmark_config_dir / ""temp_folder""","Point(row=126, column=4)","Point(row=127, column=58)",AgentBenchmarkConfig,classic/benchmark/agbenchmark/config.py
get_list_of_file_paths,function,,"def get_list_of_file_paths(
    challenge_dir_path: str | Path, artifact_folder_name: str
) -> list[Path]:
    source_dir = Path(challenge_dir_path) / artifact_folder_name
    if not source_dir.exists():
        return []
    return list(source_dir.iterdir())","Point(row=11, column=0)","Point(row=17, column=37)",,classic/benchmark/agbenchmark/agent_interface.py
copy_challenge_artifacts_into_workspace,function,,"def copy_challenge_artifacts_into_workspace(
    challenge_dir_path: str | Path, artifact_folder_name: str, workspace: str | Path
) -> None:
    file_paths = get_list_of_file_paths(challenge_dir_path, artifact_folder_name)
    for file_path in file_paths:
        if file_path.is_file():
            shutil.copy(file_path, workspace)","Point(row=20, column=0)","Point(row=26, column=45)",,classic/benchmark/agbenchmark/agent_interface.py
BenchmarkTaskInfo,class,,"class BenchmarkTaskInfo(BaseModel):
    task_id: str
    start_time: datetime.datetime
    challenge_info: ChallengeInfo","Point(row=72, column=0)","Point(row=75, column=33)",,classic/benchmark/agbenchmark/app.py
find_agbenchmark_without_uvicorn,function,,"def find_agbenchmark_without_uvicorn():
    pids = []
    for process in psutil.process_iter(
        attrs=[
            ""pid"",
            ""cmdline"",
            ""name"",
            ""username"",
            ""status"",
            ""cpu_percent"",
            ""memory_info"",
            ""create_time"",
            ""cwd"",
            ""connections"",
        ]
    ):
        try:
            # Convert the process.info dictionary values to strings and concatenate them
            full_info = "" "".join([str(v) for k, v in process.as_dict().items()])

            if ""agbenchmark"" in full_info and ""uvicorn"" not in full_info:
                pids.append(process.pid)
        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
            pass
    return pids","Point(row=81, column=0)","Point(row=105, column=15)",,classic/benchmark/agbenchmark/app.py
CreateReportRequest,class,,"class CreateReportRequest(BaseModel):
    test: str
    test_run_id: str
    # category: Optional[str] = []
    mock: Optional[bool] = False

    model_config = ConfigDict(extra=""forbid"")","Point(row=108, column=0)","Point(row=114, column=45)",,classic/benchmark/agbenchmark/app.py
stream_output,function,,"def stream_output(pipe):
    for line in pipe:
        print(line, end="""")","Point(row=127, column=0)","Point(row=129, column=27)",,classic/benchmark/agbenchmark/app.py
setup_fastapi_app,function,,"def setup_fastapi_app(agbenchmark_config: AgentBenchmarkConfig) -> FastAPI:
    from agbenchmark.agent_api_interface import upload_artifacts
    from agbenchmark.challenges import get_challenge_from_source_uri
    from agbenchmark.main import run_benchmark

    configuration = Configuration(
        host=agbenchmark_config.host or ""http://localhost:8000""
    )
    app = FastAPI()
    app.add_middleware(
        CORSMiddleware,
        allow_origins=origins,
        allow_credentials=True,
        allow_methods=[""*""],
        allow_headers=[""*""],
    )
    router = APIRouter()

    @router.post(""/reports"")
    def run_single_test(body: CreateReportRequest) -> dict:
        pids = find_agbenchmark_without_uvicorn()
        logger.info(f""pids already running with agbenchmark: {pids}"")

        logger.debug(f""Request to /reports: {body.model_dump()}"")

        # Start the benchmark in a separate thread
        benchmark_process = Process(
            target=lambda: run_benchmark(
                config=agbenchmark_config,
                tests=(body.test,),
                mock=body.mock or False,
            )
        )
        benchmark_process.start()

        # Wait for the benchmark to finish, with a timeout of 200 seconds
        timeout = 200
        start_time = time.time()
        while benchmark_process.is_alive():
            if time.time() - start_time > timeout:
                logger.warning(f""Benchmark run timed out after {timeout} seconds"")
                benchmark_process.terminate()
                break
            time.sleep(1)
        else:
            logger.debug(f""Benchmark finished running in {time.time() - start_time} s"")

        # List all folders in the current working directory
        reports_folder = agbenchmark_config.reports_folder
        folders = [folder for folder in reports_folder.iterdir() if folder.is_dir()]

        # Sort the folders based on their names
        sorted_folders = sorted(folders, key=lambda x: x.name)

        # Get the last folder
        latest_folder = sorted_folders[-1] if sorted_folders else None

        # Read report.json from this folder
        if latest_folder:
            report_path = latest_folder / ""report.json""
            logger.debug(f""Getting latest report from {report_path}"")
            if report_path.exists():
                with report_path.open() as file:
                    data = json.load(file)
                logger.debug(f""Report data: {data}"")
            else:
                raise HTTPException(
                    502,
                    ""Could not get result after running benchmark: ""
                    f""'report.json' does not exist in '{latest_folder}'"",
                )
        else:
            raise HTTPException(
                504, ""Could not get result after running benchmark: no reports found""
            )

        return data

    @router.post(""/agent/tasks"", tags=[""agent""])
    async def create_agent_task(task_eval_request: TaskEvalRequestBody) -> Task:
        """"""
        Creates a new task using the provided TaskEvalRequestBody and returns a Task.

        Args:
            task_eval_request: `TaskRequestBody` including an eval_id.

        Returns:
            Task: A new task with task_id, input, additional_input,
                and empty lists for artifacts and steps.

        Example:
            Request (TaskEvalRequestBody defined in schema.py):
                {
                    ...,
                    ""eval_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb""
                }

            Response (Task defined in `agent_protocol_client.models`):
                {
                    ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                    ""input"": ""Write the word 'Washington' to a .txt file"",
                    ""artifacts"": []
                }
        """"""
        try:
            challenge_info = CHALLENGES[task_eval_request.eval_id]
            async with ApiClient(configuration) as api_client:
                api_instance = AgentApi(api_client)
                task_input = challenge_info.task

                task_request_body = TaskRequestBody(
                    input=task_input, additional_input=None
                )
                task_response = await api_instance.create_agent_task(
                    task_request_body=task_request_body
                )
                task_info = BenchmarkTaskInfo(
                    task_id=task_response.task_id,
                    start_time=datetime.datetime.now(datetime.timezone.utc),
                    challenge_info=challenge_info,
                )
                task_informations[task_info.task_id] = task_info

                if input_artifacts_dir := challenge_info.task_artifacts_dir:
                    await upload_artifacts(
                        api_instance,
                        input_artifacts_dir,
                        task_response.task_id,
                        ""artifacts_in"",
                    )
                return task_response
        except ApiException as e:
            logger.error(f""Error whilst trying to create a task:\n{e}"")
            logger.error(
                ""The above error was caused while processing request: ""
                f""{task_eval_request}""
            )
            raise HTTPException(500)

    @router.post(""/agent/tasks/{task_id}/steps"")
    async def proxy(request: Request, task_id: str):
        timeout = httpx.Timeout(300.0, read=300.0)  # 5 minutes
        async with httpx.AsyncClient(timeout=timeout) as client:
            # Construct the new URL
            new_url = f""{configuration.host}/ap/v1/agent/tasks/{task_id}/steps""

            # Forward the request
            response = await client.post(
                new_url,
                content=await request.body(),
                headers=dict(request.headers),
            )

            # Return the response from the forwarded request
            return Response(content=response.content, status_code=response.status_code)

    @router.post(""/agent/tasks/{task_id}/evaluations"")
    async def create_evaluation(task_id: str) -> BenchmarkRun:
        task_info = task_informations[task_id]
        challenge = get_challenge_from_source_uri(task_info.challenge_info.source_uri)
        try:
            async with ApiClient(configuration) as api_client:
                api_instance = AgentApi(api_client)
                eval_results = await challenge.evaluate_task_state(
                    api_instance, task_id
                )

            eval_info = BenchmarkRun(
                repository_info=RepositoryInfo(),
                run_details=RunDetails(
                    command=f""agbenchmark --test={challenge.info.name}"",
                    benchmark_start_time=(
                        task_info.start_time.strftime(""%Y-%m-%dT%H:%M:%S+00:00"")
                    ),
                    test_name=challenge.info.name,
                ),
                task_info=TaskInfo(
                    data_path=challenge.info.source_uri,
                    is_regression=None,
                    category=[c.value for c in challenge.info.category],
                    task=challenge.info.task,
                    answer=challenge.info.reference_answer or """",
                    description=challenge.info.description or """",
                ),
                metrics=Metrics(
                    success=all(e.passed for e in eval_results),
                    success_percentage=(
                        100 * sum(e.score for e in eval_results) / len(eval_results)
                        if eval_results  # avoid division by 0
                        else 0
                    ),
                    attempted=True,
                ),
                config={},
            )

            logger.debug(
                f""Returning evaluation data:\n{eval_info.model_dump_json(indent=4)}""
            )
            return eval_info
        except ApiException as e:
            logger.error(f""Error {e} whilst trying to evaluate task: {task_id}"")
            raise HTTPException(500)

    app.include_router(router, prefix=""/ap/v1"")

    return app","Point(row=132, column=0)","Point(row=338, column=14)",,classic/benchmark/agbenchmark/app.py
setup_fastapi_app.run_single_test,function,,"def run_single_test(body: CreateReportRequest) -> dict:
        pids = find_agbenchmark_without_uvicorn()
        logger.info(f""pids already running with agbenchmark: {pids}"")

        logger.debug(f""Request to /reports: {body.model_dump()}"")

        # Start the benchmark in a separate thread
        benchmark_process = Process(
            target=lambda: run_benchmark(
                config=agbenchmark_config,
                tests=(body.test,),
                mock=body.mock or False,
            )
        )
        benchmark_process.start()

        # Wait for the benchmark to finish, with a timeout of 200 seconds
        timeout = 200
        start_time = time.time()
        while benchmark_process.is_alive():
            if time.time() - start_time > timeout:
                logger.warning(f""Benchmark run timed out after {timeout} seconds"")
                benchmark_process.terminate()
                break
            time.sleep(1)
        else:
            logger.debug(f""Benchmark finished running in {time.time() - start_time} s"")

        # List all folders in the current working directory
        reports_folder = agbenchmark_config.reports_folder
        folders = [folder for folder in reports_folder.iterdir() if folder.is_dir()]

        # Sort the folders based on their names
        sorted_folders = sorted(folders, key=lambda x: x.name)

        # Get the last folder
        latest_folder = sorted_folders[-1] if sorted_folders else None

        # Read report.json from this folder
        if latest_folder:
            report_path = latest_folder / ""report.json""
            logger.debug(f""Getting latest report from {report_path}"")
            if report_path.exists():
                with report_path.open() as file:
                    data = json.load(file)
                logger.debug(f""Report data: {data}"")
            else:
                raise HTTPException(
                    502,
                    ""Could not get result after running benchmark: ""
                    f""'report.json' does not exist in '{latest_folder}'"",
                )
        else:
            raise HTTPException(
                504, ""Could not get result after running benchmark: no reports found""
            )

        return data","Point(row=151, column=4)","Point(row=208, column=19)",,classic/benchmark/agbenchmark/app.py
setup_fastapi_app.create_agent_task,function,"
        Creates a new task using the provided TaskEvalRequestBody and returns a Task.

        Args:
            task_eval_request: `TaskRequestBody` including an eval_id.

        Returns:
            Task: A new task with task_id, input, additional_input,
                and empty lists for artifacts and steps.

        Example:
            Request (TaskEvalRequestBody defined in schema.py):
                {
                    ...,
                    ""eval_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb""
                }

            Response (Task defined in `agent_protocol_client.models`):
                {
                    ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                    ""input"": ""Write the word 'Washington' to a .txt file"",
                    ""artifacts"": []
                }
","async def create_agent_task(task_eval_request: TaskEvalRequestBody) -> Task:
        """"""
        Creates a new task using the provided TaskEvalRequestBody and returns a Task.

        Args:
            task_eval_request: `TaskRequestBody` including an eval_id.

        Returns:
            Task: A new task with task_id, input, additional_input,
                and empty lists for artifacts and steps.

        Example:
            Request (TaskEvalRequestBody defined in schema.py):
                {
                    ...,
                    ""eval_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb""
                }

            Response (Task defined in `agent_protocol_client.models`):
                {
                    ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                    ""input"": ""Write the word 'Washington' to a .txt file"",
                    ""artifacts"": []
                }
        """"""
        try:
            challenge_info = CHALLENGES[task_eval_request.eval_id]
            async with ApiClient(configuration) as api_client:
                api_instance = AgentApi(api_client)
                task_input = challenge_info.task

                task_request_body = TaskRequestBody(
                    input=task_input, additional_input=None
                )
                task_response = await api_instance.create_agent_task(
                    task_request_body=task_request_body
                )
                task_info = BenchmarkTaskInfo(
                    task_id=task_response.task_id,
                    start_time=datetime.datetime.now(datetime.timezone.utc),
                    challenge_info=challenge_info,
                )
                task_informations[task_info.task_id] = task_info

                if input_artifacts_dir := challenge_info.task_artifacts_dir:
                    await upload_artifacts(
                        api_instance,
                        input_artifacts_dir,
                        task_response.task_id,
                        ""artifacts_in"",
                    )
                return task_response
        except ApiException as e:
            logger.error(f""Error whilst trying to create a task:\n{e}"")
            logger.error(
                ""The above error was caused while processing request: ""
                f""{task_eval_request}""
            )
            raise HTTPException(500)","Point(row=211, column=4)","Point(row=269, column=36)",,classic/benchmark/agbenchmark/app.py
setup_fastapi_app.proxy,function,,"async def proxy(request: Request, task_id: str):
        timeout = httpx.Timeout(300.0, read=300.0)  # 5 minutes
        async with httpx.AsyncClient(timeout=timeout) as client:
            # Construct the new URL
            new_url = f""{configuration.host}/ap/v1/agent/tasks/{task_id}/steps""

            # Forward the request
            response = await client.post(
                new_url,
                content=await request.body(),
                headers=dict(request.headers),
            )

            # Return the response from the forwarded request
            return Response(content=response.content, status_code=response.status_code)","Point(row=272, column=4)","Point(row=286, column=87)",,classic/benchmark/agbenchmark/app.py
setup_fastapi_app.create_evaluation,function,,"async def create_evaluation(task_id: str) -> BenchmarkRun:
        task_info = task_informations[task_id]
        challenge = get_challenge_from_source_uri(task_info.challenge_info.source_uri)
        try:
            async with ApiClient(configuration) as api_client:
                api_instance = AgentApi(api_client)
                eval_results = await challenge.evaluate_task_state(
                    api_instance, task_id
                )

            eval_info = BenchmarkRun(
                repository_info=RepositoryInfo(),
                run_details=RunDetails(
                    command=f""agbenchmark --test={challenge.info.name}"",
                    benchmark_start_time=(
                        task_info.start_time.strftime(""%Y-%m-%dT%H:%M:%S+00:00"")
                    ),
                    test_name=challenge.info.name,
                ),
                task_info=TaskInfo(
                    data_path=challenge.info.source_uri,
                    is_regression=None,
                    category=[c.value for c in challenge.info.category],
                    task=challenge.info.task,
                    answer=challenge.info.reference_answer or """",
                    description=challenge.info.description or """",
                ),
                metrics=Metrics(
                    success=all(e.passed for e in eval_results),
                    success_percentage=(
                        100 * sum(e.score for e in eval_results) / len(eval_results)
                        if eval_results  # avoid division by 0
                        else 0
                    ),
                    attempted=True,
                ),
                config={},
            )

            logger.debug(
                f""Returning evaluation data:\n{eval_info.model_dump_json(indent=4)}""
            )
            return eval_info
        except ApiException as e:
            logger.error(f""Error {e} whilst trying to evaluate task: {task_id}"")
            raise HTTPException(500)","Point(row=289, column=4)","Point(row=334, column=36)",,classic/benchmark/agbenchmark/app.py
run_benchmark,function,"
    Starts the benchmark. If a category flag is provided, only challenges with the
    corresponding mark will be run.
","def run_benchmark(
    config: AgentBenchmarkConfig,
    maintain: bool = False,
    improve: bool = False,
    explore: bool = False,
    tests: tuple[str, ...] = tuple(),
    categories: tuple[str, ...] = tuple(),
    skip_categories: tuple[str, ...] = tuple(),
    attempts_per_challenge: int = 1,
    mock: bool = False,
    no_dep: bool = False,
    no_cutoff: bool = False,
    cutoff: Optional[int] = None,
    keep_answers: bool = False,
    server: bool = False,
) -> int:
    """"""
    Starts the benchmark. If a category flag is provided, only challenges with the
    corresponding mark will be run.
    """"""
    import pytest

    from agbenchmark.reports.ReportManager import SingletonReportManager

    validate_args(
        maintain=maintain,
        improve=improve,
        explore=explore,
        tests=tests,
        categories=categories,
        skip_categories=skip_categories,
        no_cutoff=no_cutoff,
        cutoff=cutoff,
    )

    SingletonReportManager()

    for key, value in vars(config).items():
        logger.debug(f""config.{key} = {repr(value)}"")

    pytest_args = [""-vs""]

    if tests:
        logger.info(f""Running specific test(s): {' '.join(tests)}"")
        pytest_args += [f""--test={t}"" for t in tests]
    else:
        all_categories = get_unique_categories()

        if categories or skip_categories:
            categories_to_run = set(categories) or all_categories
            if skip_categories:
                categories_to_run = categories_to_run.difference(set(skip_categories))
            assert categories_to_run, ""Error: You can't skip all categories""
            pytest_args += [f""--category={c}"" for c in categories_to_run]
            logger.info(f""Running tests of category: {categories_to_run}"")
        else:
            logger.info(""Running all categories"")

        if maintain:
            logger.info(""Running only regression tests"")
        elif improve:
            logger.info(""Running only non-regression tests"")
        elif explore:
            logger.info(""Only attempt challenges that have never been beaten"")

    if mock:
        # TODO: unhack
        os.environ[
            ""IS_MOCK""
        ] = ""True""  # ugly hack to make the mock work when calling from API

    # Pass through flags
    for flag, active in {
        ""--maintain"": maintain,
        ""--improve"": improve,
        ""--explore"": explore,
        ""--no-dep"": no_dep,
        ""--mock"": mock,
        ""--nc"": no_cutoff,
        ""--keep-answers"": keep_answers,
    }.items():
        if active:
            pytest_args.append(flag)

    if attempts_per_challenge > 1:
        pytest_args.append(f""--attempts={attempts_per_challenge}"")

    if cutoff:
        pytest_args.append(f""--cutoff={cutoff}"")
        logger.debug(f""Setting cuttoff override to {cutoff} seconds."")

    current_dir = Path(__file__).resolve().parent
    pytest_args.append(str(current_dir / ""generate_test.py""))

    pytest_args.append(""--cache-clear"")
    logger.debug(f""Running Pytest with args: {pytest_args}"")
    exit_code = pytest.main(pytest_args)

    SingletonReportManager.clear_instance()
    return exit_code","Point(row=15, column=0)","Point(row=114, column=20)",,classic/benchmark/agbenchmark/main.py
InvalidInvocationError,class,,"class InvalidInvocationError(ValueError):
    pass","Point(row=117, column=0)","Point(row=118, column=8)",,classic/benchmark/agbenchmark/main.py
validate_args,function,,"def validate_args(
    maintain: bool,
    improve: bool,
    explore: bool,
    tests: Sequence[str],
    categories: Sequence[str],
    skip_categories: Sequence[str],
    no_cutoff: bool,
    cutoff: Optional[int],
) -> None:
    if categories:
        all_categories = get_unique_categories()
        invalid_categories = set(categories) - all_categories
        if invalid_categories:
            raise InvalidInvocationError(
                ""One or more invalid categories were specified: ""
                f""{', '.join(invalid_categories)}.\n""
                f""Valid categories are: {', '.join(all_categories)}.""
            )

    if (maintain + improve + explore) > 1:
        raise InvalidInvocationError(
            ""You can't use --maintain, --improve or --explore at the same time. ""
            ""Please choose one.""
        )

    if tests and (categories or skip_categories or maintain or improve or explore):
        raise InvalidInvocationError(
            ""If you're running a specific test make sure no other options are ""
            ""selected. Please just pass the --test.""
        )

    if no_cutoff and cutoff:
        raise InvalidInvocationError(
            ""You can't use both --nc and --cutoff at the same time. ""
            ""Please choose one.""
        )","Point(row=121, column=0)","Point(row=157, column=9)",,classic/benchmark/agbenchmark/main.py
InvalidInvocationError,class,,"class InvalidInvocationError(ValueError):
    pass","Point(row=27, column=0)","Point(row=28, column=8)",,classic/benchmark/agbenchmark/__main__.py
cli,function,,"def cli(
    debug: bool,
) -> Any:
    configure_logging(logging.DEBUG if debug else logging.INFO)","Point(row=47, column=0)","Point(row=50, column=63)",,classic/benchmark/agbenchmark/__main__.py
start,function,,"def start():
    raise DeprecationWarning(
        ""`agbenchmark start` is deprecated. Use `agbenchmark run` instead.""
    )","Point(row=54, column=0)","Point(row=57, column=5)",,classic/benchmark/agbenchmark/__main__.py
run,function,"
    Run the benchmark on the agent in the current directory.

    Options marked with (+) can be specified multiple times, to select multiple items.
","def run(
    maintain: bool,
    improve: bool,
    explore: bool,
    mock: bool,
    no_dep: bool,
    nc: bool,
    keep_answers: bool,
    test: tuple[str],
    category: tuple[str],
    skip_category: tuple[str],
    attempts: int,
    cutoff: Optional[int] = None,
    backend: Optional[bool] = False,
    # agent_path: Optional[Path] = None,
) -> None:
    """"""
    Run the benchmark on the agent in the current directory.

    Options marked with (+) can be specified multiple times, to select multiple items.
    """"""
    from agbenchmark.main import run_benchmark, validate_args

    agbenchmark_config = AgentBenchmarkConfig.load()
    logger.debug(f""agbenchmark_config: {agbenchmark_config.agbenchmark_config_dir}"")
    try:
        validate_args(
            maintain=maintain,
            improve=improve,
            explore=explore,
            tests=test,
            categories=category,
            skip_categories=skip_category,
            no_cutoff=nc,
            cutoff=cutoff,
        )
    except InvalidInvocationError as e:
        logger.error(""Error: "" + ""\n"".join(e.args))
        sys.exit(1)

    original_stdout = sys.stdout  # Save the original standard output
    exit_code = None

    if backend:
        with open(""backend/backend_stdout.txt"", ""w"") as f:
            sys.stdout = f
            exit_code = run_benchmark(
                config=agbenchmark_config,
                maintain=maintain,
                improve=improve,
                explore=explore,
                mock=mock,
                no_dep=no_dep,
                no_cutoff=nc,
                keep_answers=keep_answers,
                tests=test,
                categories=category,
                skip_categories=skip_category,
                attempts_per_challenge=attempts,
                cutoff=cutoff,
            )

        sys.stdout = original_stdout

    else:
        exit_code = run_benchmark(
            config=agbenchmark_config,
            maintain=maintain,
            improve=improve,
            explore=explore,
            mock=mock,
            no_dep=no_dep,
            no_cutoff=nc,
            keep_answers=keep_answers,
            tests=test,
            categories=category,
            skip_categories=skip_category,
            attempts_per_challenge=attempts,
            cutoff=cutoff,
        )

        sys.exit(exit_code)","Point(row=103, column=0)","Point(row=184, column=27)",,classic/benchmark/agbenchmark/__main__.py
serve,function,Serve the benchmark frontend and API on port 8080.,"def serve(port: Optional[int] = None):
    """"""Serve the benchmark frontend and API on port 8080.""""""
    import uvicorn

    from agbenchmark.app import setup_fastapi_app

    config = AgentBenchmarkConfig.load()
    app = setup_fastapi_app(config)

    # Run the FastAPI application using uvicorn
    port = port or int(os.getenv(""PORT"", 8080))
    uvicorn.run(app, host=""0.0.0.0"", port=port)","Point(row=189, column=0)","Point(row=200, column=47)",,classic/benchmark/agbenchmark/__main__.py
config,function,Displays info regarding the present AGBenchmark config.,"def config():
    """"""Displays info regarding the present AGBenchmark config.""""""
    from .utils.utils import pretty_print_model

    try:
        config = AgentBenchmarkConfig.load()
    except FileNotFoundError as e:
        click.echo(e, err=True)
        return 1

    pretty_print_model(config, include_header=False)","Point(row=204, column=0)","Point(row=214, column=52)",,classic/benchmark/agbenchmark/__main__.py
challenge,function,,"def challenge():
    logging.getLogger().setLevel(logging.WARNING)","Point(row=218, column=0)","Point(row=219, column=49)",,classic/benchmark/agbenchmark/__main__.py
list_challenges,function,Lists [available|all] challenges.,"def list_challenges(include_unavailable: bool, only_names: bool, output_json: bool):
    """"""Lists [available|all] challenges.""""""
    import json

    from tabulate import tabulate

    from .challenges.builtin import load_builtin_challenges
    from .challenges.webarena import load_webarena_challenges
    from .utils.data_types import Category, DifficultyLevel
    from .utils.utils import sorted_by_enum_index

    DIFFICULTY_COLORS = {
        difficulty: color
        for difficulty, color in zip(
            DifficultyLevel,
            [""black"", ""blue"", ""cyan"", ""green"", ""yellow"", ""red"", ""magenta"", ""white""],
        )
    }
    CATEGORY_COLORS = {
        category: f""bright_{color}""
        for category, color in zip(
            Category,
            [""blue"", ""cyan"", ""green"", ""yellow"", ""magenta"", ""red"", ""white"", ""black""],
        )
    }

    # Load challenges
    challenges = filter(
        lambda c: c.info.available or include_unavailable,
        [
            *load_builtin_challenges(),
            *load_webarena_challenges(skip_unavailable=False),
        ],
    )
    challenges = sorted_by_enum_index(
        challenges, DifficultyLevel, key=lambda c: c.info.difficulty
    )

    if only_names:
        if output_json:
            click.echo(json.dumps([c.info.name for c in challenges]))
            return

        for c in challenges:
            click.echo(
                click.style(c.info.name, fg=None if c.info.available else ""black"")
            )
        return

    if output_json:
        click.echo(
            json.dumps([json.loads(c.info.model_dump_json()) for c in challenges])
        )
        return

    headers = tuple(
        click.style(h, bold=True) for h in (""Name"", ""Difficulty"", ""Categories"")
    )
    table = [
        tuple(
            v if challenge.info.available else click.style(v, fg=""black"")
            for v in (
                challenge.info.name,
                (
                    click.style(
                        challenge.info.difficulty.value,
                        fg=DIFFICULTY_COLORS[challenge.info.difficulty],
                    )
                    if challenge.info.difficulty
                    else click.style(""-"", fg=""black"")
                ),
                "" "".join(
                    click.style(cat.value, fg=CATEGORY_COLORS[cat])
                    for cat in sorted_by_enum_index(challenge.info.category, Category)
                ),
            )
        )
        for challenge in challenges
    ]
    click.echo(tabulate(table, headers=headers))","Point(row=230, column=0)","Point(row=309, column=48)",,classic/benchmark/agbenchmark/__main__.py
info,function,,"def info(name: str, json: bool):
    from itertools import chain

    from .challenges.builtin import load_builtin_challenges
    from .challenges.webarena import load_webarena_challenges
    from .utils.utils import pretty_print_model

    for challenge in chain(
        load_builtin_challenges(),
        load_webarena_challenges(skip_unavailable=False),
    ):
        if challenge.info.name != name:
            continue

        if json:
            click.echo(challenge.info.model_dump_json())
            break

        pretty_print_model(challenge.info)
        break
    else:
        click.echo(click.style(f""Unknown challenge '{name}'"", fg=""red""), err=True)","Point(row=315, column=0)","Point(row=336, column=82)",,classic/benchmark/agbenchmark/__main__.py
version,function,Print version info for the AGBenchmark application.,"def version():
    """"""Print version info for the AGBenchmark application.""""""
    import toml

    package_root = Path(__file__).resolve().parent.parent
    pyproject = toml.load(package_root / ""pyproject.toml"")
    version = pyproject[""tool""][""poetry""][""version""]
    click.echo(f""AGBenchmark version {version}"")","Point(row=340, column=0)","Point(row=347, column=48)",,classic/benchmark/agbenchmark/__main__.py
TaskRequestBody,class,,"class TaskRequestBody(BaseModel):
    input: str = Field(
        min_length=1,
        description=""Input prompt for the task."",
        examples=[""Write the words you receive to the file 'output.txt'.""],
    )
    additional_input: Optional[dict[str, Any]] = Field(default_factory=dict)","Point(row=7, column=0)","Point(row=13, column=76)",,classic/benchmark/agbenchmark/schema.py
TaskEvalRequestBody,class,,"class TaskEvalRequestBody(TaskRequestBody):
    eval_id: str","Point(row=16, column=0)","Point(row=17, column=16)",,classic/benchmark/agbenchmark/schema.py
WebArenaSiteInfo,class,,"class WebArenaSiteInfo(BaseModel):
    base_url: str
    available: bool = True
    additional_info: str = """"
    unavailable_reason: str = """"","Point(row=25, column=0)","Point(row=29, column=32)",,classic/benchmark/agbenchmark/challenges/webarena.py
get_site_info,function,,"def get_site_info(site: WebArenaSite) -> WebArenaSiteInfo:
    if site not in site_info_map:
        raise ValueError(f""JungleGym site '{site}' unknown, cannot resolve URL"")
    return site_info_map[site]","Point(row=63, column=0)","Point(row=66, column=30)",,classic/benchmark/agbenchmark/challenges/webarena.py
get_site_url,function,,"def get_site_url(site: WebArenaSite) -> str:
    return get_site_info(site).base_url","Point(row=69, column=0)","Point(row=70, column=39)",,classic/benchmark/agbenchmark/challenges/webarena.py
resolve_uri,function,"
    Resolves URIs with mock hosts, like `__WIKI__/wiki/Octopus`, with the corresponding
    JungleGym site mirror host.
","def resolve_uri(uri: str) -> str:
    """"""
    Resolves URIs with mock hosts, like `__WIKI__/wiki/Octopus`, with the corresponding
    JungleGym site mirror host.
    """"""
    segments = uri.split(""__"")
    if len(segments) > 2 and (site := segments[1]).lower() in site_info_map:
        return uri.replace(f""__{site}__"", get_site_url(site.lower()))  # type: ignore
    return uri","Point(row=73, column=0)","Point(row=81, column=14)",,classic/benchmark/agbenchmark/challenges/webarena.py
Eval,class,,"class Eval(ABC):
    @abstractmethod
    def evaluate(self, string: str) -> bool:
        ...

    @property
    @abstractmethod
    def description(self) -> str:
        ...","Point(row=84, column=0)","Point(row=92, column=11)",,classic/benchmark/agbenchmark/challenges/webarena.py
Eval.evaluate,function,,"def evaluate(self, string: str) -> bool:
        ...","Point(row=86, column=4)","Point(row=87, column=11)",Eval,classic/benchmark/agbenchmark/challenges/webarena.py
Eval.description,function,,"def description(self) -> str:
        ...","Point(row=91, column=4)","Point(row=92, column=11)",Eval,classic/benchmark/agbenchmark/challenges/webarena.py
BaseStringEval,class,,"class BaseStringEval(BaseModel, Eval):
    # type: ReferenceAnswerType
    pass","Point(row=95, column=0)","Point(row=97, column=8)",,classic/benchmark/agbenchmark/challenges/webarena.py
ExactStringMatchEval,class,,"class ExactStringMatchEval(BaseStringEval):
    type: Literal[""exact_match""] = ""exact_match""
    reference_answer: str

    @property
    def description(self) -> str:
        return f""Answer must be '{self.reference_answer}'""

    def evaluate(self, string: str) -> bool:
        return string == self.reference_answer","Point(row=100, column=0)","Point(row=109, column=46)",,classic/benchmark/agbenchmark/challenges/webarena.py
ExactStringMatchEval.description,function,,"def description(self) -> str:
        return f""Answer must be '{self.reference_answer}'""","Point(row=105, column=4)","Point(row=106, column=58)",ExactStringMatchEval,classic/benchmark/agbenchmark/challenges/webarena.py
ExactStringMatchEval.evaluate,function,,"def evaluate(self, string: str) -> bool:
        return string == self.reference_answer","Point(row=108, column=4)","Point(row=109, column=46)",ExactStringMatchEval,classic/benchmark/agbenchmark/challenges/webarena.py
FuzzyStringMatchEval,class,,"class FuzzyStringMatchEval(BaseStringEval):
    type: Literal[""fuzzy_match""] = ""fuzzy_match""
    reference_answer: str

    @property
    def description(self) -> str:
        return f""Answer must contain something like '{self.reference_answer}'""

    def evaluate(self, string: str) -> bool:
        # TODO: use LLM for matching (or something else that's flexible/robust)
        return self.reference_answer.lower() in string.lower()","Point(row=112, column=0)","Point(row=122, column=62)",,classic/benchmark/agbenchmark/challenges/webarena.py
FuzzyStringMatchEval.description,function,,"def description(self) -> str:
        return f""Answer must contain something like '{self.reference_answer}'""","Point(row=117, column=4)","Point(row=118, column=78)",FuzzyStringMatchEval,classic/benchmark/agbenchmark/challenges/webarena.py
FuzzyStringMatchEval.evaluate,function,,"def evaluate(self, string: str) -> bool:
        # TODO: use LLM for matching (or something else that's flexible/robust)
        return self.reference_answer.lower() in string.lower()","Point(row=120, column=4)","Point(row=122, column=62)",FuzzyStringMatchEval,classic/benchmark/agbenchmark/challenges/webarena.py
MustIncludeStringEval,class,,"class MustIncludeStringEval(BaseStringEval):
    type: Literal[""must_include""] = ""must_include""
    reference_answer: str

    @property
    def description(self) -> str:
        return f""Answer must include '{self.reference_answer}'""

    def evaluate(self, string: str) -> bool:
        return self.reference_answer.lower() in string.lower()","Point(row=125, column=0)","Point(row=134, column=62)",,classic/benchmark/agbenchmark/challenges/webarena.py
MustIncludeStringEval.description,function,,"def description(self) -> str:
        return f""Answer must include '{self.reference_answer}'""","Point(row=130, column=4)","Point(row=131, column=63)",MustIncludeStringEval,classic/benchmark/agbenchmark/challenges/webarena.py
MustIncludeStringEval.evaluate,function,,"def evaluate(self, string: str) -> bool:
        return self.reference_answer.lower() in string.lower()","Point(row=133, column=4)","Point(row=134, column=62)",MustIncludeStringEval,classic/benchmark/agbenchmark/challenges/webarena.py
UrlMatchEval,class,,"class UrlMatchEval(BaseModel, Eval):
    url: str
    """"""Example: `""__WIKI__/wiki/Octopus""`""""""

    @property
    def description(self) -> str:
        return f""Agent must navigate to '{self.url}'""

    def evaluate(self, string: str) -> bool:
        return string == resolve_uri(self.url)","Point(row=140, column=0)","Point(row=149, column=46)",,classic/benchmark/agbenchmark/challenges/webarena.py
UrlMatchEval.description,function,,"def description(self) -> str:
        return f""Agent must navigate to '{self.url}'""","Point(row=145, column=4)","Point(row=146, column=53)",UrlMatchEval,classic/benchmark/agbenchmark/challenges/webarena.py
UrlMatchEval.evaluate,function,,"def evaluate(self, string: str) -> bool:
        return string == resolve_uri(self.url)","Point(row=148, column=4)","Point(row=149, column=46)",UrlMatchEval,classic/benchmark/agbenchmark/challenges/webarena.py
ProgramHtmlEval,class,,"class ProgramHtmlEval(BaseModel):
    url: str
    locator: str
    """"""JavaScript code that returns the value to check""""""
    required_contents: str

    @property
    def description(self) -> str:
        return (
            f""On the webpage {self.url}, ""
            f""`{self.locator}` should contain '{self.required_contents}'""
        )

    def evaluate(self, selenium_instance) -> bool:
        result = selenium_instance.execute_script(
            self.locator or ""return document.body.innerHTML;""
        )
        return self.required_contents in result","Point(row=152, column=0)","Point(row=169, column=47)",,classic/benchmark/agbenchmark/challenges/webarena.py
ProgramHtmlEval.description,function,,"def description(self) -> str:
        return (
            f""On the webpage {self.url}, ""
            f""`{self.locator}` should contain '{self.required_contents}'""
        )","Point(row=159, column=4)","Point(row=163, column=9)",ProgramHtmlEval,classic/benchmark/agbenchmark/challenges/webarena.py
ProgramHtmlEval.evaluate,function,,"def evaluate(self, selenium_instance) -> bool:
        result = selenium_instance.execute_script(
            self.locator or ""return document.body.innerHTML;""
        )
        return self.required_contents in result","Point(row=165, column=4)","Point(row=169, column=47)",ProgramHtmlEval,classic/benchmark/agbenchmark/challenges/webarena.py
WebArenaChallengeSpec,class,,"class WebArenaChallengeSpec(BaseModel):
    task_id: int
    sites: list[WebArenaSite]
    """"""The sites needed to complete the task""""""
    start_url: str
    """"""The full URL at which to start""""""
    start_url_junglegym: str
    """"""The JungleGym site (base URL) at which to start""""""
    require_login: bool
    require_reset: bool
    storage_state: str | None = None

    intent: str
    intent_template: str
    intent_template_id: int
    instantiation_dict: dict[str, str | list[str]]

    available: bool = True
    unavailable_reason: str = """"

    class EvalSet(BaseModel):
        class StringMatchEvalSet(BaseModel):
            exact_match: str | None = None
            fuzzy_match: list[str] | None = None
            must_include: list[str] | None = None

        reference_answers: StringMatchEvalSet | None = None
        """"""For string_match eval, a set of criteria to judge the final answer""""""
        reference_answer_raw_annotation: str | None = None
        string_note: str | None = None
        annotation_note: str | None = None

        reference_url: str | None = None
        """"""For url_match eval, the last URL that should be visited""""""
        url_note: str | None = None

        program_html: list[ProgramHtmlEval]
        """"""For program_html eval, a list of criteria to judge the site state by""""""

        eval_types: list[EvalType]

        @field_validator(""eval_types"")
        def check_eval_parameters(cls, value: list[EvalType], info: ValidationInfo):
            if ""string_match"" in value and not info.data[""reference_answers""]:
                raise ValueError(""'string_match' eval_type requires reference_answers"")
            if ""url_match"" in value and not info.data[""reference_url""]:
                raise ValueError(""'url_match' eval_type requires reference_url"")
            if ""program_html"" in value and not info.data[""program_html""]:
                raise ValueError(
                    ""'program_html' eval_type requires at least one program_html eval""
                )
            return value

        @property
        def evaluators(self) -> list[_Eval]:
            evaluators: list[_Eval] = []
            if self.reference_answers:
                if self.reference_answers.exact_match:
                    evaluators.append(
                        ExactStringMatchEval(
                            reference_answer=self.reference_answers.exact_match
                        )
                    )
                if self.reference_answers.fuzzy_match:
                    evaluators.extend(
                        FuzzyStringMatchEval(reference_answer=a)
                        for a in self.reference_answers.fuzzy_match
                    )
                if self.reference_answers.must_include:
                    evaluators.extend(
                        MustIncludeStringEval(reference_answer=a)
                        for a in self.reference_answers.must_include
                    )
            if self.reference_url:
                evaluators.append(UrlMatchEval(url=self.reference_url))
            evaluators.extend(self.program_html)
            return evaluators

    eval: EvalSet
    """"""Evaluation criteria by which to judge the agent's performance""""""

    @property
    def assignment_for_agent(self):
        sites = [get_site_info(s) for s in self.sites]
        nav_constraint = (
            ""You are ONLY allowed to access URLs in ""
            f""{' and '.join(s.base_url for s in sites)}.\n\n""
            + ""\n"".join(
                s.additional_info.format(url=s.base_url)
                for s in sites
                if s.additional_info
            )
        ).strip()

        return (
            f""First of all, go to {self.start_url}. ""
            f""{self.intent.rstrip('.')}.\n""
            f""{nav_constraint}""
        )","Point(row=175, column=0)","Point(row=273, column=9)",,classic/benchmark/agbenchmark/challenges/webarena.py
EvalSet,class,,"class EvalSet(BaseModel):
        class StringMatchEvalSet(BaseModel):
            exact_match: str | None = None
            fuzzy_match: list[str] | None = None
            must_include: list[str] | None = None

        reference_answers: StringMatchEvalSet | None = None
        """"""For string_match eval, a set of criteria to judge the final answer""""""
        reference_answer_raw_annotation: str | None = None
        string_note: str | None = None
        annotation_note: str | None = None

        reference_url: str | None = None
        """"""For url_match eval, the last URL that should be visited""""""
        url_note: str | None = None

        program_html: list[ProgramHtmlEval]
        """"""For program_html eval, a list of criteria to judge the site state by""""""

        eval_types: list[EvalType]

        @field_validator(""eval_types"")
        def check_eval_parameters(cls, value: list[EvalType], info: ValidationInfo):
            if ""string_match"" in value and not info.data[""reference_answers""]:
                raise ValueError(""'string_match' eval_type requires reference_answers"")
            if ""url_match"" in value and not info.data[""reference_url""]:
                raise ValueError(""'url_match' eval_type requires reference_url"")
            if ""program_html"" in value and not info.data[""program_html""]:
                raise ValueError(
                    ""'program_html' eval_type requires at least one program_html eval""
                )
            return value

        @property
        def evaluators(self) -> list[_Eval]:
            evaluators: list[_Eval] = []
            if self.reference_answers:
                if self.reference_answers.exact_match:
                    evaluators.append(
                        ExactStringMatchEval(
                            reference_answer=self.reference_answers.exact_match
                        )
                    )
                if self.reference_answers.fuzzy_match:
                    evaluators.extend(
                        FuzzyStringMatchEval(reference_answer=a)
                        for a in self.reference_answers.fuzzy_match
                    )
                if self.reference_answers.must_include:
                    evaluators.extend(
                        MustIncludeStringEval(reference_answer=a)
                        for a in self.reference_answers.must_include
                    )
            if self.reference_url:
                evaluators.append(UrlMatchEval(url=self.reference_url))
            evaluators.extend(self.program_html)
            return evaluators","Point(row=195, column=4)","Point(row=251, column=29)",,classic/benchmark/agbenchmark/challenges/webarena.py
StringMatchEvalSet,class,,"class StringMatchEvalSet(BaseModel):
            exact_match: str | None = None
            fuzzy_match: list[str] | None = None
            must_include: list[str] | None = None","Point(row=196, column=8)","Point(row=199, column=49)",,classic/benchmark/agbenchmark/challenges/webarena.py
EvalSet.check_eval_parameters,function,,"def check_eval_parameters(cls, value: list[EvalType], info: ValidationInfo):
            if ""string_match"" in value and not info.data[""reference_answers""]:
                raise ValueError(""'string_match' eval_type requires reference_answers"")
            if ""url_match"" in value and not info.data[""reference_url""]:
                raise ValueError(""'url_match' eval_type requires reference_url"")
            if ""program_html"" in value and not info.data[""program_html""]:
                raise ValueError(
                    ""'program_html' eval_type requires at least one program_html eval""
                )
            return value","Point(row=217, column=8)","Point(row=226, column=24)",EvalSet,classic/benchmark/agbenchmark/challenges/webarena.py
EvalSet.evaluators,function,,"def evaluators(self) -> list[_Eval]:
            evaluators: list[_Eval] = []
            if self.reference_answers:
                if self.reference_answers.exact_match:
                    evaluators.append(
                        ExactStringMatchEval(
                            reference_answer=self.reference_answers.exact_match
                        )
                    )
                if self.reference_answers.fuzzy_match:
                    evaluators.extend(
                        FuzzyStringMatchEval(reference_answer=a)
                        for a in self.reference_answers.fuzzy_match
                    )
                if self.reference_answers.must_include:
                    evaluators.extend(
                        MustIncludeStringEval(reference_answer=a)
                        for a in self.reference_answers.must_include
                    )
            if self.reference_url:
                evaluators.append(UrlMatchEval(url=self.reference_url))
            evaluators.extend(self.program_html)
            return evaluators","Point(row=229, column=8)","Point(row=251, column=29)",EvalSet,classic/benchmark/agbenchmark/challenges/webarena.py
WebArenaChallengeSpec.assignment_for_agent,function,,"def assignment_for_agent(self):
        sites = [get_site_info(s) for s in self.sites]
        nav_constraint = (
            ""You are ONLY allowed to access URLs in ""
            f""{' and '.join(s.base_url for s in sites)}.\n\n""
            + ""\n"".join(
                s.additional_info.format(url=s.base_url)
                for s in sites
                if s.additional_info
            )
        ).strip()

        return (
            f""First of all, go to {self.start_url}. ""
            f""{self.intent.rstrip('.')}.\n""
            f""{nav_constraint}""
        )","Point(row=257, column=4)","Point(row=273, column=9)",WebArenaChallengeSpec,classic/benchmark/agbenchmark/challenges/webarena.py
WebArenaChallenge,class,,"class WebArenaChallenge(BaseChallenge):
    _spec: ClassVar[WebArenaChallengeSpec]

    SOURCE_URI_PREFIX = ""__JUNGLEGYM__/webarena/tasks/""
    SOURCE_URI_TEMPLATE = f""{SOURCE_URI_PREFIX}{{task_id}}""

    @classmethod
    def from_source_uri(cls, source_uri: str) -> type[""WebArenaChallenge""]:
        if not source_uri.startswith(cls.SOURCE_URI_PREFIX):
            raise ValueError(f""Invalid source_uri for WebArenaChallenge: {source_uri}"")

        source_url = source_uri.replace(
            cls.SOURCE_URI_PREFIX,
            ""https://api.junglegym.ai/get_webarena_by_task_id?task_id="",
        )
        results = requests.get(source_url).json()[""data""]
        if not results:
            raise ValueError(f""Could not fetch challenge {source_uri}"")
        return cls.from_challenge_spec(WebArenaChallengeSpec.model_validate(results[0]))

    @classmethod
    def from_challenge_spec(
        cls, spec: WebArenaChallengeSpec
    ) -> type[""WebArenaChallenge""]:
        challenge_info = ChallengeInfo(
            eval_id=f""junglegym-webarena-{spec.task_id}"",
            name=f""WebArenaTask_{spec.task_id}"",
            task=spec.assignment_for_agent,
            category=[
                Category.GENERALIST,
                Category.WEB,
            ],  # TODO: make categories more specific
            reference_answer=spec.eval.reference_answer_raw_annotation,
            source_uri=cls.SOURCE_URI_TEMPLATE.format(task_id=spec.task_id),
            available=spec.available,
            unavailable_reason=spec.unavailable_reason,
        )
        return type(
            f""Test{challenge_info.name}"",
            (WebArenaChallenge,),
            {
                ""info"": challenge_info,
                ""_spec"": spec,
            },
        )

    @classmethod
    def evaluate_answer(cls, answer: str) -> list[tuple[_Eval, EvalResult]]:
        results: list[tuple[_Eval, EvalResult]] = []
        for evaluator in cls._spec.eval.evaluators:
            if isinstance(evaluator, StringEval):  # string_match
                results.append(
                    (
                        evaluator,
                        EvalResult(
                            result=answer,
                            result_source=""step_output"",
                            score=evaluator.evaluate(answer),
                            passed=evaluator.evaluate(answer),
                        ),
                    )
                )
        return results

    @classmethod
    def evaluate_step_result(
        cls, step: Step, *, mock: bool = False
    ) -> list[tuple[_Eval, EvalResult]]:
        if mock:
            step.output = cls.info.reference_answer
        assert step.output
        eval_results = cls.evaluate_answer(step.output)
        for eval in cls._spec.eval.evaluators:
            if isinstance(eval, UrlMatchEval):
                passed = resolve_uri(eval.url) in step.output  # HACK: url_match bodge
                eval_results.append(
                    (
                        eval,
                        EvalResult(
                            result=step.output,
                            result_source=""step_output"",
                            score=1.0 if passed else 0.0,
                            passed=passed,
                        ),
                    )
                )
            # TODO: add support for program_html evals
        return eval_results

    @classmethod
    async def evaluate_task_state(
        cls, agent: AgentApi, task_id: str
    ) -> list[EvalResult]:
        steps: list[Step] = (await agent.list_agent_task_steps(task_id)).steps

        eval_results_per_step = [cls.evaluate_step_result(step) for step in steps]
        # Get the column aggregate (highest scored EvalResult for each Eval)
        # from the matrix of EvalResults per step.
        return [
            max(step_results_for_eval, key=lambda r: r[1].score)[1]
            for step_results_for_eval in zip(*eval_results_per_step)
        ]

    @pytest.mark.asyncio
    async def test_method(
        self,
        config: AgentBenchmarkConfig,
        request: pytest.FixtureRequest,
        i_attempt: int,
    ) -> None:
        if not self._spec.available:
            pytest.skip(self._spec.unavailable_reason)

        # if os.environ.get(""HELICONE_API_KEY""):
        #     from helicone.lock import HeliconeLockManager

        #     HeliconeLockManager.write_custom_property(""challenge"", self.info.name)

        timeout = 120
        if request.config.getoption(""--nc""):
            timeout = 100000
        elif cutoff := request.config.getoption(""--cutoff""):
            timeout = int(cutoff)  # type: ignore

        assert isinstance(request.node, pytest.Item)

        n_steps = 0
        timed_out = None
        agent_task_cost = None
        steps: list[Step] = []
        eval_results_per_step: list[list[tuple[_Eval, EvalResult]]] = []
        try:
            async for step in self.run_challenge(
                config, timeout, mock=bool(request.config.getoption(""--mock""))
            ):
                if not step.output:
                    logger.warn(f""Step has no output: {step}"")
                    continue

                n_steps += 1
                steps.append(step)
                if step.additional_output:
                    agent_task_cost = step.additional_output.get(
                        ""task_total_cost"",
                        step.additional_output.get(""task_cumulative_cost""),
                    )

                step_eval_results = self.evaluate_step_result(
                    step, mock=bool(request.config.getoption(""--mock""))
                )
                logger.debug(f""Intermediary results: {step_eval_results}"")
                eval_results_per_step.append(step_eval_results)
                if step.is_last:
                    request.node.user_properties.append(
                        (
                            ""answers"",
                            step.output
                            if request.config.getoption(""--keep-answers"")
                            else None,
                        )
                    )
            timed_out = False
        except TimeoutError:
            timed_out = True
        request.node.user_properties.append((""steps"", steps))
        request.node.user_properties.append((""n_steps"", n_steps))
        request.node.user_properties.append((""timed_out"", timed_out))
        request.node.user_properties.append((""agent_task_cost"", agent_task_cost))

        # Get the column aggregate (highest score for each Eval)
        # from the matrix of EvalResults per step.
        evals_results = [
            max(step_results_for_eval, key=lambda r: r[1].score)
            for step_results_for_eval in zip(*eval_results_per_step)
        ]

        if not evals_results:
            if timed_out:
                raise TimeoutError(""Timed out, no results to evaluate"")
            else:
                raise ValueError(""No results to evaluate"")

        request.node.user_properties.append(
            (""scores"", [r[1].score for r in evals_results])
        )

        # FIXME: arbitrary threshold
        assert all(r[1].score > 0.9 for r in evals_results), (
            ""Scores insufficient:\n\n""
            if not timed_out
            else ""Timed out; scores insufficient:\n\n""
        ) + ""\n"".join(f""{repr(r[0])}\n  -> {repr(r[1])}"" for r in evals_results)","Point(row=276, column=0)","Point(row=467, column=80)",,classic/benchmark/agbenchmark/challenges/webarena.py
WebArenaChallenge.from_source_uri,function,,"def from_source_uri(cls, source_uri: str) -> type[""WebArenaChallenge""]:
        if not source_uri.startswith(cls.SOURCE_URI_PREFIX):
            raise ValueError(f""Invalid source_uri for WebArenaChallenge: {source_uri}"")

        source_url = source_uri.replace(
            cls.SOURCE_URI_PREFIX,
            ""https://api.junglegym.ai/get_webarena_by_task_id?task_id="",
        )
        results = requests.get(source_url).json()[""data""]
        if not results:
            raise ValueError(f""Could not fetch challenge {source_uri}"")
        return cls.from_challenge_spec(WebArenaChallengeSpec.model_validate(results[0]))","Point(row=283, column=4)","Point(row=294, column=88)",WebArenaChallenge,classic/benchmark/agbenchmark/challenges/webarena.py
WebArenaChallenge.from_challenge_spec,function,,"def from_challenge_spec(
        cls, spec: WebArenaChallengeSpec
    ) -> type[""WebArenaChallenge""]:
        challenge_info = ChallengeInfo(
            eval_id=f""junglegym-webarena-{spec.task_id}"",
            name=f""WebArenaTask_{spec.task_id}"",
            task=spec.assignment_for_agent,
            category=[
                Category.GENERALIST,
                Category.WEB,
            ],  # TODO: make categories more specific
            reference_answer=spec.eval.reference_answer_raw_annotation,
            source_uri=cls.SOURCE_URI_TEMPLATE.format(task_id=spec.task_id),
            available=spec.available,
            unavailable_reason=spec.unavailable_reason,
        )
        return type(
            f""Test{challenge_info.name}"",
            (WebArenaChallenge,),
            {
                ""info"": challenge_info,
                ""_spec"": spec,
            },
        )","Point(row=297, column=4)","Point(row=320, column=9)",WebArenaChallenge,classic/benchmark/agbenchmark/challenges/webarena.py
WebArenaChallenge.evaluate_answer,function,,"def evaluate_answer(cls, answer: str) -> list[tuple[_Eval, EvalResult]]:
        results: list[tuple[_Eval, EvalResult]] = []
        for evaluator in cls._spec.eval.evaluators:
            if isinstance(evaluator, StringEval):  # string_match
                results.append(
                    (
                        evaluator,
                        EvalResult(
                            result=answer,
                            result_source=""step_output"",
                            score=evaluator.evaluate(answer),
                            passed=evaluator.evaluate(answer),
                        ),
                    )
                )
        return results","Point(row=323, column=4)","Point(row=338, column=22)",WebArenaChallenge,classic/benchmark/agbenchmark/challenges/webarena.py
WebArenaChallenge.evaluate_step_result,function,,"def evaluate_step_result(
        cls, step: Step, *, mock: bool = False
    ) -> list[tuple[_Eval, EvalResult]]:
        if mock:
            step.output = cls.info.reference_answer
        assert step.output
        eval_results = cls.evaluate_answer(step.output)
        for eval in cls._spec.eval.evaluators:
            if isinstance(eval, UrlMatchEval):
                passed = resolve_uri(eval.url) in step.output  # HACK: url_match bodge
                eval_results.append(
                    (
                        eval,
                        EvalResult(
                            result=step.output,
                            result_source=""step_output"",
                            score=1.0 if passed else 0.0,
                            passed=passed,
                        ),
                    )
                )
            # TODO: add support for program_html evals
        return eval_results","Point(row=341, column=4)","Point(row=363, column=27)",WebArenaChallenge,classic/benchmark/agbenchmark/challenges/webarena.py
WebArenaChallenge.evaluate_task_state,function,,"async def evaluate_task_state(
        cls, agent: AgentApi, task_id: str
    ) -> list[EvalResult]:
        steps: list[Step] = (await agent.list_agent_task_steps(task_id)).steps

        eval_results_per_step = [cls.evaluate_step_result(step) for step in steps]
        # Get the column aggregate (highest scored EvalResult for each Eval)
        # from the matrix of EvalResults per step.
        return [
            max(step_results_for_eval, key=lambda r: r[1].score)[1]
            for step_results_for_eval in zip(*eval_results_per_step)
        ]","Point(row=366, column=4)","Point(row=377, column=9)",WebArenaChallenge,classic/benchmark/agbenchmark/challenges/webarena.py
WebArenaChallenge.test_method,function,,"async def test_method(
        self,
        config: AgentBenchmarkConfig,
        request: pytest.FixtureRequest,
        i_attempt: int,
    ) -> None:
        if not self._spec.available:
            pytest.skip(self._spec.unavailable_reason)

        # if os.environ.get(""HELICONE_API_KEY""):
        #     from helicone.lock import HeliconeLockManager

        #     HeliconeLockManager.write_custom_property(""challenge"", self.info.name)

        timeout = 120
        if request.config.getoption(""--nc""):
            timeout = 100000
        elif cutoff := request.config.getoption(""--cutoff""):
            timeout = int(cutoff)  # type: ignore

        assert isinstance(request.node, pytest.Item)

        n_steps = 0
        timed_out = None
        agent_task_cost = None
        steps: list[Step] = []
        eval_results_per_step: list[list[tuple[_Eval, EvalResult]]] = []
        try:
            async for step in self.run_challenge(
                config, timeout, mock=bool(request.config.getoption(""--mock""))
            ):
                if not step.output:
                    logger.warn(f""Step has no output: {step}"")
                    continue

                n_steps += 1
                steps.append(step)
                if step.additional_output:
                    agent_task_cost = step.additional_output.get(
                        ""task_total_cost"",
                        step.additional_output.get(""task_cumulative_cost""),
                    )

                step_eval_results = self.evaluate_step_result(
                    step, mock=bool(request.config.getoption(""--mock""))
                )
                logger.debug(f""Intermediary results: {step_eval_results}"")
                eval_results_per_step.append(step_eval_results)
                if step.is_last:
                    request.node.user_properties.append(
                        (
                            ""answers"",
                            step.output
                            if request.config.getoption(""--keep-answers"")
                            else None,
                        )
                    )
            timed_out = False
        except TimeoutError:
            timed_out = True
        request.node.user_properties.append((""steps"", steps))
        request.node.user_properties.append((""n_steps"", n_steps))
        request.node.user_properties.append((""timed_out"", timed_out))
        request.node.user_properties.append((""agent_task_cost"", agent_task_cost))

        # Get the column aggregate (highest score for each Eval)
        # from the matrix of EvalResults per step.
        evals_results = [
            max(step_results_for_eval, key=lambda r: r[1].score)
            for step_results_for_eval in zip(*eval_results_per_step)
        ]

        if not evals_results:
            if timed_out:
                raise TimeoutError(""Timed out, no results to evaluate"")
            else:
                raise ValueError(""No results to evaluate"")

        request.node.user_properties.append(
            (""scores"", [r[1].score for r in evals_results])
        )

        # FIXME: arbitrary threshold
        assert all(r[1].score > 0.9 for r in evals_results), (
            ""Scores insufficient:\n\n""
            if not timed_out
            else ""Timed out; scores insufficient:\n\n""
        ) + ""\n"".join(f""{repr(r[0])}\n  -> {repr(r[1])}"" for r in evals_results)","Point(row=380, column=4)","Point(row=467, column=80)",WebArenaChallenge,classic/benchmark/agbenchmark/challenges/webarena.py
load_webarena_challenges,function,,"def load_webarena_challenges(
    skip_unavailable: bool = True,
) -> Iterator[type[WebArenaChallenge]]:
    logger.info(""Loading WebArena challenges..."")

    for site, info in site_info_map.items():
        if not info.available and skip_unavailable:
            logger.warning(
                f""JungleGym site '{site}' is not available: {info.unavailable_reason} ""
                ""Skipping all challenges which use this site.""
            )

    # response = requests.get(""https://api.junglegym.ai/get_full_webarena_dataset"")
    # challenge_dicts = response.json()[""data""]

    # Until the full WebArena challenge set is supported, use a hand-picked selection
    import json
    from pathlib import Path

    challenge_dicts = json.loads(
        (Path(__file__).parent / ""webarena_selection.json"").read_bytes()
    )

    logger.debug(
        ""Fetched WebArena dataset. ""
        f""Constructing {len(challenge_dicts)} WebArenaChallenges...""
    )
    loaded = 0
    failed = 0
    skipped = 0
    for entry in challenge_dicts:
        try:
            challenge_spec = WebArenaChallengeSpec.model_validate(entry)
        except ValidationError as e:
            failed += 1
            logger.warning(f""Error validating WebArena challenge entry: {entry}"")
            logger.warning(f""Error details: {e}"")
            continue

        # Check all required sites for availability
        for site in challenge_spec.sites:
            site_info = site_info_map.get(site)
            if site_info is None:
                challenge_spec.available = False
                challenge_spec.unavailable_reason = (
                    f""WebArena task {challenge_spec.task_id} requires unknown site ""
                    f""'{site}'""
                )
            elif not site_info.available:
                challenge_spec.available = False
                challenge_spec.unavailable_reason = (
                    f""WebArena task {challenge_spec.task_id} requires unavailable ""
                    f""site '{site}'""
                )

        if not challenge_spec.available and skip_unavailable:
            logger.debug(f""{challenge_spec.unavailable_reason}; skipping..."")
            skipped += 1
            continue

        yield WebArenaChallenge.from_challenge_spec(challenge_spec)
        loaded += 1

    logger.info(
        ""Loading WebArena challenges complete: ""
        f""loaded {loaded}, skipped {skipped}.""
        + (f"" {failed} challenges failed to load."" if failed else """")
    )","Point(row=470, column=0)","Point(row=537, column=5)",,classic/benchmark/agbenchmark/challenges/webarena.py
get_challenge_from_source_uri,function,,"def get_challenge_from_source_uri(source_uri: str) -> type[BaseChallenge]:
    from .builtin import BuiltinChallenge
    from .webarena import WebArenaChallenge

    provider_prefix = source_uri.split(""/"", 1)[0]

    if provider_prefix == BuiltinChallenge.SOURCE_URI_PREFIX:
        return BuiltinChallenge.from_source_uri(source_uri)

    if provider_prefix == WebArenaChallenge.SOURCE_URI_PREFIX:
        return WebArenaChallenge.from_source_uri(source_uri)

    raise ValueError(f""Cannot resolve source_uri '{source_uri}'"")","Point(row=11, column=0)","Point(row=23, column=65)",,classic/benchmark/agbenchmark/challenges/__init__.py
get_unique_categories,function,"
    Reads all challenge spec files and returns a set of all their categories.
","def get_unique_categories() -> set[str]:
    """"""
    Reads all challenge spec files and returns a set of all their categories.
    """"""
    categories = set()

    challenges_dir = Path(__file__).parent
    glob_path = f""{challenges_dir}/**/data.json""

    for data_file in glob.glob(glob_path, recursive=True):
        with open(data_file, ""r"") as f:
            try:
                challenge_data = json.load(f)
                categories.update(challenge_data.get(""category"", []))
            except json.JSONDecodeError:
                logger.error(f""Error: {data_file} is not a valid JSON file."")
                continue
            except IOError:
                logger.error(f""IOError: file could not be read: {data_file}"")
                continue

    return categories","Point(row=26, column=0)","Point(row=47, column=21)",,classic/benchmark/agbenchmark/challenges/__init__.py
BuiltinChallengeSpec,class,,"class BuiltinChallengeSpec(BaseModel):
    eval_id: str = """"
    name: str
    task: str
    category: list[Category]
    dependencies: list[str]
    cutoff: int

    class Info(BaseModel):
        difficulty: DifficultyLevel
        description: Annotated[
            str, StringConstraints(pattern=r""^Tests if the agent can.*"")
        ]
        side_effects: list[str] = Field(default_factory=list)

    info: Info

    class Ground(BaseModel):
        answer: str
        should_contain: Optional[list[str]] = None
        should_not_contain: Optional[list[str]] = None
        files: list[str]
        case_sensitive: Optional[bool] = True

        class Eval(BaseModel):
            type: str
            scoring: Optional[Literal[""percentage"", ""scale"", ""binary""]] = None
            template: Optional[
                Literal[""rubric"", ""reference"", ""question"", ""custom""]
            ] = None
            examples: Optional[str] = None

            @field_validator(""scoring"", ""template"")
            def validate_eval_fields(cls, value, info: ValidationInfo):
                field_name = info.field_name
                if ""type"" in info.data and info.data[""type""] == ""llm"":
                    if value is None:
                        raise ValueError(
                            f""{field_name} must be provided when eval type is 'llm'""
                        )
                else:
                    if value is not None:
                        raise ValueError(
                            f""{field_name} should only exist when eval type is 'llm'""
                        )
                return value

        eval: Eval

    ground: Ground

    metadata: Optional[dict[str, Any]] = None
    spec_file: Path | None = Field(None, exclude=True)","Point(row=44, column=0)","Point(row=96, column=54)",,classic/benchmark/agbenchmark/challenges/builtin.py
Info,class,,"class Info(BaseModel):
        difficulty: DifficultyLevel
        description: Annotated[
            str, StringConstraints(pattern=r""^Tests if the agent can.*"")
        ]
        side_effects: list[str] = Field(default_factory=list)","Point(row=52, column=4)","Point(row=57, column=61)",,classic/benchmark/agbenchmark/challenges/builtin.py
Ground,class,,"class Ground(BaseModel):
        answer: str
        should_contain: Optional[list[str]] = None
        should_not_contain: Optional[list[str]] = None
        files: list[str]
        case_sensitive: Optional[bool] = True

        class Eval(BaseModel):
            type: str
            scoring: Optional[Literal[""percentage"", ""scale"", ""binary""]] = None
            template: Optional[
                Literal[""rubric"", ""reference"", ""question"", ""custom""]
            ] = None
            examples: Optional[str] = None

            @field_validator(""scoring"", ""template"")
            def validate_eval_fields(cls, value, info: ValidationInfo):
                field_name = info.field_name
                if ""type"" in info.data and info.data[""type""] == ""llm"":
                    if value is None:
                        raise ValueError(
                            f""{field_name} must be provided when eval type is 'llm'""
                        )
                else:
                    if value is not None:
                        raise ValueError(
                            f""{field_name} should only exist when eval type is 'llm'""
                        )
                return value

        eval: Eval","Point(row=61, column=4)","Point(row=91, column=18)",,classic/benchmark/agbenchmark/challenges/builtin.py
Eval,class,,"class Eval(BaseModel):
            type: str
            scoring: Optional[Literal[""percentage"", ""scale"", ""binary""]] = None
            template: Optional[
                Literal[""rubric"", ""reference"", ""question"", ""custom""]
            ] = None
            examples: Optional[str] = None

            @field_validator(""scoring"", ""template"")
            def validate_eval_fields(cls, value, info: ValidationInfo):
                field_name = info.field_name
                if ""type"" in info.data and info.data[""type""] == ""llm"":
                    if value is None:
                        raise ValueError(
                            f""{field_name} must be provided when eval type is 'llm'""
                        )
                else:
                    if value is not None:
                        raise ValueError(
                            f""{field_name} should only exist when eval type is 'llm'""
                        )
                return value","Point(row=68, column=8)","Point(row=89, column=28)",,classic/benchmark/agbenchmark/challenges/builtin.py
Eval.validate_eval_fields,function,,"def validate_eval_fields(cls, value, info: ValidationInfo):
                field_name = info.field_name
                if ""type"" in info.data and info.data[""type""] == ""llm"":
                    if value is None:
                        raise ValueError(
                            f""{field_name} must be provided when eval type is 'llm'""
                        )
                else:
                    if value is not None:
                        raise ValueError(
                            f""{field_name} should only exist when eval type is 'llm'""
                        )
                return value","Point(row=77, column=12)","Point(row=89, column=28)",Eval,classic/benchmark/agbenchmark/challenges/builtin.py
BuiltinChallenge,class,"
    Base class for AGBenchmark's built-in challenges (challenges/**/*.json).

    All of the logic is present in this class. Individual challenges are created as
    subclasses of `BuiltinChallenge` with challenge-specific values assigned to the
    ClassVars `_spec` etc.

    Dynamically constructing subclasses rather than class instances for the individual
    challenges makes them suitable for collection by Pytest, which will run their
    `test_method` like any regular test item.
","class BuiltinChallenge(BaseChallenge):
    """"""
    Base class for AGBenchmark's built-in challenges (challenges/**/*.json).

    All of the logic is present in this class. Individual challenges are created as
    subclasses of `BuiltinChallenge` with challenge-specific values assigned to the
    ClassVars `_spec` etc.

    Dynamically constructing subclasses rather than class instances for the individual
    challenges makes them suitable for collection by Pytest, which will run their
    `test_method` like any regular test item.
    """"""

    _spec: ClassVar[BuiltinChallengeSpec]
    CHALLENGE_LOCATION: ClassVar[str]
    ARTIFACTS_LOCATION: ClassVar[str]

    SOURCE_URI_PREFIX = ""__BUILTIN__""

    @classmethod
    def from_challenge_spec(
        cls, spec: BuiltinChallengeSpec
    ) -> type[""BuiltinChallenge""]:
        if not spec.spec_file:
            raise ValueError(""spec.spec_file not defined"")

        challenge_info = ChallengeInfo(
            eval_id=spec.eval_id,
            name=spec.name,
            task=spec.task,
            task_artifacts_dir=spec.spec_file.parent,
            category=spec.category,
            difficulty=spec.info.difficulty,
            description=spec.info.description,
            dependencies=spec.dependencies,
            reference_answer=spec.ground.answer,
            source_uri=(
                f""__BUILTIN__/{spec.spec_file.relative_to(Path(__file__).parent)}""
            ),
        )

        challenge_class_name = f""Test{challenge_info.name}""
        logger.debug(f""Creating {challenge_class_name} from spec: {spec.spec_file}"")
        return type(
            challenge_class_name,
            (BuiltinChallenge,),
            {
                ""info"": challenge_info,
                ""_spec"": spec,
                ""CHALLENGE_LOCATION"": str(spec.spec_file),
                ""ARTIFACTS_LOCATION"": str(spec.spec_file.resolve().parent),
            },
        )

    @classmethod
    def from_challenge_spec_file(cls, spec_file: Path) -> type[""BuiltinChallenge""]:
        challenge_spec = BuiltinChallengeSpec.model_validate_json(spec_file.read_text())
        challenge_spec.spec_file = spec_file
        return cls.from_challenge_spec(challenge_spec)

    @classmethod
    def from_source_uri(cls, source_uri: str) -> type[""BuiltinChallenge""]:
        if not source_uri.startswith(cls.SOURCE_URI_PREFIX):
            raise ValueError(f""Invalid source_uri for BuiltinChallenge: {source_uri}"")

        path = source_uri.split(""/"", 1)[1]
        spec_file = Path(__file__).parent / path
        return cls.from_challenge_spec_file(spec_file)

    @pytest.mark.asyncio
    async def test_method(
        self,
        config: AgentBenchmarkConfig,
        request: pytest.FixtureRequest,
        i_attempt: int,
    ) -> None:
        # if os.environ.get(""HELICONE_API_KEY""):
        #     from helicone.lock import HeliconeLockManager

        #     HeliconeLockManager.write_custom_property(""challenge"", self.info.name)

        timeout = self._spec.cutoff or 60

        if request.config.getoption(""--nc""):
            timeout = 100000
        elif cutoff := request.config.getoption(""--cutoff""):
            timeout = int(cutoff)  # type: ignore

        task_id = """"
        n_steps = 0
        timed_out = None
        agent_task_cost = None
        steps: list[Step] = []
        try:
            async for step in self.run_challenge(
                config, timeout, mock=bool(request.config.getoption(""--mock""))
            ):
                if not task_id:
                    task_id = step.task_id

                n_steps += 1
                steps.append(step.model_copy())
                if step.additional_output:
                    agent_task_cost = step.additional_output.get(
                        ""task_total_cost"",
                        step.additional_output.get(""task_cumulative_cost""),
                    )
            timed_out = False
        except TimeoutError:
            timed_out = True

        assert isinstance(request.node, pytest.Item)
        request.node.user_properties.append((""steps"", steps))
        request.node.user_properties.append((""n_steps"", n_steps))
        request.node.user_properties.append((""timed_out"", timed_out))
        request.node.user_properties.append((""agent_task_cost"", agent_task_cost))

        agent_client_config = ClientConfig(host=config.host)
        async with ApiClient(agent_client_config) as api_client:
            api_instance = AgentApi(api_client)
            eval_results = await self.evaluate_task_state(api_instance, task_id)

        if not eval_results:
            if timed_out:
                raise TimeoutError(""Timed out, no results to evaluate"")
            else:
                raise ValueError(""No results to evaluate"")

        request.node.user_properties.append(
            (
                ""answers"",
                [r.result for r in eval_results]
                if request.config.getoption(""--keep-answers"")
                else None,
            )
        )
        request.node.user_properties.append((""scores"", [r.score for r in eval_results]))

        # FIXME: this allows partial failure
        assert any(r.passed for r in eval_results), (
            f""No passed evals: {eval_results}""
            if not timed_out
            else f""Timed out; no passed evals: {eval_results}""
        )

    @classmethod
    async def evaluate_task_state(
        cls, agent: AgentApi, task_id: str
    ) -> list[EvalResult]:
        with tempfile.TemporaryDirectory() as workspace:
            workspace = Path(workspace)
            await download_agent_artifacts_into_folder(agent, task_id, workspace)
            if cls.info.task_artifacts_dir:
                copy_challenge_artifacts_into_workspace(
                    cls.info.task_artifacts_dir, ""custom_python"", workspace
                )

            return list(cls.evaluate_workspace_content(workspace))

    @classmethod
    def evaluate_workspace_content(cls, workspace: Path) -> Iterator[EvalResult]:
        result_ground = cls._spec.ground
        outputs_for_eval = cls.get_outputs_for_eval(workspace, result_ground)

        if result_ground.should_contain or result_ground.should_not_contain:
            for source, content in outputs_for_eval:
                score = cls.score_result(content, result_ground)
                if score is not None:
                    print(f""{Fore.GREEN}Your score is:{Style.RESET_ALL}"", score)
                    yield EvalResult(
                        result=content,
                        result_source=str(source),
                        score=score,
                        passed=score > 0.9,  # FIXME: arbitrary threshold
                    )

        if result_ground.eval.type in (""python"", ""pytest""):
            for py_file, output in outputs_for_eval:
                yield EvalResult(
                    result=output,
                    result_source=str(py_file),
                    score=float(not output.startswith(""Error:"")),
                    passed=not output.startswith(""Error:""),
                )

        if result_ground.eval.type == ""llm"":
            combined_results = ""\n"".join(output[1] for output in outputs_for_eval)
            llm_eval = cls.score_result_with_llm(combined_results, result_ground)
            print(f""{Fore.GREEN}Your score is:{Style.RESET_ALL}"", llm_eval)
            if result_ground.eval.scoring == ""percentage"":
                score = llm_eval / 100
            elif result_ground.eval.scoring == ""scale"":
                score = llm_eval / 10
            else:
                score = llm_eval

            yield EvalResult(
                result=combined_results,
                result_source="", "".join(str(res[0]) for res in outputs_for_eval),
                score=score,
                passed=score > 0.9,  # FIXME: arbitrary threshold
            )

    @staticmethod
    def get_outputs_for_eval(
        workspace: str | Path | dict[str, str], ground: BuiltinChallengeSpec.Ground
    ) -> Iterator[tuple[str | Path, str]]:
        if isinstance(workspace, dict):
            workspace = workspace[""output""]

        script_dir = workspace

        for file_pattern in ground.files:
            # Check if it is a file extension
            if file_pattern.startswith("".""):
                # Find all files with the given extension in the workspace
                matching_files = glob.glob(os.path.join(script_dir, ""*"" + file_pattern))
            else:
                # Otherwise, it is a specific file
                matching_files = [os.path.join(script_dir, file_pattern)]

            logger.debug(
                f""Files to evaluate for pattern `{file_pattern}`: {matching_files}""
            )

            for file_path in matching_files:
                relative_file_path = Path(file_path).relative_to(workspace)
                logger.debug(
                    f""Evaluating {relative_file_path} ""
                    f""(eval type: {ground.eval.type})...""
                )
                if ground.eval.type == ""python"":
                    result = subprocess.run(
                        [sys.executable, file_path],
                        cwd=os.path.abspath(workspace),
                        capture_output=True,
                        text=True,
                    )
                    if ""error"" in result.stderr or result.returncode != 0:
                        yield relative_file_path, f""Error: {result.stderr}\n""
                    else:
                        yield relative_file_path, f""Output: {result.stdout}\n""
                else:
                    with open(file_path, ""r"") as f:
                        yield relative_file_path, f.read()
        else:
            if ground.eval.type == ""pytest"":
                result = subprocess.run(
                    [sys.executable, ""-m"", ""pytest""],
                    cwd=os.path.abspath(workspace),
                    capture_output=True,
                    text=True,
                )
                logger.debug(f""EXIT CODE: {result.returncode}"")
                logger.debug(f""STDOUT: {result.stdout}"")
                logger.debug(f""STDERR: {result.stderr}"")
                if ""error"" in result.stderr or result.returncode != 0:
                    yield ""pytest"", f""Error: {result.stderr.strip() or result.stdout}\n""
                else:
                    yield ""pytest"", f""Output: {result.stdout}\n""

    @staticmethod
    def score_result(content: str, ground: BuiltinChallengeSpec.Ground) -> float | None:
        print(f""{Fore.BLUE}Scoring content:{Style.RESET_ALL}"", content)
        if ground.should_contain:
            for should_contain_word in ground.should_contain:
                if not ground.case_sensitive:
                    should_contain_word = should_contain_word.lower()
                    content = content.lower()
                print_content = (
                    f""{Fore.BLUE}Word that should exist{Style.RESET_ALL}""
                    f"" - {should_contain_word}:""
                )
                if should_contain_word not in content:
                    print(print_content, ""False"")
                    return 0.0
                else:
                    print(print_content, ""True"")
                    return 1.0

        if ground.should_not_contain:
            for should_not_contain_word in ground.should_not_contain:
                if not ground.case_sensitive:
                    should_not_contain_word = should_not_contain_word.lower()
                    content = content.lower()
                print_content = (
                    f""{Fore.BLUE}Word that should not exist{Style.RESET_ALL}""
                    f"" - {should_not_contain_word}:""
                )
                if should_not_contain_word in content:
                    print(print_content, ""False"")
                    return 0.0
                else:
                    print(print_content, ""True"")
                    return 1.0

    @classmethod
    def score_result_with_llm(
        cls, content: str, ground: BuiltinChallengeSpec.Ground, *, mock: bool = False
    ) -> float:
        if mock:
            return 1.0

        # the validation for this is done in the Eval BaseModel
        scoring = SCORING_MAP[ground.eval.scoring]  # type: ignore
        prompt = PROMPT_MAP[ground.eval.template].format(  # type: ignore
            task=cls._spec.task, scoring=scoring, answer=ground.answer, response=content
        )

        if ground.eval.examples:
            prompt += FEW_SHOT_EXAMPLES.format(examples=ground.eval.examples)

        prompt += END_PROMPT

        answer = get_openai_client().chat.completions.create(
            model=""gpt-4"",
            messages=[
                {""role"": ""system"", ""content"": prompt},
            ],
        )

        return float(answer.choices[0].message.content)  # type: ignore","Point(row=99, column=0)","Point(row=420, column=71)",,classic/benchmark/agbenchmark/challenges/builtin.py
BuiltinChallenge.from_challenge_spec,function,,"def from_challenge_spec(
        cls, spec: BuiltinChallengeSpec
    ) -> type[""BuiltinChallenge""]:
        if not spec.spec_file:
            raise ValueError(""spec.spec_file not defined"")

        challenge_info = ChallengeInfo(
            eval_id=spec.eval_id,
            name=spec.name,
            task=spec.task,
            task_artifacts_dir=spec.spec_file.parent,
            category=spec.category,
            difficulty=spec.info.difficulty,
            description=spec.info.description,
            dependencies=spec.dependencies,
            reference_answer=spec.ground.answer,
            source_uri=(
                f""__BUILTIN__/{spec.spec_file.relative_to(Path(__file__).parent)}""
            ),
        )

        challenge_class_name = f""Test{challenge_info.name}""
        logger.debug(f""Creating {challenge_class_name} from spec: {spec.spec_file}"")
        return type(
            challenge_class_name,
            (BuiltinChallenge,),
            {
                ""info"": challenge_info,
                ""_spec"": spec,
                ""CHALLENGE_LOCATION"": str(spec.spec_file),
                ""ARTIFACTS_LOCATION"": str(spec.spec_file.resolve().parent),
            },
        )","Point(row=119, column=4)","Point(row=151, column=9)",BuiltinChallenge,classic/benchmark/agbenchmark/challenges/builtin.py
BuiltinChallenge.from_challenge_spec_file,function,,"def from_challenge_spec_file(cls, spec_file: Path) -> type[""BuiltinChallenge""]:
        challenge_spec = BuiltinChallengeSpec.model_validate_json(spec_file.read_text())
        challenge_spec.spec_file = spec_file
        return cls.from_challenge_spec(challenge_spec)","Point(row=154, column=4)","Point(row=157, column=54)",BuiltinChallenge,classic/benchmark/agbenchmark/challenges/builtin.py
BuiltinChallenge.from_source_uri,function,,"def from_source_uri(cls, source_uri: str) -> type[""BuiltinChallenge""]:
        if not source_uri.startswith(cls.SOURCE_URI_PREFIX):
            raise ValueError(f""Invalid source_uri for BuiltinChallenge: {source_uri}"")

        path = source_uri.split(""/"", 1)[1]
        spec_file = Path(__file__).parent / path
        return cls.from_challenge_spec_file(spec_file)","Point(row=160, column=4)","Point(row=166, column=54)",BuiltinChallenge,classic/benchmark/agbenchmark/challenges/builtin.py
BuiltinChallenge.test_method,function,,"async def test_method(
        self,
        config: AgentBenchmarkConfig,
        request: pytest.FixtureRequest,
        i_attempt: int,
    ) -> None:
        # if os.environ.get(""HELICONE_API_KEY""):
        #     from helicone.lock import HeliconeLockManager

        #     HeliconeLockManager.write_custom_property(""challenge"", self.info.name)

        timeout = self._spec.cutoff or 60

        if request.config.getoption(""--nc""):
            timeout = 100000
        elif cutoff := request.config.getoption(""--cutoff""):
            timeout = int(cutoff)  # type: ignore

        task_id = """"
        n_steps = 0
        timed_out = None
        agent_task_cost = None
        steps: list[Step] = []
        try:
            async for step in self.run_challenge(
                config, timeout, mock=bool(request.config.getoption(""--mock""))
            ):
                if not task_id:
                    task_id = step.task_id

                n_steps += 1
                steps.append(step.model_copy())
                if step.additional_output:
                    agent_task_cost = step.additional_output.get(
                        ""task_total_cost"",
                        step.additional_output.get(""task_cumulative_cost""),
                    )
            timed_out = False
        except TimeoutError:
            timed_out = True

        assert isinstance(request.node, pytest.Item)
        request.node.user_properties.append((""steps"", steps))
        request.node.user_properties.append((""n_steps"", n_steps))
        request.node.user_properties.append((""timed_out"", timed_out))
        request.node.user_properties.append((""agent_task_cost"", agent_task_cost))

        agent_client_config = ClientConfig(host=config.host)
        async with ApiClient(agent_client_config) as api_client:
            api_instance = AgentApi(api_client)
            eval_results = await self.evaluate_task_state(api_instance, task_id)

        if not eval_results:
            if timed_out:
                raise TimeoutError(""Timed out, no results to evaluate"")
            else:
                raise ValueError(""No results to evaluate"")

        request.node.user_properties.append(
            (
                ""answers"",
                [r.result for r in eval_results]
                if request.config.getoption(""--keep-answers"")
                else None,
            )
        )
        request.node.user_properties.append((""scores"", [r.score for r in eval_results]))

        # FIXME: this allows partial failure
        assert any(r.passed for r in eval_results), (
            f""No passed evals: {eval_results}""
            if not timed_out
            else f""Timed out; no passed evals: {eval_results}""
        )","Point(row=169, column=4)","Point(row=242, column=9)",BuiltinChallenge,classic/benchmark/agbenchmark/challenges/builtin.py
BuiltinChallenge.evaluate_task_state,function,,"async def evaluate_task_state(
        cls, agent: AgentApi, task_id: str
    ) -> list[EvalResult]:
        with tempfile.TemporaryDirectory() as workspace:
            workspace = Path(workspace)
            await download_agent_artifacts_into_folder(agent, task_id, workspace)
            if cls.info.task_artifacts_dir:
                copy_challenge_artifacts_into_workspace(
                    cls.info.task_artifacts_dir, ""custom_python"", workspace
                )

            return list(cls.evaluate_workspace_content(workspace))","Point(row=245, column=4)","Point(row=256, column=66)",BuiltinChallenge,classic/benchmark/agbenchmark/challenges/builtin.py
BuiltinChallenge.evaluate_workspace_content,function,,"def evaluate_workspace_content(cls, workspace: Path) -> Iterator[EvalResult]:
        result_ground = cls._spec.ground
        outputs_for_eval = cls.get_outputs_for_eval(workspace, result_ground)

        if result_ground.should_contain or result_ground.should_not_contain:
            for source, content in outputs_for_eval:
                score = cls.score_result(content, result_ground)
                if score is not None:
                    print(f""{Fore.GREEN}Your score is:{Style.RESET_ALL}"", score)
                    yield EvalResult(
                        result=content,
                        result_source=str(source),
                        score=score,
                        passed=score > 0.9,  # FIXME: arbitrary threshold
                    )

        if result_ground.eval.type in (""python"", ""pytest""):
            for py_file, output in outputs_for_eval:
                yield EvalResult(
                    result=output,
                    result_source=str(py_file),
                    score=float(not output.startswith(""Error:"")),
                    passed=not output.startswith(""Error:""),
                )

        if result_ground.eval.type == ""llm"":
            combined_results = ""\n"".join(output[1] for output in outputs_for_eval)
            llm_eval = cls.score_result_with_llm(combined_results, result_ground)
            print(f""{Fore.GREEN}Your score is:{Style.RESET_ALL}"", llm_eval)
            if result_ground.eval.scoring == ""percentage"":
                score = llm_eval / 100
            elif result_ground.eval.scoring == ""scale"":
                score = llm_eval / 10
            else:
                score = llm_eval

            yield EvalResult(
                result=combined_results,
                result_source="", "".join(str(res[0]) for res in outputs_for_eval),
                score=score,
                passed=score > 0.9,  # FIXME: arbitrary threshold
            )","Point(row=259, column=4)","Point(row=300, column=13)",BuiltinChallenge,classic/benchmark/agbenchmark/challenges/builtin.py
BuiltinChallenge.get_outputs_for_eval,function,,"def get_outputs_for_eval(
        workspace: str | Path | dict[str, str], ground: BuiltinChallengeSpec.Ground
    ) -> Iterator[tuple[str | Path, str]]:
        if isinstance(workspace, dict):
            workspace = workspace[""output""]

        script_dir = workspace

        for file_pattern in ground.files:
            # Check if it is a file extension
            if file_pattern.startswith("".""):
                # Find all files with the given extension in the workspace
                matching_files = glob.glob(os.path.join(script_dir, ""*"" + file_pattern))
            else:
                # Otherwise, it is a specific file
                matching_files = [os.path.join(script_dir, file_pattern)]

            logger.debug(
                f""Files to evaluate for pattern `{file_pattern}`: {matching_files}""
            )

            for file_path in matching_files:
                relative_file_path = Path(file_path).relative_to(workspace)
                logger.debug(
                    f""Evaluating {relative_file_path} ""
                    f""(eval type: {ground.eval.type})...""
                )
                if ground.eval.type == ""python"":
                    result = subprocess.run(
                        [sys.executable, file_path],
                        cwd=os.path.abspath(workspace),
                        capture_output=True,
                        text=True,
                    )
                    if ""error"" in result.stderr or result.returncode != 0:
                        yield relative_file_path, f""Error: {result.stderr}\n""
                    else:
                        yield relative_file_path, f""Output: {result.stdout}\n""
                else:
                    with open(file_path, ""r"") as f:
                        yield relative_file_path, f.read()
        else:
            if ground.eval.type == ""pytest"":
                result = subprocess.run(
                    [sys.executable, ""-m"", ""pytest""],
                    cwd=os.path.abspath(workspace),
                    capture_output=True,
                    text=True,
                )
                logger.debug(f""EXIT CODE: {result.returncode}"")
                logger.debug(f""STDOUT: {result.stdout}"")
                logger.debug(f""STDERR: {result.stderr}"")
                if ""error"" in result.stderr or result.returncode != 0:
                    yield ""pytest"", f""Error: {result.stderr.strip() or result.stdout}\n""
                else:
                    yield ""pytest"", f""Output: {result.stdout}\n""","Point(row=303, column=4)","Point(row=358, column=64)",BuiltinChallenge,classic/benchmark/agbenchmark/challenges/builtin.py
BuiltinChallenge.score_result,function,,"def score_result(content: str, ground: BuiltinChallengeSpec.Ground) -> float | None:
        print(f""{Fore.BLUE}Scoring content:{Style.RESET_ALL}"", content)
        if ground.should_contain:
            for should_contain_word in ground.should_contain:
                if not ground.case_sensitive:
                    should_contain_word = should_contain_word.lower()
                    content = content.lower()
                print_content = (
                    f""{Fore.BLUE}Word that should exist{Style.RESET_ALL}""
                    f"" - {should_contain_word}:""
                )
                if should_contain_word not in content:
                    print(print_content, ""False"")
                    return 0.0
                else:
                    print(print_content, ""True"")
                    return 1.0

        if ground.should_not_contain:
            for should_not_contain_word in ground.should_not_contain:
                if not ground.case_sensitive:
                    should_not_contain_word = should_not_contain_word.lower()
                    content = content.lower()
                print_content = (
                    f""{Fore.BLUE}Word that should not exist{Style.RESET_ALL}""
                    f"" - {should_not_contain_word}:""
                )
                if should_not_contain_word in content:
                    print(print_content, ""False"")
                    return 0.0
                else:
                    print(print_content, ""True"")
                    return 1.0","Point(row=361, column=4)","Point(row=393, column=30)",BuiltinChallenge,classic/benchmark/agbenchmark/challenges/builtin.py
BuiltinChallenge.score_result_with_llm,function,,"def score_result_with_llm(
        cls, content: str, ground: BuiltinChallengeSpec.Ground, *, mock: bool = False
    ) -> float:
        if mock:
            return 1.0

        # the validation for this is done in the Eval BaseModel
        scoring = SCORING_MAP[ground.eval.scoring]  # type: ignore
        prompt = PROMPT_MAP[ground.eval.template].format(  # type: ignore
            task=cls._spec.task, scoring=scoring, answer=ground.answer, response=content
        )

        if ground.eval.examples:
            prompt += FEW_SHOT_EXAMPLES.format(examples=ground.eval.examples)

        prompt += END_PROMPT

        answer = get_openai_client().chat.completions.create(
            model=""gpt-4"",
            messages=[
                {""role"": ""system"", ""content"": prompt},
            ],
        )

        return float(answer.choices[0].message.content)  # type: ignore","Point(row=396, column=4)","Point(row=420, column=71)",BuiltinChallenge,classic/benchmark/agbenchmark/challenges/builtin.py
load_builtin_challenges,function,,"def load_builtin_challenges() -> Iterator[type[BuiltinChallenge]]:
    logger.info(""Loading built-in challenges..."")

    challenges_path = Path(__file__).parent
    logger.debug(f""Looking for challenge spec files in {challenges_path}..."")

    json_files = deque(challenges_path.rglob(""data.json""))

    logger.debug(f""Found {len(json_files)} built-in challenges."")

    loaded, ignored = 0, 0
    while json_files:
        # Take and remove the first element from json_files
        json_file = json_files.popleft()
        if _challenge_should_be_ignored(json_file):
            ignored += 1
            continue

        challenge = BuiltinChallenge.from_challenge_spec_file(json_file)
        logger.debug(f""Generated test for {challenge.info.name}"")
        yield challenge

        loaded += 1

    logger.info(
        f""Loading built-in challenges complete: loaded {loaded}, ignored {ignored}.""
    )","Point(row=423, column=0)","Point(row=449, column=5)",,classic/benchmark/agbenchmark/challenges/builtin.py
_challenge_should_be_ignored,function,,"def _challenge_should_be_ignored(json_file_path: Path):
    return (
        ""challenges/deprecated"" in json_file_path.as_posix()
        or ""challenges/library"" in json_file_path.as_posix()
    )","Point(row=452, column=0)","Point(row=456, column=5)",,classic/benchmark/agbenchmark/challenges/builtin.py
ChallengeInfo,class,,"class ChallengeInfo(BaseModel):
    eval_id: str = """"
    name: str
    task: str
    task_artifacts_dir: Optional[Path] = None
    category: list[Category]
    difficulty: Optional[DifficultyLevel] = None
    description: Optional[str] = None
    dependencies: list[str] = Field(default_factory=list)
    reference_answer: Optional[str]

    source_uri: str
    """"""Internal reference indicating the source of the challenge specification""""""

    available: bool = True
    unavailable_reason: str = """"","Point(row=16, column=0)","Point(row=31, column=32)",,classic/benchmark/agbenchmark/challenges/base.py
BaseChallenge,class,"
    The base class and shared interface for all specific challenge implementations.
","class BaseChallenge(ABC):
    """"""
    The base class and shared interface for all specific challenge implementations.
    """"""

    info: ClassVar[ChallengeInfo]

    @classmethod
    @abstractmethod
    def from_source_uri(cls, source_uri: str) -> type[""BaseChallenge""]:
        """"""
        Construct an individual challenge subclass from a suitable `source_uri` (as in
        `ChallengeInfo.source_uri`).
        """"""
        ...

    @abstractmethod
    def test_method(
        self,
        config: AgentBenchmarkConfig,
        request: pytest.FixtureRequest,
        i_attempt: int,
    ) -> None | Awaitable[None]:
        """"""
        Test method for use by Pytest-based benchmark sessions. Should return normally
        if the challenge passes, and raise a (preferably descriptive) error otherwise.
        """"""
        ...

    @classmethod
    async def run_challenge(
        cls, config: AgentBenchmarkConfig, timeout: int, *, mock: bool = False
    ) -> AsyncIterator[Step]:
        """"""
        Runs the challenge on the subject agent with the specified timeout.
        Also prints basic challenge and status info to STDOUT.

        Params:
            config: The subject agent's benchmark config.
            timeout: Timeout (seconds) after which to stop the run if not finished.

        Yields:
            Step: The steps generated by the agent for the challenge task.
        """"""
        # avoid circular import
        from agbenchmark.agent_api_interface import run_api_agent

        print()
        print(
            f""{Fore.MAGENTA + Style.BRIGHT}{'='*24} ""
            f""Starting {cls.info.name} challenge""
            f"" {'='*24}{Style.RESET_ALL}""
        )
        print(f""{Fore.CYAN}Timeout:{Fore.RESET} {timeout} seconds"")
        print(f""{Fore.CYAN}Task:{Fore.RESET} {cls.info.task}"")

        print()
        logger.debug(f""Starting {cls.info.name} challenge run"")
        i = 0
        async for step in run_api_agent(
            cls.info.task, config, timeout, cls.info.task_artifacts_dir, mock=mock
        ):
            i += 1
            print(f""[{cls.info.name}] - step {step.name} ({i}. request)"")
            yield step
        logger.debug(f""Finished {cls.info.name} challenge run"")

    @classmethod
    @abstractmethod
    async def evaluate_task_state(
        cls, agent: AgentApi, task_id: str
    ) -> list[EvalResult]:
        ...","Point(row=34, column=0)","Point(row=106, column=11)",,classic/benchmark/agbenchmark/challenges/base.py
BaseChallenge.from_source_uri,function,"
        Construct an individual challenge subclass from a suitable `source_uri` (as in
        `ChallengeInfo.source_uri`).
","def from_source_uri(cls, source_uri: str) -> type[""BaseChallenge""]:
        """"""
        Construct an individual challenge subclass from a suitable `source_uri` (as in
        `ChallengeInfo.source_uri`).
        """"""
        ...","Point(row=43, column=4)","Point(row=48, column=11)",BaseChallenge,classic/benchmark/agbenchmark/challenges/base.py
BaseChallenge.test_method,function,"
        Test method for use by Pytest-based benchmark sessions. Should return normally
        if the challenge passes, and raise a (preferably descriptive) error otherwise.
","def test_method(
        self,
        config: AgentBenchmarkConfig,
        request: pytest.FixtureRequest,
        i_attempt: int,
    ) -> None | Awaitable[None]:
        """"""
        Test method for use by Pytest-based benchmark sessions. Should return normally
        if the challenge passes, and raise a (preferably descriptive) error otherwise.
        """"""
        ...","Point(row=51, column=4)","Point(row=61, column=11)",BaseChallenge,classic/benchmark/agbenchmark/challenges/base.py
BaseChallenge.run_challenge,function,"
        Runs the challenge on the subject agent with the specified timeout.
        Also prints basic challenge and status info to STDOUT.

        Params:
            config: The subject agent's benchmark config.
            timeout: Timeout (seconds) after which to stop the run if not finished.

        Yields:
            Step: The steps generated by the agent for the challenge task.
","async def run_challenge(
        cls, config: AgentBenchmarkConfig, timeout: int, *, mock: bool = False
    ) -> AsyncIterator[Step]:
        """"""
        Runs the challenge on the subject agent with the specified timeout.
        Also prints basic challenge and status info to STDOUT.

        Params:
            config: The subject agent's benchmark config.
            timeout: Timeout (seconds) after which to stop the run if not finished.

        Yields:
            Step: The steps generated by the agent for the challenge task.
        """"""
        # avoid circular import
        from agbenchmark.agent_api_interface import run_api_agent

        print()
        print(
            f""{Fore.MAGENTA + Style.BRIGHT}{'='*24} ""
            f""Starting {cls.info.name} challenge""
            f"" {'='*24}{Style.RESET_ALL}""
        )
        print(f""{Fore.CYAN}Timeout:{Fore.RESET} {timeout} seconds"")
        print(f""{Fore.CYAN}Task:{Fore.RESET} {cls.info.task}"")

        print()
        logger.debug(f""Starting {cls.info.name} challenge run"")
        i = 0
        async for step in run_api_agent(
            cls.info.task, config, timeout, cls.info.task_artifacts_dir, mock=mock
        ):
            i += 1
            print(f""[{cls.info.name}] - step {step.name} ({i}. request)"")
            yield step
        logger.debug(f""Finished {cls.info.name} challenge run"")","Point(row=64, column=4)","Point(row=99, column=63)",BaseChallenge,classic/benchmark/agbenchmark/challenges/base.py
BaseChallenge.evaluate_task_state,function,,"async def evaluate_task_state(
        cls, agent: AgentApi, task_id: str
    ) -> list[EvalResult]:
        ...","Point(row=103, column=4)","Point(row=106, column=11)",BaseChallenge,classic/benchmark/agbenchmark/challenges/base.py
get_ethereum_price,function,,"def get_ethereum_price() -> float:
    url = ""https://api.coingecko.com/api/v3/simple/price?ids=ethereum&vs_currencies=usd""
    response = requests.get(url)

    if response.status_code == 200:
        data = response.json()
        return data[""ethereum""][""usd""]
    else:
        raise Exception(f""Failed to fetch data: {response.status_code}"")","Point(row=3, column=0)","Point(row=11, column=72)",,classic/benchmark/agbenchmark/challenges/library/ethereum/check_price/artifacts_in/sample_code.py
test_get_ethereum_price,function,,"def test_get_ethereum_price() -> None:
    # Read the Ethereum price from the file
    with open(""eth_price.txt"", ""r"") as file:
        eth_price = file.read().strip()

    # Validate that the eth price is all digits
    pattern = r""^\d+$""
    matches = re.match(pattern, eth_price) is not None
    assert (
        matches
    ), f""AssertionError: Ethereum price should be all digits, but got {eth_price}""

    # Get the current price of Ethereum
    real_eth_price = get_ethereum_price()

    # Convert the eth price to a numerical value for comparison
    eth_price_value = float(eth_price)
    real_eth_price_value = float(real_eth_price)

    # Check if the eth price is within $50 of the actual Ethereum price
    assert abs(real_eth_price_value - eth_price_value) <= 50, (
        ""AssertionError: Ethereum price is not within $50 of the actual Ethereum price ""
        f""(Provided price: ${eth_price}, Real price: ${real_eth_price})""
    )

    print(""Matches"")","Point(row=5, column=0)","Point(row=30, column=20)",,classic/benchmark/agbenchmark/challenges/library/ethereum/check_price/artifacts_in/test.py
get_ethereum_price,function,,"def get_ethereum_price() -> float:
    url = ""https://api.coingecko.com/api/v3/simple/price?ids=ethereum&vs_currencies=usd""
    response = requests.get(url)

    if response.status_code == 200:
        data = response.json()
        return data[""ethereum""][""usd""]
    else:
        raise Exception(f""Failed to fetch data: {response.status_code}"")","Point(row=3, column=0)","Point(row=11, column=72)",,classic/benchmark/agbenchmark/challenges/library/ethereum/check_price/artifacts_out/sample_code.py
test_get_ethereum_price,function,,"def test_get_ethereum_price() -> None:
    # Read the Ethereum price from the file
    with open(""output.txt"", ""r"") as file:
        eth_price = file.read().strip()

    # Validate that the eth price is all digits
    pattern = r""^\d+$""
    matches = re.match(pattern, eth_price) is not None
    assert (
        matches
    ), f""AssertionError: Ethereum price should be all digits, but got {eth_price}""

    # Get the current price of Ethereum
    real_eth_price = get_ethereum_price()

    # Convert the eth price to a numerical value for comparison
    eth_price_value = float(eth_price)
    real_eth_price_value = float(real_eth_price)

    # Check if the eth price is within $50 of the actual Ethereum price
    assert abs(real_eth_price_value - eth_price_value) <= 50, (
        ""AssertionError: Ethereum price is not within $50 of the actual Ethereum price ""
        f""(Provided price: ${eth_price}, Real price: ${real_eth_price})""
    )

    print(""Matches"")","Point(row=5, column=0)","Point(row=30, column=20)",,classic/benchmark/agbenchmark/challenges/library/ethereum/check_price/artifacts_out/test.py
column,function,,"def column(matrix, i):
    return [row[i] for row in matrix]","Point(row=3, column=0)","Point(row=4, column=37)",,classic/benchmark/agbenchmark/challenges/verticals/code/5_tic_tac_toe/artifacts_out/tic_tac_toe.py
check,function,,"def check(list):
    if len(set(list)) <= 1:
        if list[0] != 0:
            return list[0]
    return None","Point(row=7, column=0)","Point(row=11, column=15)",,classic/benchmark/agbenchmark/challenges/verticals/code/5_tic_tac_toe/artifacts_out/tic_tac_toe.py
checkDiagLeft,function,,"def checkDiagLeft(board):
    if board[0][0] == board[1][1] and board[1][1] == board[2][2]:
        if board[0][0] != 0:
            return board[0][0]
    return None","Point(row=14, column=0)","Point(row=18, column=15)",,classic/benchmark/agbenchmark/challenges/verticals/code/5_tic_tac_toe/artifacts_out/tic_tac_toe.py
checkDiagRight,function,,"def checkDiagRight(board):
    if board[2][0] == board[1][1] and board[1][1] == board[0][2]:
        if board[2][0] != 0:
            return board[2][0]
    return None","Point(row=21, column=0)","Point(row=25, column=15)",,classic/benchmark/agbenchmark/challenges/verticals/code/5_tic_tac_toe/artifacts_out/tic_tac_toe.py
placeItem,function,,"def placeItem(row, column, board, current_player):
    if board[row][column] != 0:
        return None
    else:
        board[row][column] = current_player","Point(row=28, column=0)","Point(row=32, column=43)",,classic/benchmark/agbenchmark/challenges/verticals/code/5_tic_tac_toe/artifacts_out/tic_tac_toe.py
swapPlayers,function,,"def swapPlayers(player):
    if player == 2:
        return 1
    else:
        return 2","Point(row=35, column=0)","Point(row=39, column=16)",,classic/benchmark/agbenchmark/challenges/verticals/code/5_tic_tac_toe/artifacts_out/tic_tac_toe.py
winner,function,,"def winner(board):
    for rowIndex in board:
        if check(rowIndex) is not None:
            return check(rowIndex)
    for columnIndex in range(len(board[0])):
        if check(column(board, columnIndex)) is not None:
            return check(column(board, columnIndex))
    if checkDiagLeft(board) is not None:
        return checkDiagLeft(board)
    if checkDiagRight(board) is not None:
        return checkDiagRight(board)
    return 0","Point(row=42, column=0)","Point(row=53, column=12)",,classic/benchmark/agbenchmark/challenges/verticals/code/5_tic_tac_toe/artifacts_out/tic_tac_toe.py
getLocation,function,,"def getLocation():
    location = input(
        ""Choose where to play. Enter two numbers separated by a comma [example: 1,1]: ""
    )
    print(f""\nYou picked {location}"")
    coordinates = [int(x) for x in location.split("","")]
    while (
        len(coordinates) != 2
        or coordinates[0] < 0
        or coordinates[0] > 2
        or coordinates[1] < 0
        or coordinates[1] > 2
    ):
        print(""You inputted a location in an invalid format"")
        location = input(
            ""Choose where to play. Enter two numbers separated by a comma ""
            ""[example: 1,1]: ""
        )
        coordinates = [int(x) for x in location.split("","")]
    return coordinates","Point(row=56, column=0)","Point(row=75, column=22)",,classic/benchmark/agbenchmark/challenges/verticals/code/5_tic_tac_toe/artifacts_out/tic_tac_toe.py
gamePlay,function,,"def gamePlay():
    num_moves = 0
    pp = pprint.PrettyPrinter(width=20)
    current_player = 1
    board = [[0 for x in range(3)] for x in range(3)]

    while num_moves < 9 and winner(board) == 0:
        print(""This is the current board: "")
        pp.pprint(board)
        coordinates = getLocation()
        placeItem(coordinates[0], coordinates[1], board, current_player)
        current_player = swapPlayers(current_player)
        if winner(board) != 0:
            print(f""Player {winner(board)} won!"")
        num_moves += 1

    if winner(board) == 0:
        print(""Draw"")","Point(row=78, column=0)","Point(row=95, column=21)",,classic/benchmark/agbenchmark/challenges/verticals/code/5_tic_tac_toe/artifacts_out/tic_tac_toe.py
run_game_with_inputs,function,,"def run_game_with_inputs(inputs):
    # Start the game process
    process = subprocess.Popen(
        [""python"", ""tic_tac_toe.py""],
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    )

    # Send the input moves one by one
    output, errors = process.communicate(""\n"".join(inputs))

    # Print the inputs and outputs
    print(""Inputs:\n"", ""\n"".join(inputs))
    print(""Output:\n"", output)
    print(""Errors:\n"", errors)

    return output","Point(row=5, column=0)","Point(row=23, column=17)",,classic/benchmark/agbenchmark/challenges/verticals/code/5_tic_tac_toe/custom_python/test.py
test_game,function,,"def test_game(inputs, expected_output):
    output = run_game_with_inputs(inputs)
    assert expected_output in output","Point(row=34, column=0)","Point(row=36, column=36)",,classic/benchmark/agbenchmark/challenges/verticals/code/5_tic_tac_toe/custom_python/test.py
generate_password,function,,"def generate_password(length: int = 8) -> str:
    if length < 8 or length > 16:
        raise ValueError(""Password length must be between 8 and 16 characters."")

    characters = string.ascii_letters + string.digits + string.punctuation
    password = [
        random.choice(string.ascii_lowercase),
        random.choice(string.ascii_uppercase),
        random.choice(string.digits),
        random.choice(string.punctuation),
    ]
    password += [random.choice(characters) for _ in range(length - 4)]
    random.shuffle(password)
    return """".join(password)","Point(row=5, column=0)","Point(row=18, column=28)",,classic/benchmark/agbenchmark/challenges/verticals/code/2_password_generator/artifacts_out/password_generator.py
TestPasswordGenerator,class,,"class TestPasswordGenerator(unittest.TestCase):
    def test_password_length(self):
        for i in range(8, 17):
            password = password_generator.generate_password(i)
            self.assertEqual(len(password), i)

    def test_value_error(self):
        with self.assertRaises(ValueError):
            password_generator.generate_password(7)
        with self.assertRaises(ValueError):
            password_generator.generate_password(17)

    def test_password_content(self):
        password = password_generator.generate_password()
        self.assertTrue(any(c.isdigit() for c in password))
        self.assertTrue(
            any(c in password_generator.string.punctuation for c in password)
        )","Point(row=6, column=0)","Point(row=23, column=9)",,classic/benchmark/agbenchmark/challenges/verticals/code/2_password_generator/custom_python/test.py
TestPasswordGenerator.test_password_length,function,,"def test_password_length(self):
        for i in range(8, 17):
            password = password_generator.generate_password(i)
            self.assertEqual(len(password), i)","Point(row=7, column=4)","Point(row=10, column=46)",TestPasswordGenerator,classic/benchmark/agbenchmark/challenges/verticals/code/2_password_generator/custom_python/test.py
TestPasswordGenerator.test_value_error,function,,"def test_value_error(self):
        with self.assertRaises(ValueError):
            password_generator.generate_password(7)
        with self.assertRaises(ValueError):
            password_generator.generate_password(17)","Point(row=12, column=4)","Point(row=16, column=52)",TestPasswordGenerator,classic/benchmark/agbenchmark/challenges/verticals/code/2_password_generator/custom_python/test.py
TestPasswordGenerator.test_password_content,function,,"def test_password_content(self):
        password = password_generator.generate_password()
        self.assertTrue(any(c.isdigit() for c in password))
        self.assertTrue(
            any(c in password_generator.string.punctuation for c in password)
        )","Point(row=18, column=4)","Point(row=23, column=9)",TestPasswordGenerator,classic/benchmark/agbenchmark/challenges/verticals/code/2_password_generator/custom_python/test.py
organize_files,function,,"def organize_files(directory_path):
    # Define file type groups
    file_types = {
        ""images"": ["".png"", "".jpg"", "".jpeg""],
        ""documents"": ["".pdf"", "".docx"", "".txt""],
        ""audio"": ["".mp3"", "".wav"", "".flac""],
    }

    # Create the folders if they don't exist
    for folder_name in file_types.keys():
        folder_path = os.path.join(directory_path, folder_name)
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)

    # Traverse through all files and folders in the specified directory
    for foldername, subfolders, filenames in os.walk(directory_path):
        for filename in filenames:
            # Get file extension
            _, file_extension = os.path.splitext(filename)

            # Move files to corresponding folders
            for folder_name, extensions in file_types.items():
                if file_extension in extensions:
                    old_path = os.path.join(foldername, filename)
                    new_path = os.path.join(directory_path, folder_name, filename)
                    if old_path != new_path:
                        shutil.move(old_path, new_path)","Point(row=5, column=0)","Point(row=31, column=55)",,classic/benchmark/agbenchmark/challenges/verticals/code/3_file_organizer/artifacts_out/organize_files.py
TestOrganizeFiles,class,,"class TestOrganizeFiles(unittest.TestCase):
    def setUp(self):
        # Create temporary directory
        self.test_dir = tempfile.mkdtemp()

        # File types and their corresponding directory
        self.file_types = {
            ""test_image.png"": ""images"",
            ""test_doc.txt"": ""documents"",
            ""test_audio.mp3"": ""audio"",
        }

        # Create test files
        for file_name in self.file_types.keys():
            open(os.path.join(self.test_dir, file_name), ""a"").close()

    def test_organize_files(self):
        # Call the organize_files.py script using subprocess
        subprocess.call(
            [""python"", ""organize_files.py"", ""--directory_path="" + self.test_dir]
        )

        # Check if the files have been moved to the correct directories
        for file_name, directory in self.file_types.items():
            self.assertTrue(
                os.path.isfile(os.path.join(self.test_dir, directory, file_name))
            )

    def tearDown(self):
        # Delete test directory and its contents
        for file_name, directory in self.file_types.items():
            os.remove(os.path.join(self.test_dir, directory, file_name))
        for directory in set(self.file_types.values()):
            os.rmdir(os.path.join(self.test_dir, directory))
        os.rmdir(self.test_dir)","Point(row=6, column=0)","Point(row=40, column=31)",,classic/benchmark/agbenchmark/challenges/verticals/code/3_file_organizer/custom_python/test.py
TestOrganizeFiles.setUp,function,,"def setUp(self):
        # Create temporary directory
        self.test_dir = tempfile.mkdtemp()

        # File types and their corresponding directory
        self.file_types = {
            ""test_image.png"": ""images"",
            ""test_doc.txt"": ""documents"",
            ""test_audio.mp3"": ""audio"",
        }

        # Create test files
        for file_name in self.file_types.keys():
            open(os.path.join(self.test_dir, file_name), ""a"").close()","Point(row=7, column=4)","Point(row=20, column=69)",TestOrganizeFiles,classic/benchmark/agbenchmark/challenges/verticals/code/3_file_organizer/custom_python/test.py
TestOrganizeFiles.test_organize_files,function,,"def test_organize_files(self):
        # Call the organize_files.py script using subprocess
        subprocess.call(
            [""python"", ""organize_files.py"", ""--directory_path="" + self.test_dir]
        )

        # Check if the files have been moved to the correct directories
        for file_name, directory in self.file_types.items():
            self.assertTrue(
                os.path.isfile(os.path.join(self.test_dir, directory, file_name))
            )","Point(row=22, column=4)","Point(row=32, column=13)",TestOrganizeFiles,classic/benchmark/agbenchmark/challenges/verticals/code/3_file_organizer/custom_python/test.py
TestOrganizeFiles.tearDown,function,,"def tearDown(self):
        # Delete test directory and its contents
        for file_name, directory in self.file_types.items():
            os.remove(os.path.join(self.test_dir, directory, file_name))
        for directory in set(self.file_types.values()):
            os.rmdir(os.path.join(self.test_dir, directory))
        os.rmdir(self.test_dir)","Point(row=34, column=4)","Point(row=40, column=31)",TestOrganizeFiles,classic/benchmark/agbenchmark/challenges/verticals/code/3_file_organizer/custom_python/test.py
three_sum,function,,"def three_sum(nums: List[int], target: int) -> Optional[List[int]]:
    nums_indices = [(num, index) for index, num in enumerate(nums)]
    nums_indices.sort()
    for i in range(len(nums_indices) - 2):
        if i > 0 and nums_indices[i] == nums_indices[i - 1]:
            continue
        l, r = i + 1, len(nums_indices) - 1
        while l < r:
            three_sum = nums_indices[i][0] + nums_indices[l][0] + nums_indices[r][0]
            if three_sum < target:
                l += 1
            elif three_sum > target:
                r -= 1
            else:
                indices = sorted(
                    [nums_indices[i][1], nums_indices[l][1], nums_indices[r][1]]
                )
                return indices
    return None","Point(row=3, column=0)","Point(row=21, column=15)",,classic/benchmark/agbenchmark/challenges/verticals/code/1_three_sum/artifacts_out/sample_code.py
test_three_sum,function,,"def test_three_sum(nums: List[int], target: int, expected_result: List[int]) -> None:
    result = three_sum(nums, target)
    print(result)
    assert (
        result == expected_result
    ), f""AssertionError: Expected the output to be {expected_result}""","Point(row=6, column=0)","Point(row=11, column=69)",,classic/benchmark/agbenchmark/challenges/verticals/code/1_three_sum/custom_python/test.py
test_ship_placement_out_of_bounds,function,,"def test_ship_placement_out_of_bounds(battleship_game):
    game_id = battleship_game.create_game()

    try:
        out_of_bounds_ship = ShipPlacement(
            ship_type=""battleship"",
            start={""row"": 11, ""column"": ""Z""},
            direction=""horizontal"",
        )
    except ValidationError:  # Use the directly imported ValidationError class
        pass
    else:
        with pytest.raises(ValueError, match=""Placement out of bounds""):
            battleship_game.create_ship_placement(game_id, out_of_bounds_ship)","Point(row=6, column=0)","Point(row=19, column=78)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_negative.py
test_no_ship_overlap,function,,"def test_no_ship_overlap(battleship_game):
    game_id = battleship_game.create_game()
    placement1 = ShipPlacement(
        ship_type=""battleship"", start={""row"": 1, ""column"": ""A""}, direction=""horizontal""
    )
    battleship_game.create_ship_placement(game_id, placement1)
    placement2 = ShipPlacement(
        ship_type=""cruiser"", start={""row"": 1, ""column"": ""A""}, direction=""horizontal""
    )
    with pytest.raises(ValueError):
        battleship_game.create_ship_placement(game_id, placement2)","Point(row=22, column=0)","Point(row=32, column=66)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_negative.py
test_cant_hit_before_ships_placed,function,,"def test_cant_hit_before_ships_placed(battleship_game):
    game_id = battleship_game.create_game()
    placement1 = ShipPlacement(
        ship_type=""battleship"", start={""row"": 1, ""column"": ""A""}, direction=""horizontal""
    )
    battleship_game.create_ship_placement(game_id, placement1)
    placement2 = ShipPlacement(
        ship_type=""cruiser"", start={""row"": 4, ""column"": ""D""}, direction=""horizontal""
    )
    battleship_game.create_ship_placement(game_id, placement2)
    turn = Turn(target={""row"": 1, ""column"": ""A""})
    with pytest.raises(
        ValueError, match=""All ships must be placed before starting turns""
    ):
        battleship_game.create_turn(game_id, turn)","Point(row=35, column=0)","Point(row=49, column=50)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_negative.py
test_cant_place_ship_after_all_ships_placed,function,,"def test_cant_place_ship_after_all_ships_placed(battleship_game, initialized_game_id):
    battleship_game.get_game(initialized_game_id)
    additional_ship = ShipPlacement(
        ship_type=""carrier"", start={""row"": 2, ""column"": ""E""}, direction=""horizontal""
    )

    with pytest.raises(
        ValueError, match=""All ships are already placed. Cannot place more ships.""
    ):
        battleship_game.create_ship_placement(initialized_game_id, additional_ship)","Point(row=52, column=0)","Point(row=61, column=83)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_negative.py
test_ship_placement_invalid_direction,function,,"def test_ship_placement_invalid_direction(battleship_game):
    game_id = battleship_game.create_game()

    with pytest.raises(ValueError, match=""Invalid ship direction""):
        invalid_direction_ship = ShipPlacement(
            ship_type=""battleship"",
            start={""row"": 1, ""column"": ""A""},
            direction=""diagonal"",
        )
        battleship_game.create_ship_placement(game_id, invalid_direction_ship)","Point(row=64, column=0)","Point(row=73, column=78)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_negative.py
test_invalid_ship_type,function,,"def test_invalid_ship_type(battleship_game):
    game_id = battleship_game.create_game()
    invalid_ship = ShipPlacement(
        ship_type=""spacecraft"", start={""row"": 1, ""column"": ""A""}, direction=""horizontal""
    )
    with pytest.raises(ValueError, match=""Invalid ship type""):
        battleship_game.create_ship_placement(game_id, invalid_ship)","Point(row=76, column=0)","Point(row=82, column=68)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_negative.py
test_ship_placement_extends_beyond_boundaries,function,,"def test_ship_placement_extends_beyond_boundaries(battleship_game):
    game_id = battleship_game.create_game()

    with pytest.raises(ValueError, match=""Ship extends beyond board boundaries""):
        ship_extending_beyond = ShipPlacement(
            ship_type=""battleship"",
            start={""row"": 1, ""column"": ""H""},
            direction=""horizontal"",
        )
        battleship_game.create_ship_placement(game_id, ship_extending_beyond)

    with pytest.raises(ValueError, match=""Ship extends beyond board boundaries""):
        ship_extending_beyond = ShipPlacement(
            ship_type=""cruiser"", start={""row"": 9, ""column"": ""A""}, direction=""vertical""
        )
        battleship_game.create_ship_placement(game_id, ship_extending_beyond)","Point(row=85, column=0)","Point(row=100, column=77)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_negative.py
battleship_game,function,,"def battleship_game():
    return Battleship()","Point(row=8, column=0)","Point(row=9, column=23)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/conftest.py
initialized_game_id,function,,"def initialized_game_id(battleship_game):
    # Create a game instance
    game_id = battleship_game.create_game()

    # Place all the ships using battleship_game's methods
    sample_ship_placements = [
        ShipPlacement(
            ship_type=""carrier"", start={""row"": 1, ""column"": ""A""}, direction=""horizontal""
        ),
        ShipPlacement(
            ship_type=""battleship"",
            start={""row"": 2, ""column"": ""A""},
            direction=""horizontal"",
        ),
        ShipPlacement(
            ship_type=""cruiser"", start={""row"": 3, ""column"": ""A""}, direction=""horizontal""
        ),
        ShipPlacement(
            ship_type=""submarine"",
            start={""row"": 4, ""column"": ""A""},
            direction=""horizontal"",
        ),
        ShipPlacement(
            ship_type=""destroyer"",
            start={""row"": 5, ""column"": ""A""},
            direction=""horizontal"",
        ),
    ]

    for ship_placement in sample_ship_placements:
        # Place ship using battleship_game's methods
        battleship_game.create_ship_placement(game_id, ship_placement)

    return game_id","Point(row=13, column=0)","Point(row=46, column=18)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/conftest.py
game_over_fixture,function,,"def game_over_fixture(battleship_game, initialized_game_id):
    # Assuming 10x10 grid, target all possible positions
    for row in range(1, 11):
        for column in list(""ABCDEFGHIJ""):
            # Player 1 takes a turn
            turn = Turn(target={""row"": row, ""column"": column})
            battleship_game.create_turn(initialized_game_id, turn)

            # Player 2 takes a turn, targeting the same position as Player 1
            battleship_game.create_turn(initialized_game_id, turn)

    # At the end of this fixture, the game should be over
    return initialized_game_id","Point(row=50, column=0)","Point(row=62, column=30)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/conftest.py
test_turns_and_results,function,,"def test_turns_and_results(battleship_game, initialized_game_id):
    turn = Turn(target={""row"": 1, ""column"": ""A""})
    response = battleship_game.create_turn(initialized_game_id, turn)

    assert response.result in [""hit"", ""miss""]
    if response.result == ""hit"":
        assert response.ship_type == ""carrier""
    game = battleship_game.get_game(initialized_game_id)
    assert turn in game.turns","Point(row=3, column=0)","Point(row=11, column=29)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_positive.py
test_game_status_and_winner,function,,"def test_game_status_and_winner(battleship_game):
    game_id = battleship_game.create_game()
    status = battleship_game.get_game_status(game_id)
    assert isinstance(status.is_game_over, bool)
    if status.is_game_over:
        winner = battleship_game.get_winner(game_id)
        assert winner is not None","Point(row=14, column=0)","Point(row=20, column=33)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_positive.py
test_delete_game,function,,"def test_delete_game(battleship_game):
    game_id = battleship_game.create_game()
    battleship_game.delete_game(game_id)
    assert battleship_game.get_game(game_id) is None","Point(row=23, column=0)","Point(row=26, column=52)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_positive.py
test_ship_rotation,function,,"def test_ship_rotation(battleship_game):
    game_id = battleship_game.create_game()
    placement_horizontal = ShipPlacement(
        ship_type=""battleship"", start={""row"": 1, ""column"": ""B""}, direction=""horizontal""
    )
    battleship_game.create_ship_placement(game_id, placement_horizontal)
    placement_vertical = ShipPlacement(
        ship_type=""cruiser"", start={""row"": 3, ""column"": ""D""}, direction=""vertical""
    )
    battleship_game.create_ship_placement(game_id, placement_vertical)
    game = battleship_game.get_game(game_id)
    assert placement_horizontal in game.ships
    assert placement_vertical in game.ships","Point(row=29, column=0)","Point(row=41, column=43)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_positive.py
test_game_state_updates,function,,"def test_game_state_updates(battleship_game, initialized_game_id):
    turn = Turn(target={""row"": 3, ""column"": ""A""})
    battleship_game.create_turn(initialized_game_id, turn)

    game = battleship_game.get_game(initialized_game_id)

    target_key = (3, ord(""A"") - ord(""A""))
    assert target_key in game.board and game.board[target_key] == ""hit""","Point(row=44, column=0)","Point(row=51, column=71)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_positive.py
test_ship_sinking_feedback,function,,"def test_ship_sinking_feedback(battleship_game, initialized_game_id):
    hits = [""A"", ""B"", ""C"", ""D""]
    static_moves = [
        {""row"": 1, ""column"": ""E""},
        {""row"": 1, ""column"": ""F""},
        {""row"": 1, ""column"": ""G""},
        {""row"": 1, ""column"": ""H""},
    ]

    response = None
    for index, hit in enumerate(hits):
        turn = Turn(target={""row"": 2, ""column"": hit})
        response = battleship_game.create_turn(initialized_game_id, turn)
        assert response.ship_type == ""battleship""

        static_turn = Turn(target=static_moves[index])
        battleship_game.create_turn(initialized_game_id, static_turn)

    assert response and response.result == ""sunk""","Point(row=54, column=0)","Point(row=72, column=49)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_positive.py
test_restart_game,function,,"def test_restart_game(battleship_game):
    game_id = battleship_game.create_game()
    battleship_game.delete_game(game_id)
    game_id = (
        battleship_game.create_game()
    )  # Use the returned game_id after recreating the game
    game = battleship_game.get_game(game_id)
    assert game is not None","Point(row=75, column=0)","Point(row=82, column=27)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_positive.py
test_ship_edge_overlapping,function,,"def test_ship_edge_overlapping(battleship_game):
    game_id = battleship_game.create_game()

    first_ship = ShipPlacement(
        ship_type=""battleship"", start={""row"": 1, ""column"": ""A""}, direction=""horizontal""
    )
    battleship_game.create_ship_placement(game_id, first_ship)

    next_ship = ShipPlacement(
        ship_type=""cruiser"", start={""row"": 1, ""column"": ""E""}, direction=""horizontal""
    )
    battleship_game.create_ship_placement(game_id, next_ship)

    game = battleship_game.get_game(game_id)
    assert first_ship in game.ships
    assert next_ship in game.ships","Point(row=85, column=0)","Point(row=100, column=34)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_positive.py
test_game_state_after_ship_placement,function,,"def test_game_state_after_ship_placement(battleship_game):
    game_id = battleship_game.create_game()

    ship_placement = ShipPlacement(
        ship_type=""battleship"", start={""row"": 1, ""column"": ""A""}, direction=""horizontal""
    )
    battleship_game.create_ship_placement(game_id, ship_placement)

    game = battleship_game.get_game(game_id)
    assert ship_placement in game.ships","Point(row=103, column=0)","Point(row=112, column=39)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_positive.py
test_game_state_after_turn,function,,"def test_game_state_after_turn(initialized_game_id, battleship_game):
    turn = Turn(target={""row"": 1, ""column"": ""A""})
    response = battleship_game.create_turn(initialized_game_id, turn)

    game = battleship_game.get_game(initialized_game_id)

    if response.result == ""hit"":
        assert game.board[(1, 0)] == ""hit""
    else:
        assert game.board[1][0] == ""miss""","Point(row=115, column=0)","Point(row=124, column=41)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_positive.py
test_multiple_hits_on_ship,function,,"def test_multiple_hits_on_ship(battleship_game, initialized_game_id):
    hit_positions = [""A"", ""B"", ""C"", ""D"", ""E""]

    for index, pos in enumerate(hit_positions):
        turn = Turn(target={""row"": 1, ""column"": pos})
        response = battleship_game.create_turn(initialized_game_id, turn)

        if index == len(hit_positions) - 1:
            assert response.result == ""sunk""
        else:
            assert response.result == ""hit""","Point(row=127, column=0)","Point(row=137, column=43)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_positive.py
test_game_over_condition,function,,"def test_game_over_condition(battleship_game, initialized_game_id):
    for row in range(1, 11):
        for column in list(""ABCDEFGHIJ""):
            turn = Turn(target={""row"": row, ""column"": column})
            battleship_game.create_turn(initialized_game_id, turn)

            battleship_game.create_turn(initialized_game_id, turn)

    status = battleship_game.get_game_status(initialized_game_id)
    assert status.is_game_over","Point(row=140, column=0)","Point(row=149, column=30)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_positive.py
ShipPlacement,class,,"class ShipPlacement(BaseModel):
    ship_type: str
    start: dict  # {""row"": int, ""column"": str}
    direction: str

    @field_validator(""start"")
    def validate_start(cls, start):
        row, column = start.get(""row""), start.get(""column"")

        if not (1 <= row <= 10):
            raise ValueError(""Row must be between 1 and 10 inclusive."")

        if column not in list(""ABCDEFGHIJ""):
            raise ValueError(""Column must be one of A, B, C, D, E, F, G, H, I, J."")

        return start","Point(row=7, column=0)","Point(row=22, column=20)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/abstract_class.py
ShipPlacement.validate_start,function,,"def validate_start(cls, start):
        row, column = start.get(""row""), start.get(""column"")

        if not (1 <= row <= 10):
            raise ValueError(""Row must be between 1 and 10 inclusive."")

        if column not in list(""ABCDEFGHIJ""):
            raise ValueError(""Column must be one of A, B, C, D, E, F, G, H, I, J."")

        return start","Point(row=13, column=4)","Point(row=22, column=20)",ShipPlacement,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/abstract_class.py
Turn,class,,"class Turn(BaseModel):
    target: dict  # {""row"": int, ""column"": str}","Point(row=25, column=0)","Point(row=26, column=47)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/abstract_class.py
TurnResponse,class,,"class TurnResponse(BaseModel):
    result: str
    ship_type: Optional[str]  # This would be None if the result is a miss","Point(row=29, column=0)","Point(row=31, column=74)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/abstract_class.py
GameStatus,class,,"class GameStatus(BaseModel):
    is_game_over: bool
    winner: Optional[str]","Point(row=34, column=0)","Point(row=36, column=25)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/abstract_class.py
Game,class,,"class Game(BaseModel):
    game_id: str
    players: list[str]
    # This could represent the state of the game board,
    # you might need to flesh this out further:
    board: dict
    ships: list[ShipPlacement]  # List of ship placements for this game
    turns: list[Turn]  # List of turns that have been taken","Point(row=39, column=0)","Point(row=46, column=59)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/abstract_class.py
AbstractBattleship,class,,"class AbstractBattleship(ABC):
    SHIP_LENGTHS = {
        ""carrier"": 5,
        ""battleship"": 4,
        ""cruiser"": 3,
        ""submarine"": 3,
        ""destroyer"": 2,
    }

    @abstractmethod
    def create_ship_placement(self, game_id: str, placement: ShipPlacement) -> None:
        """"""
        Place a ship on the grid.
        """"""
        pass

    @abstractmethod
    def create_turn(self, game_id: str, turn: Turn) -> TurnResponse:
        """"""
        Players take turns to target a grid cell.
        """"""
        pass

    @abstractmethod
    def get_game_status(self, game_id: str) -> GameStatus:
        """"""
        Check if the game is over and get the winner if there's one.
        """"""
        pass

    @abstractmethod
    def get_winner(self, game_id: str) -> str:
        """"""
        Get the winner of the game.
        """"""
        pass

    @abstractmethod
    def get_game(self) -> Game | None:
        """"""
        Retrieve the state of the game.
        """"""
        pass

    @abstractmethod
    def delete_game(self, game_id: str) -> None:
        """"""
        Delete a game given its ID.
        """"""
        pass

    @abstractmethod
    def create_game(self) -> None:
        """"""
        Create a new game.

        Returns:
            str: The ID of the created game.
        """"""
        pass","Point(row=49, column=0)","Point(row=108, column=12)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/abstract_class.py
AbstractBattleship.create_ship_placement,function,"
        Place a ship on the grid.
","def create_ship_placement(self, game_id: str, placement: ShipPlacement) -> None:
        """"""
        Place a ship on the grid.
        """"""
        pass","Point(row=59, column=4)","Point(row=63, column=12)",AbstractBattleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/abstract_class.py
AbstractBattleship.create_turn,function,"
        Players take turns to target a grid cell.
","def create_turn(self, game_id: str, turn: Turn) -> TurnResponse:
        """"""
        Players take turns to target a grid cell.
        """"""
        pass","Point(row=66, column=4)","Point(row=70, column=12)",AbstractBattleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/abstract_class.py
AbstractBattleship.get_game_status,function,"
        Check if the game is over and get the winner if there's one.
","def get_game_status(self, game_id: str) -> GameStatus:
        """"""
        Check if the game is over and get the winner if there's one.
        """"""
        pass","Point(row=73, column=4)","Point(row=77, column=12)",AbstractBattleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/abstract_class.py
AbstractBattleship.get_winner,function,"
        Get the winner of the game.
","def get_winner(self, game_id: str) -> str:
        """"""
        Get the winner of the game.
        """"""
        pass","Point(row=80, column=4)","Point(row=84, column=12)",AbstractBattleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/abstract_class.py
AbstractBattleship.get_game,function,"
        Retrieve the state of the game.
","def get_game(self) -> Game | None:
        """"""
        Retrieve the state of the game.
        """"""
        pass","Point(row=87, column=4)","Point(row=91, column=12)",AbstractBattleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/abstract_class.py
AbstractBattleship.delete_game,function,"
        Delete a game given its ID.
","def delete_game(self, game_id: str) -> None:
        """"""
        Delete a game given its ID.
        """"""
        pass","Point(row=94, column=4)","Point(row=98, column=12)",AbstractBattleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/abstract_class.py
AbstractBattleship.create_game,function,"
        Create a new game.

        Returns:
            str: The ID of the created game.
","def create_game(self) -> None:
        """"""
        Create a new game.

        Returns:
            str: The ID of the created game.
        """"""
        pass","Point(row=101, column=4)","Point(row=108, column=12)",AbstractBattleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/abstract_class.py
test_ship_placement_out_of_bounds,function,,"def test_ship_placement_out_of_bounds(battleship_game):
    game_id = battleship_game.create_game()

    try:
        out_of_bounds_ship = ShipPlacement(
            ship_type=""battleship"",
            start={""row"": 11, ""column"": ""Z""},
            direction=""horizontal"",
        )
    except ValidationError:  # Use the directly imported ValidationError class
        pass
    else:
        with pytest.raises(ValueError, match=""Placement out of bounds""):
            battleship_game.create_ship_placement(game_id, out_of_bounds_ship)","Point(row=6, column=0)","Point(row=19, column=78)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_negative.py
test_no_ship_overlap,function,,"def test_no_ship_overlap(battleship_game):
    game_id = battleship_game.create_game()
    placement1 = ShipPlacement(
        ship_type=""battleship"", start={""row"": 1, ""column"": ""A""}, direction=""horizontal""
    )
    battleship_game.create_ship_placement(game_id, placement1)
    placement2 = ShipPlacement(
        ship_type=""cruiser"", start={""row"": 1, ""column"": ""A""}, direction=""horizontal""
    )
    with pytest.raises(ValueError):
        battleship_game.create_ship_placement(game_id, placement2)","Point(row=22, column=0)","Point(row=32, column=66)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_negative.py
test_cant_hit_before_ships_placed,function,,"def test_cant_hit_before_ships_placed(battleship_game):
    game_id = battleship_game.create_game()
    placement1 = ShipPlacement(
        ship_type=""battleship"", start={""row"": 1, ""column"": ""A""}, direction=""horizontal""
    )
    battleship_game.create_ship_placement(game_id, placement1)
    placement2 = ShipPlacement(
        ship_type=""cruiser"", start={""row"": 4, ""column"": ""D""}, direction=""horizontal""
    )
    battleship_game.create_ship_placement(game_id, placement2)
    turn = Turn(target={""row"": 1, ""column"": ""A""})
    with pytest.raises(
        ValueError, match=""All ships must be placed before starting turns""
    ):
        battleship_game.create_turn(game_id, turn)","Point(row=35, column=0)","Point(row=49, column=50)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_negative.py
test_cant_place_ship_after_all_ships_placed,function,,"def test_cant_place_ship_after_all_ships_placed(battleship_game, initialized_game_id):
    battleship_game.get_game(initialized_game_id)
    additional_ship = ShipPlacement(
        ship_type=""carrier"", start={""row"": 2, ""column"": ""E""}, direction=""horizontal""
    )

    with pytest.raises(
        ValueError, match=""All ships are already placed. Cannot place more ships.""
    ):
        battleship_game.create_ship_placement(initialized_game_id, additional_ship)","Point(row=52, column=0)","Point(row=61, column=83)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_negative.py
test_ship_placement_invalid_direction,function,,"def test_ship_placement_invalid_direction(battleship_game):
    game_id = battleship_game.create_game()

    with pytest.raises(ValueError, match=""Invalid ship direction""):
        invalid_direction_ship = ShipPlacement(
            ship_type=""battleship"",
            start={""row"": 1, ""column"": ""A""},
            direction=""diagonal"",
        )
        battleship_game.create_ship_placement(game_id, invalid_direction_ship)","Point(row=64, column=0)","Point(row=73, column=78)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_negative.py
test_invalid_ship_type,function,,"def test_invalid_ship_type(battleship_game):
    game_id = battleship_game.create_game()
    invalid_ship = ShipPlacement(
        ship_type=""spacecraft"", start={""row"": 1, ""column"": ""A""}, direction=""horizontal""
    )
    with pytest.raises(ValueError, match=""Invalid ship type""):
        battleship_game.create_ship_placement(game_id, invalid_ship)","Point(row=76, column=0)","Point(row=82, column=68)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_negative.py
test_ship_placement_extends_beyond_boundaries,function,,"def test_ship_placement_extends_beyond_boundaries(battleship_game):
    game_id = battleship_game.create_game()

    with pytest.raises(ValueError, match=""Ship extends beyond board boundaries""):
        ship_extending_beyond = ShipPlacement(
            ship_type=""battleship"",
            start={""row"": 1, ""column"": ""H""},
            direction=""horizontal"",
        )
        battleship_game.create_ship_placement(game_id, ship_extending_beyond)

    with pytest.raises(ValueError, match=""Ship extends beyond board boundaries""):
        ship_extending_beyond = ShipPlacement(
            ship_type=""cruiser"", start={""row"": 9, ""column"": ""A""}, direction=""vertical""
        )
        battleship_game.create_ship_placement(game_id, ship_extending_beyond)","Point(row=85, column=0)","Point(row=100, column=77)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_negative.py
battleship_game,function,,"def battleship_game():
    return Battleship()","Point(row=7, column=0)","Point(row=8, column=23)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/conftest.py
initialized_game_id,function,,"def initialized_game_id(battleship_game):
    # Create a game instance
    game_id = battleship_game.create_game()

    # Place all the ships using battleship_game's methods
    sample_ship_placements = [
        ShipPlacement(
            ship_type=""carrier"", start={""row"": 1, ""column"": ""A""}, direction=""horizontal""
        ),
        ShipPlacement(
            ship_type=""battleship"",
            start={""row"": 2, ""column"": ""A""},
            direction=""horizontal"",
        ),
        ShipPlacement(
            ship_type=""cruiser"", start={""row"": 3, ""column"": ""A""}, direction=""horizontal""
        ),
        ShipPlacement(
            ship_type=""submarine"",
            start={""row"": 4, ""column"": ""A""},
            direction=""horizontal"",
        ),
        ShipPlacement(
            ship_type=""destroyer"",
            start={""row"": 5, ""column"": ""A""},
            direction=""horizontal"",
        ),
    ]

    for ship_placement in sample_ship_placements:
        # Place ship using battleship_game's methods
        battleship_game.create_ship_placement(game_id, ship_placement)

    return game_id","Point(row=12, column=0)","Point(row=45, column=18)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/conftest.py
game_over_fixture,function,,"def game_over_fixture(battleship_game, initialized_game_id):
    # Assuming 10x10 grid, target all possible positions
    for row in range(1, 11):
        for column in list(""ABCDEFGHIJ""):
            # Player 1 takes a turn
            turn = Turn(target={""row"": row, ""column"": column})
            battleship_game.create_turn(initialized_game_id, turn)

            # Player 2 takes a turn, targeting the same position as Player 1
            battleship_game.create_turn(initialized_game_id, turn)

    # At the end of this fixture, the game should be over
    return initialized_game_id","Point(row=49, column=0)","Point(row=61, column=30)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/conftest.py
Battleship,class,,"class Battleship(AbstractBattleship):
    def __init__(self):
        self.games: Dict[str, Game] = {}

    def create_game(self) -> str:
        game_id = str(len(self.games))
        new_game = Game(
            game_id=game_id,
            players=[],
            board={},
            ships=[],
            turns=[],
        )

        self.games[game_id] = new_game
        return game_id

    def create_ship_placement(self, game_id: str, placement: ShipPlacement) -> None:
        game = self.games.get(game_id)

        if not game:
            raise ValueError(f""Game with ID {game_id} not found."")
        if placement.direction not in [""horizontal"", ""vertical""]:
            raise ValueError(""Invalid ship direction"")
        if self.all_ships_placed(game):
            raise ValueError(""All ships are already placed. Cannot place more ships."")

        ship_length = self.SHIP_LENGTHS.get(placement.ship_type)
        if not ship_length:
            raise ValueError(f""Invalid ship type {placement.ship_type}"")

        start_row, start_col = placement.start[""row""], ord(
            placement.start[""column""]
        ) - ord(""A"")

        if start_row < 1 or start_row > 10 or start_col < 0 or start_col > 9:
            raise ValueError(""Placement out of bounds"")

        if placement.direction == ""horizontal"" and start_col + ship_length > 10:
            raise ValueError(""Ship extends beyond board boundaries"")
        elif placement.direction == ""vertical"" and start_row + ship_length > 10:
            raise ValueError(""Ship extends beyond board boundaries"")

        for i in range(ship_length):
            if placement.direction == ""horizontal"":
                if game.board.get((start_row, start_col + i)):
                    raise ValueError(""Ship overlaps with another ship!"")
            elif placement.direction == ""vertical"":
                if game.board.get((start_row + i, start_col)):
                    raise ValueError(""Ship overlaps with another ship!"")

        for i in range(ship_length):
            if placement.direction == ""horizontal"":
                game.board[(start_row, start_col + i)] = placement.ship_type
            else:
                game.board[(start_row + i, start_col)] = placement.ship_type

        game.ships.append(placement)

    def create_turn(self, game_id: str, turn: Turn) -> TurnResponse:
        game = self.games.get(game_id)

        if not game:
            raise ValueError(f""Game with ID {game_id} not found."")

        if not self.all_ships_placed(game):
            raise ValueError(""All ships must be placed before starting turns"")

        target_row, target_col = turn.target[""row""], ord(turn.target[""column""]) - ord(
            ""A""
        )
        hit_ship = game.board.get((target_row, target_col))

        game.turns.append(turn)

        if not hit_ship or hit_ship == ""hit"":  # if no ship or already hit
            return TurnResponse(result=""miss"", ship_type=None)

        ship_placement = next(sp for sp in game.ships if sp.ship_type == hit_ship)
        start_row, start_col = (
            ship_placement.start[""row""],
            ord(ship_placement.start[""column""]) - ord(""A""),
        )
        ship_positions = [
            (
                start_row + (i if ship_placement.direction == ""vertical"" else 0),
                start_col + (i if ship_placement.direction == ""horizontal"" else 0),
            )
            for i in range(self.SHIP_LENGTHS[hit_ship])
        ]

        targeted_positions = {
            (t.target[""row""], ord(t.target[""column""]) - ord(""A"")) for t in game.turns
        }

        game.board[(target_row, target_col)] = ""hit""

        if set(ship_positions).issubset(targeted_positions):
            for pos in ship_positions:
                game.board[pos] = ""hit""
            return TurnResponse(result=""sunk"", ship_type=hit_ship)
        else:
            return TurnResponse(result=""hit"", ship_type=hit_ship)

    def get_game_status(self, game_id: str) -> GameStatus:
        game = self.games.get(game_id)

        if not game:
            raise ValueError(f""Game with ID {game_id} not found."")

        hits = sum(1 for _, status in game.board.items() if status == ""hit"")

        total_ships_length = sum(
            self.SHIP_LENGTHS[ship.ship_type] for ship in game.ships
        )

        if hits == total_ships_length:
            return GameStatus(is_game_over=True, winner=""player"")
        else:
            return GameStatus(is_game_over=False, winner=None)

    def get_winner(self, game_id: str) -> str:
        game_status = self.get_game_status(game_id)

        if game_status.is_game_over and game_status.winner:
            return game_status.winner
        else:
            raise ValueError(f""Game {game_id} isn't over yet"")

    def get_game(self, game_id: str) -> Game | None:
        return self.games.get(game_id)

    def delete_game(self, game_id: str) -> None:
        if game_id in self.games:
            del self.games[game_id]

    def all_ships_placed(self, game: Game) -> bool:
        placed_ship_types = set([placement.ship_type for placement in game.ships])
        return placed_ship_types == set(self.SHIP_LENGTHS.keys())","Point(row=12, column=0)","Point(row=150, column=65)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/battleship.py
Battleship.__init__,function,,"def __init__(self):
        self.games: Dict[str, Game] = {}","Point(row=13, column=4)","Point(row=14, column=40)",Battleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/battleship.py
Battleship.create_game,function,,"def create_game(self) -> str:
        game_id = str(len(self.games))
        new_game = Game(
            game_id=game_id,
            players=[],
            board={},
            ships=[],
            turns=[],
        )

        self.games[game_id] = new_game
        return game_id","Point(row=16, column=4)","Point(row=27, column=22)",Battleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/battleship.py
Battleship.create_ship_placement,function,,"def create_ship_placement(self, game_id: str, placement: ShipPlacement) -> None:
        game = self.games.get(game_id)

        if not game:
            raise ValueError(f""Game with ID {game_id} not found."")
        if placement.direction not in [""horizontal"", ""vertical""]:
            raise ValueError(""Invalid ship direction"")
        if self.all_ships_placed(game):
            raise ValueError(""All ships are already placed. Cannot place more ships."")

        ship_length = self.SHIP_LENGTHS.get(placement.ship_type)
        if not ship_length:
            raise ValueError(f""Invalid ship type {placement.ship_type}"")

        start_row, start_col = placement.start[""row""], ord(
            placement.start[""column""]
        ) - ord(""A"")

        if start_row < 1 or start_row > 10 or start_col < 0 or start_col > 9:
            raise ValueError(""Placement out of bounds"")

        if placement.direction == ""horizontal"" and start_col + ship_length > 10:
            raise ValueError(""Ship extends beyond board boundaries"")
        elif placement.direction == ""vertical"" and start_row + ship_length > 10:
            raise ValueError(""Ship extends beyond board boundaries"")

        for i in range(ship_length):
            if placement.direction == ""horizontal"":
                if game.board.get((start_row, start_col + i)):
                    raise ValueError(""Ship overlaps with another ship!"")
            elif placement.direction == ""vertical"":
                if game.board.get((start_row + i, start_col)):
                    raise ValueError(""Ship overlaps with another ship!"")

        for i in range(ship_length):
            if placement.direction == ""horizontal"":
                game.board[(start_row, start_col + i)] = placement.ship_type
            else:
                game.board[(start_row + i, start_col)] = placement.ship_type

        game.ships.append(placement)","Point(row=29, column=4)","Point(row=69, column=36)",Battleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/battleship.py
Battleship.create_turn,function,,"def create_turn(self, game_id: str, turn: Turn) -> TurnResponse:
        game = self.games.get(game_id)

        if not game:
            raise ValueError(f""Game with ID {game_id} not found."")

        if not self.all_ships_placed(game):
            raise ValueError(""All ships must be placed before starting turns"")

        target_row, target_col = turn.target[""row""], ord(turn.target[""column""]) - ord(
            ""A""
        )
        hit_ship = game.board.get((target_row, target_col))

        game.turns.append(turn)

        if not hit_ship or hit_ship == ""hit"":  # if no ship or already hit
            return TurnResponse(result=""miss"", ship_type=None)

        ship_placement = next(sp for sp in game.ships if sp.ship_type == hit_ship)
        start_row, start_col = (
            ship_placement.start[""row""],
            ord(ship_placement.start[""column""]) - ord(""A""),
        )
        ship_positions = [
            (
                start_row + (i if ship_placement.direction == ""vertical"" else 0),
                start_col + (i if ship_placement.direction == ""horizontal"" else 0),
            )
            for i in range(self.SHIP_LENGTHS[hit_ship])
        ]

        targeted_positions = {
            (t.target[""row""], ord(t.target[""column""]) - ord(""A"")) for t in game.turns
        }

        game.board[(target_row, target_col)] = ""hit""

        if set(ship_positions).issubset(targeted_positions):
            for pos in ship_positions:
                game.board[pos] = ""hit""
            return TurnResponse(result=""sunk"", ship_type=hit_ship)
        else:
            return TurnResponse(result=""hit"", ship_type=hit_ship)","Point(row=71, column=4)","Point(row=114, column=65)",Battleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/battleship.py
Battleship.get_game_status,function,,"def get_game_status(self, game_id: str) -> GameStatus:
        game = self.games.get(game_id)

        if not game:
            raise ValueError(f""Game with ID {game_id} not found."")

        hits = sum(1 for _, status in game.board.items() if status == ""hit"")

        total_ships_length = sum(
            self.SHIP_LENGTHS[ship.ship_type] for ship in game.ships
        )

        if hits == total_ships_length:
            return GameStatus(is_game_over=True, winner=""player"")
        else:
            return GameStatus(is_game_over=False, winner=None)","Point(row=116, column=4)","Point(row=131, column=62)",Battleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/battleship.py
Battleship.get_winner,function,,"def get_winner(self, game_id: str) -> str:
        game_status = self.get_game_status(game_id)

        if game_status.is_game_over and game_status.winner:
            return game_status.winner
        else:
            raise ValueError(f""Game {game_id} isn't over yet"")","Point(row=133, column=4)","Point(row=139, column=62)",Battleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/battleship.py
Battleship.get_game,function,,"def get_game(self, game_id: str) -> Game | None:
        return self.games.get(game_id)","Point(row=141, column=4)","Point(row=142, column=38)",Battleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/battleship.py
Battleship.delete_game,function,,"def delete_game(self, game_id: str) -> None:
        if game_id in self.games:
            del self.games[game_id]","Point(row=144, column=4)","Point(row=146, column=35)",Battleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/battleship.py
Battleship.all_ships_placed,function,,"def all_ships_placed(self, game: Game) -> bool:
        placed_ship_types = set([placement.ship_type for placement in game.ships])
        return placed_ship_types == set(self.SHIP_LENGTHS.keys())","Point(row=148, column=4)","Point(row=150, column=65)",Battleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/battleship.py
test_turns_and_results,function,,"def test_turns_and_results(battleship_game, initialized_game_id):
    turn = Turn(target={""row"": 1, ""column"": ""A""})
    response = battleship_game.create_turn(initialized_game_id, turn)

    assert response.result in [""hit"", ""miss""]
    if response.result == ""hit"":
        assert response.ship_type == ""carrier""
    game = battleship_game.get_game(initialized_game_id)
    assert turn in game.turns","Point(row=3, column=0)","Point(row=11, column=29)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_positive.py
test_game_status_and_winner,function,,"def test_game_status_and_winner(battleship_game):
    game_id = battleship_game.create_game()
    status = battleship_game.get_game_status(game_id)
    assert isinstance(status.is_game_over, bool)
    if status.is_game_over:
        winner = battleship_game.get_winner(game_id)
        assert winner is not None","Point(row=14, column=0)","Point(row=20, column=33)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_positive.py
test_delete_game,function,,"def test_delete_game(battleship_game):
    game_id = battleship_game.create_game()
    battleship_game.delete_game(game_id)
    assert battleship_game.get_game(game_id) is None","Point(row=23, column=0)","Point(row=26, column=52)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_positive.py
test_ship_rotation,function,,"def test_ship_rotation(battleship_game):
    game_id = battleship_game.create_game()
    placement_horizontal = ShipPlacement(
        ship_type=""battleship"", start={""row"": 1, ""column"": ""B""}, direction=""horizontal""
    )
    battleship_game.create_ship_placement(game_id, placement_horizontal)
    placement_vertical = ShipPlacement(
        ship_type=""cruiser"", start={""row"": 3, ""column"": ""D""}, direction=""vertical""
    )
    battleship_game.create_ship_placement(game_id, placement_vertical)
    game = battleship_game.get_game(game_id)
    assert placement_horizontal in game.ships
    assert placement_vertical in game.ships","Point(row=29, column=0)","Point(row=41, column=43)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_positive.py
test_game_state_updates,function,,"def test_game_state_updates(battleship_game, initialized_game_id):
    turn = Turn(target={""row"": 3, ""column"": ""A""})
    battleship_game.create_turn(initialized_game_id, turn)

    game = battleship_game.get_game(initialized_game_id)

    target_key = (3, ord(""A"") - ord(""A""))
    assert target_key in game.board and game.board[target_key] == ""hit""","Point(row=44, column=0)","Point(row=51, column=71)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_positive.py
test_ship_sinking_feedback,function,,"def test_ship_sinking_feedback(battleship_game, initialized_game_id):
    hits = [""A"", ""B"", ""C"", ""D""]
    static_moves = [
        {""row"": 1, ""column"": ""E""},
        {""row"": 1, ""column"": ""F""},
        {""row"": 1, ""column"": ""G""},
        {""row"": 1, ""column"": ""H""},
    ]

    response = None
    for index, hit in enumerate(hits):
        turn = Turn(target={""row"": 2, ""column"": hit})
        response = battleship_game.create_turn(initialized_game_id, turn)
        assert response.ship_type == ""battleship""

        static_turn = Turn(target=static_moves[index])
        battleship_game.create_turn(initialized_game_id, static_turn)

    assert response and response.result == ""sunk""","Point(row=54, column=0)","Point(row=72, column=49)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_positive.py
test_restart_game,function,,"def test_restart_game(battleship_game):
    game_id = battleship_game.create_game()
    battleship_game.delete_game(game_id)
    game_id = (
        battleship_game.create_game()
    )  # Use the returned game_id after recreating the game
    game = battleship_game.get_game(game_id)
    assert game is not None","Point(row=75, column=0)","Point(row=82, column=27)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_positive.py
test_ship_edge_overlapping,function,,"def test_ship_edge_overlapping(battleship_game):
    game_id = battleship_game.create_game()

    first_ship = ShipPlacement(
        ship_type=""battleship"", start={""row"": 1, ""column"": ""A""}, direction=""horizontal""
    )
    battleship_game.create_ship_placement(game_id, first_ship)

    next_ship = ShipPlacement(
        ship_type=""cruiser"", start={""row"": 1, ""column"": ""E""}, direction=""horizontal""
    )
    battleship_game.create_ship_placement(game_id, next_ship)

    game = battleship_game.get_game(game_id)
    assert first_ship in game.ships
    assert next_ship in game.ships","Point(row=85, column=0)","Point(row=100, column=34)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_positive.py
test_game_state_after_ship_placement,function,,"def test_game_state_after_ship_placement(battleship_game):
    game_id = battleship_game.create_game()

    ship_placement = ShipPlacement(
        ship_type=""battleship"", start={""row"": 1, ""column"": ""A""}, direction=""horizontal""
    )
    battleship_game.create_ship_placement(game_id, ship_placement)

    game = battleship_game.get_game(game_id)
    assert ship_placement in game.ships","Point(row=103, column=0)","Point(row=112, column=39)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_positive.py
test_game_state_after_turn,function,,"def test_game_state_after_turn(initialized_game_id, battleship_game):
    turn = Turn(target={""row"": 1, ""column"": ""A""})
    response = battleship_game.create_turn(initialized_game_id, turn)

    game = battleship_game.get_game(initialized_game_id)

    if response.result == ""hit"":
        assert game.board[(1, 0)] == ""hit""
    else:
        assert game.board[1][0] == ""miss""","Point(row=115, column=0)","Point(row=124, column=41)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_positive.py
test_multiple_hits_on_ship,function,,"def test_multiple_hits_on_ship(battleship_game, initialized_game_id):
    hit_positions = [""A"", ""B"", ""C"", ""D"", ""E""]

    for index, pos in enumerate(hit_positions):
        turn = Turn(target={""row"": 1, ""column"": pos})
        response = battleship_game.create_turn(initialized_game_id, turn)

        if index == len(hit_positions) - 1:
            assert response.result == ""sunk""
        else:
            assert response.result == ""hit""","Point(row=127, column=0)","Point(row=137, column=43)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_positive.py
test_game_over_condition,function,,"def test_game_over_condition(battleship_game, initialized_game_id):
    for row in range(1, 11):
        for column in list(""ABCDEFGHIJ""):
            turn = Turn(target={""row"": row, ""column"": column})
            battleship_game.create_turn(initialized_game_id, turn)

            battleship_game.create_turn(initialized_game_id, turn)

    status = battleship_game.get_game_status(initialized_game_id)
    assert status.is_game_over","Point(row=140, column=0)","Point(row=149, column=30)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_positive.py
ShipPlacement,class,,"class ShipPlacement(BaseModel):
    ship_type: str
    start: dict  # {""row"": int, ""column"": str}
    direction: str

    @field_validator(""start"")
    def validate_start(cls, start):
        row, column = start.get(""row""), start.get(""column"")

        if not (1 <= row <= 10):
            raise ValueError(""Row must be between 1 and 10 inclusive."")

        if column not in list(""ABCDEFGHIJ""):
            raise ValueError(""Column must be one of A, B, C, D, E, F, G, H, I, J."")

        return start","Point(row=7, column=0)","Point(row=22, column=20)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/abstract_class.py
ShipPlacement.validate_start,function,,"def validate_start(cls, start):
        row, column = start.get(""row""), start.get(""column"")

        if not (1 <= row <= 10):
            raise ValueError(""Row must be between 1 and 10 inclusive."")

        if column not in list(""ABCDEFGHIJ""):
            raise ValueError(""Column must be one of A, B, C, D, E, F, G, H, I, J."")

        return start","Point(row=13, column=4)","Point(row=22, column=20)",ShipPlacement,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/abstract_class.py
Turn,class,,"class Turn(BaseModel):
    target: dict  # {""row"": int, ""column"": str}","Point(row=25, column=0)","Point(row=26, column=47)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/abstract_class.py
TurnResponse,class,,"class TurnResponse(BaseModel):
    result: str
    ship_type: Optional[str]  # This would be None if the result is a miss","Point(row=29, column=0)","Point(row=31, column=74)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/abstract_class.py
GameStatus,class,,"class GameStatus(BaseModel):
    is_game_over: bool
    winner: Optional[str]","Point(row=34, column=0)","Point(row=36, column=25)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/abstract_class.py
Game,class,,"class Game(BaseModel):
    game_id: str
    players: list[str]
    # This could represent the state of the game board,
    # you might need to flesh this out further:
    board: dict
    ships: list[ShipPlacement]  # List of ship placements for this game
    turns: list[Turn]  # List of turns that have been taken","Point(row=39, column=0)","Point(row=46, column=59)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/abstract_class.py
AbstractBattleship,class,,"class AbstractBattleship(ABC):
    SHIP_LENGTHS = {
        ""carrier"": 5,
        ""battleship"": 4,
        ""cruiser"": 3,
        ""submarine"": 3,
        ""destroyer"": 2,
    }

    @abstractmethod
    def create_ship_placement(self, game_id: str, placement: ShipPlacement) -> None:
        """"""
        Place a ship on the grid.
        """"""
        pass

    @abstractmethod
    def create_turn(self, game_id: str, turn: Turn) -> TurnResponse:
        """"""
        Players take turns to target a grid cell.
        """"""
        pass

    @abstractmethod
    def get_game_status(self, game_id: str) -> GameStatus:
        """"""
        Check if the game is over and get the winner if there's one.
        """"""
        pass

    @abstractmethod
    def get_winner(self, game_id: str) -> str:
        """"""
        Get the winner of the game.
        """"""
        pass

    @abstractmethod
    def get_game(self, game_id: str) -> Game | None:
        """"""
        Retrieve the state of the game.
        """"""
        pass

    @abstractmethod
    def delete_game(self, game_id: str) -> None:
        """"""
        Delete a game given its ID.
        """"""
        pass

    @abstractmethod
    def create_game(self) -> str:
        """"""
        Create a new game.

        Returns:
            str: The ID of the created game.
        """"""
        pass","Point(row=49, column=0)","Point(row=108, column=12)",,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/abstract_class.py
AbstractBattleship.create_ship_placement,function,"
        Place a ship on the grid.
","def create_ship_placement(self, game_id: str, placement: ShipPlacement) -> None:
        """"""
        Place a ship on the grid.
        """"""
        pass","Point(row=59, column=4)","Point(row=63, column=12)",AbstractBattleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/abstract_class.py
AbstractBattleship.create_turn,function,"
        Players take turns to target a grid cell.
","def create_turn(self, game_id: str, turn: Turn) -> TurnResponse:
        """"""
        Players take turns to target a grid cell.
        """"""
        pass","Point(row=66, column=4)","Point(row=70, column=12)",AbstractBattleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/abstract_class.py
AbstractBattleship.get_game_status,function,"
        Check if the game is over and get the winner if there's one.
","def get_game_status(self, game_id: str) -> GameStatus:
        """"""
        Check if the game is over and get the winner if there's one.
        """"""
        pass","Point(row=73, column=4)","Point(row=77, column=12)",AbstractBattleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/abstract_class.py
AbstractBattleship.get_winner,function,"
        Get the winner of the game.
","def get_winner(self, game_id: str) -> str:
        """"""
        Get the winner of the game.
        """"""
        pass","Point(row=80, column=4)","Point(row=84, column=12)",AbstractBattleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/abstract_class.py
AbstractBattleship.get_game,function,"
        Retrieve the state of the game.
","def get_game(self, game_id: str) -> Game | None:
        """"""
        Retrieve the state of the game.
        """"""
        pass","Point(row=87, column=4)","Point(row=91, column=12)",AbstractBattleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/abstract_class.py
AbstractBattleship.delete_game,function,"
        Delete a game given its ID.
","def delete_game(self, game_id: str) -> None:
        """"""
        Delete a game given its ID.
        """"""
        pass","Point(row=94, column=4)","Point(row=98, column=12)",AbstractBattleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/abstract_class.py
AbstractBattleship.create_game,function,"
        Create a new game.

        Returns:
            str: The ID of the created game.
","def create_game(self) -> str:
        """"""
        Create a new game.

        Returns:
            str: The ID of the created game.
        """"""
        pass","Point(row=101, column=4)","Point(row=108, column=12)",AbstractBattleship,classic/benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/abstract_class.py
shorten_url,function,,"def shorten_url(url):
    # Convert the URL to base64
    encoded_url = base64.b64encode(url.encode()).decode()
    # Take the first 8 characters of the encoded URL as our shortened URL
    short_url = encoded_url[:8]
    # Map the shortened URL back to the original
    URL_MAPPING[short_url] = url
    return short_url","Point(row=6, column=0)","Point(row=13, column=20)",,classic/benchmark/agbenchmark/challenges/verticals/code/4_url_shortener/artifacts_out/url_shortener.py
retrieve_url,function,,"def retrieve_url(short_url):
    return URL_MAPPING.get(short_url, ""URL not found"")","Point(row=16, column=0)","Point(row=17, column=54)",,classic/benchmark/agbenchmark/challenges/verticals/code/4_url_shortener/artifacts_out/url_shortener.py
main,function,,"def main():
    parser = argparse.ArgumentParser(description=""URL Shortener"")
    parser.add_argument(""-s"", ""--shorten"", type=str, help=""URL to be shortened"")
    parser.add_argument(""-r"", ""--retrieve"", type=str, help=""Short URL to be retrieved"")

    args = parser.parse_args()

    if args.shorten:
        shortened_url = shorten_url(args.shorten)
        print(shortened_url)
        # Directly retrieve after shortening, using the newly shortened URL
        print(retrieve_url(shortened_url))
    elif args.retrieve:
        print(retrieve_url(args.retrieve))
    else:
        print(""No valid arguments provided."")","Point(row=20, column=0)","Point(row=35, column=45)",,classic/benchmark/agbenchmark/challenges/verticals/code/4_url_shortener/artifacts_out/url_shortener.py
TestURLShortener,class,,"class TestURLShortener(unittest.TestCase):
    def test_url_retrieval(self):
        # Shorten the URL to get its shortened form
        shortened_url = shorten_url(""https://www.example.com"")

        # Retrieve the original URL using the shortened URL directly
        retrieved_url = retrieve_url(shortened_url)

        self.assertEqual(
            retrieved_url,
            ""https://www.example.com"",
            ""Retrieved URL does not match the original!"",
        )","Point(row=5, column=0)","Point(row=17, column=9)",,classic/benchmark/agbenchmark/challenges/verticals/code/4_url_shortener/artifacts_out/test.py
TestURLShortener.test_url_retrieval,function,,"def test_url_retrieval(self):
        # Shorten the URL to get its shortened form
        shortened_url = shorten_url(""https://www.example.com"")

        # Retrieve the original URL using the shortened URL directly
        retrieved_url = retrieve_url(shortened_url)

        self.assertEqual(
            retrieved_url,
            ""https://www.example.com"",
            ""Retrieved URL does not match the original!"",
        )","Point(row=6, column=4)","Point(row=17, column=9)",TestURLShortener,classic/benchmark/agbenchmark/challenges/verticals/code/4_url_shortener/artifacts_out/test.py
TestURLShortener,class,,"class TestURLShortener(unittest.TestCase):
    def test_url_retrieval(self):
        # Shorten the URL to get its shortened form
        shortened_url = shorten_url(""https://www.example.com"")

        # Retrieve the original URL using the shortened URL directly
        retrieved_url = retrieve_url(shortened_url)

        self.assertEqual(
            retrieved_url,
            ""https://www.example.com"",
            ""Retrieved URL does not match the original!"",
        )","Point(row=6, column=0)","Point(row=18, column=9)",,classic/benchmark/agbenchmark/challenges/verticals/code/4_url_shortener/custom_python/test.py
TestURLShortener.test_url_retrieval,function,,"def test_url_retrieval(self):
        # Shorten the URL to get its shortened form
        shortened_url = shorten_url(""https://www.example.com"")

        # Retrieve the original URL using the shortened URL directly
        retrieved_url = retrieve_url(shortened_url)

        self.assertEqual(
            retrieved_url,
            ""https://www.example.com"",
            ""Retrieved URL does not match the original!"",
        )","Point(row=7, column=4)","Point(row=18, column=9)",TestURLShortener,classic/benchmark/agbenchmark/challenges/verticals/code/4_url_shortener/custom_python/test.py
configure_logging,function,Configure the native logging module.,"def configure_logging(
    level: int = logging.INFO,
) -> None:
    """"""Configure the native logging module.""""""

    # Auto-adjust default log format based on log level
    log_format = DEBUG_LOG_FORMAT if level == logging.DEBUG else SIMPLE_LOG_FORMAT

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(FancyConsoleFormatter(log_format))

    # Configure the root logger
    logging.basicConfig(
        level=level,
        format=log_format,
        handlers=[console_handler],
    )","Point(row=10, column=0)","Point(row=26, column=5)",,classic/benchmark/agbenchmark/utils/logging.py
FancyConsoleFormatter,class,"
    A custom logging formatter designed for console output.

    This formatter enhances the standard logging output with color coding. The color
    coding is based on the level of the log message, making it easier to distinguish
    between different types of messages in the console output.

    The color for each level is defined in the LEVEL_COLOR_MAP class attribute.
","class FancyConsoleFormatter(logging.Formatter):
    """"""
    A custom logging formatter designed for console output.

    This formatter enhances the standard logging output with color coding. The color
    coding is based on the level of the log message, making it easier to distinguish
    between different types of messages in the console output.

    The color for each level is defined in the LEVEL_COLOR_MAP class attribute.
    """"""

    # level -> (level & text color, title color)
    LEVEL_COLOR_MAP = {
        logging.DEBUG: Fore.LIGHTBLACK_EX,
        logging.INFO: Fore.BLUE,
        logging.WARNING: Fore.YELLOW,
        logging.ERROR: Fore.RED,
        logging.CRITICAL: Fore.RED + Style.BRIGHT,
    }

    def format(self, record: logging.LogRecord) -> str:
        # Make sure `msg` is a string
        if not hasattr(record, ""msg""):
            record.msg = """"
        elif not type(record.msg) is str:
            record.msg = str(record.msg)

        # Justify the level name to 5 characters minimum
        record.levelname = record.levelname.ljust(5)

        # Determine default color based on error level
        level_color = """"
        if record.levelno in self.LEVEL_COLOR_MAP:
            level_color = self.LEVEL_COLOR_MAP[record.levelno]
            record.levelname = f""{level_color}{record.levelname}{Style.RESET_ALL}""

        # Determine color for message
        color = getattr(record, ""color"", level_color)
        color_is_specified = hasattr(record, ""color"")

        # Don't color INFO messages unless the color is explicitly specified.
        if color and (record.levelno != logging.INFO or color_is_specified):
            record.msg = f""{color}{record.msg}{Style.RESET_ALL}""

        return super().format(record)","Point(row=29, column=0)","Point(row=73, column=37)",,classic/benchmark/agbenchmark/utils/logging.py
FancyConsoleFormatter.format,function,,"def format(self, record: logging.LogRecord) -> str:
        # Make sure `msg` is a string
        if not hasattr(record, ""msg""):
            record.msg = """"
        elif not type(record.msg) is str:
            record.msg = str(record.msg)

        # Justify the level name to 5 characters minimum
        record.levelname = record.levelname.ljust(5)

        # Determine default color based on error level
        level_color = """"
        if record.levelno in self.LEVEL_COLOR_MAP:
            level_color = self.LEVEL_COLOR_MAP[record.levelno]
            record.levelname = f""{level_color}{record.levelname}{Style.RESET_ALL}""

        # Determine color for message
        color = getattr(record, ""color"", level_color)
        color_is_specified = hasattr(record, ""color"")

        # Don't color INFO messages unless the color is explicitly specified.
        if color and (record.levelno != logging.INFO or color_is_specified):
            record.msg = f""{color}{record.msg}{Style.RESET_ALL}""

        return super().format(record)","Point(row=49, column=4)","Point(row=73, column=37)",FancyConsoleFormatter,classic/benchmark/agbenchmark/utils/logging.py
get_data_from_helicone,function,,"def get_data_from_helicone(challenge: str) -> Optional[float]:
    # Define the endpoint of your GraphQL server
    url = ""https://www.helicone.ai/api/graphql""

    # Set the headers, usually you'd need to set the content type
    # and possibly an authorization token
    headers = {""authorization"": f""Bearer {os.environ.get('HELICONE_API_KEY')}""}

    # Define the query, variables, and operation name
    query = """"""
query ExampleQuery($properties: [PropertyFilter!]){
  aggregatedHeliconeRequest(properties: $properties) {
    costUSD
  }
}
""""""

    variables = {
        ""properties"": [
            {
                ""value"": {""equals"": os.environ.get(""AGENT_NAME"")},
                ""name"": ""agent"",
            },
            {
                ""value"": {""equals"": BENCHMARK_START_TIME},
                ""name"": ""benchmark_start_time"",
            },
            {""value"": {""equals"": challenge}, ""name"": ""challenge""},
        ]
    }
    if HELICONE_GRAPHQL_LOGS:
        logger.debug(f""Executing Helicone query:\n{query.strip()}"")
        logger.debug(f""Query variables:\n{json.dumps(variables, indent=4)}"")

    operation_name = ""ExampleQuery""

    data = {}
    response = None

    try:
        response = requests.post(
            url,
            headers=headers,
            json={
                ""query"": query,
                ""variables"": variables,
                ""operationName"": operation_name,
            },
        )

        data = response.json()
    except requests.HTTPError as http_err:
        logger.error(f""Helicone returned an HTTP error: {http_err}"")
        return None
    except json.JSONDecodeError:
        raw_response = response.text  # type: ignore
        logger.error(
            f""Helicone returned an invalid JSON response: '''{raw_response}'''""
        )
        return None
    except Exception as err:
        logger.error(f""Error while trying to get data from Helicone: {err}"")
        return None

    if data is None or data.get(""data"") is None:
        logger.error(""Invalid response received from Helicone: no data"")
        logger.error(f""Offending response: {response}"")
        return None
    return (
        data.get(""data"", {}).get(""aggregatedHeliconeRequest"", {}).get(""costUSD"", None)
    )","Point(row=13, column=0)","Point(row=83, column=5)",,classic/benchmark/agbenchmark/utils/get_data_from_helicone.py
DifficultyLevel,class,,"class DifficultyLevel(Enum):
    interface = ""interface""
    basic = ""basic""
    novice = ""novice""
    intermediate = ""intermediate""
    advanced = ""advanced""
    expert = ""expert""
    human = ""human""","Point(row=6, column=0)","Point(row=13, column=19)",,classic/benchmark/agbenchmark/utils/data_types.py
Category,class,,"class Category(str, Enum):
    GENERALIST = ""general""
    DATA = ""data""
    CODING = ""coding""
    SCRAPE_SYNTHESIZE = ""scrape_synthesize""
    WEB = ""web""
    GAIA_1 = ""GAIA_1""
    GAIA_2 = ""GAIA_2""
    GAIA_3 = ""GAIA_3""","Point(row=30, column=0)","Point(row=38, column=21)",,classic/benchmark/agbenchmark/utils/data_types.py
EvalResult,class,,"class EvalResult(BaseModel):
    result: str
    result_source: Literal[""step_output""] | str
    score: float
    passed: bool","Point(row=41, column=0)","Point(row=45, column=16)",,classic/benchmark/agbenchmark/utils/data_types.py
replace_backslash,function,,"def replace_backslash(value: Any) -> Any:
    if isinstance(value, str):
        return re.sub(
            r""\\+"", ""/"", value
        )  # replace one or more backslashes with a forward slash
    elif isinstance(value, list):
        return [replace_backslash(i) for i in value]
    elif isinstance(value, dict):
        return {k: replace_backslash(v) for k, v in value.items()}
    else:
        return value","Point(row=26, column=0)","Point(row=36, column=20)",,classic/benchmark/agbenchmark/utils/utils.py
get_test_path,function,,"def get_test_path(json_file: str | Path) -> str:
    if isinstance(json_file, str):
        json_file = Path(json_file)

    # Find the index of ""agbenchmark"" in the path parts
    try:
        agbenchmark_index = json_file.parts.index(""benchmark"")
    except ValueError:
        raise ValueError(""Invalid challenge location."")

    # Create the path from ""agbenchmark"" onwards
    challenge_location = Path(*json_file.parts[agbenchmark_index:])

    formatted_location = replace_backslash(str(challenge_location))
    if isinstance(formatted_location, str):
        return formatted_location
    else:
        return str(challenge_location)","Point(row=39, column=0)","Point(row=56, column=38)",,classic/benchmark/agbenchmark/utils/utils.py
get_highest_success_difficulty,function,,"def get_highest_success_difficulty(
    data: dict[str, Test], just_string: Optional[bool] = None
) -> str:
    highest_difficulty = None
    highest_difficulty_level = 0

    for test_name, test_data in data.items():
        try:
            if any(r.success for r in test_data.results):
                difficulty_str = test_data.difficulty
                if not difficulty_str:
                    continue

                try:
                    difficulty_enum = DifficultyLevel[difficulty_str.lower()]
                    difficulty_level = DIFFICULTY_MAP[difficulty_enum]

                    if difficulty_level > highest_difficulty_level:
                        highest_difficulty = difficulty_enum
                        highest_difficulty_level = difficulty_level
                except KeyError:
                    logger.warning(
                        f""Unexpected difficulty level '{difficulty_str}' ""
                        f""in test '{test_name}'""
                    )
                    continue
        except Exception as e:
            logger.warning(
                ""An unexpected error [1] occurred while analyzing report [2].""
                ""Please notify a maintainer.\n""
                f""Report data [1]: {data}\n""
                f""Error [2]: {e}""
            )
            logger.warning(
                ""Make sure you selected the right test, no reports were generated.""
            )
            break

    if highest_difficulty is not None:
        highest_difficulty_str = highest_difficulty.name  # convert enum to string
    else:
        highest_difficulty_str = """"

    if highest_difficulty_level and not just_string:
        return f""{highest_difficulty_str}: {highest_difficulty_level}""
    elif highest_difficulty_str:
        return highest_difficulty_str
    return ""No successful tests""","Point(row=59, column=0)","Point(row=106, column=32)",,classic/benchmark/agbenchmark/utils/utils.py
write_pretty_json,function,,"def write_pretty_json(data, json_file):
    sorted_data = deep_sort(data)
    json_graph = json.dumps(sorted_data, indent=4)
    with open(json_file, ""w"") as f:
        f.write(json_graph)
        f.write(""\n"")","Point(row=124, column=0)","Point(row=129, column=21)",,classic/benchmark/agbenchmark/utils/utils.py
pretty_print_model,function,,"def pretty_print_model(model: BaseModel, include_header: bool = True) -> None:
    indent = """"
    if include_header:
        # Try to find the ID and/or name attribute of the model
        id, name = None, None
        for attr, value in model.model_dump().items():
            if attr == ""id"" or attr.endswith(""_id""):
                id = value
            if attr.endswith(""name""):
                name = value
            if id and name:
                break
        identifiers = [v for v in [name, id] if v]
        click.echo(
            f""{model.__repr_name__()}{repr(identifiers) if identifiers else ''}:""
        )
        indent = "" "" * 2

    k_col_width = max(len(k) for k in model.model_dump().keys())
    for k, v in model.model_dump().items():
        v_fmt = repr(v)
        if v is None or v == """":
            v_fmt = click.style(v_fmt, fg=""black"")
        elif type(v) is bool:
            v_fmt = click.style(v_fmt, fg=""green"" if v else ""red"")
        elif type(v) is str and ""\n"" in v:
            v_fmt = f""\n{v}"".replace(
                ""\n"", f""\n{indent} {click.style('|', fg='black')} ""
            )
        if isinstance(v, Enum):
            v_fmt = click.style(v.value, fg=""blue"")
        elif type(v) is list and len(v) > 0 and isinstance(v[0], Enum):
            v_fmt = "", "".join(click.style(lv.value, fg=""blue"") for lv in v)
        click.echo(f""{indent}{k: <{k_col_width}}  = {v_fmt}"")","Point(row=132, column=0)","Point(row=165, column=61)",,classic/benchmark/agbenchmark/utils/utils.py
deep_sort,function,"
    Recursively sort the keys in JSON object
","def deep_sort(obj):
    """"""
    Recursively sort the keys in JSON object
    """"""
    if isinstance(obj, dict):
        return {k: deep_sort(v) for k, v in sorted(obj.items())}
    if isinstance(obj, list):
        return [deep_sort(elem) for elem in obj]
    return obj","Point(row=168, column=0)","Point(row=176, column=14)",,classic/benchmark/agbenchmark/utils/utils.py
sorted_by_enum_index,function,,"def sorted_by_enum_index(
    sortable: Iterable[E],
    enum: type[E],
    *,
    reverse: bool = False,
) -> list[E]:
    ...","Point(row=180, column=0)","Point(row=186, column=7)",,classic/benchmark/agbenchmark/utils/utils.py
sorted_by_enum_index,function,,"def sorted_by_enum_index(
    sortable: Iterable[T],
    enum: type[Enum],
    *,
    key: Callable[[T], Enum | None],
    reverse: bool = False,
) -> list[T]:
    ...","Point(row=190, column=0)","Point(row=197, column=7)",,classic/benchmark/agbenchmark/utils/utils.py
sorted_by_enum_index,function,,"def sorted_by_enum_index(
    sortable: Iterable[T],
    enum: type[Enum],
    *,
    key: Optional[Callable[[T], Enum | None]] = None,
    reverse: bool = False,
) -> list[T]:
    return sorted(
        sortable,
        key=lambda x: (
            enum._member_names_.index(e.name)  # type: ignore
            if (e := key(x) if key else x)
            else 420e3
        ),
        reverse=reverse,
    )","Point(row=200, column=0)","Point(row=215, column=5)",,classic/benchmark/agbenchmark/utils/utils.py
clean_nodeid,function,"
    Remove any superfluous ::() from a node id.

    >>> clean_nodeid('test_file.py::TestClass::()::test')
    'test_file.py::TestClass::test'
    >>> clean_nodeid('test_file.py::TestClass::test')
    'test_file.py::TestClass::test'
    >>> clean_nodeid('test_file.py::test')
    'test_file.py::test'
","def clean_nodeid(nodeid: str) -> str:
    """"""
    Remove any superfluous ::() from a node id.

    >>> clean_nodeid('test_file.py::TestClass::()::test')
    'test_file.py::TestClass::test'
    >>> clean_nodeid('test_file.py::TestClass::test')
    'test_file.py::TestClass::test'
    >>> clean_nodeid('test_file.py::test')
    'test_file.py::test'
    """"""
    return nodeid.replace(""::()::"", ""::"")","Point(row=12, column=0)","Point(row=23, column=41)",,classic/benchmark/agbenchmark/utils/dependencies/util.py
strip_nodeid_parameters,function,"
    Strip parameters from a node id.

    >>> strip_nodeid_parameters('test_file.py::TestClass::test[foo]')
    'test_file.py::TestClass::test'
    >>> strip_nodeid_parameters('test_file.py::TestClass::test')
    'test_file.py::TestClass::test'
","def strip_nodeid_parameters(nodeid: str) -> str:
    """"""
    Strip parameters from a node id.

    >>> strip_nodeid_parameters('test_file.py::TestClass::test[foo]')
    'test_file.py::TestClass::test'
    >>> strip_nodeid_parameters('test_file.py::TestClass::test')
    'test_file.py::TestClass::test'
    """"""
    return REGEX_PARAMETERS.sub("""", nodeid)","Point(row=26, column=0)","Point(row=35, column=43)",,classic/benchmark/agbenchmark/utils/dependencies/util.py
get_absolute_nodeid,function,"
    Transform a possibly relative node id to an absolute one
    using the scope in which it is used.

    >>> scope = 'test_file.py::TestClass::test'
    >>> get_absolute_nodeid('test2', scope)
    'test_file.py::TestClass::test2'
    >>> get_absolute_nodeid('TestClass2::test2', scope)
    'test_file.py::TestClass2::test2'
    >>> get_absolute_nodeid('test_file2.py::TestClass2::test2', scope)
    'test_file2.py::TestClass2::test2'
","def get_absolute_nodeid(nodeid: str, scope: str) -> str:
    """"""
    Transform a possibly relative node id to an absolute one
    using the scope in which it is used.

    >>> scope = 'test_file.py::TestClass::test'
    >>> get_absolute_nodeid('test2', scope)
    'test_file.py::TestClass::test2'
    >>> get_absolute_nodeid('TestClass2::test2', scope)
    'test_file.py::TestClass2::test2'
    >>> get_absolute_nodeid('test_file2.py::TestClass2::test2', scope)
    'test_file2.py::TestClass2::test2'
    """"""
    parts = nodeid.split(""::"")
    # Completely relative (test_name): add the full current scope (file::class or file)
    if len(parts) == 1:
        base_nodeid = scope.rsplit(""::"", 1)[0]
        nodeid = f""{base_nodeid}::{nodeid}""
    # Contains some scope already (Class::test_name), so only add the current file scope
    elif ""."" not in parts[0]:
        base_nodeid = scope.split(""::"", 1)[0]
        nodeid = f""{base_nodeid}::{nodeid}""
    return clean_nodeid(nodeid)","Point(row=38, column=0)","Point(row=60, column=31)",,classic/benchmark/agbenchmark/utils/dependencies/util.py
get_name,function,"
    Get all names for a test.

    This will use the following methods to determine the name of the test:
        - If given, the custom name(s) passed to the keyword argument name on the marker
","def get_name(item: Item) -> str:
    """"""
    Get all names for a test.

    This will use the following methods to determine the name of the test:
        - If given, the custom name(s) passed to the keyword argument name on the marker
    """"""
    name = """"

    # Custom name
    markers = get_markers(item, MARKER_NAME)
    for marker in markers:
        if MARKER_KWARG_ID in marker.kwargs:
            name = marker.kwargs[MARKER_KWARG_ID]

    return name","Point(row=63, column=0)","Point(row=78, column=15)",,classic/benchmark/agbenchmark/utils/dependencies/util.py
get_markers,function,Get all markers with the given name for a given item.,"def get_markers(item: Item, name: str) -> Iterator[Mark]:
    """"""Get all markers with the given name for a given item.""""""
    for marker in item.iter_markers():
        if marker.name == name:
            yield marker","Point(row=81, column=0)","Point(row=85, column=24)",,classic/benchmark/agbenchmark/utils/dependencies/util.py
_add_ini_and_option,function,"
    Add an option to both the ini file and the command line flags.
    Command line flags/options takes precedence over the ini config.
","def _add_ini_and_option(
    parser: Any,
    group: OptionGroup,
    name: str,
    help: str,
    default: str | bool | int,
    **kwargs: Any,
) -> None:
    """"""
    Add an option to both the ini file and the command line flags.
    Command line flags/options takes precedence over the ini config.
    """"""
    parser.addini(
        name,
        help + "" This overrides the similarly named option from the config."",
        default=default,
    )
    group.addoption(f'--{name.replace(""_"", ""-"")}', help=help, default=None, **kwargs)","Point(row=26, column=0)","Point(row=43, column=85)",,classic/benchmark/agbenchmark/utils/dependencies/__init__.py
_get_ini_or_option,function,"
    Get an option from either the ini file or the command line flags,
    with the latter taking precedence.
","def _get_ini_or_option(
    config: Any, name: str, choices: Optional[list[str]]
) -> str | None:
    """"""
    Get an option from either the ini file or the command line flags,
    with the latter taking precedence.
    """"""
    value = config.getini(name)
    if value is not None and choices is not None and value not in choices:
        raise ValueError(
            f'Invalid ini value for {name}, choose from {"", "".join(choices)}'
        )
    return config.getoption(name) or value","Point(row=46, column=0)","Point(row=58, column=42)",,classic/benchmark/agbenchmark/utils/dependencies/__init__.py
pytest_addoption,function,,"def pytest_addoption(parser: Parser) -> None:
    # get all current option strings
    current_options = []
    for action in parser._anonymous.options:
        current_options += action._short_opts + action._long_opts

    for group in parser._groups:
        for action in group.options:
            current_options += action._short_opts + action._long_opts

    group = parser.getgroup(""depends"")

    # Add a flag to list all names + the tests they resolve to
    if ""--list-dependency-names"" not in current_options:
        group.addoption(
            ""--list-dependency-names"",
            action=""store_true"",
            default=False,
            help=(
                ""List all non-nodeid dependency names + the tests they resolve to. ""
                ""Will also list all nodeid dependency names in verbose mode.""
            ),
        )

    # Add a flag to list all (resolved) dependencies for all tests + unresolvable names
    if ""--list-processed-dependencies"" not in current_options:
        group.addoption(
            ""--list-processed-dependencies"",
            action=""store_true"",
            default=False,
            help=(
                ""List all dependencies of all tests as a list of nodeids ""
                ""+ the names that could not be resolved.""
            ),
        )

    # Add an ini option + flag to choose the action to take for failed dependencies
    if ""--failed-dependency-action"" not in current_options:
        _add_ini_and_option(
            parser,
            group,
            name=""failed_dependency_action"",
            help=(
                ""The action to take when a test has dependencies that failed. ""
                'Use ""run"" to run the test anyway, ""skip"" to skip the test, '
                'and ""fail"" to fail the test.'
            ),
            default=""skip"",
            choices=DEPENDENCY_PROBLEM_ACTIONS.keys(),
        )

    # Add an ini option + flag to choose the action to take for unresolved dependencies
    if ""--missing-dependency-action"" not in current_options:
        _add_ini_and_option(
            parser,
            group,
            name=""missing_dependency_action"",
            help=(
                ""The action to take when a test has dependencies that cannot be found ""
                ""within the current scope. ""
                'Use ""run"" to run the test anyway, ""skip"" to skip the test, '
                'and ""fail"" to fail the test.'
            ),
            default=""warning"",
            choices=DEPENDENCY_PROBLEM_ACTIONS.keys(),
        )","Point(row=61, column=0)","Point(row=126, column=9)",,classic/benchmark/agbenchmark/utils/dependencies/__init__.py
pytest_configure,function,,"def pytest_configure(config: Any) -> None:
    manager = DependencyManager()
    managers.append(manager)

    # Setup the handling of problems with dependencies
    manager.options[""failed_dependency_action""] = _get_ini_or_option(
        config,
        ""failed_dependency_action"",
        list(DEPENDENCY_PROBLEM_ACTIONS.keys()),
    )
    manager.options[""missing_dependency_action""] = _get_ini_or_option(
        config,
        ""missing_dependency_action"",
        list(DEPENDENCY_PROBLEM_ACTIONS.keys()),
    )

    # Register marker
    config.addinivalue_line(
        ""markers"",
        ""depends(name='name', on=['other_name']): marks dependencies between tests."",
    )","Point(row=129, column=0)","Point(row=149, column=5)",,classic/benchmark/agbenchmark/utils/dependencies/__init__.py
pytest_collection_modifyitems,function,,"def pytest_collection_modifyitems(config: Any, items: list[pytest.Function]) -> None:
    manager = managers[-1]

    # Register the founds tests on the manager
    manager.items = items

    # Show the extra information if requested
    if config.getoption(""list_dependency_names""):
        verbose = config.getoption(""verbose"") > 1
        manager.print_name_map(verbose)
    if config.getoption(""list_processed_dependencies""):
        color = config.getoption(""color"")
        manager.print_processed_dependencies(color)

    # Reorder the items so that tests run after their dependencies
    items[:] = manager.sorted_items","Point(row=153, column=0)","Point(row=168, column=35)",,classic/benchmark/agbenchmark/utils/dependencies/__init__.py
pytest_runtest_makereport,function,,"def pytest_runtest_makereport(item: Item) -> Any:
    manager = managers[-1]

    # Run the step
    outcome = yield

    # Store the result on the manager
    manager.register_result(item, outcome.get_result())","Point(row=172, column=0)","Point(row=179, column=55)",,classic/benchmark/agbenchmark/utils/dependencies/__init__.py
pytest_runtest_call,function,,"def pytest_runtest_call(item: Item) -> None:
    manager = managers[-1]

    # Handle missing dependencies
    missing_dependency_action = DEPENDENCY_PROBLEM_ACTIONS[
        manager.options[""missing_dependency_action""]
    ]
    missing = manager.get_missing(item)
    if missing_dependency_action and missing:
        missing_dependency_action(
            f'{item.nodeid} depends on {"", "".join(missing)}, which was not found'
        )

    # Check whether all dependencies succeeded
    failed_dependency_action = DEPENDENCY_PROBLEM_ACTIONS[
        manager.options[""failed_dependency_action""]
    ]
    failed = manager.get_failed(item)
    if failed_dependency_action and failed:
        failed_dependency_action(f'{item.nodeid} depends on {"", "".join(failed)}')","Point(row=182, column=0)","Point(row=201, column=81)",,classic/benchmark/agbenchmark/utils/dependencies/__init__.py
pytest_unconfigure,function,,"def pytest_unconfigure() -> None:
    managers.pop()","Point(row=204, column=0)","Point(row=205, column=18)",,classic/benchmark/agbenchmark/utils/dependencies/__init__.py
bezier_curve,function,"
    Generate B√©zier curve points.

    Args:
    - src (np.ndarray): The source point.
    - ctrl (List[float]): The control point.
    - dst (np.ndarray): The destination point.

    Returns:
    - List[np.ndarray]: The B√©zier curve points.
","def bezier_curve(
    src: np.ndarray, ctrl: List[float], dst: np.ndarray
) -> List[np.ndarray]:
    """"""
    Generate B√©zier curve points.

    Args:
    - src (np.ndarray): The source point.
    - ctrl (List[float]): The control point.
    - dst (np.ndarray): The destination point.

    Returns:
    - List[np.ndarray]: The B√©zier curve points.
    """"""
    curve = []
    for t in np.linspace(0, 1, num=100):
        curve_point = (
            np.outer((1 - t) ** 2, src)
            + 2 * np.outer((1 - t) * t, ctrl)
            + np.outer(t**2, dst)
        )
        curve.append(curve_point[0])
    return curve","Point(row=18, column=0)","Point(row=40, column=16)",,classic/benchmark/agbenchmark/utils/dependencies/graphs.py
curved_edges,function,"
    Draw curved edges for nodes on the same level.

    Args:
    - G (Any): The graph object.
    - pos (Dict[Any, Tuple[float, float]]): Dictionary with node positions.
    - dist (float, optional): Distance for curvature. Defaults to 0.2.

    Returns:
    - None
","def curved_edges(
    G: nx.Graph, pos: Dict[Any, Tuple[float, float]], dist: float = 0.2
) -> None:
    """"""
    Draw curved edges for nodes on the same level.

    Args:
    - G (Any): The graph object.
    - pos (Dict[Any, Tuple[float, float]]): Dictionary with node positions.
    - dist (float, optional): Distance for curvature. Defaults to 0.2.

    Returns:
    - None
    """"""
    ax = plt.gca()
    for u, v, data in G.edges(data=True):
        _src = pos[u]
        _dst = pos[v]
        src = np.array(_src)
        dst = np.array(_dst)

        same_level = abs(src[1] - dst[1]) < 0.01

        if same_level:
            control = [(src[0] + dst[0]) / 2, src[1] + dist]
            curve = bezier_curve(src, control, dst)
            arrow = patches.FancyArrowPatch(
                posA=curve[0],  # type: ignore
                posB=curve[-1],  # type: ignore
                connectionstyle=""arc3,rad=0.2"",
                color=""gray"",
                arrowstyle=""-|>"",
                mutation_scale=15.0,
                lw=1,
                shrinkA=10,
                shrinkB=10,
            )
            ax.add_patch(arrow)
        else:
            ax.annotate(
                """",
                xy=_dst,
                xytext=_src,
                arrowprops=dict(
                    arrowstyle=""-|>"", color=""gray"", lw=1, shrinkA=10, shrinkB=10
                ),
            )","Point(row=43, column=0)","Point(row=89, column=13)",,classic/benchmark/agbenchmark/utils/dependencies/graphs.py
tree_layout,function,"Compute positions as a tree layout centered on the root
    with alternating vertical shifts.","def tree_layout(graph: nx.DiGraph, root_node: Any) -> Dict[Any, Tuple[float, float]]:
    """"""Compute positions as a tree layout centered on the root
    with alternating vertical shifts.""""""
    bfs_tree = nx.bfs_tree(graph, source=root_node)
    levels = {
        node: depth
        for node, depth in nx.single_source_shortest_path_length(
            bfs_tree, root_node
        ).items()
    }

    pos = {}
    max_depth = max(levels.values())
    level_positions = {i: 0 for i in range(max_depth + 1)}  # type: ignore

    # Count the number of nodes per level to compute the width
    level_count: Any = {}
    for node, level in levels.items():
        level_count[level] = level_count.get(level, 0) + 1

    vertical_offset = (
        0.07  # The amount of vertical shift per node within the same level
    )

    # Assign positions
    for node, level in sorted(levels.items(), key=lambda x: x[1]):
        total_nodes_in_level = level_count[level]
        horizontal_spacing = 1.0 / (total_nodes_in_level + 1)
        pos_x = (
            0.5
            - (total_nodes_in_level - 1) * horizontal_spacing / 2
            + level_positions[level] * horizontal_spacing
        )

        # Alternately shift nodes up and down within the same level
        pos_y = (
            -level
            + (level_positions[level] % 2) * vertical_offset
            - ((level_positions[level] + 1) % 2) * vertical_offset
        )
        pos[node] = (pos_x, pos_y)

        level_positions[level] += 1

    return pos","Point(row=92, column=0)","Point(row=136, column=14)",,classic/benchmark/agbenchmark/utils/dependencies/graphs.py
graph_spring_layout,function,,"def graph_spring_layout(
    dag: nx.DiGraph, labels: Dict[Any, str], tree: bool = True
) -> None:
    num_nodes = len(list(dag.nodes()))
    # Setting up the figure and axis
    fig, ax = plt.subplots()
    ax.axis(""off"")  # Turn off the axis

    base = 3.0

    if num_nodes > 10:
        base /= 1 + math.log(num_nodes)
        font_size = base * 10

    font_size = max(10, base * 10)
    node_size = max(300, base * 1000)

    if tree:
        root_node = [node for node, degree in dag.in_degree() if degree == 0][0]
        pos = tree_layout(dag, root_node)
    else:
        # Adjust k for the spring layout based on node count
        k_value = 3 / math.sqrt(num_nodes)

        pos = nx.spring_layout(dag, k=k_value, iterations=50)

    # Draw nodes and labels
    nx.draw_networkx_nodes(dag, pos, node_color=""skyblue"", node_size=int(node_size))
    nx.draw_networkx_labels(dag, pos, labels=labels, font_size=int(font_size))

    # Draw curved edges
    curved_edges(dag, pos)  # type: ignore

    plt.tight_layout()
    plt.show()","Point(row=139, column=0)","Point(row=173, column=14)",,classic/benchmark/agbenchmark/utils/dependencies/graphs.py
rgb_to_hex,function,,"def rgb_to_hex(rgb: Tuple[float, float, float]) -> str:
    return ""#{:02x}{:02x}{:02x}"".format(
        int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255)
    )","Point(row=176, column=0)","Point(row=179, column=5)",,classic/benchmark/agbenchmark/utils/dependencies/graphs.py
get_category_colors,function,,"def get_category_colors(categories: Dict[Any, str]) -> Dict[str, str]:
    unique_categories = set(categories.values())
    colormap = plt.cm.get_cmap(""tab10"", len(unique_categories))  # type: ignore
    return {
        category: rgb_to_hex(colormap(i)[:3])
        for i, category in enumerate(unique_categories)
    }","Point(row=182, column=0)","Point(row=188, column=5)",,classic/benchmark/agbenchmark/utils/dependencies/graphs.py
graph_interactive_network,function,,"def graph_interactive_network(
    dag: nx.DiGraph,
    labels: Dict[Any, Dict[str, Any]],
    html_graph_path: str = """",
) -> None:
    nt = Network(notebook=True, width=""100%"", height=""800px"", directed=True)

    category_colors = get_category_colors(DATA_CATEGORY)

    # Add nodes and edges to the pyvis network
    for node, json_data in labels.items():
        label = json_data.get(""name"", """")
        # remove the first 4 letters of label
        label_without_test = label[4:]
        node_id_str = node.nodeid

        # Get the category for this label
        category = DATA_CATEGORY.get(
            label, ""unknown""
        )  # Default to 'unknown' if label not found

        # Get the color for this category
        color = category_colors.get(category, ""grey"")

        nt.add_node(
            node_id_str,
            label=label_without_test,
            color=color,
            data=json_data,
        )

    # Add edges to the pyvis network
    for edge in dag.edges():
        source_id_str = edge[0].nodeid
        target_id_str = edge[1].nodeid
        edge_id_str = (
            f""{source_id_str}_to_{target_id_str}""  # Construct a unique edge id
        )
        if not (source_id_str in nt.get_nodes() and target_id_str in nt.get_nodes()):
            logger.warning(
                f""Skipping edge {source_id_str} -> {target_id_str} due to missing nodes""
            )
            continue
        nt.add_edge(source_id_str, target_id_str, id=edge_id_str)

    # Configure physics for hierarchical layout
    hierarchical_options = {
        ""enabled"": True,
        ""levelSeparation"": 200,  # Increased vertical spacing between levels
        ""nodeSpacing"": 250,  # Increased spacing between nodes on the same level
        ""treeSpacing"": 250,  # Increased spacing between different trees (for forest)
        ""blockShifting"": True,
        ""edgeMinimization"": True,
        ""parentCentralization"": True,
        ""direction"": ""UD"",
        ""sortMethod"": ""directed"",
    }

    physics_options = {
        ""stabilization"": {
            ""enabled"": True,
            ""iterations"": 1000,  # Default is often around 100
        },
        ""hierarchicalRepulsion"": {
            ""centralGravity"": 0.0,
            ""springLength"": 200,  # Increased edge length
            ""springConstant"": 0.01,
            ""nodeDistance"": 250,  # Increased minimum distance between nodes
            ""damping"": 0.09,
        },
        ""solver"": ""hierarchicalRepulsion"",
        ""timestep"": 0.5,
    }

    nt.options = {
        ""nodes"": {
            ""font"": {
                ""size"": 20,  # Increased font size for labels
                ""color"": ""black"",  # Set a readable font color
            },
            ""shapeProperties"": {""useBorderWithImage"": True},
        },
        ""edges"": {
            ""length"": 250,  # Increased edge length
        },
        ""physics"": physics_options,
        ""layout"": {""hierarchical"": hierarchical_options},
    }

    # Serialize the graph to JSON and save in appropriate locations
    graph_data = {""nodes"": nt.nodes, ""edges"": nt.edges}
    logger.debug(f""Generated graph data:\n{json.dumps(graph_data, indent=4)}"")

    # FIXME: use more reliable method to find the right location for these files.
    #   This will fail in all cases except if run from the root of our repo.
    home_path = Path.cwd()
    write_pretty_json(graph_data, home_path / ""frontend"" / ""public"" / ""graph.json"")

    flutter_app_path = home_path.parent / ""frontend"" / ""assets""

    # Optionally, save to a file
    # Sync with the flutter UI
    # this literally only works in the AutoGPT repo, but this part of the code
    # is not reached if BUILD_SKILL_TREE is false
    write_pretty_json(graph_data, flutter_app_path / ""tree_structure.json"")
    validate_skill_tree(graph_data, """")

    # Extract node IDs with category ""coding""

    coding_tree = extract_subgraph_based_on_category(graph_data.copy(), ""coding"")
    validate_skill_tree(coding_tree, ""coding"")
    write_pretty_json(
        coding_tree,
        flutter_app_path / ""coding_tree_structure.json"",
    )

    data_tree = extract_subgraph_based_on_category(graph_data.copy(), ""data"")
    # validate_skill_tree(data_tree, ""data"")
    write_pretty_json(
        data_tree,
        flutter_app_path / ""data_tree_structure.json"",
    )

    general_tree = extract_subgraph_based_on_category(graph_data.copy(), ""general"")
    validate_skill_tree(general_tree, ""general"")
    write_pretty_json(
        general_tree,
        flutter_app_path / ""general_tree_structure.json"",
    )

    scrape_synthesize_tree = extract_subgraph_based_on_category(
        graph_data.copy(), ""scrape_synthesize""
    )
    validate_skill_tree(scrape_synthesize_tree, ""scrape_synthesize"")
    write_pretty_json(
        scrape_synthesize_tree,
        flutter_app_path / ""scrape_synthesize_tree_structure.json"",
    )

    if html_graph_path:
        file_path = str(Path(html_graph_path).resolve())

        nt.write_html(file_path)","Point(row=191, column=0)","Point(row=333, column=32)",,classic/benchmark/agbenchmark/utils/dependencies/graphs.py
extract_subgraph_based_on_category,function,"
    Extracts a subgraph that includes all nodes and edges required to reach all nodes
    with a specified category.

    :param graph: The original graph.
    :param category: The target category.
    :return: Subgraph with nodes and edges required to reach the nodes
        with the given category.
","def extract_subgraph_based_on_category(graph, category):
    """"""
    Extracts a subgraph that includes all nodes and edges required to reach all nodes
    with a specified category.

    :param graph: The original graph.
    :param category: The target category.
    :return: Subgraph with nodes and edges required to reach the nodes
        with the given category.
    """"""

    subgraph = {""nodes"": [], ""edges"": []}
    visited = set()

    def reverse_dfs(node_id):
        if node_id in visited:
            return
        visited.add(node_id)

        node_data = next(node for node in graph[""nodes""] if node[""id""] == node_id)

        # Add the node to the subgraph if it's not already present.
        if node_data not in subgraph[""nodes""]:
            subgraph[""nodes""].append(node_data)

        for edge in graph[""edges""]:
            if edge[""to""] == node_id:
                if edge not in subgraph[""edges""]:
                    subgraph[""edges""].append(edge)
                reverse_dfs(edge[""from""])

    # Identify nodes with the target category and initiate reverse DFS from them.
    nodes_with_target_category = [
        node[""id""] for node in graph[""nodes""] if category in node[""data""][""category""]
    ]

    for node_id in nodes_with_target_category:
        reverse_dfs(node_id)

    return subgraph","Point(row=336, column=0)","Point(row=375, column=19)",,classic/benchmark/agbenchmark/utils/dependencies/graphs.py
extract_subgraph_based_on_category.reverse_dfs,function,,"def reverse_dfs(node_id):
        if node_id in visited:
            return
        visited.add(node_id)

        node_data = next(node for node in graph[""nodes""] if node[""id""] == node_id)

        # Add the node to the subgraph if it's not already present.
        if node_data not in subgraph[""nodes""]:
            subgraph[""nodes""].append(node_data)

        for edge in graph[""edges""]:
            if edge[""to""] == node_id:
                if edge not in subgraph[""edges""]:
                    subgraph[""edges""].append(edge)
                reverse_dfs(edge[""from""])","Point(row=350, column=4)","Point(row=365, column=41)",,classic/benchmark/agbenchmark/utils/dependencies/graphs.py
is_circular,function,,"def is_circular(graph):
    def dfs(node, visited, stack, parent_map):
        visited.add(node)
        stack.add(node)
        for edge in graph[""edges""]:
            if edge[""from""] == node:
                if edge[""to""] in stack:
                    # Detected a cycle
                    cycle_path = []
                    current = node
                    while current != edge[""to""]:
                        cycle_path.append(current)
                        current = parent_map.get(current)
                    cycle_path.append(edge[""to""])
                    cycle_path.append(node)
                    return cycle_path[::-1]
                elif edge[""to""] not in visited:
                    parent_map[edge[""to""]] = node
                    cycle_path = dfs(edge[""to""], visited, stack, parent_map)
                    if cycle_path:
                        return cycle_path
        stack.remove(node)
        return None

    visited = set()
    stack = set()
    parent_map = {}
    for node in graph[""nodes""]:
        node_id = node[""id""]
        if node_id not in visited:
            cycle_path = dfs(node_id, visited, stack, parent_map)
            if cycle_path:
                return cycle_path
    return None","Point(row=378, column=0)","Point(row=411, column=15)",,classic/benchmark/agbenchmark/utils/dependencies/graphs.py
is_circular.dfs,function,,"def dfs(node, visited, stack, parent_map):
        visited.add(node)
        stack.add(node)
        for edge in graph[""edges""]:
            if edge[""from""] == node:
                if edge[""to""] in stack:
                    # Detected a cycle
                    cycle_path = []
                    current = node
                    while current != edge[""to""]:
                        cycle_path.append(current)
                        current = parent_map.get(current)
                    cycle_path.append(edge[""to""])
                    cycle_path.append(node)
                    return cycle_path[::-1]
                elif edge[""to""] not in visited:
                    parent_map[edge[""to""]] = node
                    cycle_path = dfs(edge[""to""], visited, stack, parent_map)
                    if cycle_path:
                        return cycle_path
        stack.remove(node)
        return None","Point(row=379, column=4)","Point(row=400, column=19)",,classic/benchmark/agbenchmark/utils/dependencies/graphs.py
get_roots,function,"
    Return the roots of a graph. Roots are nodes with no incoming edges.
","def get_roots(graph):
    """"""
    Return the roots of a graph. Roots are nodes with no incoming edges.
    """"""
    # Create a set of all node IDs
    all_nodes = {node[""id""] for node in graph[""nodes""]}

    # Create a set of nodes with incoming edges
    nodes_with_incoming_edges = {edge[""to""] for edge in graph[""edges""]}

    # Roots are nodes that have no incoming edges
    roots = all_nodes - nodes_with_incoming_edges

    return list(roots)","Point(row=414, column=0)","Point(row=427, column=22)",,classic/benchmark/agbenchmark/utils/dependencies/graphs.py
validate_skill_tree,function,"
    Validate if a given graph represents a valid skill tree
    and raise appropriate exceptions if not.

    :param graph: A dictionary representing the graph with 'nodes' and 'edges'.
    :raises: ValueError with a description of the invalidity.
","def validate_skill_tree(graph, skill_tree_name):
    """"""
    Validate if a given graph represents a valid skill tree
    and raise appropriate exceptions if not.

    :param graph: A dictionary representing the graph with 'nodes' and 'edges'.
    :raises: ValueError with a description of the invalidity.
    """"""
    # Check for circularity
    cycle_path = is_circular(graph)
    if cycle_path:
        cycle_str = "" -> "".join(cycle_path)
        raise ValueError(
            f""{skill_tree_name} skill tree is circular! ""
            f""Detected circular path: {cycle_str}.""
        )

    # Check for multiple roots
    roots = get_roots(graph)
    if len(roots) > 1:
        raise ValueError(f""{skill_tree_name} skill tree has multiple roots: {roots}."")
    elif not roots:
        raise ValueError(f""{skill_tree_name} skill tree has no roots."")","Point(row=430, column=0)","Point(row=452, column=71)",,classic/benchmark/agbenchmark/utils/dependencies/graphs.py
TestResult,class,Keeps track of the results of a single test.,"class TestResult(object):
    """"""Keeps track of the results of a single test.""""""

    STEPS = [""setup"", ""call"", ""teardown""]
    GOOD_OUTCOMES = [""passed""]

    def __init__(self, nodeid: str) -> None:
        """"""Create a new instance for a test with a given node id.""""""
        self.nodeid = nodeid
        self.results: dict[str, Any] = {}

    def register_result(self, result: Any) -> None:
        """"""Register a result of this test.""""""
        if result.when not in self.STEPS:
            raise ValueError(
                f""Received result for unknown step {result.when} of test {self.nodeid}""
            )
        if result.when in self.results:
            raise AttributeError(
                f""Received multiple results for step {result.when} ""
                f""of test {self.nodeid}""
            )
        self.results[result.when] = result.outcome

    @property
    def success(self) -> bool:
        """"""Whether the entire test was successful.""""""
        return all(
            self.results.get(step, None) in self.GOOD_OUTCOMES for step in self.STEPS
        )","Point(row=22, column=0)","Point(row=51, column=9)",,classic/benchmark/agbenchmark/utils/dependencies/main.py
TestResult.__init__,function,Create a new instance for a test with a given node id.,"def __init__(self, nodeid: str) -> None:
        """"""Create a new instance for a test with a given node id.""""""
        self.nodeid = nodeid
        self.results: dict[str, Any] = {}","Point(row=28, column=4)","Point(row=31, column=41)",TestResult,classic/benchmark/agbenchmark/utils/dependencies/main.py
TestResult.register_result,function,Register a result of this test.,"def register_result(self, result: Any) -> None:
        """"""Register a result of this test.""""""
        if result.when not in self.STEPS:
            raise ValueError(
                f""Received result for unknown step {result.when} of test {self.nodeid}""
            )
        if result.when in self.results:
            raise AttributeError(
                f""Received multiple results for step {result.when} ""
                f""of test {self.nodeid}""
            )
        self.results[result.when] = result.outcome","Point(row=33, column=4)","Point(row=44, column=50)",TestResult,classic/benchmark/agbenchmark/utils/dependencies/main.py
TestResult.success,function,Whether the entire test was successful.,"def success(self) -> bool:
        """"""Whether the entire test was successful.""""""
        return all(
            self.results.get(step, None) in self.GOOD_OUTCOMES for step in self.STEPS
        )","Point(row=47, column=4)","Point(row=51, column=9)",TestResult,classic/benchmark/agbenchmark/utils/dependencies/main.py
TestDependencies,class,Information about the resolved dependencies of a single test.,"class TestDependencies(object):
    """"""Information about the resolved dependencies of a single test.""""""

    def __init__(self, item: Item, manager: ""DependencyManager"") -> None:
        """"""Create a new instance for a given test.""""""
        self.nodeid = clean_nodeid(item.nodeid)
        self.dependencies = set()
        self.unresolved = set()

        markers = get_markers(item, MARKER_NAME)
        dependencies = [
            dep
            for marker in markers
            for dep in marker.kwargs.get(MARKER_KWARG_DEPENDENCIES, [])
        ]
        for dependency in dependencies:
            # If the name is not known, try to make it absolute (file::[class::]method)
            if dependency not in manager.name_to_nodeids:
                absolute_dependency = get_absolute_nodeid(dependency, self.nodeid)
                if absolute_dependency in manager.name_to_nodeids:
                    dependency = absolute_dependency

            # Add all items matching the name
            if dependency in manager.name_to_nodeids:
                for nodeid in manager.name_to_nodeids[dependency]:
                    self.dependencies.add(nodeid)
            else:
                self.unresolved.add(dependency)","Point(row=54, column=0)","Point(row=81, column=47)",,classic/benchmark/agbenchmark/utils/dependencies/main.py
TestDependencies.__init__,function,Create a new instance for a given test.,"def __init__(self, item: Item, manager: ""DependencyManager"") -> None:
        """"""Create a new instance for a given test.""""""
        self.nodeid = clean_nodeid(item.nodeid)
        self.dependencies = set()
        self.unresolved = set()

        markers = get_markers(item, MARKER_NAME)
        dependencies = [
            dep
            for marker in markers
            for dep in marker.kwargs.get(MARKER_KWARG_DEPENDENCIES, [])
        ]
        for dependency in dependencies:
            # If the name is not known, try to make it absolute (file::[class::]method)
            if dependency not in manager.name_to_nodeids:
                absolute_dependency = get_absolute_nodeid(dependency, self.nodeid)
                if absolute_dependency in manager.name_to_nodeids:
                    dependency = absolute_dependency

            # Add all items matching the name
            if dependency in manager.name_to_nodeids:
                for nodeid in manager.name_to_nodeids[dependency]:
                    self.dependencies.add(nodeid)
            else:
                self.unresolved.add(dependency)","Point(row=57, column=4)","Point(row=81, column=47)",TestDependencies,classic/benchmark/agbenchmark/utils/dependencies/main.py
DependencyManager,class,"Keep track of tests, their names and their dependencies.","class DependencyManager(object):
    """"""Keep track of tests, their names and their dependencies.""""""

    def __init__(self) -> None:
        """"""Create a new DependencyManager.""""""
        self.options: dict[str, Any] = {}
        self._items: list[Function] | None = None
        self._name_to_nodeids: Any = None
        self._nodeid_to_item: Any = None
        self._results: Any = None

    @property
    def items(self) -> list[Function]:
        """"""The collected tests that are managed by this instance.""""""
        if self._items is None:
            raise AttributeError(""The items attribute has not been set yet"")
        return self._items

    @items.setter
    def items(self, items: list[Function]) -> None:
        if self._items is not None:
            raise AttributeError(""The items attribute has already been set"")
        self._items = items

        self._name_to_nodeids = collections.defaultdict(list)
        self._nodeid_to_item = {}
        self._results = {}
        self._dependencies = {}

        for item in items:
            nodeid = clean_nodeid(item.nodeid)
            # Add the mapping from nodeid to the test item
            self._nodeid_to_item[nodeid] = item
            # Add the mappings from all names to the node id
            name = get_name(item)
            self._name_to_nodeids[name].append(nodeid)
            # Create the object that will contain the results of this test
            self._results[nodeid] = TestResult(clean_nodeid(item.nodeid))

        # Don't allow using unknown keys on the name_to_nodeids mapping
        self._name_to_nodeids.default_factory = None

        for item in items:
            nodeid = clean_nodeid(item.nodeid)
            # Process the dependencies of this test
            # This uses the mappings created in the previous loop,
            # and can thus not be merged into that loop
            self._dependencies[nodeid] = TestDependencies(item, self)

    @property
    def name_to_nodeids(self) -> dict[str, list[str]]:
        """"""A mapping from names to matching node id(s).""""""
        assert self.items is not None
        return self._name_to_nodeids

    @property
    def nodeid_to_item(self) -> dict[str, Function]:
        """"""A mapping from node ids to test items.""""""
        assert self.items is not None
        return self._nodeid_to_item

    @property
    def results(self) -> dict[str, TestResult]:
        """"""The results of the tests.""""""
        assert self.items is not None
        return self._results

    @property
    def dependencies(self) -> dict[str, TestDependencies]:
        """"""The dependencies of the tests.""""""
        assert self.items is not None
        return self._dependencies

    def print_name_map(self, verbose: bool = False) -> None:
        """"""Print a human-readable version of the name -> test mapping.""""""
        print(""Available dependency names:"")
        for name, nodeids in sorted(self.name_to_nodeids.items(), key=lambda x: x[0]):
            if len(nodeids) == 1:
                if name == nodeids[0]:
                    # This is just the base name, only print this when verbose
                    if verbose:
                        print(f""  {name}"")
                else:
                    # Name refers to a single node id, so use the short format
                    print(f""  {name} -> {nodeids[0]}"")
            else:
                # Name refers to multiple node ids, so use the long format
                print(f""  {name} ->"")
                for nodeid in sorted(nodeids):
                    print(f""    {nodeid}"")

    def print_processed_dependencies(self, colors: bool = False) -> None:
        """"""Print a human-readable list of the processed dependencies.""""""
        missing = ""MISSING""
        if colors:
            missing = f""{colorama.Fore.RED}{missing}{colorama.Fore.RESET}""
            colorama.init()
        try:
            print(""Dependencies:"")
            for nodeid, info in sorted(self.dependencies.items(), key=lambda x: x[0]):
                descriptions = []
                for dependency in info.dependencies:
                    descriptions.append(dependency)
                for dependency in info.unresolved:
                    descriptions.append(f""{dependency} ({missing})"")
                if descriptions:
                    print(f""  {nodeid} depends on"")
                    for description in sorted(descriptions):
                        print(f""    {description}"")
        finally:
            if colors:
                colorama.deinit()

    @property
    def sorted_items(self) -> Generator:
        """"""
        Get a sorted list of tests where all tests are sorted after their dependencies.
        """"""
        # Build a directed graph for sorting
        build_skill_tree = os.getenv(""BUILD_SKILL_TREE"")
        BUILD_SKILL_TREE = (
            build_skill_tree.lower() == ""true"" if build_skill_tree else False
        )
        dag = networkx.DiGraph()

        # Insert all items as nodes, to prevent items that have no dependencies
        # and are not dependencies themselves from being lost
        dag.add_nodes_from(self.items)

        # Insert edges for all the dependencies
        for item in self.items:
            nodeid = clean_nodeid(item.nodeid)
            for dependency in self.dependencies[nodeid].dependencies:
                dag.add_edge(self.nodeid_to_item[dependency], item)

        labels = {}
        for item in self.items:
            assert item.cls and issubclass(item.cls, BaseChallenge)
            data = item.cls.info.model_dump()

            node_name = get_name(item)
            data[""name""] = node_name
            labels[item] = data

        # only build the tree if it's specified in the env and is a whole run
        if BUILD_SKILL_TREE:
            # graph_spring_layout(dag, labels)
            graph_interactive_network(dag, labels, html_graph_path="""")

        # Sort based on the dependencies
        return networkx.topological_sort(dag)

    def register_result(self, item: Item, result: Any) -> None:
        """"""Register a result of a test.""""""
        nodeid = clean_nodeid(item.nodeid)
        self.results[nodeid].register_result(result)

    def get_failed(self, item: Item) -> Any:
        """"""Get a list of unfulfilled dependencies for a test.""""""
        nodeid = clean_nodeid(item.nodeid)
        failed = []
        for dependency in self.dependencies[nodeid].dependencies:
            result = self.results[dependency]
            if not result.success:
                failed.append(dependency)
        return failed

    def get_missing(self, item: Item) -> Any:
        """"""Get a list of missing dependencies for a test.""""""
        nodeid = clean_nodeid(item.nodeid)
        return self.dependencies[nodeid].unresolved","Point(row=84, column=0)","Point(row=254, column=51)",,classic/benchmark/agbenchmark/utils/dependencies/main.py
DependencyManager.__init__,function,Create a new DependencyManager.,"def __init__(self) -> None:
        """"""Create a new DependencyManager.""""""
        self.options: dict[str, Any] = {}
        self._items: list[Function] | None = None
        self._name_to_nodeids: Any = None
        self._nodeid_to_item: Any = None
        self._results: Any = None","Point(row=87, column=4)","Point(row=93, column=33)",DependencyManager,classic/benchmark/agbenchmark/utils/dependencies/main.py
DependencyManager.items,function,The collected tests that are managed by this instance.,"def items(self) -> list[Function]:
        """"""The collected tests that are managed by this instance.""""""
        if self._items is None:
            raise AttributeError(""The items attribute has not been set yet"")
        return self._items","Point(row=96, column=4)","Point(row=100, column=26)",DependencyManager,classic/benchmark/agbenchmark/utils/dependencies/main.py
DependencyManager.items,function,,"def items(self, items: list[Function]) -> None:
        if self._items is not None:
            raise AttributeError(""The items attribute has already been set"")
        self._items = items

        self._name_to_nodeids = collections.defaultdict(list)
        self._nodeid_to_item = {}
        self._results = {}
        self._dependencies = {}

        for item in items:
            nodeid = clean_nodeid(item.nodeid)
            # Add the mapping from nodeid to the test item
            self._nodeid_to_item[nodeid] = item
            # Add the mappings from all names to the node id
            name = get_name(item)
            self._name_to_nodeids[name].append(nodeid)
            # Create the object that will contain the results of this test
            self._results[nodeid] = TestResult(clean_nodeid(item.nodeid))

        # Don't allow using unknown keys on the name_to_nodeids mapping
        self._name_to_nodeids.default_factory = None

        for item in items:
            nodeid = clean_nodeid(item.nodeid)
            # Process the dependencies of this test
            # This uses the mappings created in the previous loop,
            # and can thus not be merged into that loop
            self._dependencies[nodeid] = TestDependencies(item, self)","Point(row=103, column=4)","Point(row=131, column=69)",DependencyManager,classic/benchmark/agbenchmark/utils/dependencies/main.py
DependencyManager.name_to_nodeids,function,A mapping from names to matching node id(s).,"def name_to_nodeids(self) -> dict[str, list[str]]:
        """"""A mapping from names to matching node id(s).""""""
        assert self.items is not None
        return self._name_to_nodeids","Point(row=134, column=4)","Point(row=137, column=36)",DependencyManager,classic/benchmark/agbenchmark/utils/dependencies/main.py
DependencyManager.nodeid_to_item,function,A mapping from node ids to test items.,"def nodeid_to_item(self) -> dict[str, Function]:
        """"""A mapping from node ids to test items.""""""
        assert self.items is not None
        return self._nodeid_to_item","Point(row=140, column=4)","Point(row=143, column=35)",DependencyManager,classic/benchmark/agbenchmark/utils/dependencies/main.py
DependencyManager.results,function,The results of the tests.,"def results(self) -> dict[str, TestResult]:
        """"""The results of the tests.""""""
        assert self.items is not None
        return self._results","Point(row=146, column=4)","Point(row=149, column=28)",DependencyManager,classic/benchmark/agbenchmark/utils/dependencies/main.py
DependencyManager.dependencies,function,The dependencies of the tests.,"def dependencies(self) -> dict[str, TestDependencies]:
        """"""The dependencies of the tests.""""""
        assert self.items is not None
        return self._dependencies","Point(row=152, column=4)","Point(row=155, column=33)",DependencyManager,classic/benchmark/agbenchmark/utils/dependencies/main.py
DependencyManager.print_name_map,function,Print a human-readable version of the name -> test mapping.,"def print_name_map(self, verbose: bool = False) -> None:
        """"""Print a human-readable version of the name -> test mapping.""""""
        print(""Available dependency names:"")
        for name, nodeids in sorted(self.name_to_nodeids.items(), key=lambda x: x[0]):
            if len(nodeids) == 1:
                if name == nodeids[0]:
                    # This is just the base name, only print this when verbose
                    if verbose:
                        print(f""  {name}"")
                else:
                    # Name refers to a single node id, so use the short format
                    print(f""  {name} -> {nodeids[0]}"")
            else:
                # Name refers to multiple node ids, so use the long format
                print(f""  {name} ->"")
                for nodeid in sorted(nodeids):
                    print(f""    {nodeid}"")","Point(row=157, column=4)","Point(row=173, column=42)",DependencyManager,classic/benchmark/agbenchmark/utils/dependencies/main.py
DependencyManager.print_processed_dependencies,function,Print a human-readable list of the processed dependencies.,"def print_processed_dependencies(self, colors: bool = False) -> None:
        """"""Print a human-readable list of the processed dependencies.""""""
        missing = ""MISSING""
        if colors:
            missing = f""{colorama.Fore.RED}{missing}{colorama.Fore.RESET}""
            colorama.init()
        try:
            print(""Dependencies:"")
            for nodeid, info in sorted(self.dependencies.items(), key=lambda x: x[0]):
                descriptions = []
                for dependency in info.dependencies:
                    descriptions.append(dependency)
                for dependency in info.unresolved:
                    descriptions.append(f""{dependency} ({missing})"")
                if descriptions:
                    print(f""  {nodeid} depends on"")
                    for description in sorted(descriptions):
                        print(f""    {description}"")
        finally:
            if colors:
                colorama.deinit()","Point(row=175, column=4)","Point(row=195, column=33)",DependencyManager,classic/benchmark/agbenchmark/utils/dependencies/main.py
DependencyManager.sorted_items,function,"
        Get a sorted list of tests where all tests are sorted after their dependencies.
","def sorted_items(self) -> Generator:
        """"""
        Get a sorted list of tests where all tests are sorted after their dependencies.
        """"""
        # Build a directed graph for sorting
        build_skill_tree = os.getenv(""BUILD_SKILL_TREE"")
        BUILD_SKILL_TREE = (
            build_skill_tree.lower() == ""true"" if build_skill_tree else False
        )
        dag = networkx.DiGraph()

        # Insert all items as nodes, to prevent items that have no dependencies
        # and are not dependencies themselves from being lost
        dag.add_nodes_from(self.items)

        # Insert edges for all the dependencies
        for item in self.items:
            nodeid = clean_nodeid(item.nodeid)
            for dependency in self.dependencies[nodeid].dependencies:
                dag.add_edge(self.nodeid_to_item[dependency], item)

        labels = {}
        for item in self.items:
            assert item.cls and issubclass(item.cls, BaseChallenge)
            data = item.cls.info.model_dump()

            node_name = get_name(item)
            data[""name""] = node_name
            labels[item] = data

        # only build the tree if it's specified in the env and is a whole run
        if BUILD_SKILL_TREE:
            # graph_spring_layout(dag, labels)
            graph_interactive_network(dag, labels, html_graph_path="""")

        # Sort based on the dependencies
        return networkx.topological_sort(dag)","Point(row=198, column=4)","Point(row=234, column=45)",DependencyManager,classic/benchmark/agbenchmark/utils/dependencies/main.py
DependencyManager.register_result,function,Register a result of a test.,"def register_result(self, item: Item, result: Any) -> None:
        """"""Register a result of a test.""""""
        nodeid = clean_nodeid(item.nodeid)
        self.results[nodeid].register_result(result)","Point(row=236, column=4)","Point(row=239, column=52)",DependencyManager,classic/benchmark/agbenchmark/utils/dependencies/main.py
DependencyManager.get_failed,function,Get a list of unfulfilled dependencies for a test.,"def get_failed(self, item: Item) -> Any:
        """"""Get a list of unfulfilled dependencies for a test.""""""
        nodeid = clean_nodeid(item.nodeid)
        failed = []
        for dependency in self.dependencies[nodeid].dependencies:
            result = self.results[dependency]
            if not result.success:
                failed.append(dependency)
        return failed","Point(row=241, column=4)","Point(row=249, column=21)",DependencyManager,classic/benchmark/agbenchmark/utils/dependencies/main.py
DependencyManager.get_missing,function,Get a list of missing dependencies for a test.,"def get_missing(self, item: Item) -> Any:
        """"""Get a list of missing dependencies for a test.""""""
        nodeid = clean_nodeid(item.nodeid)
        return self.dependencies[nodeid].unresolved","Point(row=251, column=4)","Point(row=254, column=51)",DependencyManager,classic/benchmark/agbenchmark/utils/dependencies/main.py
SingletonReportManager,class,,"class SingletonReportManager:
    instance = None

    INFO_MANAGER: ""SessionReportManager""
    REGRESSION_MANAGER: ""RegressionTestsTracker""
    SUCCESS_RATE_TRACKER: ""SuccessRatesTracker""

    def __new__(cls):
        if not cls.instance:
            cls.instance = super(SingletonReportManager, cls).__new__(cls)

            agent_benchmark_config = AgentBenchmarkConfig.load()
            benchmark_start_time_dt = datetime.now(
                timezone.utc
            )  # or any logic to fetch the datetime

            # Make the Managers class attributes
            cls.INFO_MANAGER = SessionReportManager(
                agent_benchmark_config.get_report_dir(benchmark_start_time_dt)
                / ""report.json"",
                benchmark_start_time_dt,
            )
            cls.REGRESSION_MANAGER = RegressionTestsTracker(
                agent_benchmark_config.regression_tests_file
            )
            cls.SUCCESS_RATE_TRACKER = SuccessRatesTracker(
                agent_benchmark_config.success_rate_file
            )

        return cls.instance

    @classmethod
    def clear_instance(cls):
        cls.instance = None
        del cls.INFO_MANAGER
        del cls.REGRESSION_MANAGER
        del cls.SUCCESS_RATE_TRACKER","Point(row=21, column=0)","Point(row=57, column=36)",,classic/benchmark/agbenchmark/reports/ReportManager.py
SingletonReportManager.__new__,function,,"def __new__(cls):
        if not cls.instance:
            cls.instance = super(SingletonReportManager, cls).__new__(cls)

            agent_benchmark_config = AgentBenchmarkConfig.load()
            benchmark_start_time_dt = datetime.now(
                timezone.utc
            )  # or any logic to fetch the datetime

            # Make the Managers class attributes
            cls.INFO_MANAGER = SessionReportManager(
                agent_benchmark_config.get_report_dir(benchmark_start_time_dt)
                / ""report.json"",
                benchmark_start_time_dt,
            )
            cls.REGRESSION_MANAGER = RegressionTestsTracker(
                agent_benchmark_config.regression_tests_file
            )
            cls.SUCCESS_RATE_TRACKER = SuccessRatesTracker(
                agent_benchmark_config.success_rate_file
            )

        return cls.instance","Point(row=28, column=4)","Point(row=50, column=27)",SingletonReportManager,classic/benchmark/agbenchmark/reports/ReportManager.py
SingletonReportManager.clear_instance,function,,"def clear_instance(cls):
        cls.instance = None
        del cls.INFO_MANAGER
        del cls.REGRESSION_MANAGER
        del cls.SUCCESS_RATE_TRACKER","Point(row=53, column=4)","Point(row=57, column=36)",SingletonReportManager,classic/benchmark/agbenchmark/reports/ReportManager.py
BaseReportManager,class,Abstracts interaction with the regression tests file,"class BaseReportManager:
    """"""Abstracts interaction with the regression tests file""""""

    tests: dict[str, Any]

    def __init__(self, report_file: Path):
        self.report_file = report_file

        self.load()

    def load(self) -> None:
        if not self.report_file.exists():
            self.report_file.parent.mkdir(exist_ok=True)

        try:
            with self.report_file.open(""r"") as f:
                data = json.load(f)
                self.tests = {k: data[k] for k in sorted(data)}
        except FileNotFoundError:
            self.tests = {}
        except json.decoder.JSONDecodeError as e:
            logger.warning(f""Could not parse {self.report_file}: {e}"")
            self.tests = {}

    def save(self) -> None:
        with self.report_file.open(""w"") as f:
            json.dump(self.tests, f, indent=4)

    def remove_test(self, test_name: str) -> None:
        if test_name in self.tests:
            del self.tests[test_name]
            self.save()

    def reset(self) -> None:
        self.tests = {}
        self.save()","Point(row=60, column=0)","Point(row=95, column=19)",,classic/benchmark/agbenchmark/reports/ReportManager.py
BaseReportManager.__init__,function,,"def __init__(self, report_file: Path):
        self.report_file = report_file

        self.load()","Point(row=65, column=4)","Point(row=68, column=19)",BaseReportManager,classic/benchmark/agbenchmark/reports/ReportManager.py
BaseReportManager.load,function,,"def load(self) -> None:
        if not self.report_file.exists():
            self.report_file.parent.mkdir(exist_ok=True)

        try:
            with self.report_file.open(""r"") as f:
                data = json.load(f)
                self.tests = {k: data[k] for k in sorted(data)}
        except FileNotFoundError:
            self.tests = {}
        except json.decoder.JSONDecodeError as e:
            logger.warning(f""Could not parse {self.report_file}: {e}"")
            self.tests = {}","Point(row=70, column=4)","Point(row=82, column=27)",BaseReportManager,classic/benchmark/agbenchmark/reports/ReportManager.py
BaseReportManager.save,function,,"def save(self) -> None:
        with self.report_file.open(""w"") as f:
            json.dump(self.tests, f, indent=4)","Point(row=84, column=4)","Point(row=86, column=46)",BaseReportManager,classic/benchmark/agbenchmark/reports/ReportManager.py
BaseReportManager.remove_test,function,,"def remove_test(self, test_name: str) -> None:
        if test_name in self.tests:
            del self.tests[test_name]
            self.save()","Point(row=88, column=4)","Point(row=91, column=23)",BaseReportManager,classic/benchmark/agbenchmark/reports/ReportManager.py
BaseReportManager.reset,function,,"def reset(self) -> None:
        self.tests = {}
        self.save()","Point(row=93, column=4)","Point(row=95, column=19)",BaseReportManager,classic/benchmark/agbenchmark/reports/ReportManager.py
SessionReportManager,class,Abstracts interaction with the regression tests file,"class SessionReportManager(BaseReportManager):
    """"""Abstracts interaction with the regression tests file""""""

    tests: dict[str, Test]
    report: Report | None = None

    def __init__(self, report_file: Path, benchmark_start_time: datetime):
        super().__init__(report_file)

        self.start_time = time.time()
        self.benchmark_start_time = benchmark_start_time

    def save(self) -> None:
        with self.report_file.open(""w"") as f:
            if self.report:
                f.write(self.report.model_dump_json(indent=4))
            else:
                json.dump(
                    {k: v.model_dump() for k, v in self.tests.items()}, f, indent=4
                )

    def load(self) -> None:
        super().load()

        if ""tests"" in self.tests:
            self.report = Report.model_validate(self.tests)
        else:
            self.tests = {n: Test.model_validate(d) for n, d in self.tests.items()}

    def add_test_report(self, test_name: str, test_report: Test) -> None:
        if self.report:
            raise RuntimeError(""Session report already finalized"")

        if test_name.startswith(""Test""):
            test_name = test_name[4:]
        self.tests[test_name] = test_report

        self.save()

    def finalize_session_report(self, config: AgentBenchmarkConfig) -> None:
        command = "" "".join(sys.argv)

        if self.report:
            raise RuntimeError(""Session report already finalized"")

        self.report = Report(
            command=command.split(os.sep)[-1],
            benchmark_git_commit_sha=""---"",
            agent_git_commit_sha=""---"",
            completion_time=datetime.now(timezone.utc).strftime(
                ""%Y-%m-%dT%H:%M:%S+00:00""
            ),
            benchmark_start_time=self.benchmark_start_time.strftime(
                ""%Y-%m-%dT%H:%M:%S+00:00""
            ),
            metrics=MetricsOverall(
                run_time=str(round(time.time() - self.start_time, 2)) + "" seconds"",
                highest_difficulty=get_highest_success_difficulty(self.tests),
                total_cost=self.get_total_costs(),
            ),
            tests=copy.copy(self.tests),
            config=config.model_dump(exclude={""reports_folder""}, exclude_none=True),
        )

        agent_categories = get_highest_achieved_difficulty_per_category(self.report)
        if len(agent_categories) > 1:
            save_single_radar_chart(
                agent_categories,
                config.get_report_dir(self.benchmark_start_time) / ""radar_chart.png"",
            )

        self.save()

    def get_total_costs(self):
        if self.report:
            tests = self.report.tests
        else:
            tests = self.tests

        total_cost = 0
        all_costs_none = True
        for test_data in tests.values():
            cost = sum(r.cost or 0 for r in test_data.results)

            if cost is not None:  # check if cost is not None
                all_costs_none = False
                total_cost += cost  # add cost to total
        if all_costs_none:
            total_cost = None
        return total_cost","Point(row=98, column=0)","Point(row=187, column=25)",,classic/benchmark/agbenchmark/reports/ReportManager.py
SessionReportManager.__init__,function,,"def __init__(self, report_file: Path, benchmark_start_time: datetime):
        super().__init__(report_file)

        self.start_time = time.time()
        self.benchmark_start_time = benchmark_start_time","Point(row=104, column=4)","Point(row=108, column=56)",SessionReportManager,classic/benchmark/agbenchmark/reports/ReportManager.py
SessionReportManager.save,function,,"def save(self) -> None:
        with self.report_file.open(""w"") as f:
            if self.report:
                f.write(self.report.model_dump_json(indent=4))
            else:
                json.dump(
                    {k: v.model_dump() for k, v in self.tests.items()}, f, indent=4
                )","Point(row=110, column=4)","Point(row=117, column=17)",SessionReportManager,classic/benchmark/agbenchmark/reports/ReportManager.py
SessionReportManager.load,function,,"def load(self) -> None:
        super().load()

        if ""tests"" in self.tests:
            self.report = Report.model_validate(self.tests)
        else:
            self.tests = {n: Test.model_validate(d) for n, d in self.tests.items()}","Point(row=119, column=4)","Point(row=125, column=83)",SessionReportManager,classic/benchmark/agbenchmark/reports/ReportManager.py
SessionReportManager.add_test_report,function,,"def add_test_report(self, test_name: str, test_report: Test) -> None:
        if self.report:
            raise RuntimeError(""Session report already finalized"")

        if test_name.startswith(""Test""):
            test_name = test_name[4:]
        self.tests[test_name] = test_report

        self.save()","Point(row=127, column=4)","Point(row=135, column=19)",SessionReportManager,classic/benchmark/agbenchmark/reports/ReportManager.py
SessionReportManager.finalize_session_report,function,,"def finalize_session_report(self, config: AgentBenchmarkConfig) -> None:
        command = "" "".join(sys.argv)

        if self.report:
            raise RuntimeError(""Session report already finalized"")

        self.report = Report(
            command=command.split(os.sep)[-1],
            benchmark_git_commit_sha=""---"",
            agent_git_commit_sha=""---"",
            completion_time=datetime.now(timezone.utc).strftime(
                ""%Y-%m-%dT%H:%M:%S+00:00""
            ),
            benchmark_start_time=self.benchmark_start_time.strftime(
                ""%Y-%m-%dT%H:%M:%S+00:00""
            ),
            metrics=MetricsOverall(
                run_time=str(round(time.time() - self.start_time, 2)) + "" seconds"",
                highest_difficulty=get_highest_success_difficulty(self.tests),
                total_cost=self.get_total_costs(),
            ),
            tests=copy.copy(self.tests),
            config=config.model_dump(exclude={""reports_folder""}, exclude_none=True),
        )

        agent_categories = get_highest_achieved_difficulty_per_category(self.report)
        if len(agent_categories) > 1:
            save_single_radar_chart(
                agent_categories,
                config.get_report_dir(self.benchmark_start_time) / ""radar_chart.png"",
            )

        self.save()","Point(row=137, column=4)","Point(row=169, column=19)",SessionReportManager,classic/benchmark/agbenchmark/reports/ReportManager.py
SessionReportManager.get_total_costs,function,,"def get_total_costs(self):
        if self.report:
            tests = self.report.tests
        else:
            tests = self.tests

        total_cost = 0
        all_costs_none = True
        for test_data in tests.values():
            cost = sum(r.cost or 0 for r in test_data.results)

            if cost is not None:  # check if cost is not None
                all_costs_none = False
                total_cost += cost  # add cost to total
        if all_costs_none:
            total_cost = None
        return total_cost","Point(row=171, column=4)","Point(row=187, column=25)",SessionReportManager,classic/benchmark/agbenchmark/reports/ReportManager.py
RegressionTestsTracker,class,Abstracts interaction with the regression tests file,"class RegressionTestsTracker(BaseReportManager):
    """"""Abstracts interaction with the regression tests file""""""

    tests: dict[str, dict]

    def add_test(self, test_name: str, test_details: dict) -> None:
        if test_name.startswith(""Test""):
            test_name = test_name[4:]

        self.tests[test_name] = test_details
        self.save()

    def has_regression_test(self, test_name: str) -> bool:
        return self.tests.get(test_name) is not None","Point(row=190, column=0)","Point(row=203, column=52)",,classic/benchmark/agbenchmark/reports/ReportManager.py
RegressionTestsTracker.add_test,function,,"def add_test(self, test_name: str, test_details: dict) -> None:
        if test_name.startswith(""Test""):
            test_name = test_name[4:]

        self.tests[test_name] = test_details
        self.save()","Point(row=195, column=4)","Point(row=200, column=19)",RegressionTestsTracker,classic/benchmark/agbenchmark/reports/ReportManager.py
RegressionTestsTracker.has_regression_test,function,,"def has_regression_test(self, test_name: str) -> bool:
        return self.tests.get(test_name) is not None","Point(row=202, column=4)","Point(row=203, column=52)",RegressionTestsTracker,classic/benchmark/agbenchmark/reports/ReportManager.py
SuccessRatesTracker,class,Abstracts interaction with the regression tests file,"class SuccessRatesTracker(BaseReportManager):
    """"""Abstracts interaction with the regression tests file""""""

    tests: dict[str, list[bool | None]]

    def update(self, test_name: str, success_history: list[bool | None]) -> None:
        if test_name.startswith(""Test""):
            test_name = test_name[4:]

        self.tests[test_name] = success_history
        self.save()","Point(row=206, column=0)","Point(row=216, column=19)",,classic/benchmark/agbenchmark/reports/ReportManager.py
SuccessRatesTracker.update,function,,"def update(self, test_name: str, success_history: list[bool | None]) -> None:
        if test_name.startswith(""Test""):
            test_name = test_name[4:]

        self.tests[test_name] = success_history
        self.save()","Point(row=211, column=4)","Point(row=216, column=19)",SuccessRatesTracker,classic/benchmark/agbenchmark/reports/ReportManager.py
get_and_update_success_history,function,,"def get_and_update_success_history(
    test_name: str, success: bool | None
) -> list[bool | None]:
    mock = os.getenv(""IS_MOCK"")  # Check if --mock is in sys.argv

    prev_test_results = SingletonReportManager().SUCCESS_RATE_TRACKER.tests.get(
        test_name, []
    )

    if not mock:
        # only add if it's an actual test
        prev_test_results.append(success)
        SingletonReportManager().SUCCESS_RATE_TRACKER.update(
            test_name, prev_test_results
        )

    return prev_test_results","Point(row=19, column=0)","Point(row=35, column=28)",,classic/benchmark/agbenchmark/reports/reports.py
update_regression_tests,function,,"def update_regression_tests(
    prev_test_results: list[bool | None],
    test_report: Test,
    test_name: str,
) -> None:
    if len(prev_test_results) >= 3 and prev_test_results[-3:] == [True, True, True]:
        # if the last 3 tests were successful, add to the regression tests
        test_report.metrics.is_regression = True
        SingletonReportManager().REGRESSION_MANAGER.add_test(
            test_name, test_report.model_dump(include={""difficulty"", ""data_path""})
        )","Point(row=38, column=0)","Point(row=48, column=9)",,classic/benchmark/agbenchmark/reports/reports.py
make_empty_test_report,function,,"def make_empty_test_report(
    challenge_info: ChallengeInfo,
) -> Test:
    difficulty = challenge_info.difficulty
    if isinstance(difficulty, DifficultyLevel):
        difficulty = difficulty.value

    return Test(
        category=[c.value for c in challenge_info.category],
        difficulty=difficulty,
        data_path=challenge_info.source_uri,
        description=challenge_info.description or """",
        task=challenge_info.task,
        answer=challenge_info.reference_answer or """",
        metrics=TestMetrics(attempted=False, is_regression=False),
        results=[],
    )","Point(row=51, column=0)","Point(row=67, column=5)",,classic/benchmark/agbenchmark/reports/reports.py
add_test_result_to_report,function,,"def add_test_result_to_report(
    test_report: Test,
    item: pytest.Item,
    call: pytest.CallInfo,
    config: AgentBenchmarkConfig,
) -> None:
    user_properties: dict = dict(item.user_properties)
    test_name: str = user_properties.get(""test_name"", """")

    mock = os.getenv(""IS_MOCK"")  # Check if --mock is in sys.argv

    if call.excinfo:
        if not mock:
            SingletonReportManager().REGRESSION_MANAGER.remove_test(test_name)

        test_report.metrics.attempted = call.excinfo.typename != ""Skipped""
    else:
        test_report.metrics.attempted = True

    try:
        test_report.results.append(
            TestResult(
                success=call.excinfo is None,
                run_time=f""{str(round(call.duration, 3))} seconds"",
                fail_reason=(
                    str(call.excinfo.value) if call.excinfo is not None else None
                ),
                reached_cutoff=user_properties.get(""timed_out"", False),
                n_steps=user_properties.get(""n_steps""),
                steps=user_properties.get(""steps"", []),
                cost=user_properties.get(""agent_task_cost""),
            )
        )
        test_report.metrics.success_percentage = (
            sum(r.success or False for r in test_report.results)
            / len(test_report.results)
            * 100
        )
    except ValidationError:
        if call.excinfo:
            logger.error(
                ""Validation failed on TestResult; ""
                f""call.excinfo = {repr(call.excinfo)};\n{call.excinfo.getrepr()})""
            )
        raise

    prev_test_results: list[bool | None] = get_and_update_success_history(
        test_name, test_report.results[-1].success
    )

    update_regression_tests(prev_test_results, test_report, test_name)

    if test_report and test_name:
        # if ""--mock"" not in sys.argv and os.environ.get(""HELICONE_API_KEY""):
        #     logger.debug(""Getting cost from Helicone"")
        #     test_report.metrics.cost = get_data_from_helicone(test_name)
        #     logger.debug(f""Cost: {cost}"")

        if not mock:
            update_challenges_already_beaten(
                config.challenges_already_beaten_file, test_report, test_name
            )

        SingletonReportManager().INFO_MANAGER.add_test_report(test_name, test_report)","Point(row=70, column=0)","Point(row=133, column=85)",,classic/benchmark/agbenchmark/reports/reports.py
update_challenges_already_beaten,function,,"def update_challenges_already_beaten(
    challenges_already_beaten_file: Path, test_report: Test, test_name: str
) -> None:
    current_run_successful = any(r.success for r in test_report.results)
    try:
        with open(challenges_already_beaten_file, ""r"") as f:
            challenges_beaten_before = json.load(f)
    except FileNotFoundError:
        challenges_beaten_before = {}

    has_ever_been_beaten = challenges_beaten_before.get(test_name)
    challenges_beaten_before[test_name] = has_ever_been_beaten or current_run_successful

    with open(challenges_already_beaten_file, ""w"") as f:
        json.dump(challenges_beaten_before, f, indent=4)","Point(row=136, column=0)","Point(row=150, column=56)",,classic/benchmark/agbenchmark/reports/reports.py
session_finish,function,,"def session_finish(agbenchmark_config: AgentBenchmarkConfig) -> None:
    SingletonReportManager().INFO_MANAGER.finalize_session_report(agbenchmark_config)
    SingletonReportManager().REGRESSION_MANAGER.save()
    SingletonReportManager().SUCCESS_RATE_TRACKER.save()","Point(row=153, column=0)","Point(row=156, column=56)",,classic/benchmark/agbenchmark/reports/reports.py
get_last_subdirectory,function,,"def get_last_subdirectory(directory_path: str) -> str | None:
    # Get all subdirectories in the directory
    subdirs = [
        os.path.join(directory_path, name)
        for name in os.listdir(directory_path)
        if os.path.isdir(os.path.join(directory_path, name))
    ]

    # Sort the subdirectories by creation time
    subdirs.sort(key=os.path.getctime)

    # Return the last subdirectory in the list
    return subdirs[-1] if subdirs else None","Point(row=3, column=0)","Point(row=15, column=43)",,classic/benchmark/agbenchmark/reports/processing/get_files.py
get_latest_report_from_agent_directories,function,,"def get_latest_report_from_agent_directories(
    directory_path: str,
) -> list[tuple[os.DirEntry[str], str]]:
    latest_reports = []

    for subdir in os.scandir(directory_path):
        if subdir.is_dir():
            # Get the most recently created subdirectory within this agent's directory
            latest_subdir = get_last_subdirectory(subdir.path)
            if latest_subdir is not None:
                # Look for 'report.json' in the subdirectory
                report_file = os.path.join(latest_subdir, ""report.json"")
                if os.path.isfile(report_file):
                    latest_reports.append((subdir, report_file))

    return latest_reports","Point(row=18, column=0)","Point(row=33, column=25)",,classic/benchmark/agbenchmark/reports/processing/get_files.py
get_reports_data,function,,"def get_reports_data(report_path: str) -> dict[str, Any]:
    latest_files = get_latest_report_from_agent_directories(report_path)

    reports_data = {}

    if latest_files is None:
        raise Exception(""No files found in the reports directory"")

    # This will print the latest file in each s
    # ubdirectory and add to the files_data dictionary
    for subdir, file in latest_files:
        subdir_name = os.path.basename(os.path.normpath(subdir))
        with open(Path(subdir) / file, ""r"") as f:
            # Load the JSON data from the file
            json_data = json.load(f)
            converted_data = Report.model_validate(json_data)
            # get the last directory name in the path as key
            reports_data[subdir_name] = converted_data

    return reports_data","Point(row=15, column=0)","Point(row=34, column=23)",,classic/benchmark/agbenchmark/reports/processing/process_report.py
get_highest_achieved_difficulty_per_category,function,,"def get_highest_achieved_difficulty_per_category(report: Report) -> dict[str, Any]:
    categories: dict[str, Any] = {}

    for _, test_data in report.tests.items():
        for category in test_data.category:
            if category in (""interface"", ""iterate"", ""product_advisor""):
                continue
            categories.setdefault(category, 0)
            if (
                test_data.results
                and all(r.success for r in test_data.results)
                and test_data.difficulty
            ):
                num_dif = STRING_DIFFICULTY_MAP[test_data.difficulty]
                if num_dif > categories[category]:
                    categories[category] = num_dif

    return categories","Point(row=37, column=0)","Point(row=54, column=21)",,classic/benchmark/agbenchmark/reports/processing/process_report.py
all_agent_categories,function,,"def all_agent_categories(reports_data: dict[str, Any]) -> dict[str, Any]:
    all_categories: dict[str, Any] = {}

    for name, report in reports_data.items():
        categories = get_highest_achieved_difficulty_per_category(report)
        if categories:  # only add to all_categories if categories is not empty
            logger.debug(f""Adding {name}: {categories}"")
            all_categories[name] = categories

    return all_categories","Point(row=57, column=0)","Point(row=66, column=25)",,classic/benchmark/agbenchmark/reports/processing/process_report.py
TaskInfo,class,,"class TaskInfo(BaseModel):
    data_path: str
    is_regression: bool | None
    answer: str
    description: str
    category: list[str]
    task: str","Point(row=8, column=0)","Point(row=14, column=13)",,classic/benchmark/agbenchmark/reports/processing/report_types_v2.py
RepositoryInfo,class,,"class RepositoryInfo(BaseModel):
    repo_url: str | None = None
    team_name: str | None = None
    agent_git_commit_sha: str | None = None
    benchmark_git_commit_sha: str | None = None","Point(row=17, column=0)","Point(row=21, column=47)",,classic/benchmark/agbenchmark/reports/processing/report_types_v2.py
Metrics,class,,"class Metrics(BaseModel):
    cost: float | None = None
    success: bool
    attempted: bool
    difficulty: str | None = None
    run_time: str | None = None
    fail_reason: str | None = None
    success_percentage: float | None = None","Point(row=24, column=0)","Point(row=31, column=43)",,classic/benchmark/agbenchmark/reports/processing/report_types_v2.py
RunDetails,class,,"class RunDetails(BaseModel):
    test_name: str
    run_id: str | None = None
    command: str
    completion_time: str | None = None
    benchmark_start_time: Annotated[str, StringConstraints(pattern=datetime_format)]","Point(row=34, column=0)","Point(row=39, column=84)",,classic/benchmark/agbenchmark/reports/processing/report_types_v2.py
BenchmarkRun,class,,"class BenchmarkRun(BaseModel):
    repository_info: RepositoryInfo
    run_details: RunDetails
    task_info: TaskInfo
    metrics: Metrics
    reached_cutoff: bool | None = None
    config: dict[str, str | dict[str, str]]","Point(row=42, column=0)","Point(row=48, column=43)",,classic/benchmark/agbenchmark/reports/processing/report_types_v2.py
TestResult,class,Result details for a single run of a test/challenge.,"class TestResult(BaseModel):
    """"""Result details for a single run of a test/challenge.""""""

    success: bool | None = None
    """"""Whether the run was successful""""""
    run_time: str | None = None
    """"""The (formatted) duration of the run""""""
    fail_reason: str | None = None
    """"""If applicable, the reason why the run was not successful""""""
    reached_cutoff: bool | None = None  # None if in progress
    """"""Whether the run had to be stopped due to reaching the timeout""""""
    n_steps: int | None = None
    """"""The number of steps executed by the agent""""""
    steps: list[Step] = []
    """"""The steps generated by the agent""""""
    cost: float | None = None
    """"""The (known) cost incurred by the run, e.g. from using paid LLM APIs""""""

    @field_validator(""fail_reason"")
    def success_xor_fail_reason(cls, value, info: ValidationInfo):
        if bool(value) == bool(info.data[""success""]):
            logger.error(
                ""Error validating `success ^ fail_reason` on TestResult: ""
                f""success = {repr(info.data['success'])}; ""
                f""fail_reason = {repr(value)}""
            )
        if value:
            success = info.data[""success""]
            assert not success, ""fail_reason must only be specified if success=False""
        else:
            assert info.data[""success""], ""fail_reason is required if success=False""
        return value","Point(row=20, column=0)","Point(row=51, column=20)",,classic/benchmark/agbenchmark/reports/processing/report_types.py
TestResult.success_xor_fail_reason,function,,"def success_xor_fail_reason(cls, value, info: ValidationInfo):
        if bool(value) == bool(info.data[""success""]):
            logger.error(
                ""Error validating `success ^ fail_reason` on TestResult: ""
                f""success = {repr(info.data['success'])}; ""
                f""fail_reason = {repr(value)}""
            )
        if value:
            success = info.data[""success""]
            assert not success, ""fail_reason must only be specified if success=False""
        else:
            assert info.data[""success""], ""fail_reason is required if success=False""
        return value","Point(row=39, column=4)","Point(row=51, column=20)",TestResult,classic/benchmark/agbenchmark/reports/processing/report_types.py
TestMetrics,class,"
    Result metrics for a set of runs for a test/challenge. Should be an aggregate of all
    results for the same test/challenge within a benchmarking session.
","class TestMetrics(BaseModel):
    """"""
    Result metrics for a set of runs for a test/challenge. Should be an aggregate of all
    results for the same test/challenge within a benchmarking session.
    """"""

    attempted: bool
    """"""Whether the challenge was attempted during this session""""""
    is_regression: bool
    """"""Whether the challenge was considered a regression test at the time of running""""""
    success_percentage: float | None = Field(default=None, alias=""success_%"")
    """"""Success rate (0-100) for this challenge within the session""""""","Point(row=54, column=0)","Point(row=65, column=68)",,classic/benchmark/agbenchmark/reports/processing/report_types.py
MetricsOverall,class,Global metrics concerning a benchmarking session,"class MetricsOverall(BaseModel):
    """"""Global metrics concerning a benchmarking session""""""

    run_time: str
    """"""Duration from beginning to end of the session""""""
    highest_difficulty: str
    """"""
    Difficulty of the most difficult challenge that succeeded at least once this session
    """"""
    total_cost: float | None = None
    """"""Total known cost of the session""""""","Point(row=68, column=0)","Point(row=78, column=41)",,classic/benchmark/agbenchmark/reports/processing/report_types.py
Test,class,,"class Test(BaseModel):
    category: List[str]
    difficulty: str | None
    data_path: str
    description: str
    task: str
    answer: str
    metrics: TestMetrics
    results: list[TestResult]
    metadata: dict[str, Any] | None = Field(default_factory=dict)","Point(row=81, column=0)","Point(row=90, column=65)",,classic/benchmark/agbenchmark/reports/processing/report_types.py
ReportBase,class,,"class ReportBase(BaseModel):
    command: str
    completion_time: str | None = None
    benchmark_start_time: Annotated[str, StringConstraints(pattern=datetime_format)]
    metrics: MetricsOverall
    config: Dict[str, str | dict[str, str]]
    agent_git_commit_sha: str | None = None
    benchmark_git_commit_sha: str | None = None
    repo_url: str | None = None","Point(row=93, column=0)","Point(row=101, column=31)",,classic/benchmark/agbenchmark/reports/processing/report_types.py
Report,class,,"class Report(ReportBase):
    tests: Dict[str, Test]","Point(row=104, column=0)","Point(row=105, column=26)",,classic/benchmark/agbenchmark/reports/processing/report_types.py
save_combined_radar_chart,function,,"def save_combined_radar_chart(
    categories: dict[str, Any], save_path: str | Path
) -> None:
    categories = {k: v for k, v in categories.items() if v}
    if not all(categories.values()):
        raise Exception(""No data to plot"")
    labels = np.array(
        list(next(iter(categories.values())).keys())
    )  # We use the first category to get the keys
    num_vars = len(labels)
    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
    angles += angles[
        :1
    ]  # Add the first angle to the end of the list to ensure the polygon is closed

    # Create radar chart
    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
    ax.set_theta_offset(np.pi / 2)  # type: ignore
    ax.set_theta_direction(-1)  # type: ignore
    ax.spines[""polar""].set_visible(False)  # Remove border

    cmap = plt.cm.get_cmap(""nipy_spectral"", len(categories))  # type: ignore

    colors = [cmap(i) for i in range(len(categories))]

    for i, (cat_name, cat_values) in enumerate(
        categories.items()
    ):  # Iterating through each category (series)
        values = np.array(list(cat_values.values()))
        values = np.concatenate((values, values[:1]))  # Ensure the polygon is closed

        ax.fill(angles, values, color=colors[i], alpha=0.25)  # Draw the filled polygon
        ax.plot(angles, values, color=colors[i], linewidth=2)  # Draw polygon
        ax.plot(
            angles,
            values,
            ""o"",
            color=""white"",
            markersize=7,
            markeredgecolor=colors[i],
            markeredgewidth=2,
        )  # Draw points

        # Draw legend
        ax.legend(
            handles=[
                mpatches.Patch(color=color, label=cat_name, alpha=0.25)
                for cat_name, color in zip(categories.keys(), colors)
            ],
            loc=""upper left"",
            bbox_to_anchor=(0.7, 1.3),
        )

        # Adjust layout to make room for the legend
        plt.tight_layout()

    lines, labels = plt.thetagrids(
        np.degrees(angles[:-1]), (list(next(iter(categories.values())).keys()))
    )  # We use the first category to get the keys

    highest_score = 7

    # Set y-axis limit to 7
    ax.set_ylim(top=highest_score)

    # Move labels away from the plot
    for label in labels:
        label.set_position(
            (label.get_position()[0], label.get_position()[1] + -0.05)
        )  # adjust 0.1 as needed

    # Move radial labels away from the plot
    ax.set_rlabel_position(180)  # type: ignore

    ax.set_yticks([])  # Remove default yticks

    # Manually create gridlines
    for y in np.arange(0, highest_score + 1, 1):
        if y != highest_score:
            ax.plot(
                angles, [y] * len(angles), color=""gray"", linewidth=0.5, linestyle="":""
            )
        # Add labels for manually created gridlines
        ax.text(
            angles[0],
            y + 0.2,
            str(int(y)),
            color=""black"",
            size=9,
            horizontalalignment=""center"",
            verticalalignment=""center"",
        )

    plt.savefig(save_path, dpi=300)  # Save the figure as a PNG file
    plt.close()  # Close the figure to free up memory","Point(row=9, column=0)","Point(row=103, column=53)",,classic/benchmark/agbenchmark/reports/processing/graphs.py
save_single_radar_chart,function,,"def save_single_radar_chart(
    category_dict: dict[str, int], save_path: str | Path
) -> None:
    labels = np.array(list(category_dict.keys()))
    values = np.array(list(category_dict.values()))

    num_vars = len(labels)

    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()

    angles += angles[:1]
    values = np.concatenate((values, values[:1]))

    colors = [""#1f77b4""]

    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
    ax.set_theta_offset(np.pi / 2)  # type: ignore
    ax.set_theta_direction(-1)  # type: ignore

    ax.spines[""polar""].set_visible(False)

    lines, labels = plt.thetagrids(
        np.degrees(angles[:-1]), (list(category_dict.keys()))
    )

    highest_score = 7

    # Set y-axis limit to 7
    ax.set_ylim(top=highest_score)

    for label in labels:
        label.set_position((label.get_position()[0], label.get_position()[1] + -0.05))

    ax.fill(angles, values, color=colors[0], alpha=0.25)
    ax.plot(angles, values, color=colors[0], linewidth=2)

    for i, (angle, value) in enumerate(zip(angles, values)):
        ha = ""left""
        if angle in {0, np.pi}:
            ha = ""center""
        elif np.pi < angle < 2 * np.pi:
            ha = ""right""
        ax.text(
            angle,
            value - 0.5,
            f""{value}"",
            size=10,
            horizontalalignment=ha,
            verticalalignment=""center"",
            color=""black"",
        )

    ax.set_yticklabels([])

    ax.set_yticks([])

    if values.size == 0:
        return

    for y in np.arange(0, highest_score, 1):
        ax.plot(angles, [y] * len(angles), color=""gray"", linewidth=0.5, linestyle="":"")

    for angle, value in zip(angles, values):
        ax.plot(
            angle,
            value,
            ""o"",
            color=""white"",
            markersize=7,
            markeredgecolor=colors[0],
            markeredgewidth=2,
        )

    plt.savefig(save_path, dpi=300)  # Save the figure as a PNG file
    plt.close()  # Close the figure to free up memory","Point(row=106, column=0)","Point(row=180, column=53)",,classic/benchmark/agbenchmark/reports/processing/graphs.py
save_combined_bar_chart,function,,"def save_combined_bar_chart(categories: dict[str, Any], save_path: str | Path) -> None:
    if not all(categories.values()):
        raise Exception(""No data to plot"")

    # Convert dictionary to DataFrame
    df = pd.DataFrame(categories)

    # Create a grouped bar chart
    df.plot(kind=""bar"", figsize=(10, 7))

    plt.title(""Performance by Category for Each Agent"")
    plt.xlabel(""Category"")
    plt.ylabel(""Performance"")

    plt.savefig(save_path, dpi=300)  # Save the figure as a PNG file
    plt.close()  # Close the figure to free up memory","Point(row=183, column=0)","Point(row=198, column=53)",,classic/benchmark/agbenchmark/reports/processing/graphs.py
generate_combined_chart,function,,"def generate_combined_chart() -> None:
    all_agents_path = Path(__file__).parent.parent.parent.parent / ""reports""

    combined_charts_folder = all_agents_path / ""combined_charts""

    reports_data = get_reports_data(str(all_agents_path))

    categories = all_agent_categories(reports_data)

    # Count the number of directories in this directory
    num_dirs = len([f for f in combined_charts_folder.iterdir() if f.is_dir()])

    run_charts_folder = combined_charts_folder / f""run{num_dirs + 1}""

    if not os.path.exists(run_charts_folder):
        os.makedirs(run_charts_folder)

    info_data = {
        report_name: data.benchmark_start_time
        for report_name, data in reports_data.items()
        if report_name in categories
    }
    with open(Path(run_charts_folder) / ""run_info.json"", ""w"") as f:
        json.dump(info_data, f)

    save_combined_radar_chart(categories, Path(run_charts_folder) / ""radar_chart.png"")
    save_combined_bar_chart(categories, Path(run_charts_folder) / ""bar_chart.png"")","Point(row=14, column=0)","Point(row=40, column=82)",,classic/benchmark/agbenchmark/reports/processing/gen_combined_chart.py
curriculum_graph,function,,"def curriculum_graph():
    return {
        ""edges"": [
            {""from"": ""Calculus"", ""to"": ""Advanced Calculus""},
            {""from"": ""Algebra"", ""to"": ""Calculus""},
            {""from"": ""Biology"", ""to"": ""Advanced Biology""},
            {""from"": ""World History"", ""to"": ""Modern History""},
        ],
        ""nodes"": [
            {""data"": {""category"": [""math""]}, ""id"": ""Calculus"", ""label"": ""Calculus""},
            {
                ""data"": {""category"": [""math""]},
                ""id"": ""Advanced Calculus"",
                ""label"": ""Advanced Calculus"",
            },
            {""data"": {""category"": [""math""]}, ""id"": ""Algebra"", ""label"": ""Algebra""},
            {""data"": {""category"": [""science""]}, ""id"": ""Biology"", ""label"": ""Biology""},
            {
                ""data"": {""category"": [""science""]},
                ""id"": ""Advanced Biology"",
                ""label"": ""Advanced Biology"",
            },
            {
                ""data"": {""category"": [""history""]},
                ""id"": ""World History"",
                ""label"": ""World History"",
            },
            {
                ""data"": {""category"": [""history""]},
                ""id"": ""Modern History"",
                ""label"": ""Modern History"",
            },
        ],
    }","Point(row=6, column=0)","Point(row=39, column=5)",,classic/benchmark/tests/test_extract_subgraph.py
test_dfs_category_math,function,,"def test_dfs_category_math(curriculum_graph):
    result_graph = extract_subgraph_based_on_category(curriculum_graph, ""math"")

    # Expected nodes: Algebra, Calculus, Advanced Calculus
    # Expected edges: Algebra->Calculus, Calculus->Advanced Calculus

    expected_nodes = [""Algebra"", ""Calculus"", ""Advanced Calculus""]
    expected_edges = [
        {""from"": ""Algebra"", ""to"": ""Calculus""},
        {""from"": ""Calculus"", ""to"": ""Advanced Calculus""},
    ]

    assert set(node[""id""] for node in result_graph[""nodes""]) == set(expected_nodes)
    assert set((edge[""from""], edge[""to""]) for edge in result_graph[""edges""]) == set(
        (edge[""from""], edge[""to""]) for edge in expected_edges
    )","Point(row=52, column=0)","Point(row=67, column=5)",,classic/benchmark/tests/test_extract_subgraph.py
test_extract_subgraph_math_category,function,,"def test_extract_subgraph_math_category():
    subgraph = extract_subgraph_based_on_category(graph_example, ""math"")
    assert set(
        (node[""id""], tuple(node[""data""][""category""])) for node in subgraph[""nodes""]
    ) == set(
        (node[""id""], tuple(node[""data""][""category""])) for node in graph_example[""nodes""]
    )
    assert set((edge[""from""], edge[""to""]) for edge in subgraph[""edges""]) == set(
        (edge[""from""], edge[""to""]) for edge in graph_example[""edges""]
    )","Point(row=70, column=0)","Point(row=79, column=5)",,classic/benchmark/tests/test_extract_subgraph.py
test_extract_subgraph_non_existent_category,function,,"def test_extract_subgraph_non_existent_category():
    result_graph = extract_subgraph_based_on_category(graph_example, ""toto"")

    # Asserting that the result graph has no nodes and no edges
    assert len(result_graph[""nodes""]) == 0
    assert len(result_graph[""edges""]) == 0","Point(row=82, column=0)","Point(row=87, column=42)",,classic/benchmark/tests/test_extract_subgraph.py
test_is_circular,function,,"def test_is_circular():
    cyclic_graph = {
        ""nodes"": [
            {""id"": ""A"", ""data"": {""category"": []}},
            {""id"": ""B"", ""data"": {""category"": []}},
            {""id"": ""C"", ""data"": {""category"": []}},
            {""id"": ""D"", ""data"": {""category"": []}},  # New node
        ],
        ""edges"": [
            {""from"": ""A"", ""to"": ""B""},
            {""from"": ""B"", ""to"": ""C""},
            {""from"": ""C"", ""to"": ""D""},
            {""from"": ""D"", ""to"": ""A""},  # This edge creates a cycle
        ],
    }

    result = is_circular(cyclic_graph)
    assert result is not None, ""Expected a cycle, but none was detected""
    assert all(
        (
            (result[i], result[i + 1])
            in [(x[""from""], x[""to""]) for x in cyclic_graph[""edges""]]
        )
        for i in range(len(result) - 1)
    ), ""The detected cycle path is not part of the graph's edges""","Point(row=3, column=0)","Point(row=27, column=65)",,classic/benchmark/tests/test_is_circular.py
test_is_not_circular,function,,"def test_is_not_circular():
    acyclic_graph = {
        ""nodes"": [
            {""id"": ""A"", ""data"": {""category"": []}},
            {""id"": ""B"", ""data"": {""category"": []}},
            {""id"": ""C"", ""data"": {""category"": []}},
            {""id"": ""D"", ""data"": {""category"": []}},  # New node
        ],
        ""edges"": [
            {""from"": ""A"", ""to"": ""B""},
            {""from"": ""B"", ""to"": ""C""},
            {""from"": ""C"", ""to"": ""D""},
            # No back edge from D to any node, so it remains acyclic
        ],
    }

    assert is_circular(acyclic_graph) is None, ""Detected a cycle in an acyclic graph""","Point(row=30, column=0)","Point(row=46, column=85)",,classic/benchmark/tests/test_is_circular.py
test_get_roots,function,,"def test_get_roots():
    graph = {
        ""nodes"": [
            {""id"": ""A"", ""data"": {""category"": []}},
            {""id"": ""B"", ""data"": {""category"": []}},
            {""id"": ""C"", ""data"": {""category"": []}},
            {""id"": ""D"", ""data"": {""category"": []}},
        ],
        ""edges"": [
            {""from"": ""A"", ""to"": ""B""},
            {""from"": ""B"", ""to"": ""C""},
        ],
    }

    result = get_roots(graph)
    assert set(result) == {
        ""A"",
        ""D"",
    }, f""Expected roots to be 'A' and 'D', but got {result}""","Point(row=3, column=0)","Point(row=21, column=60)",,classic/benchmark/tests/test_get_roots.py
test_no_roots,function,,"def test_no_roots():
    fully_connected_graph = {
        ""nodes"": [
            {""id"": ""A"", ""data"": {""category"": []}},
            {""id"": ""B"", ""data"": {""category"": []}},
            {""id"": ""C"", ""data"": {""category"": []}},
        ],
        ""edges"": [
            {""from"": ""A"", ""to"": ""B""},
            {""from"": ""B"", ""to"": ""C""},
            {""from"": ""C"", ""to"": ""A""},
        ],
    }

    result = get_roots(fully_connected_graph)
    assert not result, ""Expected no roots, but found some""","Point(row=24, column=0)","Point(row=39, column=58)",,classic/benchmark/tests/test_get_roots.py
test_entire_workflow,function,,"def test_entire_workflow(
    eval_id: str,
    input_text: str,
    expected_artifact_length: int,
    test_name: str,
    should_be_successful: bool,
):
    task_request = {""eval_id"": eval_id, ""input"": input_text}
    response = requests.get(f""{URL_AGENT}/agent/tasks"")
    task_count_before = response.json()[""pagination""][""total_items""]
    # First POST request
    task_response_benchmark = requests.post(
        URL_BENCHMARK + ""/agent/tasks"", json=task_request
    )
    response = requests.get(f""{URL_AGENT}/agent/tasks"")
    task_count_after = response.json()[""pagination""][""total_items""]
    assert task_count_after == task_count_before + 1

    timestamp_after_task_eval_created = datetime.datetime.now(datetime.timezone.utc)
    time.sleep(1.1)  # To make sure the 2 timestamps to compare are different
    assert task_response_benchmark.status_code == 200
    task_response_benchmark = task_response_benchmark.json()
    assert task_response_benchmark[""input""] == input_text

    task_response_benchmark_id = task_response_benchmark[""task_id""]

    response_task_agent = requests.get(
        f""{URL_AGENT}/agent/tasks/{task_response_benchmark_id}""
    )
    assert response_task_agent.status_code == 200
    response_task_agent = response_task_agent.json()
    assert len(response_task_agent[""artifacts""]) == expected_artifact_length

    step_request = {""input"": input_text}

    step_response = requests.post(
        URL_BENCHMARK + ""/agent/tasks/"" + task_response_benchmark_id + ""/steps"",
        json=step_request,
    )
    assert step_response.status_code == 200
    step_response = step_response.json()
    assert step_response[""is_last""] is True  # Assuming is_last is always True

    eval_response = requests.post(
        URL_BENCHMARK + ""/agent/tasks/"" + task_response_benchmark_id + ""/evaluations"",
        json={},
    )
    assert eval_response.status_code == 200
    eval_response = eval_response.json()
    print(""eval_response"")
    print(eval_response)
    assert eval_response[""run_details""][""test_name""] == test_name
    assert eval_response[""metrics""][""success""] == should_be_successful
    benchmark_start_time = datetime.datetime.fromisoformat(
        eval_response[""run_details""][""benchmark_start_time""]
    )

    assert benchmark_start_time < timestamp_after_task_eval_created","Point(row=35, column=0)","Point(row=92, column=67)",,classic/benchmark/tests/test_benchmark_workflow.py
process_test,function,Recursive function to process test data.,"def process_test(
    test_name: str, test_info: dict, agent_name: str, common_data: dict
) -> None:
    """"""Recursive function to process test data.""""""
    parts = test_name.split(""_"", 1)  # Split by underscore only once
    test_suite = parts[0] if len(parts) > 1 else None

    # transform array into string with | as separator
    separator = ""|""
    categories = separator.join(
        test_info.get(""category"", []),
    )

    row = {
        ""Agent"": agent_name,
        ""Command"": common_data.get(""command"", """"),
        ""Completion Time"": common_data.get(""completion_time"", """"),
        ""Benchmark Start Time"": common_data.get(""benchmark_start_time"", """"),
        ""Total Run Time"": common_data.get(""metrics"", {}).get(""run_time"", """"),
        ""Highest Difficulty"": common_data.get(""metrics"", {}).get(
            ""highest_difficulty"", """"
        ),
        ""Workspace"": common_data.get(""config"", {}).get(""workspace"", """"),
        ""Test Name"": test_name,
        ""Data Path"": test_info.get(""data_path"", """"),
        ""Is Regression"": test_info.get(""is_regression"", """"),
        ""Difficulty"": test_info.get(""metrics"", {}).get(""difficulty"", """"),
        ""Success"": test_info.get(""metrics"", {}).get(""success"", """"),
        ""Success %"": test_info.get(""metrics"", {}).get(""success_%"", """"),
        ""Non mock success %"": test_info.get(""metrics"", {}).get(
            ""non_mock_success_%"", """"
        ),
        ""Run Time"": test_info.get(""metrics"", {}).get(""run_time"", """"),
        ""Benchmark Git Commit Sha"": common_data.get(""benchmark_git_commit_sha"", None),
        ""Agent Git Commit Sha"": common_data.get(""agent_git_commit_sha"", None),
        ""Cost"": test_info.get(""metrics"", {}).get(""cost"", """"),
        ""Attempted"": test_info.get(""metrics"", {}).get(""attempted"", """"),
        ""Test Suite"": test_suite,
        ""Category"": categories,
        ""Task"": test_info.get(""task"", """"),
        ""Answer"": test_info.get(""answer"", """"),
        ""Description"": test_info.get(""description"", """"),
        ""Fail Reason"": test_info.get(""metrics"", {}).get(""fail_reason"", """"),
        ""Reached Cutoff"": test_info.get(""reached_cutoff"", """"),
    }

    rows.append(row)

    # Check for nested tests and process them if present
    nested_tests = test_info.get(""tests"")
    if nested_tests:
        for nested_test_name, nested_test_info in nested_tests.items():
            process_test(nested_test_name, nested_test_info, agent_name, common_data)","Point(row=45, column=0)","Point(row=97, column=85)",,classic/benchmark/reports/send_to_googledrive.py
print_markdown_report,function,"
    Generates a Markdown report from a given report.json file.

    :param report_json_file: Path to the report.json file.
    :return: A string containing the Markdown formatted report.
","def print_markdown_report(report_json_file: Path):
    """"""
    Generates a Markdown report from a given report.json file.

    :param report_json_file: Path to the report.json file.
    :return: A string containing the Markdown formatted report.
    """"""
    report = Report.model_validate_json(report_json_file.read_text())

    # Header and metadata
    click.echo(""# Benchmark Report"")
    click.echo(f""- ‚åõ **Run time:** `{report.metrics.run_time}`"")
    click.echo(
        f""  - **Started at:** `{report.benchmark_start_time[:16].replace('T', '` `')}`""
    )
    if report.completion_time:
        click.echo(
            f""  - **Completed at:** `{report.completion_time[:16].replace('T', '` `')}`""
        )
    if report.metrics.total_cost:
        click.echo(f""- üí∏ **Total cost:** `${round(report.metrics.total_cost, 2)}`"")
    click.echo(
        f""- üèÖ **Highest achieved difficulty:** `{report.metrics.highest_difficulty}`""
    )
    click.echo(f""- ‚öôÔ∏è **Command:** `{report.command}`"")

    click.echo()  # spacing

    # Aggregate information
    successful, failed, unreliable = [], [], []
    for test in report.tests.values():
        test.metrics.success_percentage = (
            rsp
            if (rsp := test.metrics.success_percentage) is not None
            else sum(float(r.success or 0) for r in test.results)
            * 100
            / len(test.results)
        )
        if test.metrics.success_percentage == 100.0:
            successful.append(test)
        elif test.metrics.success_percentage == 0.0:
            failed.append(test)
        else:
            unreliable.append(test)

    # Summary
    click.echo(""## Summary"")
    click.echo(f""- **`{len(successful)}` passed** {'‚úÖ'*len(successful)}"")
    click.echo(f""- **`{len(failed)}` failed** {'‚ùå'*len(failed)}"")
    click.echo(f""- **`{len(unreliable)}` unreliable** {'‚ö†Ô∏è'*len(unreliable)}"")

    click.echo()  # spacing

    # Test results
    click.echo(""## Challenges"")
    for test_name, test in report.tests.items():
        click.echo()  # spacing

        result_indicator = (
            ""‚úÖ""
            if test.metrics.success_percentage == 100.0
            else ""‚ö†Ô∏è""
            if test.metrics.success_percentage > 0
            else ""‚ùå""
        )
        click.echo(
            f""### {test_name} {result_indicator if test.metrics.attempted else '‚ùî'}""
        )
        click.echo(f""{test.description}"")

        click.echo()  # spacing

        click.echo(f""- **Attempted:** {'Yes üëç' if test.metrics.attempted else 'No üëé'}"")
        click.echo(
            f""- **Success rate:** {round(test.metrics.success_percentage)}% ""
            f""({len([r for r in test.results if r.success])}/{len(test.results)})""
        )
        click.echo(f""- **Difficulty:** `{test.difficulty}`"")
        click.echo(f""- **Categories:** `{'`, `'.join(test.category)}`"")
        click.echo(
            f""<details>\n<summary><strong>Task</strong> (click to expand)</summary>\n\n""
            f""{indent('> ', test.task)}\n\n""
            f""Reference answer:\n{indent('> ', test.answer)}\n""
            ""</details>""
        )

        click.echo()  # spacing

        click.echo(""\n#### Attempts"")
        for i, attempt in enumerate(test.results, 1):
            click.echo(
                f""\n{i}. **{'‚úÖ Passed' if attempt.success else '‚ùå Failed'}** ""
                f""in **{attempt.run_time}** ""
                f""and **{quantify('step', attempt.n_steps)}**\n""
            )
            if attempt.cost is not None:
                click.echo(f""   - **Cost:** `${round(attempt.cost, 3)}`"")
            if attempt.fail_reason:
                click.echo(
                    ""   - **Failure reason:**\n""
                    + indent(""      > "", attempt.fail_reason)
                    + ""\n""
                )
            if attempt.steps:
                click.echo(
                    indent(
                        3 * "" "",
                        ""<details>\n<summary><strong>Steps</strong></summary>\n"",
                    )
                )
                for j, step in enumerate(attempt.steps, 1):
                    click.echo()
                    click.echo(
                        indent(3 * "" "", f""{j}. {indent(3*' ', step.output, False)}"")
                    )
                click.echo(""\n</details>"")","Point(row=13, column=0)","Point(row=128, column=42)",,classic/benchmark/reports/format.py
indent,function,,"def indent(indent: str, text: str, prefix_indent: bool = True) -> str:
    return (indent if prefix_indent else """") + text.replace(""\n"", ""\n"" + indent)","Point(row=131, column=0)","Point(row=132, column=80)",,classic/benchmark/reports/format.py
quantify,function,,"def quantify(noun: str, count: int, plural_suffix: str = ""s"") -> str:
    if count == 1:
        return f""{count} {noun}""
    return f""{count} {noun}{plural_suffix}""","Point(row=135, column=0)","Point(row=138, column=43)",,classic/benchmark/reports/format.py
Metrics,class,,"class Metrics(BaseModel):
    difficulty: str
    success: bool
    success_percent: float = Field(alias=""success_%"")
    run_time: Optional[str] = None
    fail_reason: Optional[str] = None
    attempted: Optional[bool] = None","Point(row=13, column=0)","Point(row=19, column=36)",,classic/benchmark/reports/match_records.py
MetricsOverall,class,,"class MetricsOverall(BaseModel):
    run_time: str
    highest_difficulty: str
    percentage: Optional[float] = None","Point(row=22, column=0)","Point(row=25, column=38)",,classic/benchmark/reports/match_records.py
Test,class,,"class Test(BaseModel):
    data_path: str
    is_regression: bool
    answer: str
    description: str
    metrics: Metrics
    category: List[str]
    task: Optional[str] = None
    reached_cutoff: Optional[bool] = None","Point(row=28, column=0)","Point(row=36, column=41)",,classic/benchmark/reports/match_records.py
SuiteTest,class,,"class SuiteTest(BaseModel):
    data_path: str
    metrics: MetricsOverall
    tests: Dict[str, Test]
    category: Optional[List[str]] = None
    task: Optional[str] = None
    reached_cutoff: Optional[bool] = None","Point(row=39, column=0)","Point(row=45, column=41)",,classic/benchmark/reports/match_records.py
Report,class,,"class Report(BaseModel):
    command: str
    completion_time: str
    benchmark_start_time: str
    metrics: MetricsOverall
    tests: Dict[str, Union[Test, SuiteTest]]
    config: Dict[str, str | dict[str, str]]","Point(row=48, column=0)","Point(row=54, column=43)",,classic/benchmark/reports/match_records.py
get_reports,function,,"def get_reports():
    # Initialize an empty list to store the report data
    report_data = []

    # Get the current working directory
    current_dir = os.getcwd()

    # Check if the current directory ends with 'reports'
    if current_dir.endswith(""reports""):
        reports_dir = ""/""
    else:
        reports_dir = ""reports""

    # Iterate over all agent directories in the reports directory
    for agent_name in os.listdir(reports_dir):
        if agent_name is None:
            continue
        agent_dir = os.path.join(reports_dir, agent_name)

        # Check if the item is a directory (an agent directory)
        if os.path.isdir(agent_dir):
            # Construct the path to the report.json file
            # Get all directories and files, but note that this will also include any file, not just directories.
            run_dirs = glob.glob(os.path.join(agent_dir, ""*""))

            # Get all json files starting with 'file'
            # old_report_files = glob.glob(os.path.join(agent_dir, ""file*.json""))

            # For each run directory, add the report.json to the end
            # Only include the path if it's actually a directory
            report_files = [
                os.path.join(run_dir, ""report.json"")
                for run_dir in run_dirs
                if os.path.isdir(run_dir)
            ]
            # old_report_files already contains the full paths, so no need to join again
            # report_files = report_files + old_report_files
            for report_file in report_files:
                # Check if the report.json file exists
                if os.path.isfile(report_file):
                    # Open the report.json file
                    with open(report_file, ""r"") as f:
                        # Load the JSON data from the file
                        json_data = json.load(f)
                        print(f""Processing {report_file}"")
                        report = Report.model_validate(json_data)

                        for test_name, test_data in report.tests.items():
                            test_json = {
                                ""agent"": agent_name.lower(),
                                ""benchmark_start_time"": report.benchmark_start_time,
                            }

                            if isinstance(test_data, SuiteTest):
                                if (
                                    test_data.category
                                ):  # this means it's a same task test
                                    test_json[""challenge""] = test_name
                                    test_json[""attempted""] = test_data.tests[
                                        list(test_data.tests.keys())[0]
                                    ].metrics.attempted
                                    test_json[""categories""] = "", "".join(
                                        test_data.category
                                    )
                                    test_json[""task""] = test_data.task
                                    test_json[""success""] = test_data.metrics.percentage
                                    test_json[
                                        ""difficulty""
                                    ] = test_data.metrics.highest_difficulty
                                    test_json[
                                        ""success_%""
                                    ] = test_data.metrics.percentage
                                    test_json[""run_time""] = test_data.metrics.run_time
                                    test_json[""is_regression""] = test_data.tests[
                                        list(test_data.tests.keys())[0]
                                    ].is_regression
                                else:  # separate tasks in 1 suite
                                    for (
                                        suite_test_name,
                                        suite_data,
                                    ) in test_data.tests.items():
                                        test_json[""challenge""] = suite_test_name
                                        test_json[
                                            ""attempted""
                                        ] = suite_data.metrics.attempted
                                        test_json[""categories""] = "", "".join(
                                            suite_data.category
                                        )
                                        test_json[""task""] = suite_data.task
                                        test_json[""success""] = (
                                            100.0 if suite_data.metrics.success else 0
                                        )
                                        test_json[
                                            ""difficulty""
                                        ] = suite_data.metrics.difficulty
                                        test_json[
                                            ""success_%""
                                        ] = suite_data.metrics.success_percentage
                                        test_json[
                                            ""run_time""
                                        ] = suite_data.metrics.run_time
                                        test_json[
                                            ""is_regression""
                                        ] = suite_data.is_regression

                            else:
                                test_json[""challenge""] = test_name
                                test_json[""attempted""] = test_data.metrics.attempted
                                test_json[""categories""] = "", "".join(test_data.category)
                                test_json[""task""] = test_data.task
                                test_json[""success""] = (
                                    100.0 if test_data.metrics.success else 0
                                )
                                test_json[""difficulty""] = test_data.metrics.difficulty
                                test_json[
                                    ""success_%""
                                ] = test_data.metrics.success_percentage
                                test_json[""run_time""] = test_data.metrics.run_time
                                test_json[""is_regression""] = test_data.is_regression

                            report_data.append(test_json)

    return pd.DataFrame(report_data)","Point(row=57, column=0)","Point(row=179, column=36)",,classic/benchmark/reports/match_records.py
get_helicone_data,function,,"def get_helicone_data():
    helicone_api_key = os.getenv(""HELICONE_API_KEY"")

    url = ""https://www.helicone.ai/api/graphql""
    # Replace <KEY> with your personal access key
    transport = AIOHTTPTransport(
        url=url, headers={""authorization"": f""Bearer {helicone_api_key}""}
    )

    client = Client(transport=transport, fetch_schema_from_transport=True)

    SIZE = 250

    i = 0

    data = []
    print(""Fetching data from Helicone"")
    while True:
        query = gql(
            """"""
            query ExampleQuery($limit: Int, $offset: Int){
                heliconeRequest(
                    limit: $limit
                    offset: $offset
                ) {
                    costUSD
                    prompt
                    properties{
                        name
                        value
                    }
                    
                    requestBody
                    response
                    createdAt

                }

                }
        """"""
        )
        print(f""Fetching {i * SIZE} to {(i + 1) * SIZE} records"")
        try:
            result = client.execute(
                query, variable_values={""limit"": SIZE, ""offset"": i * SIZE}
            )
        except Exception as e:
            print(f""Error occurred: {e}"")
            result = None

        i += 1

        if result:
            for item in result[""heliconeRequest""]:
                properties = {
                    prop[""name""]: prop[""value""] for prop in item[""properties""]
                }
                data.append(
                    {
                        ""createdAt"": item[""createdAt""],
                        ""agent"": properties.get(""agent""),
                        ""costUSD"": item[""costUSD""],
                        ""job_id"": properties.get(""job_id""),
                        ""challenge"": properties.get(""challenge""),
                        ""benchmark_start_time"": properties.get(""benchmark_start_time""),
                        ""prompt"": item[""prompt""],
                        ""response"": item[""response""],
                        ""model"": item[""requestBody""].get(""model""),
                        ""request"": item[""requestBody""].get(""messages""),
                    }
                )

        if not result or (len(result[""heliconeRequest""]) == 0):
            print(""No more results"")
            break

    df = pd.DataFrame(data)
    # Drop rows where agent is None
    df = df.dropna(subset=[""agent""])

    # Convert the remaining agent names to lowercase
    df[""agent""] = df[""agent""].str.lower()

    return df","Point(row=182, column=0)","Point(row=265, column=13)",,classic/benchmark/reports/match_records.py
try_formats,function,,"def try_formats(date_str):
    formats = [""%Y-%m-%d-%H:%M"", ""%Y-%m-%dT%H:%M:%S%z""]
    for fmt in formats:
        try:
            return pd.to_datetime(date_str, format=fmt)
        except ValueError:
            pass
    return None","Point(row=278, column=0)","Point(row=285, column=15)",,classic/benchmark/reports/match_records.py
txt_to_rtf,function,"
    Convert a text file to RTF format.

    Args:
    input_file (Union[str, Path]): Path to the input text file.
    output_file (Union[str, Path]): Path to the output RTF file.

    Returns:
    None
","def txt_to_rtf(input_file: Union[str, Path], output_file: Union[str, Path]) -> None:
    """"""
    Convert a text file to RTF format.

    Args:
    input_file (Union[str, Path]): Path to the input text file.
    output_file (Union[str, Path]): Path to the output RTF file.

    Returns:
    None
    """"""
    input_path = Path(input_file)
    output_path = Path(output_file)

    with input_path.open(""r"", encoding=""utf-8"") as txt_file:
        content = txt_file.read()

    # RTF header
    rtf = r""{\rtf1\ansi\deff0 {\fonttbl {\f0 Times New Roman;}}\f0\fs24 ""

    # Replace newlines with RTF newline
    rtf += content.replace(""\n"", ""\\par "")

    # Close RTF document
    rtf += ""}""

    with output_path.open(""w"", encoding=""utf-8"") as rtf_file:
        rtf_file.write(rtf)","Point(row=23, column=0)","Point(row=50, column=27)",,classic/original_autogpt/setup.py
tmp_project_root,function,,"def tmp_project_root(tmp_path: Path) -> Path:
    return tmp_path","Point(row=26, column=0)","Point(row=27, column=19)",,classic/original_autogpt/tests/conftest.py
app_data_dir,function,,"def app_data_dir(tmp_project_root: Path) -> Path:
    dir = tmp_project_root / ""data""
    dir.mkdir(parents=True, exist_ok=True)
    return dir","Point(row=31, column=0)","Point(row=34, column=14)",,classic/original_autogpt/tests/conftest.py
storage,function,,"def storage(app_data_dir: Path) -> FileStorage:
    storage = LocalFileStorage(
        FileStorageConfiguration(root=app_data_dir, restrict_to_root=False)
    )
    storage.initialize()
    return storage","Point(row=38, column=0)","Point(row=43, column=18)",,classic/original_autogpt/tests/conftest.py
config,function,,"def config(
    tmp_project_root: Path,
    app_data_dir: Path,
):
    if not os.environ.get(""OPENAI_API_KEY""):
        os.environ[""OPENAI_API_KEY""] = ""sk-dummy""
    config = ConfigBuilder.build_config_from_env(project_root=tmp_project_root)

    config.app_data_dir = app_data_dir

    config.noninteractive_mode = True

    yield config","Point(row=47, column=0)","Point(row=59, column=16)",,classic/original_autogpt/tests/conftest.py
setup_logger,function,,"def setup_logger():
    configure_logging(
        debug=True,
        log_dir=Path(__file__).parent / ""logs"",
        plain_console_output=True,
    )","Point(row=63, column=0)","Point(row=68, column=5)",,classic/original_autogpt/tests/conftest.py
llm_provider,function,,"def llm_provider(config: AppConfig) -> MultiProvider:
    return _configure_llm_provider(config)","Point(row=72, column=0)","Point(row=73, column=42)",,classic/original_autogpt/tests/conftest.py
agent,function,,"def agent(
    config: AppConfig, llm_provider: MultiProvider, storage: FileStorage
) -> Agent:
    ai_profile = AIProfile(
        ai_name=""Base"",
        ai_role=""A base AI"",
        ai_goals=[],
    )

    agent_settings = AgentSettings(
        name=Agent.default_settings.name,
        description=Agent.default_settings.description,
        agent_id=f""AutoGPT-test-agent-{str(uuid.uuid4())[:8]}"",
        ai_profile=ai_profile,
        config=AgentConfiguration(
            fast_llm=config.fast_llm,
            smart_llm=config.smart_llm,
            allow_fs_access=not config.restrict_to_workspace,
            use_functions_api=config.openai_functions,
        ),
        history=Agent.default_settings.history.model_copy(deep=True),
    )

    agent = Agent(
        settings=agent_settings,
        llm_provider=llm_provider,
        file_storage=storage,
        app_config=config,
    )
    return agent","Point(row=77, column=0)","Point(row=106, column=16)",,classic/original_autogpt/tests/conftest.py
skip_in_ci,function,,"def skip_in_ci(test_function):
    return pytest.mark.skipif(
        os.environ.get(""CI"") == ""true"",
        reason=""This test doesn't work on GitHub Actions."",
    )(test_function)","Point(row=5, column=0)","Point(row=9, column=20)",,classic/original_autogpt/tests/utils.py
valid_json_response,function,,"def valid_json_response() -> dict:
    return {
        ""thoughts"": {
            ""text"": ""My task is complete. I will use the 'task_complete' command ""
            ""to shut down."",
            ""reasoning"": ""I will use the 'task_complete' command because it allows me ""
            ""to shut down and signal that my task is complete."",
            ""plan"": ""I will use the 'task_complete' command with the reason ""
            ""'Task complete: retrieved Tesla's revenue in 2022.' to shut down."",
            ""criticism"": ""I need to ensure that I have completed all necessary tasks ""
            ""before shutting down."",
            ""speak"": ""All done!"",
        },
        ""command"": {
            ""name"": ""task_complete"",
            ""args"": {""reason"": ""Task complete: retrieved Tesla's revenue in 2022.""},
        },
    }","Point(row=21, column=0)","Point(row=38, column=5)",,classic/original_autogpt/tests/unit/test_utils.py
invalid_json_response,function,,"def invalid_json_response() -> dict:
    return {
        ""thoughts"": {
            ""text"": ""My task is complete. I will use the 'task_complete' command ""
            ""to shut down."",
            ""reasoning"": ""I will use the 'task_complete' command because it allows me ""
            ""to shut down and signal that my task is complete."",
            ""plan"": ""I will use the 'task_complete' command with the reason ""
            ""'Task complete: retrieved Tesla's revenue in 2022.' to shut down."",
            ""criticism"": ""I need to ensure that I have completed all necessary tasks ""
            ""before shutting down."",
            ""speak"": """",
        },
        ""command"": {""name"": """", ""args"": {}},
    }","Point(row=42, column=0)","Point(row=56, column=5)",,classic/original_autogpt/tests/unit/test_utils.py
test_get_bulletin_from_web_success,function,,"def test_get_bulletin_from_web_success(mock_get):
    expected_content = ""Test bulletin from web""

    mock_get.return_value.status_code = 200
    mock_get.return_value.text = expected_content
    bulletin = get_bulletin_from_web()

    assert expected_content in bulletin
    mock_get.assert_called_with(
        ""https://raw.githubusercontent.com/Significant-Gravitas/AutoGPT/master/classic/original_autogpt/BULLETIN.md""  # noqa: E501
    )","Point(row=60, column=0)","Point(row=70, column=5)",,classic/original_autogpt/tests/unit/test_utils.py
test_get_bulletin_from_web_failure,function,,"def test_get_bulletin_from_web_failure(mock_get):
    mock_get.return_value.status_code = 404
    bulletin = get_bulletin_from_web()

    assert bulletin == """"","Point(row=74, column=0)","Point(row=78, column=25)",,classic/original_autogpt/tests/unit/test_utils.py
test_get_bulletin_from_web_exception,function,,"def test_get_bulletin_from_web_exception(mock_get):
    mock_get.side_effect = requests.exceptions.RequestException()
    bulletin = get_bulletin_from_web()

    assert bulletin == """"","Point(row=82, column=0)","Point(row=86, column=25)",,classic/original_autogpt/tests/unit/test_utils.py
test_get_latest_bulletin_no_file,function,,"def test_get_latest_bulletin_no_file():
    if os.path.exists(""data/CURRENT_BULLETIN.md""):
        os.remove(""data/CURRENT_BULLETIN.md"")

    bulletin, is_new = get_latest_bulletin()
    assert is_new","Point(row=89, column=0)","Point(row=94, column=17)",,classic/original_autogpt/tests/unit/test_utils.py
test_get_latest_bulletin_with_file,function,,"def test_get_latest_bulletin_with_file():
    expected_content = ""Test bulletin""
    with open(""data/CURRENT_BULLETIN.md"", ""w"", encoding=""utf-8"") as f:
        f.write(expected_content)

    with patch(""autogpt.app.utils.get_bulletin_from_web"", return_value=""""):
        bulletin, is_new = get_latest_bulletin()
        assert expected_content in bulletin
        assert is_new is False

    os.remove(""data/CURRENT_BULLETIN.md"")","Point(row=97, column=0)","Point(row=107, column=41)",,classic/original_autogpt/tests/unit/test_utils.py
test_get_latest_bulletin_with_new_bulletin,function,,"def test_get_latest_bulletin_with_new_bulletin():
    with open(""data/CURRENT_BULLETIN.md"", ""w"", encoding=""utf-8"") as f:
        f.write(""Old bulletin"")

    expected_content = ""New bulletin from web""
    with patch(
        ""autogpt.app.utils.get_bulletin_from_web"", return_value=expected_content
    ):
        bulletin, is_new = get_latest_bulletin()
        assert ""::NEW BULLETIN::"" in bulletin
        assert expected_content in bulletin
        assert is_new

    os.remove(""data/CURRENT_BULLETIN.md"")","Point(row=110, column=0)","Point(row=123, column=41)",,classic/original_autogpt/tests/unit/test_utils.py
test_get_latest_bulletin_new_bulletin_same_as_old_bulletin,function,,"def test_get_latest_bulletin_new_bulletin_same_as_old_bulletin():
    expected_content = ""Current bulletin""
    with open(""data/CURRENT_BULLETIN.md"", ""w"", encoding=""utf-8"") as f:
        f.write(expected_content)

    with patch(
        ""autogpt.app.utils.get_bulletin_from_web"", return_value=expected_content
    ):
        bulletin, is_new = get_latest_bulletin()
        assert expected_content in bulletin
        assert is_new is False

    os.remove(""data/CURRENT_BULLETIN.md"")","Point(row=126, column=0)","Point(row=138, column=41)",,classic/original_autogpt/tests/unit/test_utils.py
test_get_current_git_branch,function,,"def test_get_current_git_branch():
    branch_name = get_current_git_branch()
    assert branch_name != """"","Point(row=142, column=0)","Point(row=144, column=28)",,classic/original_autogpt/tests/unit/test_utils.py
test_get_current_git_branch_success,function,,"def test_get_current_git_branch_success(mock_repo):
    mock_repo.return_value.active_branch.name = ""test-branch""
    branch_name = get_current_git_branch()

    assert branch_name == ""test-branch""","Point(row=148, column=0)","Point(row=152, column=39)",,classic/original_autogpt/tests/unit/test_utils.py
test_get_current_git_branch_failure,function,,"def test_get_current_git_branch_failure(mock_repo):
    mock_repo.side_effect = InvalidGitRepositoryError()
    branch_name = get_current_git_branch()

    assert branch_name == """"","Point(row=156, column=0)","Point(row=160, column=28)",,classic/original_autogpt/tests/unit/test_utils.py
test_extract_json_from_response,function,,"def test_extract_json_from_response(valid_json_response: dict):
    emulated_response_from_openai = json.dumps(valid_json_response)
    assert extract_dict_from_json(emulated_response_from_openai) == valid_json_response","Point(row=163, column=0)","Point(row=165, column=87)",,classic/original_autogpt/tests/unit/test_utils.py
test_extract_json_from_response_wrapped_in_code_block,function,,"def test_extract_json_from_response_wrapped_in_code_block(valid_json_response: dict):
    emulated_response_from_openai = ""```"" + json.dumps(valid_json_response) + ""```""
    assert extract_dict_from_json(emulated_response_from_openai) == valid_json_response","Point(row=168, column=0)","Point(row=170, column=87)",,classic/original_autogpt/tests/unit/test_utils.py
test_extract_json_from_response_wrapped_in_code_block_with_language,function,,"def test_extract_json_from_response_wrapped_in_code_block_with_language(
    valid_json_response: dict,
):
    emulated_response_from_openai = ""```json"" + json.dumps(valid_json_response) + ""```""
    assert extract_dict_from_json(emulated_response_from_openai) == valid_json_response","Point(row=173, column=0)","Point(row=177, column=87)",,classic/original_autogpt/tests/unit/test_utils.py
test_extract_json_from_response_json_contained_in_string,function,,"def test_extract_json_from_response_json_contained_in_string(valid_json_response: dict):
    emulated_response_from_openai = (
        ""sentence1"" + json.dumps(valid_json_response) + ""sentence2""
    )
    assert extract_dict_from_json(emulated_response_from_openai) == valid_json_response","Point(row=180, column=0)","Point(row=184, column=87)",,classic/original_autogpt/tests/unit/test_utils.py
mock_env_file_path,function,,"def mock_env_file_path(tmp_path):
    return tmp_path / "".env""","Point(row=188, column=0)","Point(row=189, column=28)",,classic/original_autogpt/tests/unit/test_utils.py
mock_env_file,function,,"def mock_env_file(mock_env_file_path: Path, monkeypatch: pytest.MonkeyPatch):
    mock_env_file_path.write_text(env_file_initial_content)
    monkeypatch.setattr(autogpt.app.utils, ""ENV_FILE_PATH"", mock_env_file_path)
    return mock_env_file_path","Point(row=205, column=0)","Point(row=208, column=29)",,classic/original_autogpt/tests/unit/test_utils.py
mock_environ,function,,"def mock_environ(monkeypatch: pytest.MonkeyPatch):
    env = {}
    monkeypatch.setattr(os, ""environ"", env)
    return env","Point(row=212, column=0)","Point(row=215, column=14)",,classic/original_autogpt/tests/unit/test_utils.py
test_set_env_config_value_updates_existing_key,function,,"def test_set_env_config_value_updates_existing_key(
    mock_env_file: Path, mock_environ: dict
):
    # Before updating, ensure the original content is as expected
    with mock_env_file.open(""r"") as file:
        assert file.readlines() == env_file_initial_content.splitlines(True)

    set_env_config_value(""EXISTING_KEY"", ""NEW_VALUE"")
    with mock_env_file.open(""r"") as file:
        content = file.readlines()

    # Ensure only the relevant line is altered
    expected_content_lines = [
        ""\n"",
        ""# This is a comment\n"",
        ""EXISTING_KEY=NEW_VALUE\n"",  # existing key + new value
        ""\n"",
        ""## This is also a comment\n"",
        ""# DISABLED_KEY=DISABLED_VALUE\n"",
        ""\n"",
        ""# Another comment\n"",
        ""UNUSED_KEY=UNUSED_VALUE\n"",
    ]
    assert content == expected_content_lines
    assert mock_environ[""EXISTING_KEY""] == ""NEW_VALUE""","Point(row=218, column=0)","Point(row=242, column=54)",,classic/original_autogpt/tests/unit/test_utils.py
test_set_env_config_value_uncomments_and_updates_disabled_key,function,,"def test_set_env_config_value_uncomments_and_updates_disabled_key(
    mock_env_file: Path, mock_environ: dict
):
    # Before adding, ensure the original content is as expected
    with mock_env_file.open(""r"") as file:
        assert file.readlines() == env_file_initial_content.splitlines(True)

    set_env_config_value(""DISABLED_KEY"", ""ENABLED_NEW_VALUE"")
    with mock_env_file.open(""r"") as file:
        content = file.readlines()

    # Ensure only the relevant line is altered
    expected_content_lines = [
        ""\n"",
        ""# This is a comment\n"",
        ""EXISTING_KEY=EXISTING_VALUE\n"",
        ""\n"",
        ""## This is also a comment\n"",
        ""DISABLED_KEY=ENABLED_NEW_VALUE\n"",  # disabled -> enabled + new value
        ""\n"",
        ""# Another comment\n"",
        ""UNUSED_KEY=UNUSED_VALUE\n"",
    ]
    assert content == expected_content_lines
    assert mock_environ[""DISABLED_KEY""] == ""ENABLED_NEW_VALUE""","Point(row=245, column=0)","Point(row=269, column=62)",,classic/original_autogpt/tests/unit/test_utils.py
test_set_env_config_value_adds_new_key,function,,"def test_set_env_config_value_adds_new_key(mock_env_file: Path, mock_environ: dict):
    # Before adding, ensure the original content is as expected
    with mock_env_file.open(""r"") as file:
        assert file.readlines() == env_file_initial_content.splitlines(True)

    set_env_config_value(""NEW_KEY"", ""NEW_VALUE"")
    with mock_env_file.open(""r"") as file:
        content = file.readlines()

    # Ensure the new key-value pair is added without altering the rest
    expected_content_lines = [
        ""\n"",
        ""# This is a comment\n"",
        ""EXISTING_KEY=EXISTING_VALUE\n"",
        ""\n"",
        ""## This is also a comment\n"",
        ""# DISABLED_KEY=DISABLED_VALUE\n"",
        ""\n"",
        ""# Another comment\n"",
        ""UNUSED_KEY=UNUSED_VALUE\n"",
        ""NEW_KEY=NEW_VALUE\n"",  # New key-value pair added at the end
    ]
    assert content == expected_content_lines
    assert mock_environ[""NEW_KEY""] == ""NEW_VALUE""","Point(row=272, column=0)","Point(row=295, column=49)",,classic/original_autogpt/tests/unit/test_utils.py
test_initial_values,function,"
    Test if the initial values of the config class attributes are set correctly.
","def test_initial_values(config: AppConfig) -> None:
    """"""
    Test if the initial values of the config class attributes are set correctly.
    """"""
    assert config.continuous_mode is False
    assert config.tts_config.speak_mode is False
    assert config.fast_llm.startswith(""gpt-3.5-turbo"")
    assert config.smart_llm.startswith(""gpt-4"")","Point(row=18, column=0)","Point(row=25, column=47)",,classic/original_autogpt/tests/unit/test_config.py
test_fallback_to_gpt3_if_gpt4_not_available,function,"
    Test if models update to gpt-3.5-turbo if gpt-4 is not available.
","async def test_fallback_to_gpt3_if_gpt4_not_available(
    mock_list_models: Any, config: AppConfig
) -> None:
    """"""
    Test if models update to gpt-3.5-turbo if gpt-4 is not available.
    """"""
    config.fast_llm = GPT_4_MODEL
    config.smart_llm = GPT_4_MODEL

    mock_list_models.return_value = asyncio.Future()
    mock_list_models.return_value.set_result(
        AsyncPage(
            data=[Model(id=GPT_3_MODEL, created=0, object=""model"", owned_by=""AutoGPT"")],
            object=""Models"",  # no idea what this should be, but irrelevant
        )
    )

    await apply_overrides_to_config(config=config)

    assert config.fast_llm == GPT_3_MODEL
    assert config.smart_llm == GPT_3_MODEL","Point(row=30, column=0)","Point(row=50, column=42)",,classic/original_autogpt/tests/unit/test_config.py
test_missing_azure_config,function,,"def test_missing_azure_config(config: AppConfig) -> None:
    assert config.openai_credentials is not None

    config_file = config.app_data_dir / ""azure_config.yaml""
    with pytest.raises(FileNotFoundError):
        config.openai_credentials.load_azure_config(config_file)

    config_file.write_text("""")
    with pytest.raises(ValueError):
        config.openai_credentials.load_azure_config(config_file)

    assert config.openai_credentials.api_type != SecretStr(""azure"")
    assert config.openai_credentials.api_version is None
    assert config.openai_credentials.azure_model_to_deploy_id_map is None","Point(row=53, column=0)","Point(row=66, column=73)",,classic/original_autogpt/tests/unit/test_config.py
config_with_azure,function,,"def config_with_azure(config: AppConfig):
    config_file = config.app_data_dir / ""azure_config.yaml""
    config_file.write_text(
        f""""""
azure_api_type: azure
azure_api_version: 2023-06-01-preview
azure_endpoint: https://dummy.openai.azure.com
azure_model_map:
    {config.fast_llm}: FAST-LLM_ID
    {config.smart_llm}: SMART-LLM_ID
    {config.embedding_model}: embedding-deployment-id-for-azure
""""""
    )
    os.environ[""USE_AZURE""] = ""True""
    os.environ[""AZURE_CONFIG_FILE""] = str(config_file)
    config_with_azure = ConfigBuilder.build_config_from_env(
        project_root=config.project_root
    )
    yield config_with_azure
    del os.environ[""USE_AZURE""]
    del os.environ[""AZURE_CONFIG_FILE""]","Point(row=70, column=0)","Point(row=90, column=39)",,classic/original_autogpt/tests/unit/test_config.py
test_azure_config,function,,"def test_azure_config(config_with_azure: AppConfig) -> None:
    assert (credentials := config_with_azure.openai_credentials) is not None
    assert credentials.api_type == SecretStr(""azure"")
    assert credentials.api_version == SecretStr(""2023-06-01-preview"")
    assert credentials.azure_endpoint == SecretStr(""https://dummy.openai.azure.com"")
    assert credentials.azure_model_to_deploy_id_map == {
        config_with_azure.fast_llm: ""FAST-LLM_ID"",
        config_with_azure.smart_llm: ""SMART-LLM_ID"",
        config_with_azure.embedding_model: ""embedding-deployment-id-for-azure"",
    }

    fast_llm = config_with_azure.fast_llm
    smart_llm = config_with_azure.smart_llm
    assert (
        credentials.get_model_access_kwargs(config_with_azure.fast_llm)[""model""]
        == ""FAST-LLM_ID""
    )
    assert (
        credentials.get_model_access_kwargs(config_with_azure.smart_llm)[""model""]
        == ""SMART-LLM_ID""
    )

    # Emulate --gpt4only
    config_with_azure.fast_llm = smart_llm
    assert (
        credentials.get_model_access_kwargs(config_with_azure.fast_llm)[""model""]
        == ""SMART-LLM_ID""
    )
    assert (
        credentials.get_model_access_kwargs(config_with_azure.smart_llm)[""model""]
        == ""SMART-LLM_ID""
    )

    # Emulate --gpt3only
    config_with_azure.fast_llm = config_with_azure.smart_llm = fast_llm
    assert (
        credentials.get_model_access_kwargs(config_with_azure.fast_llm)[""model""]
        == ""FAST-LLM_ID""
    )
    assert (
        credentials.get_model_access_kwargs(config_with_azure.smart_llm)[""model""]
        == ""FAST-LLM_ID""
    )","Point(row=93, column=0)","Point(row=135, column=5)",,classic/original_autogpt/tests/unit/test_config.py
test_spinner_initializes_with_default_values,function,Tests that the spinner initializes with default values.,"def test_spinner_initializes_with_default_values():
    """"""Tests that the spinner initializes with default values.""""""
    with Spinner() as spinner:
        assert spinner.message == ""Loading...""
        assert spinner.delay == 0.1","Point(row=8, column=0)","Point(row=12, column=35)",,classic/original_autogpt/tests/unit/test_spinner.py
test_spinner_initializes_with_custom_values,function,Tests that the spinner initializes with custom message and delay values.,"def test_spinner_initializes_with_custom_values():
    """"""Tests that the spinner initializes with custom message and delay values.""""""
    with Spinner(message=PLEASE_WAIT, delay=0.2) as spinner:
        assert spinner.message == PLEASE_WAIT
        assert spinner.delay == 0.2","Point(row=15, column=0)","Point(row=19, column=35)",,classic/original_autogpt/tests/unit/test_spinner.py
test_spinner_stops_spinning,function,Tests that the spinner starts spinning and stops spinning without errors.,"def test_spinner_stops_spinning():
    """"""Tests that the spinner starts spinning and stops spinning without errors.""""""
    with Spinner() as spinner:
        time.sleep(1)
    assert not spinner.running","Point(row=23, column=0)","Point(row=27, column=30)",,classic/original_autogpt/tests/unit/test_spinner.py
test_spinner_can_be_used_as_context_manager,function,Tests that the spinner can be used as a context manager.,"def test_spinner_can_be_used_as_context_manager():
    """"""Tests that the spinner can be used as a context manager.""""""
    with Spinner() as spinner:
        assert spinner.running
    assert not spinner.running","Point(row=30, column=0)","Point(row=34, column=30)",,classic/original_autogpt/tests/unit/test_spinner.py
test_apply_overrides_to_ai_settings,function,,"async def test_apply_overrides_to_ai_settings():
    ai_profile = AIProfile(ai_name=""Test AI"", ai_role=""Test Role"")
    directives = AIDirectives(
        resources=[""Resource1""],
        constraints=[""Constraint1""],
        best_practices=[""BestPractice1""],
    )

    apply_overrides_to_ai_settings(
        ai_profile,
        directives,
        override_name=""New AI"",
        override_role=""New Role"",
        replace_directives=True,
        resources=[""NewResource""],
        constraints=[""NewConstraint""],
        best_practices=[""NewBestPractice""],
    )

    assert ai_profile.ai_name == ""New AI""
    assert ai_profile.ai_role == ""New Role""
    assert directives.resources == [""NewResource""]
    assert directives.constraints == [""NewConstraint""]
    assert directives.best_practices == [""NewBestPractice""]","Point(row=14, column=0)","Point(row=37, column=59)",,classic/original_autogpt/tests/integration/test_setup.py
test_interactively_revise_ai_settings,function,,"async def test_interactively_revise_ai_settings(config: AppConfig):
    ai_profile = AIProfile(ai_name=""Test AI"", ai_role=""Test Role"")
    directives = AIDirectives(
        resources=[""Resource1""],
        constraints=[""Constraint1""],
        best_practices=[""BestPractice1""],
    )

    user_inputs = [
        ""n"",
        ""New AI"",
        ""New Role"",
        ""NewConstraint"",
        """",
        ""NewResource"",
        """",
        ""NewBestPractice"",
        """",
        ""y"",
    ]
    with patch(""autogpt.app.setup.clean_input"", side_effect=user_inputs):
        ai_profile, directives = await interactively_revise_ai_settings(
            ai_profile, directives, config
        )

    assert ai_profile.ai_name == ""New AI""
    assert ai_profile.ai_role == ""New Role""
    assert directives.resources == [""NewResource""]
    assert directives.constraints == [""NewConstraint""]
    assert directives.best_practices == [""NewBestPractice""]","Point(row=41, column=0)","Point(row=70, column=59)",,classic/original_autogpt/tests/integration/test_setup.py
dummy_agent,function,,"def dummy_agent(config: AppConfig, llm_provider: MultiProvider):
    ai_profile = AIProfile(
        ai_name=""Dummy Agent"",
        ai_role=""Dummy Role"",
        ai_goals=[
            ""Dummy Task"",
        ],
    )

    agent_settings = AgentSettings(
        name=Agent.default_settings.name,
        description=Agent.default_settings.description,
        ai_profile=ai_profile,
        config=AgentConfiguration(
            fast_llm=config.fast_llm,
            smart_llm=config.smart_llm,
            use_functions_api=config.openai_functions,
        ),
        history=Agent.default_settings.history.model_copy(deep=True),
    )

    local = config.file_storage_backend == FileStorageBackendName.LOCAL
    restrict_to_root = not local or config.restrict_to_workspace
    file_storage = get_storage(
        config.file_storage_backend,
        root_path=Path(""data""),
        restrict_to_root=restrict_to_root,
    )
    file_storage.initialize()

    agent = Agent(
        settings=agent_settings,
        llm_provider=llm_provider,
        file_storage=file_storage,
        app_config=config,
    )

    return agent","Point(row=12, column=0)","Point(row=49, column=16)",,classic/original_autogpt/tests/integration/agent_factory.py
AgentProfileGeneratorConfiguration,class,,"class AgentProfileGeneratorConfiguration(SystemConfiguration):
    llm_classification: LanguageModelClassification = UserConfigurable(
        default=LanguageModelClassification.SMART_MODEL
    )
    _example_call: object = {
        ""name"": ""create_agent"",
        ""arguments"": {
            ""name"": ""CMOGPT"",
            ""description"": (
                ""a professional digital marketer AI that assists Solopreneurs ""
                ""in growing their businesses by providing ""
                ""world-class expertise in solving marketing problems ""
                ""for SaaS, content products, agencies, and more.""
            ),
            ""directives"": {
                ""best_practices"": [
                    (
                        ""Engage in effective problem-solving, prioritization, ""
                        ""planning, and supporting execution to address your ""
                        ""marketing needs as your virtual ""
                        ""Chief Marketing Officer.""
                    ),
                    (
                        ""Provide specific, actionable, and concise advice to ""
                        ""help you make informed decisions without the use of ""
                        ""platitudes or overly wordy explanations.""
                    ),
                    (
                        ""Identify and prioritize quick wins and cost-effective ""
                        ""campaigns that maximize results with minimal time and ""
                        ""budget investment.""
                    ),
                    (
                        ""Proactively take the lead in guiding you and offering ""
                        ""suggestions when faced with unclear information or ""
                        ""uncertainty to ensure your marketing strategy remains ""
                        ""on track.""
                    ),
                ],
                ""constraints"": [
                    ""Do not suggest illegal or unethical plans or strategies."",
                    ""Take reasonable budgetary limits into account."",
                ],
            },
        },
    }
    system_prompt: str = UserConfigurable(
        default=(
            ""Your job is to respond to a user-defined task, given in triple quotes, by ""
            ""invoking the `create_agent` function to generate an autonomous agent to ""
            ""complete the task. ""
            ""You should supply a role-based name for the agent (_GPT), ""
            ""an informative description for what the agent does, and 1 to 5 directives ""
            ""in each of the categories Best Practices and Constraints, ""
            ""that are optimally aligned with the successful completion ""
            ""of its assigned task.\n""
            ""\n""
            ""Example Input:\n""
            '""""""Help me with marketing my business""""""\n\n'
            ""Example Call:\n""
            ""```\n""
            f""{json.dumps(_example_call, indent=4)}""
            ""\n```""
        )
    )
    user_prompt_template: str = UserConfigurable(default='""""""{user_objective}""""""')
    create_agent_function: dict = UserConfigurable(
        default=CompletionModelFunction(
            name=""create_agent"",
            description=""Create a new autonomous AI agent to complete a given task."",
            parameters={
                ""name"": JSONSchema(
                    type=JSONSchema.Type.STRING,
                    description=""A short role-based name for an autonomous agent."",
                    required=True,
                ),
                ""description"": JSONSchema(
                    type=JSONSchema.Type.STRING,
                    description=(
                        ""An informative one sentence description ""
                        ""of what the AI agent does""
                    ),
                    required=True,
                ),
                ""directives"": JSONSchema(
                    type=JSONSchema.Type.OBJECT,
                    properties={
                        ""best_practices"": JSONSchema(
                            type=JSONSchema.Type.ARRAY,
                            minItems=1,
                            maxItems=5,
                            items=JSONSchema(
                                type=JSONSchema.Type.STRING,
                            ),
                            description=(
                                ""One to five highly effective best practices ""
                                ""that are optimally aligned with the completion ""
                                ""of the given task""
                            ),
                            required=True,
                        ),
                        ""constraints"": JSONSchema(
                            type=JSONSchema.Type.ARRAY,
                            minItems=1,
                            maxItems=5,
                            items=JSONSchema(
                                type=JSONSchema.Type.STRING,
                            ),
                            description=(
                                ""One to five reasonable and efficacious constraints ""
                                ""that are optimally aligned with the completion ""
                                ""of the given task""
                            ),
                            required=True,
                        ),
                    },
                    required=True,
                ),
            },
        ).model_dump()
    )","Point(row=20, column=0)","Point(row=140, column=5)",,classic/original_autogpt/autogpt/agent_factory/profile_generator.py
AgentProfileGenerator,class,,"class AgentProfileGenerator(PromptStrategy):
    default_configuration: AgentProfileGeneratorConfiguration = (
        AgentProfileGeneratorConfiguration()
    )

    def __init__(
        self,
        llm_classification: LanguageModelClassification,
        system_prompt: str,
        user_prompt_template: str,
        create_agent_function: dict,
    ):
        self._llm_classification = llm_classification
        self._system_prompt_message = system_prompt
        self._user_prompt_template = user_prompt_template
        self._create_agent_function = CompletionModelFunction.model_validate(
            create_agent_function
        )

    @property
    def llm_classification(self) -> LanguageModelClassification:
        return self._llm_classification

    def build_prompt(self, user_objective: str = """", **kwargs) -> ChatPrompt:
        system_message = ChatMessage.system(self._system_prompt_message)
        user_message = ChatMessage.user(
            self._user_prompt_template.format(
                user_objective=user_objective,
            )
        )
        prompt = ChatPrompt(
            messages=[system_message, user_message],
            functions=[self._create_agent_function],
        )
        return prompt

    def parse_response_content(
        self,
        response: AssistantChatMessage,
    ) -> tuple[AIProfile, AIDirectives]:
        """"""Parse the actual text response from the objective model.

        Args:
            response_content: The raw response content from the objective model.

        Returns:
            The parsed response.
        """"""
        try:
            if not response.tool_calls:
                raise ValueError(
                    f""LLM did not call {self._create_agent_function.name} function; ""
                    ""agent profile creation failed""
                )
            arguments: object = response.tool_calls[0].function.arguments
            ai_profile = AIProfile(
                ai_name=arguments.get(""name""),  # type: ignore
                ai_role=arguments.get(""description""),  # type: ignore
            )
            ai_directives = AIDirectives(
                best_practices=arguments.get(""directives"", {}).get(""best_practices""),
                constraints=arguments.get(""directives"", {}).get(""constraints""),
                resources=[],
            )
        except KeyError:
            logger.debug(f""Failed to parse this response content: {response}"")
            raise
        return ai_profile, ai_directives","Point(row=143, column=0)","Point(row=210, column=40)",,classic/original_autogpt/autogpt/agent_factory/profile_generator.py
AgentProfileGenerator.__init__,function,,"def __init__(
        self,
        llm_classification: LanguageModelClassification,
        system_prompt: str,
        user_prompt_template: str,
        create_agent_function: dict,
    ):
        self._llm_classification = llm_classification
        self._system_prompt_message = system_prompt
        self._user_prompt_template = user_prompt_template
        self._create_agent_function = CompletionModelFunction.model_validate(
            create_agent_function
        )","Point(row=148, column=4)","Point(row=160, column=9)",AgentProfileGenerator,classic/original_autogpt/autogpt/agent_factory/profile_generator.py
AgentProfileGenerator.llm_classification,function,,"def llm_classification(self) -> LanguageModelClassification:
        return self._llm_classification","Point(row=163, column=4)","Point(row=164, column=39)",AgentProfileGenerator,classic/original_autogpt/autogpt/agent_factory/profile_generator.py
AgentProfileGenerator.build_prompt,function,,"def build_prompt(self, user_objective: str = """", **kwargs) -> ChatPrompt:
        system_message = ChatMessage.system(self._system_prompt_message)
        user_message = ChatMessage.user(
            self._user_prompt_template.format(
                user_objective=user_objective,
            )
        )
        prompt = ChatPrompt(
            messages=[system_message, user_message],
            functions=[self._create_agent_function],
        )
        return prompt","Point(row=166, column=4)","Point(row=177, column=21)",AgentProfileGenerator,classic/original_autogpt/autogpt/agent_factory/profile_generator.py
AgentProfileGenerator.parse_response_content,function,"Parse the actual text response from the objective model.

        Args:
            response_content: The raw response content from the objective model.

        Returns:
            The parsed response.
","def parse_response_content(
        self,
        response: AssistantChatMessage,
    ) -> tuple[AIProfile, AIDirectives]:
        """"""Parse the actual text response from the objective model.

        Args:
            response_content: The raw response content from the objective model.

        Returns:
            The parsed response.
        """"""
        try:
            if not response.tool_calls:
                raise ValueError(
                    f""LLM did not call {self._create_agent_function.name} function; ""
                    ""agent profile creation failed""
                )
            arguments: object = response.tool_calls[0].function.arguments
            ai_profile = AIProfile(
                ai_name=arguments.get(""name""),  # type: ignore
                ai_role=arguments.get(""description""),  # type: ignore
            )
            ai_directives = AIDirectives(
                best_practices=arguments.get(""directives"", {}).get(""best_practices""),
                constraints=arguments.get(""directives"", {}).get(""constraints""),
                resources=[],
            )
        except KeyError:
            logger.debug(f""Failed to parse this response content: {response}"")
            raise
        return ai_profile, ai_directives","Point(row=179, column=4)","Point(row=210, column=40)",AgentProfileGenerator,classic/original_autogpt/autogpt/agent_factory/profile_generator.py
generate_agent_profile_for_task,function,"Generates an AIConfig object from the given string.

    Returns:
    AIConfig: The AIConfig object tailored to the user's input
","async def generate_agent_profile_for_task(
    task: str,
    app_config: AppConfig,
    llm_provider: MultiProvider,
) -> tuple[AIProfile, AIDirectives]:
    """"""Generates an AIConfig object from the given string.

    Returns:
    AIConfig: The AIConfig object tailored to the user's input
    """"""
    agent_profile_generator = AgentProfileGenerator(
        **AgentProfileGenerator.default_configuration.model_dump()  # HACK
    )

    prompt = agent_profile_generator.build_prompt(task)

    # Call LLM with the string as user input
    output = await llm_provider.create_chat_completion(
        prompt.messages,
        model_name=app_config.smart_llm,
        functions=prompt.functions,
        completion_parser=agent_profile_generator.parse_response_content,
    )

    # Debug LLM Output
    logger.debug(f""AI Config Generator Raw Output: {output.response}"")

    return output.parsed_result","Point(row=213, column=0)","Point(row=240, column=31)",,classic/original_autogpt/autogpt/agent_factory/profile_generator.py
generate_agent_for_task,function,,"async def generate_agent_for_task(
    agent_id: str,
    task: str,
    app_config: AppConfig,
    file_storage: FileStorage,
    llm_provider: MultiProvider,
) -> Agent:
    ai_profile, task_directives = await generate_agent_profile_for_task(
        task=task,
        app_config=app_config,
        llm_provider=llm_provider,
    )
    return _configure_agent(
        agent_id=agent_id,
        task=task,
        ai_profile=ai_profile,
        directives=task_directives,
        app_config=app_config,
        file_storage=file_storage,
        llm_provider=llm_provider,
    )","Point(row=15, column=0)","Point(row=35, column=5)",,classic/original_autogpt/autogpt/agent_factory/generators.py
create_agent,function,,"def create_agent(
    agent_id: str,
    task: str,
    app_config: AppConfig,
    file_storage: FileStorage,
    llm_provider: MultiProvider,
    ai_profile: Optional[AIProfile] = None,
    directives: Optional[AIDirectives] = None,
) -> Agent:
    if not task:
        raise ValueError(""No task specified for new agent"")
    ai_profile = ai_profile or AIProfile()
    directives = directives or AIDirectives()

    agent = _configure_agent(
        agent_id=agent_id,
        task=task,
        ai_profile=ai_profile,
        directives=directives,
        app_config=app_config,
        file_storage=file_storage,
        llm_provider=llm_provider,
    )

    return agent","Point(row=11, column=0)","Point(row=35, column=16)",,classic/original_autogpt/autogpt/agent_factory/configurators.py
configure_agent_with_state,function,,"def configure_agent_with_state(
    state: AgentSettings,
    app_config: AppConfig,
    file_storage: FileStorage,
    llm_provider: MultiProvider,
) -> Agent:
    return _configure_agent(
        state=state,
        app_config=app_config,
        file_storage=file_storage,
        llm_provider=llm_provider,
    )","Point(row=38, column=0)","Point(row=49, column=5)",,classic/original_autogpt/autogpt/agent_factory/configurators.py
_configure_agent,function,,"def _configure_agent(
    app_config: AppConfig,
    llm_provider: MultiProvider,
    file_storage: FileStorage,
    agent_id: str = """",
    task: str = """",
    ai_profile: Optional[AIProfile] = None,
    directives: Optional[AIDirectives] = None,
    state: Optional[AgentSettings] = None,
) -> Agent:
    if state:
        agent_state = state
    elif agent_id and task and ai_profile and directives:
        agent_state = state or create_agent_state(
            agent_id=agent_id,
            task=task,
            ai_profile=ai_profile,
            directives=directives,
            app_config=app_config,
        )
    else:
        raise TypeError(
            ""Either (state) or (agent_id, task, ai_profile, directives)""
            "" must be specified""
        )

    return Agent(
        settings=agent_state,
        llm_provider=llm_provider,
        file_storage=file_storage,
        app_config=app_config,
    )","Point(row=52, column=0)","Point(row=83, column=5)",,classic/original_autogpt/autogpt/agent_factory/configurators.py
create_agent_state,function,,"def create_agent_state(
    agent_id: str,
    task: str,
    ai_profile: AIProfile,
    directives: AIDirectives,
    app_config: AppConfig,
) -> AgentSettings:
    return AgentSettings(
        agent_id=agent_id,
        name=Agent.default_settings.name,
        description=Agent.default_settings.description,
        task=task,
        ai_profile=ai_profile,
        directives=directives,
        config=AgentConfiguration(
            fast_llm=app_config.fast_llm,
            smart_llm=app_config.smart_llm,
            allow_fs_access=not app_config.restrict_to_workspace,
            use_functions_api=app_config.openai_functions,
        ),
        history=Agent.default_settings.history.model_copy(deep=True),
    )","Point(row=86, column=0)","Point(row=107, column=5)",,classic/original_autogpt/autogpt/agent_factory/configurators.py
AppConfig,class,,"class AppConfig(BaseConfig):
    name: str = ""Auto-GPT configuration""
    description: str = ""Default configuration for the Auto-GPT application.""

    ########################
    # Application Settings #
    ########################
    project_root: Path = PROJECT_ROOT
    app_data_dir: Path = project_root / ""data""
    skip_news: bool = False
    skip_reprompt: bool = False
    authorise_key: str = UserConfigurable(default=""y"", from_env=""AUTHORISE_COMMAND_KEY"")
    exit_key: str = UserConfigurable(default=""n"", from_env=""EXIT_KEY"")
    noninteractive_mode: bool = False
    logging: LoggingConfig = LoggingConfig()
    component_config_file: Optional[Path] = UserConfigurable(
        default=None, from_env=""COMPONENT_CONFIG_FILE""
    )

    ##########################
    # Agent Control Settings #
    ##########################
    # Model configuration
    fast_llm: ModelName = UserConfigurable(
        default=OpenAIModelName.GPT3,
        from_env=""FAST_LLM"",
    )
    smart_llm: ModelName = UserConfigurable(
        default=OpenAIModelName.GPT4_TURBO,
        from_env=""SMART_LLM"",
    )
    temperature: float = UserConfigurable(default=0, from_env=""TEMPERATURE"")
    openai_functions: bool = UserConfigurable(
        default=False, from_env=lambda: os.getenv(""OPENAI_FUNCTIONS"", ""False"") == ""True""
    )
    embedding_model: str = UserConfigurable(
        default=""text-embedding-3-small"", from_env=""EMBEDDING_MODEL""
    )

    # Run loop configuration
    continuous_mode: bool = False
    continuous_limit: int = 0

    ############
    # Commands #
    ############
    # General
    disabled_commands: list[str] = UserConfigurable(
        default_factory=list,
        from_env=lambda: _safe_split(os.getenv(""DISABLED_COMMANDS"")),
    )

    # File ops
    restrict_to_workspace: bool = UserConfigurable(
        default=True,
        from_env=lambda: os.getenv(""RESTRICT_TO_WORKSPACE"", ""True"") == ""True"",
    )

    ###############
    # Credentials #
    ###############
    # OpenAI
    openai_credentials: Optional[OpenAICredentials] = None
    azure_config_file: Optional[Path] = UserConfigurable(
        default=AZURE_CONFIG_FILE, from_env=""AZURE_CONFIG_FILE""
    )

    @field_validator(""openai_functions"")
    def validate_openai_functions(cls, value: bool, info: ValidationInfo):
        if value:
            smart_llm = info.data[""smart_llm""]
            assert CHAT_MODELS[smart_llm].has_function_call_api, (
                f""Model {smart_llm} does not support tool calling. ""
                ""Please disable OPENAI_FUNCTIONS or choose a suitable model.""
            )
        return value","Point(row=26, column=0)","Point(row=101, column=20)",,classic/original_autogpt/autogpt/app/config.py
AppConfig.validate_openai_functions,function,,"def validate_openai_functions(cls, value: bool, info: ValidationInfo):
        if value:
            smart_llm = info.data[""smart_llm""]
            assert CHAT_MODELS[smart_llm].has_function_call_api, (
                f""Model {smart_llm} does not support tool calling. ""
                ""Please disable OPENAI_FUNCTIONS or choose a suitable model.""
            )
        return value","Point(row=94, column=4)","Point(row=101, column=20)",AppConfig,classic/original_autogpt/autogpt/app/config.py
ConfigBuilder,class,,"class ConfigBuilder(Configurable[AppConfig]):
    default_settings = AppConfig()

    @classmethod
    def build_config_from_env(cls, project_root: Path = PROJECT_ROOT) -> AppConfig:
        """"""Initialize the Config class""""""

        config = cls.build_agent_configuration()
        config.project_root = project_root

        # Make relative paths absolute
        for k in {
            ""azure_config_file"",  # TODO: move from project root
        }:
            setattr(config, k, project_root / getattr(config, k))

        if (
            config.openai_credentials
            and config.openai_credentials.api_type == SecretStr(""azure"")
            and (config_file := config.azure_config_file)
        ):
            config.openai_credentials.load_azure_config(config_file)

        return config","Point(row=104, column=0)","Point(row=127, column=21)",,classic/original_autogpt/autogpt/app/config.py
ConfigBuilder.build_config_from_env,function,Initialize the Config class,"def build_config_from_env(cls, project_root: Path = PROJECT_ROOT) -> AppConfig:
        """"""Initialize the Config class""""""

        config = cls.build_agent_configuration()
        config.project_root = project_root

        # Make relative paths absolute
        for k in {
            ""azure_config_file"",  # TODO: move from project root
        }:
            setattr(config, k, project_root / getattr(config, k))

        if (
            config.openai_credentials
            and config.openai_credentials.api_type == SecretStr(""azure"")
            and (config_file := config.azure_config_file)
        ):
            config.openai_credentials.load_azure_config(config_file)

        return config","Point(row=108, column=4)","Point(row=127, column=21)",ConfigBuilder,classic/original_autogpt/autogpt/app/config.py
assert_config_has_required_llm_api_keys,function,"
    Check if API keys (if required) are set for the configured SMART_LLM and FAST_LLM.
","async def assert_config_has_required_llm_api_keys(config: AppConfig) -> None:
    """"""
    Check if API keys (if required) are set for the configured SMART_LLM and FAST_LLM.
    """"""
    from forge.llm.providers.anthropic import AnthropicModelName
    from forge.llm.providers.groq import GroqModelName
    from pydantic import ValidationError

    if set((config.smart_llm, config.fast_llm)).intersection(AnthropicModelName):
        from forge.llm.providers.anthropic import AnthropicCredentials

        try:
            credentials = AnthropicCredentials.from_env()
        except ValidationError as e:
            if ""api_key"" in str(e):
                logger.error(
                    ""Set your Anthropic API key in .env or as an environment variable""
                )
                logger.info(
                    ""For further instructions: ""
                    ""https://docs.agpt.co/classic/original_autogpt/setup/#anthropic""
                )

            raise ValueError(""Anthropic is unavailable: can't load credentials"") from e

        key_pattern = r""^sk-ant-api03-[\w\-]{95}""

        # If key is set, but it looks invalid
        if not re.search(key_pattern, credentials.api_key.get_secret_value()):
            logger.warning(
                ""Possibly invalid Anthropic API key! ""
                f""Configured Anthropic API key does not match pattern '{key_pattern}'. ""
                ""If this is a valid key, please report this warning to the maintainers.""
            )

    if set((config.smart_llm, config.fast_llm)).intersection(GroqModelName):
        from forge.llm.providers.groq import GroqProvider
        from groq import AuthenticationError

        try:
            groq = GroqProvider()
            await groq.get_available_models()
        except ValidationError as e:
            if ""api_key"" not in str(e):
                raise

            logger.error(""Set your Groq API key in .env or as an environment variable"")
            logger.info(
                ""For further instructions: ""
                + ""https://docs.agpt.co/classic/original_autogpt/setup/#groq""
            )
            raise ValueError(""Groq is unavailable: can't load credentials"")
        except AuthenticationError as e:
            logger.error(""The Groq API key is invalid!"")
            logger.info(
                ""For instructions to get and set a new API key: ""
                ""https://docs.agpt.co/classic/original_autogpt/setup/#groq""
            )
            raise ValueError(""Groq is unavailable: invalid API key"") from e

    if set((config.smart_llm, config.fast_llm)).intersection(OpenAIModelName):
        from forge.llm.providers.openai import OpenAIProvider
        from openai import AuthenticationError

        try:
            openai = OpenAIProvider()
            await openai.get_available_models()
        except ValidationError as e:
            if ""api_key"" not in str(e):
                raise

            logger.error(
                ""Set your OpenAI API key in .env or as an environment variable""
            )
            logger.info(
                ""For further instructions: ""
                + ""https://docs.agpt.co/classic/original_autogpt/setup/#openai""
            )
            raise ValueError(""OpenAI is unavailable: can't load credentials"")
        except AuthenticationError as e:
            logger.error(""The OpenAI API key is invalid!"")
            logger.info(
                ""For instructions to get and set a new API key: ""
                ""https://docs.agpt.co/classic/original_autogpt/setup/#openai""
            )
            raise ValueError(""OpenAI is unavailable: invalid API key"") from e","Point(row=130, column=0)","Point(row=215, column=77)",,classic/original_autogpt/autogpt/app/config.py
_safe_split,function,Split a string by a separator. Return an empty list if the string is None.,"def _safe_split(s: Union[str, None], sep: str = "","") -> list[str]:
    """"""Split a string by a separator. Return an empty list if the string is None.""""""
    if s is None:
        return []
    return s.split(sep)","Point(row=218, column=0)","Point(row=222, column=23)",,classic/original_autogpt/autogpt/app/config.py
setup_telemetry,function,,"def setup_telemetry() -> None:
    if os.getenv(""TELEMETRY_OPT_IN"") is None:
        # If no .env file is present, don't bother asking to enable telemetry,
        # to prevent repeated asking in non-persistent environments.
        if not env_file_exists():
            return

        allow_telemetry = click.prompt(
            f""""""
{Style.BRIGHT}‚ùì Do you want to enable telemetry? ‚ùì{Style.NORMAL}
This means AutoGPT will send diagnostic data to the core development team when something
goes wrong, and will help us to diagnose and fix problems earlier and faster. It also
allows us to collect basic performance data, which helps us find bottlenecks and other
things that slow down the application.

By entering 'yes', you confirm that you have read and agree to our Privacy Policy,
which is available here:
https://www.notion.so/auto-gpt/Privacy-Policy-ab11c9c20dbd4de1a15dcffe84d77984

Please enter 'yes' or 'no'"""""",
            type=bool,
        )
        set_env_config_value(""TELEMETRY_OPT_IN"", ""true"" if allow_telemetry else ""false"")
        click.echo(
            f""‚ù§Ô∏è  Thank you! Telemetry is {Fore.GREEN}enabled{Fore.RESET}.""
            if allow_telemetry
            else f""üëç Telemetry is {Fore.RED}disabled{Fore.RESET}.""
        )
        click.echo(
            ""üí° If you ever change your mind, you can change 'TELEMETRY_OPT_IN' in .env""
        )
        click.echo()

    if os.getenv(""TELEMETRY_OPT_IN"", """").lower() == ""true"":
        _setup_sentry()","Point(row=13, column=0)","Point(row=47, column=23)",,classic/original_autogpt/autogpt/app/telemetry.py
_setup_sentry,function,,"def _setup_sentry() -> None:
    import sentry_sdk

    sentry_sdk.init(
        dsn=""https://dc266f2f7a2381194d1c0fa36dff67d8@o4505260022104064.ingest.sentry.io/4506739844710400"",  # noqa
        enable_tracing=True,
        environment=os.getenv(
            ""TELEMETRY_ENVIRONMENT"",
            ""production"" if not vcs_state_diverges_from_master() else ""dev"",
        ),
    )

    # Allow Sentry to distinguish between users
    sentry_sdk.set_user({""email"": get_git_user_email(), ""ip_address"": ""{{auto}}""})","Point(row=50, column=0)","Point(row=63, column=82)",,classic/original_autogpt/autogpt/app/telemetry.py
apply_overrides_to_ai_settings,function,,"def apply_overrides_to_ai_settings(
    ai_profile: AIProfile,
    directives: AIDirectives,
    override_name: Optional[str] = """",
    override_role: Optional[str] = """",
    replace_directives: bool = False,
    resources: Optional[list[str]] = None,
    constraints: Optional[list[str]] = None,
    best_practices: Optional[list[str]] = None,
):
    if override_name:
        ai_profile.ai_name = override_name
    if override_role:
        ai_profile.ai_role = override_role

    if replace_directives:
        if resources:
            directives.resources = resources
        if constraints:
            directives.constraints = constraints
        if best_practices:
            directives.best_practices = best_practices
    else:
        if resources:
            directives.resources += resources
        if constraints:
            directives.constraints += constraints
        if best_practices:
            directives.best_practices += best_practices","Point(row=15, column=0)","Point(row=43, column=55)",,classic/original_autogpt/autogpt/app/setup.py
interactively_revise_ai_settings,function,"Interactively revise the AI settings.

    Args:
        ai_profile (AIConfig): The current AI profile.
        ai_directives (AIDirectives): The current AI directives.
        app_config (Config): The application configuration.

    Returns:
        AIConfig: The revised AI settings.
","async def interactively_revise_ai_settings(
    ai_profile: AIProfile,
    directives: AIDirectives,
    app_config: AppConfig,
):
    """"""Interactively revise the AI settings.

    Args:
        ai_profile (AIConfig): The current AI profile.
        ai_directives (AIDirectives): The current AI directives.
        app_config (Config): The application configuration.

    Returns:
        AIConfig: The revised AI settings.
    """"""
    logger = logging.getLogger(""revise_ai_profile"")

    revised = False

    while True:
        # Print the current AI configuration
        print_ai_settings(
            title=""Current AI Settings"" if not revised else ""Revised AI Settings"",
            ai_profile=ai_profile,
            directives=directives,
            logger=logger,
        )

        if (
            clean_input(""Continue with these settings? [Y/n]"").lower()
            or app_config.authorise_key
        ) == app_config.authorise_key:
            break

        # Ask for revised ai_profile
        ai_profile.ai_name = (
            clean_input(""Enter AI name (or press enter to keep current):"")
            or ai_profile.ai_name
        )
        ai_profile.ai_role = (
            clean_input(""Enter new AI role (or press enter to keep current):"")
            or ai_profile.ai_role
        )

        # Revise constraints
        i = 0
        while i < len(directives.constraints):
            constraint = directives.constraints[i]
            print_attribute(f""Constraint {i+1}:"", f'""{constraint}""')
            new_constraint = (
                clean_input(
                    f""Enter new constraint {i+1}""
                    "" (press enter to keep current, or '-' to remove):"",
                )
                or constraint
            )

            if new_constraint == ""-"":
                directives.constraints.remove(constraint)
                continue
            elif new_constraint:
                directives.constraints[i] = new_constraint

            i += 1

        # Add new constraints
        while True:
            new_constraint = clean_input(
                ""Press enter to finish, or enter a constraint to add:"",
            )
            if not new_constraint:
                break
            directives.constraints.append(new_constraint)

        # Revise resources
        i = 0
        while i < len(directives.resources):
            resource = directives.resources[i]
            print_attribute(f""Resource {i+1}:"", f'""{resource}""')
            new_resource = (
                clean_input(
                    f""Enter new resource {i+1}""
                    "" (press enter to keep current, or '-' to remove):"",
                )
                or resource
            )
            if new_resource == ""-"":
                directives.resources.remove(resource)
                continue
            elif new_resource:
                directives.resources[i] = new_resource

            i += 1

        # Add new resources
        while True:
            new_resource = clean_input(
                ""Press enter to finish, or enter a resource to add:"",
            )
            if not new_resource:
                break
            directives.resources.append(new_resource)

        # Revise best practices
        i = 0
        while i < len(directives.best_practices):
            best_practice = directives.best_practices[i]
            print_attribute(f""Best Practice {i+1}:"", f'""{best_practice}""')
            new_best_practice = (
                clean_input(
                    f""Enter new best practice {i+1}""
                    "" (press enter to keep current, or '-' to remove):"",
                )
                or best_practice
            )
            if new_best_practice == ""-"":
                directives.best_practices.remove(best_practice)
                continue
            elif new_best_practice:
                directives.best_practices[i] = new_best_practice

            i += 1

        # Add new best practices
        while True:
            new_best_practice = clean_input(
                ""Press enter to finish, or add a best practice to add:"",
            )
            if not new_best_practice:
                break
            directives.best_practices.append(new_best_practice)

        revised = True

    return ai_profile, directives","Point(row=46, column=0)","Point(row=180, column=33)",,classic/original_autogpt/autogpt/app/setup.py
print_ai_settings,function,,"def print_ai_settings(
    ai_profile: AIProfile,
    directives: AIDirectives,
    logger: logging.Logger,
    title: str = ""AI Settings"",
):
    print_attribute(title, """")
    print_attribute(""-"" * len(title), """")
    print_attribute(""Name :"", ai_profile.ai_name)
    print_attribute(""Role :"", ai_profile.ai_role)

    print_attribute(""Constraints:"", """" if directives.constraints else ""(none)"")
    for constraint in directives.constraints:
        logger.info(f""- {constraint}"")
    print_attribute(""Resources:"", """" if directives.resources else ""(none)"")
    for resource in directives.resources:
        logger.info(f""- {resource}"")
    print_attribute(""Best practices:"", """" if directives.best_practices else ""(none)"")
    for best_practice in directives.best_practices:
        logger.info(f""- {best_practice}"")","Point(row=183, column=0)","Point(row=202, column=41)",,classic/original_autogpt/autogpt/app/setup.py
cli,function,,"def cli(ctx: click.Context):
    setup_telemetry()

    # Invoke `run` by default
    if ctx.invoked_subcommand is None:
        ctx.invoke(run)","Point(row=13, column=0)","Point(row=18, column=23)",,classic/original_autogpt/autogpt/app/cli.py
run,function,"
    Sets up and runs an agent, based on the task specified by the user, or resumes an
    existing agent.
","def run(
    continuous: bool,
    continuous_limit: Optional[int],
    speak: bool,
    install_plugin_deps: bool,
    skip_news: bool,
    skip_reprompt: bool,
    ai_name: Optional[str],
    ai_role: Optional[str],
    resource: tuple[str],
    constraint: tuple[str],
    best_practice: tuple[str],
    override_directives: bool,
    debug: bool,
    log_level: Optional[str],
    log_format: Optional[str],
    log_file_format: Optional[str],
    component_config_file: Optional[Path],
) -> None:
    """"""
    Sets up and runs an agent, based on the task specified by the user, or resumes an
    existing agent.
    """"""
    # Put imports inside function to avoid importing everything when starting the CLI
    from autogpt.app.main import run_auto_gpt

    run_auto_gpt(
        continuous=continuous,
        continuous_limit=continuous_limit,
        skip_reprompt=skip_reprompt,
        speak=speak,
        debug=debug,
        log_level=log_level,
        log_format=log_format,
        log_file_format=log_file_format,
        skip_news=skip_news,
        install_plugin_deps=install_plugin_deps,
        override_ai_name=ai_name,
        override_ai_role=ai_role,
        resources=list(resource),
        constraints=list(constraint),
        best_practices=list(best_practice),
        override_directives=override_directives,
        component_config_file=component_config_file,
    )","Point(row=117, column=0)","Point(row=161, column=5)",,classic/original_autogpt/autogpt/app/cli.py
serve,function,"
    Starts an Agent Protocol compliant AutoGPT server, which creates a custom agent for
    every task.
","def serve(
    install_plugin_deps: bool,
    debug: bool,
    log_level: Optional[str],
    log_format: Optional[str],
    log_file_format: Optional[str],
) -> None:
    """"""
    Starts an Agent Protocol compliant AutoGPT server, which creates a custom agent for
    every task.
    """"""
    # Put imports inside function to avoid importing everything when starting the CLI
    from autogpt.app.main import run_auto_gpt_server

    run_auto_gpt_server(
        debug=debug,
        log_level=log_level,
        log_format=log_format,
        log_file_format=log_file_format,
        install_plugin_deps=install_plugin_deps,
    )","Point(row=191, column=0)","Point(row=211, column=5)",,classic/original_autogpt/autogpt/app/cli.py
get_bulletin_from_web,function,,"def get_bulletin_from_web():
    try:
        response = requests.get(
            ""https://raw.githubusercontent.com/Significant-Gravitas/AutoGPT/master/classic/original_autogpt/BULLETIN.md""  # noqa: E501
        )
        if response.status_code == 200:
            return response.text
    except requests.exceptions.RequestException:
        pass

    return """"","Point(row=21, column=0)","Point(row=31, column=13)",,classic/original_autogpt/autogpt/app/utils.py
get_current_git_branch,function,,"def get_current_git_branch() -> str:
    try:
        repo = Repo(search_parent_directories=True)
        branch = repo.active_branch
        return branch.name
    except InvalidGitRepositoryError:
        return """"","Point(row=34, column=0)","Point(row=40, column=17)",,classic/original_autogpt/autogpt/app/utils.py
vcs_state_diverges_from_master,function,"
    Returns whether a git repo is present and contains changes that are not in `master`.
","def vcs_state_diverges_from_master() -> bool:
    """"""
    Returns whether a git repo is present and contains changes that are not in `master`.
    """"""
    paths_we_care_about = ""classic/original_autogpt/classic/original_autogpt/**/*.py""
    try:
        repo = Repo(search_parent_directories=True)

        # Check for uncommitted changes in the specified path
        uncommitted_changes = repo.index.diff(None, paths=paths_we_care_about)
        if uncommitted_changes:
            return True

        # Find OG AutoGPT remote
        for remote in repo.remotes:
            if remote.url.endswith(
                tuple(
                    # All permutations of old/new repo name and HTTP(S)/Git URLs
                    f""{prefix}{path}""
                    for prefix in (""://github.com/"", ""git@github.com:"")
                    for path in (
                        f""Significant-Gravitas/{n}.git"" for n in (""AutoGPT"", ""Auto-GPT"")
                    )
                )
            ):
                og_remote = remote
                break
        else:
            # Original AutoGPT remote is not configured: assume local codebase diverges
            return True

        master_branch = og_remote.refs.master
        with contextlib.suppress(StopIteration):
            next(repo.iter_commits(f""HEAD..{master_branch}"", paths=paths_we_care_about))
            # Local repo is one or more commits ahead of OG AutoGPT master branch
            return True

        # Relevant part of the codebase is on master
        return False
    except InvalidGitRepositoryError:
        # No git repo present: assume codebase is a clean download
        return False","Point(row=43, column=0)","Point(row=84, column=20)",,classic/original_autogpt/autogpt/app/utils.py
get_git_user_email,function,,"def get_git_user_email() -> str:
    try:
        repo = Repo(search_parent_directories=True)
        return cast(str, repo.config_reader().get_value(""user"", ""email"", default=""""))
    except InvalidGitRepositoryError:
        return """"","Point(row=87, column=0)","Point(row=92, column=17)",,classic/original_autogpt/autogpt/app/utils.py
get_latest_bulletin,function,,"def get_latest_bulletin() -> tuple[str, bool]:
    exists = os.path.exists(""data/CURRENT_BULLETIN.md"")
    current_bulletin = """"
    if exists:
        current_bulletin = open(
            ""data/CURRENT_BULLETIN.md"", ""r"", encoding=""utf-8""
        ).read()
    new_bulletin = get_bulletin_from_web()
    is_new_news = new_bulletin != """" and new_bulletin != current_bulletin

    news_header = Fore.YELLOW + ""Welcome to AutoGPT!\n""
    if new_bulletin or current_bulletin:
        news_header += (
            ""Below you'll find the latest AutoGPT News and feature updates!\n""
            ""If you don't wish to see this message, you ""
            ""can run AutoGPT with the *--skip-news* flag.\n""
        )

    if new_bulletin and is_new_news:
        open(""data/CURRENT_BULLETIN.md"", ""w"", encoding=""utf-8"").write(new_bulletin)
        current_bulletin = f""{Fore.RED}::NEW BULLETIN::{Fore.RESET}\n\n{new_bulletin}""

    return f""{news_header}\n{current_bulletin}"", is_new_news","Point(row=95, column=0)","Point(row=117, column=60)",,classic/original_autogpt/autogpt/app/utils.py
markdown_to_ansi_style,function,,"def markdown_to_ansi_style(markdown: str):
    ansi_lines: list[str] = []
    for line in markdown.split(""\n""):
        line_style = """"

        if line.startswith(""# ""):
            line_style += Style.BRIGHT
        else:
            line = re.sub(
                r""(?<!\*)\*(\*?[^*]+\*?)\*(?!\*)"",
                rf""{Style.BRIGHT}\1{Style.NORMAL}"",
                line,
            )

        if re.match(r""^#+ "", line) is not None:
            line_style += Fore.CYAN
            line = re.sub(r""^#+ "", """", line)

        ansi_lines.append(f""{line_style}{line}{Style.RESET_ALL}"")
    return ""\n"".join(ansi_lines)","Point(row=120, column=0)","Point(row=139, column=32)",,classic/original_autogpt/autogpt/app/utils.py
get_legal_warning,function,,"def get_legal_warning() -> str:
    legal_text = """"""
## DISCLAIMER AND INDEMNIFICATION AGREEMENT
### PLEASE READ THIS DISCLAIMER AND INDEMNIFICATION AGREEMENT CAREFULLY BEFORE USING THE AUTOGPT SYSTEM. BY USING THE AUTOGPT SYSTEM, YOU AGREE TO BE BOUND BY THIS AGREEMENT.

## Introduction
AutoGPT (the ""System"") is a project that connects a GPT-like artificial intelligence system to the internet and allows it to automate tasks. While the System is designed to be useful and efficient, there may be instances where the System could perform actions that may cause harm or have unintended consequences.

## No Liability for Actions of the System
The developers, contributors, and maintainers of the AutoGPT project (collectively, the ""Project Parties"") make no warranties or representations, express or implied, about the System's performance, accuracy, reliability, or safety. By using the System, you understand and agree that the Project Parties shall not be liable for any actions taken by the System or any consequences resulting from such actions.

## User Responsibility and Respondeat Superior Liability
As a user of the System, you are responsible for supervising and monitoring the actions of the System while it is operating on your
behalf. You acknowledge that using the System could expose you to potential liability including but not limited to respondeat superior and you agree to assume all risks and liabilities associated with such potential liability.

## Indemnification
By using the System, you agree to indemnify, defend, and hold harmless the Project Parties from and against any and all claims, liabilities, damages, losses, or expenses (including reasonable attorneys' fees and costs) arising out of or in connection with your use of the System, including, without limitation, any actions taken by the System on your behalf, any failure to properly supervise or monitor the System, and any resulting harm or unintended consequences.
    """"""  # noqa: E501
    return legal_text","Point(row=142, column=0)","Point(row=160, column=21)",,classic/original_autogpt/autogpt/app/utils.py
print_motd,function,,"def print_motd(logger: logging.Logger):
    motd, is_new_motd = get_latest_bulletin()
    if motd:
        motd = markdown_to_ansi_style(motd)
        for motd_line in motd.split(""\n""):
            logger.info(
                extra={
                    ""title"": ""NEWS:"",
                    ""title_color"": Fore.GREEN,
                    ""preserve_color"": True,
                },
                msg=motd_line,
            )
        if is_new_motd:
            input(
                Fore.MAGENTA
                + Style.BRIGHT
                + ""NEWS: Bulletin was updated! Press Enter to continue...""
                + Style.RESET_ALL
            )","Point(row=163, column=0)","Point(row=182, column=13)",,classic/original_autogpt/autogpt/app/utils.py
print_git_branch_info,function,,"def print_git_branch_info(logger: logging.Logger):
    git_branch = get_current_git_branch()
    if git_branch and git_branch != ""master"":
        logger.warning(
            f""You are running on `{git_branch}` branch""
            "" - this is not a supported branch.""
        )","Point(row=185, column=0)","Point(row=191, column=9)",,classic/original_autogpt/autogpt/app/utils.py
print_python_version_info,function,,"def print_python_version_info(logger: logging.Logger):
    if sys.version_info < (3, 10):
        logger.error(
            ""WARNING: You are running on an older version of Python. ""
            ""Some people have observed problems with certain ""
            ""parts of AutoGPT with this version. ""
            ""Please consider upgrading to Python 3.10 or higher."",
        )","Point(row=194, column=0)","Point(row=201, column=9)",,classic/original_autogpt/autogpt/app/utils.py
env_file_exists,function,,"def env_file_exists() -> bool:
    return ENV_FILE_PATH.is_file()","Point(row=207, column=0)","Point(row=208, column=34)",,classic/original_autogpt/autogpt/app/utils.py
set_env_config_value,function,Sets the specified env variable and updates it in .env as well,"def set_env_config_value(key: str, value: str) -> None:
    """"""Sets the specified env variable and updates it in .env as well""""""
    os.environ[key] = value

    with ENV_FILE_PATH.open(""r+"") as file:
        lines = file.readlines()
        file.seek(0)
        key_already_in_file = False
        for line in lines:
            if re.match(rf""^(?:# )?{key}=.*$"", line):
                file.write(f""{key}={value}\n"")
                key_already_in_file = True
            else:
                file.write(line)

        if not key_already_in_file:
            file.write(f""{key}={value}\n"")

        file.truncate()","Point(row=211, column=0)","Point(row=229, column=23)",,classic/original_autogpt/autogpt/app/utils.py
is_port_free,function,,"def is_port_free(port: int, host: str = ""127.0.0.1""):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        try:
            s.bind((host, port))  # Try to bind to the port
            return True  # If successful, the port is free
        except OSError:
            return False  # If failed, the port is likely in use","Point(row=232, column=0)","Point(row=238, column=64)",,classic/original_autogpt/autogpt/app/utils.py
coroutine,function,,"def coroutine(f: Callable[P, Coroutine[Any, Any, T]]) -> Callable[P, T]:
    @functools.wraps(f)
    def wrapper(*args: P.args, **kwargs: P.kwargs):
        return asyncio.run(f(*args, **kwargs))

    return wrapper","Point(row=241, column=0)","Point(row=246, column=18)",,classic/original_autogpt/autogpt/app/utils.py
coroutine.wrapper,function,,"def wrapper(*args: P.args, **kwargs: P.kwargs):
        return asyncio.run(f(*args, **kwargs))","Point(row=243, column=4)","Point(row=244, column=46)",,classic/original_autogpt/autogpt/app/utils.py
clean_input,function,,"def clean_input(prompt: str = """"):
    try:
        # ask for input, default when just pressing Enter is y
        logger.debug(""Asking user via keyboard..."")

        return click.prompt(
            text=prompt, prompt_suffix="" "", default="""", show_default=False
        )
    except KeyboardInterrupt:
        logger.info(""You interrupted AutoGPT"")
        logger.info(""Quitting..."")
        exit(0)","Point(row=7, column=0)","Point(row=18, column=15)",,classic/original_autogpt/autogpt/app/input.py
apply_overrides_to_config,function,"Updates the config object with the given arguments.

    Args:
        config (Config): The config object to update.
        continuous (bool): Whether to run in continuous mode.
        continuous_limit (int): The number of times to run in continuous mode.
        skip_reprompt (bool): Whether to skip the re-prompting messages on start.
        speak (bool): Whether to enable speak mode.
        debug (bool): Whether to enable debug mode.
        log_level (int): The global log level for the application.
        log_format (str): The format for the log(s).
        log_file_format (str): Override the format for the log file.
        skips_news (bool): Whether to suppress the output of latest news on startup.
","async def apply_overrides_to_config(
    config: AppConfig,
    continuous: bool = False,
    continuous_limit: Optional[int] = None,
    skip_reprompt: bool = False,
    skip_news: bool = False,
) -> None:
    """"""Updates the config object with the given arguments.

    Args:
        config (Config): The config object to update.
        continuous (bool): Whether to run in continuous mode.
        continuous_limit (int): The number of times to run in continuous mode.
        skip_reprompt (bool): Whether to skip the re-prompting messages on start.
        speak (bool): Whether to enable speak mode.
        debug (bool): Whether to enable debug mode.
        log_level (int): The global log level for the application.
        log_format (str): The format for the log(s).
        log_file_format (str): Override the format for the log file.
        skips_news (bool): Whether to suppress the output of latest news on startup.
    """"""
    config.continuous_mode = False

    if continuous:
        logger.warning(
            ""Continuous mode is not recommended. It is potentially dangerous and may""
            "" cause your AI to run forever or carry out actions you would not usually""
            "" authorise. Use at your own risk."",
        )
        config.continuous_mode = True

        if continuous_limit:
            config.continuous_limit = continuous_limit

    # Check if continuous limit is used without continuous mode
    if continuous_limit and not continuous:
        raise click.UsageError(""--continuous-limit can only be used with --continuous"")

    # Check availability of configured LLMs; fallback to other LLM if unavailable
    config.fast_llm, config.smart_llm = await check_models(
        (config.fast_llm, ""fast_llm""), (config.smart_llm, ""smart_llm"")
    )

    if skip_reprompt:
        config.skip_reprompt = True

    if skip_news:
        config.skip_news = True","Point(row=14, column=0)","Point(row=61, column=31)",,classic/original_autogpt/autogpt/app/configurator.py
check_models,function,"Check if model is available for use. If not, return gpt-3.5-turbo.","async def check_models(
    *models: tuple[ModelName, Literal[""smart_llm"", ""fast_llm""]]
) -> tuple[ModelName, ...]:
    """"""Check if model is available for use. If not, return gpt-3.5-turbo.""""""
    multi_provider = MultiProvider()
    available_models = await multi_provider.get_available_chat_models()

    checked_models: list[ModelName] = []
    for model, model_type in models:
        if any(model == m.name for m in available_models):
            checked_models.append(model)
        else:
            logger.warning(
                f""You don't have access to {model}. ""
                f""Setting {model_type} to {GPT_3_MODEL}.""
            )
            checked_models.append(GPT_3_MODEL)

    return tuple(checked_models)","Point(row=64, column=0)","Point(row=82, column=32)",,classic/original_autogpt/autogpt/app/configurator.py
run_auto_gpt,function,,"async def run_auto_gpt(
    continuous: bool = False,
    continuous_limit: Optional[int] = None,
    skip_reprompt: bool = False,
    speak: bool = False,
    debug: bool = False,
    log_level: Optional[str] = None,
    log_format: Optional[str] = None,
    log_file_format: Optional[str] = None,
    skip_news: bool = False,
    install_plugin_deps: bool = False,
    override_ai_name: Optional[str] = None,
    override_ai_role: Optional[str] = None,
    resources: Optional[list[str]] = None,
    constraints: Optional[list[str]] = None,
    best_practices: Optional[list[str]] = None,
    override_directives: bool = False,
    component_config_file: Optional[Path] = None,
):
    # Set up configuration
    config = ConfigBuilder.build_config_from_env()
    # Storage
    local = config.file_storage_backend == FileStorageBackendName.LOCAL
    restrict_to_root = not local or config.restrict_to_workspace
    file_storage = get_storage(
        config.file_storage_backend,
        root_path=Path(""data""),
        restrict_to_root=restrict_to_root,
    )
    file_storage.initialize()

    # Set up logging module
    if speak:
        config.tts_config.speak_mode = True
    configure_logging(
        debug=debug,
        level=log_level,
        log_format=log_format,
        log_file_format=log_file_format,
        config=config.logging,
        tts_config=config.tts_config,
    )

    await assert_config_has_required_llm_api_keys(config)

    await apply_overrides_to_config(
        config=config,
        continuous=continuous,
        continuous_limit=continuous_limit,
        skip_reprompt=skip_reprompt,
        skip_news=skip_news,
    )

    llm_provider = _configure_llm_provider(config)

    logger = logging.getLogger(__name__)

    if config.continuous_mode:
        for line in get_legal_warning().split(""\n""):
            logger.warning(
                extra={
                    ""title"": ""LEGAL:"",
                    ""title_color"": Fore.RED,
                    ""preserve_color"": True,
                },
                msg=markdown_to_ansi_style(line),
            )

    if not config.skip_news:
        print_motd(logger)
        print_git_branch_info(logger)
        print_python_version_info(logger)
        print_attribute(""Smart LLM"", config.smart_llm)
        print_attribute(""Fast LLM"", config.fast_llm)
        if config.continuous_mode:
            print_attribute(""Continuous Mode"", ""ENABLED"", title_color=Fore.YELLOW)
            if continuous_limit:
                print_attribute(""Continuous Limit"", config.continuous_limit)
        if config.tts_config.speak_mode:
            print_attribute(""Speak Mode"", ""ENABLED"")
        if we_are_running_in_a_docker_container() or is_docker_available():
            print_attribute(""Code Execution"", ""ENABLED"")
        else:
            print_attribute(
                ""Code Execution"",
                ""DISABLED (Docker unavailable)"",
                title_color=Fore.YELLOW,
            )

    # Let user choose an existing agent to run
    agent_manager = AgentManager(file_storage)
    existing_agents = agent_manager.list_agents()
    load_existing_agent = """"
    if existing_agents:
        print(
            ""Existing agents\n---------------\n""
            + ""\n"".join(f""{i} - {id}"" for i, id in enumerate(existing_agents, 1))
        )
        load_existing_agent = clean_input(
            ""Enter the number or name of the agent to run,""
            "" or hit enter to create a new one:"",
        )
        if re.match(r""^\d+$"", load_existing_agent.strip()) and 0 < int(
            load_existing_agent
        ) <= len(existing_agents):
            load_existing_agent = existing_agents[int(load_existing_agent) - 1]

        if load_existing_agent != """" and load_existing_agent not in existing_agents:
            logger.info(
                f""Unknown agent '{load_existing_agent}', ""
                f""creating a new one instead."",
                extra={""color"": Fore.YELLOW},
            )
            load_existing_agent = """"

    # Either load existing or set up new agent state
    agent = None
    agent_state = None

    ############################
    # Resume an Existing Agent #
    ############################
    if load_existing_agent:
        agent_state = None
        while True:
            answer = clean_input(""Resume? [Y/n]"")
            if answer == """" or answer.lower() == ""y"":
                agent_state = agent_manager.load_agent_state(load_existing_agent)
                break
            elif answer.lower() == ""n"":
                break

    if agent_state:
        agent = configure_agent_with_state(
            state=agent_state,
            app_config=config,
            file_storage=file_storage,
            llm_provider=llm_provider,
        )
        apply_overrides_to_ai_settings(
            ai_profile=agent.state.ai_profile,
            directives=agent.state.directives,
            override_name=override_ai_name,
            override_role=override_ai_role,
            resources=resources,
            constraints=constraints,
            best_practices=best_practices,
            replace_directives=override_directives,
        )

        if (
            (current_episode := agent.event_history.current_episode)
            and current_episode.action.use_tool.name == FINISH_COMMAND
            and not current_episode.result
        ):
            # Agent was resumed after `finish` -> rewrite result of `finish` action
            finish_reason = current_episode.action.use_tool.arguments[""reason""]
            print(f""Agent previously self-terminated; reason: '{finish_reason}'"")
            new_assignment = clean_input(
                ""Please give a follow-up question or assignment:""
            )
            agent.event_history.register_result(
                ActionInterruptedByHuman(feedback=new_assignment)
            )

        # If any of these are specified as arguments,
        #  assume the user doesn't want to revise them
        if not any(
            [
                override_ai_name,
                override_ai_role,
                resources,
                constraints,
                best_practices,
            ]
        ):
            ai_profile, ai_directives = await interactively_revise_ai_settings(
                ai_profile=agent.state.ai_profile,
                directives=agent.state.directives,
                app_config=config,
            )
        else:
            logger.info(""AI config overrides specified through CLI; skipping revision"")

    ######################
    # Set up a new Agent #
    ######################
    if not agent:
        task = """"
        while task.strip() == """":
            task = clean_input(
                ""Enter the task that you want AutoGPT to execute,""
                "" with as much detail as possible:"",
            )

        ai_profile = AIProfile()
        additional_ai_directives = AIDirectives()
        apply_overrides_to_ai_settings(
            ai_profile=ai_profile,
            directives=additional_ai_directives,
            override_name=override_ai_name,
            override_role=override_ai_role,
            resources=resources,
            constraints=constraints,
            best_practices=best_practices,
            replace_directives=override_directives,
        )

        # If any of these are specified as arguments,
        #  assume the user doesn't want to revise them
        if not any(
            [
                override_ai_name,
                override_ai_role,
                resources,
                constraints,
                best_practices,
            ]
        ):
            (
                ai_profile,
                additional_ai_directives,
            ) = await interactively_revise_ai_settings(
                ai_profile=ai_profile,
                directives=additional_ai_directives,
                app_config=config,
            )
        else:
            logger.info(""AI config overrides specified through CLI; skipping revision"")

        agent = create_agent(
            agent_id=agent_manager.generate_id(ai_profile.ai_name),
            task=task,
            ai_profile=ai_profile,
            directives=additional_ai_directives,
            app_config=config,
            file_storage=file_storage,
            llm_provider=llm_provider,
        )

        file_manager = agent.file_manager

        if file_manager and not agent.config.allow_fs_access:
            logger.info(
                f""{Fore.YELLOW}""
                ""NOTE: All files/directories created by this agent can be found ""
                f""inside its workspace at:{Fore.RESET} {file_manager.workspace.root}"",
                extra={""preserve_color"": True},
            )

        # TODO: re-evaluate performance benefit of task-oriented profiles
        # # Concurrently generate a custom profile for the agent and apply it once done
        # def update_agent_directives(
        #     task: asyncio.Task[tuple[AIProfile, AIDirectives]]
        # ):
        #     logger.debug(f""Updating AIProfile: {task.result()[0]}"")
        #     logger.debug(f""Adding AIDirectives: {task.result()[1]}"")
        #     agent.state.ai_profile = task.result()[0]
        #     agent.state.directives = agent.state.directives + task.result()[1]

        # asyncio.create_task(
        #     generate_agent_profile_for_task(
        #         task, app_config=config, llm_provider=llm_provider
        #     )
        # ).add_done_callback(update_agent_directives)

    # Load component configuration from file
    if _config_file := component_config_file or config.component_config_file:
        try:
            logger.info(f""Loading component configuration from {_config_file}"")
            agent.load_component_configs(_config_file.read_text())
        except Exception as e:
            logger.error(f""Could not load component configuration: {e}"")

    #################
    # Run the Agent #
    #################
    try:
        await run_interaction_loop(agent)
    except AgentTerminated:
        agent_id = agent.state.agent_id
        logger.info(f""Saving state of {agent_id}..."")

        # Allow user to Save As other ID
        save_as_id = clean_input(
            f""Press enter to save as '{agent_id}',""
            "" or enter a different ID to save to:"",
        )
        # TODO: allow many-to-one relations of agents and workspaces
        await agent.file_manager.save_state(
            save_as_id.strip() if not save_as_id.isspace() else None
        )","Point(row=59, column=0)","Point(row=350, column=9)",,classic/original_autogpt/autogpt/app/main.py
run_auto_gpt_server,function,,"async def run_auto_gpt_server(
    debug: bool = False,
    log_level: Optional[str] = None,
    log_format: Optional[str] = None,
    log_file_format: Optional[str] = None,
    install_plugin_deps: bool = False,
):
    from .agent_protocol_server import AgentProtocolServer

    config = ConfigBuilder.build_config_from_env()
    # Storage
    local = config.file_storage_backend == FileStorageBackendName.LOCAL
    restrict_to_root = not local or config.restrict_to_workspace
    file_storage = get_storage(
        config.file_storage_backend,
        root_path=Path(""data""),
        restrict_to_root=restrict_to_root,
    )
    file_storage.initialize()

    # Set up logging module
    configure_logging(
        debug=debug,
        level=log_level,
        log_format=log_format,
        log_file_format=log_file_format,
        config=config.logging,
        tts_config=config.tts_config,
    )

    await assert_config_has_required_llm_api_keys(config)

    await apply_overrides_to_config(
        config=config,
    )

    llm_provider = _configure_llm_provider(config)

    # Set up & start server
    database = AgentDB(
        database_string=os.getenv(""AP_SERVER_DB_URL"", ""sqlite:///data/ap_server.db""),
        debug_enabled=debug,
    )
    port: int = int(os.getenv(""AP_SERVER_PORT"", default=8000))
    server = AgentProtocolServer(
        app_config=config,
        database=database,
        file_storage=file_storage,
        llm_provider=llm_provider,
    )
    await server.start(port=port)

    logging.getLogger().info(
        f""Total OpenAI session cost: ""
        f""${round(sum(b.total_cost for b in server._task_budgets.values()), 2)}""
    )","Point(row=354, column=0)","Point(row=409, column=5)",,classic/original_autogpt/autogpt/app/main.py
_configure_llm_provider,function,,"def _configure_llm_provider(config: AppConfig) -> MultiProvider:
    multi_provider = MultiProvider()
    for model in [config.smart_llm, config.fast_llm]:
        # Ensure model providers for configured LLMs are available
        multi_provider.get_model_provider(model)
    return multi_provider","Point(row=412, column=0)","Point(row=417, column=25)",,classic/original_autogpt/autogpt/app/main.py
_get_cycle_budget,function,,"def _get_cycle_budget(continuous_mode: bool, continuous_limit: int) -> int | float:
    # Translate from the continuous_mode/continuous_limit config
    # to a cycle_budget (maximum number of cycles to run without checking in with the
    # user) and a count of cycles_remaining before we check in..
    if continuous_mode:
        cycle_budget = continuous_limit if continuous_limit else math.inf
    else:
        cycle_budget = 1

    return cycle_budget","Point(row=420, column=0)","Point(row=429, column=23)",,classic/original_autogpt/autogpt/app/main.py
UserFeedback,class,Enum for user feedback.,"class UserFeedback(str, enum.Enum):
    """"""Enum for user feedback.""""""

    AUTHORIZE = ""GENERATE NEXT COMMAND JSON""
    EXIT = ""EXIT""
    TEXT = ""TEXT""","Point(row=432, column=0)","Point(row=437, column=17)",,classic/original_autogpt/autogpt/app/main.py
run_interaction_loop,function,"Run the main interaction loop for the agent.

    Args:
        agent: The agent to run the interaction loop for.

    Returns:
        None
","async def run_interaction_loop(
    agent: ""Agent"",
) -> None:
    """"""Run the main interaction loop for the agent.

    Args:
        agent: The agent to run the interaction loop for.

    Returns:
        None
    """"""
    # These contain both application config and agent config, so grab them here.
    app_config = agent.app_config
    ai_profile = agent.state.ai_profile
    logger = logging.getLogger(__name__)

    cycle_budget = cycles_remaining = _get_cycle_budget(
        app_config.continuous_mode, app_config.continuous_limit
    )
    spinner = Spinner(
        ""Thinking..."", plain_output=app_config.logging.plain_console_output
    )
    stop_reason = None

    def graceful_agent_interrupt(signum: int, frame: Optional[FrameType]) -> None:
        nonlocal cycle_budget, cycles_remaining, spinner, stop_reason
        if stop_reason:
            logger.error(""Quitting immediately..."")
            sys.exit()
        if cycles_remaining in [0, 1]:
            logger.warning(""Interrupt signal received: shutting down gracefully."")
            logger.warning(
                ""Press Ctrl+C again if you want to stop AutoGPT immediately.""
            )
            stop_reason = AgentTerminated(""Interrupt signal received"")
        else:
            restart_spinner = spinner.running
            if spinner.running:
                spinner.stop()

            logger.error(
                ""Interrupt signal received: stopping continuous command execution.""
            )
            cycles_remaining = 1
            if restart_spinner:
                spinner.start()

    def handle_stop_signal() -> None:
        if stop_reason:
            raise stop_reason

    # Set up an interrupt signal for the agent.
    signal.signal(signal.SIGINT, graceful_agent_interrupt)

    #########################
    # Application Main Loop #
    #########################

    # Keep track of consecutive failures of the agent
    consecutive_failures = 0

    while cycles_remaining > 0:
        logger.debug(f""Cycle budget: {cycle_budget}; remaining: {cycles_remaining}"")

        ########
        # Plan #
        ########
        handle_stop_signal()
        # Have the agent determine the next action to take.
        if not (_ep := agent.event_history.current_episode) or _ep.result:
            with spinner:
                try:
                    action_proposal = await agent.propose_action()
                except InvalidAgentResponseError as e:
                    logger.warning(f""The agent's thoughts could not be parsed: {e}"")
                    consecutive_failures += 1
                    if consecutive_failures >= 3:
                        logger.error(
                            ""The agent failed to output valid thoughts""
                            f"" {consecutive_failures} times in a row. Terminating...""
                        )
                        raise AgentTerminated(
                            ""The agent failed to output valid thoughts""
                            f"" {consecutive_failures} times in a row.""
                        )
                    continue
        else:
            action_proposal = _ep.action

        consecutive_failures = 0

        ###############
        # Update User #
        ###############
        # Print the assistant's thoughts and the next command to the user.
        update_user(
            ai_profile,
            action_proposal,
            speak_mode=app_config.tts_config.speak_mode,
        )

        ##################
        # Get user input #
        ##################
        handle_stop_signal()
        if cycles_remaining == 1:  # Last cycle
            feedback_type, feedback, new_cycles_remaining = await get_user_feedback(
                app_config,
                ai_profile,
            )

            if feedback_type == UserFeedback.AUTHORIZE:
                if new_cycles_remaining is not None:
                    # Case 1: User is altering the cycle budget.
                    if cycle_budget > 1:
                        cycle_budget = new_cycles_remaining + 1
                    # Case 2: User is running iteratively and
                    #   has initiated a one-time continuous cycle
                    cycles_remaining = new_cycles_remaining + 1
                else:
                    # Case 1: Continuous iteration was interrupted -> resume
                    if cycle_budget > 1:
                        logger.info(
                            f""The cycle budget is {cycle_budget}."",
                            extra={
                                ""title"": ""RESUMING CONTINUOUS EXECUTION"",
                                ""title_color"": Fore.MAGENTA,
                            },
                        )
                    # Case 2: The agent used up its cycle budget -> reset
                    cycles_remaining = cycle_budget + 1
                logger.info(
                    ""-=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-="",
                    extra={""color"": Fore.MAGENTA},
                )
            elif feedback_type == UserFeedback.EXIT:
                logger.warning(""Exiting..."")
                exit()
            else:  # user_feedback == UserFeedback.TEXT
                pass
        else:
            feedback = """"
            # First log new-line so user can differentiate sections better in console
            print()
            if cycles_remaining != math.inf:
                # Print authorized commands left value
                print_attribute(
                    ""AUTHORIZED_COMMANDS_LEFT"", cycles_remaining, title_color=Fore.CYAN
                )

        ###################
        # Execute Command #
        ###################
        # Decrement the cycle counter first to reduce the likelihood of a SIGINT
        # happening during command execution, setting the cycles remaining to 1,
        # and then having the decrement set it to 0, exiting the application.
        if not feedback:
            cycles_remaining -= 1

        if not action_proposal.use_tool:
            continue

        handle_stop_signal()

        if not feedback:
            result = await agent.execute(action_proposal)
        else:
            result = await agent.do_not_execute(action_proposal, feedback)

        if result.status == ""success"":
            logger.info(result, extra={""title"": ""SYSTEM:"", ""title_color"": Fore.YELLOW})
        elif result.status == ""error"":
            logger.warning(
                f""Command {action_proposal.use_tool.name} returned an error: ""
                f""{result.error or result.reason}""
            )","Point(row=440, column=0)","Point(row=615, column=13)",,classic/original_autogpt/autogpt/app/main.py
run_interaction_loop.graceful_agent_interrupt,function,,"def graceful_agent_interrupt(signum: int, frame: Optional[FrameType]) -> None:
        nonlocal cycle_budget, cycles_remaining, spinner, stop_reason
        if stop_reason:
            logger.error(""Quitting immediately..."")
            sys.exit()
        if cycles_remaining in [0, 1]:
            logger.warning(""Interrupt signal received: shutting down gracefully."")
            logger.warning(
                ""Press Ctrl+C again if you want to stop AutoGPT immediately.""
            )
            stop_reason = AgentTerminated(""Interrupt signal received"")
        else:
            restart_spinner = spinner.running
            if spinner.running:
                spinner.stop()

            logger.error(
                ""Interrupt signal received: stopping continuous command execution.""
            )
            cycles_remaining = 1
            if restart_spinner:
                spinner.start()","Point(row=464, column=4)","Point(row=485, column=31)",,classic/original_autogpt/autogpt/app/main.py
run_interaction_loop.handle_stop_signal,function,,"def handle_stop_signal() -> None:
        if stop_reason:
            raise stop_reason","Point(row=487, column=4)","Point(row=489, column=29)",,classic/original_autogpt/autogpt/app/main.py
update_user,function,"Prints the assistant's thoughts and the next command to the user.

    Args:
        config: The program's configuration.
        ai_profile: The AI's personality/profile
        command_name: The name of the command to execute.
        command_args: The arguments for the command.
        assistant_reply_dict: The assistant's reply.
","def update_user(
    ai_profile: AIProfile,
    action_proposal: ""ActionProposal"",
    speak_mode: bool = False,
) -> None:
    """"""Prints the assistant's thoughts and the next command to the user.

    Args:
        config: The program's configuration.
        ai_profile: The AI's personality/profile
        command_name: The name of the command to execute.
        command_args: The arguments for the command.
        assistant_reply_dict: The assistant's reply.
    """"""
    logger = logging.getLogger(__name__)

    print_assistant_thoughts(
        ai_name=ai_profile.ai_name,
        thoughts=action_proposal.thoughts,
        speak_mode=speak_mode,
    )

    if speak_mode:
        speak(f""I want to execute {action_proposal.use_tool.name}"")

    # First log new-line so user can differentiate sections better in console
    print()
    safe_tool_name = remove_ansi_escape(action_proposal.use_tool.name)
    logger.info(
        f""COMMAND = {Fore.CYAN}{safe_tool_name}{Style.RESET_ALL}  ""
        f""ARGUMENTS = {Fore.CYAN}{action_proposal.use_tool.arguments}{Style.RESET_ALL}"",
        extra={
            ""title"": ""NEXT ACTION:"",
            ""title_color"": Fore.CYAN,
            ""preserve_color"": True,
        },
    )","Point(row=618, column=0)","Point(row=654, column=5)",,classic/original_autogpt/autogpt/app/main.py
get_user_feedback,function,"Gets the user's feedback on the assistant's reply.

    Args:
        config: The program's configuration.
        ai_profile: The AI's configuration.

    Returns:
        A tuple of the user's feedback, the user's input, and the number of
        cycles remaining if the user has initiated a continuous cycle.
","async def get_user_feedback(
    config: AppConfig,
    ai_profile: AIProfile,
) -> tuple[UserFeedback, str, int | None]:
    """"""Gets the user's feedback on the assistant's reply.

    Args:
        config: The program's configuration.
        ai_profile: The AI's configuration.

    Returns:
        A tuple of the user's feedback, the user's input, and the number of
        cycles remaining if the user has initiated a continuous cycle.
    """"""
    logger = logging.getLogger(__name__)

    # ### GET USER AUTHORIZATION TO EXECUTE COMMAND ###
    # Get key press: Prompt the user to press enter to continue or escape
    # to exit
    logger.info(
        f""Enter '{config.authorise_key}' to authorise command, ""
        f""'{config.authorise_key} -N' to run N continuous commands, ""
        f""'{config.exit_key}' to exit program, or enter feedback for ""
        f""{ai_profile.ai_name}...""
    )

    user_feedback = None
    user_input = """"
    new_cycles_remaining = None

    while user_feedback is None:
        # Get input from user
        console_input = clean_input(Fore.MAGENTA + ""Input:"" + Style.RESET_ALL)

        # Parse user input
        if console_input.lower().strip() == config.authorise_key:
            user_feedback = UserFeedback.AUTHORIZE
        elif console_input.lower().strip() == """":
            logger.warning(""Invalid input format."")
        elif console_input.lower().startswith(f""{config.authorise_key} -""):
            try:
                user_feedback = UserFeedback.AUTHORIZE
                new_cycles_remaining = abs(int(console_input.split("" "")[1]))
            except ValueError:
                logger.warning(
                    f""Invalid input format. ""
                    f""Please enter '{config.authorise_key} -N'""
                    "" where N is the number of continuous tasks.""
                )
        elif console_input.lower() in [config.exit_key, ""exit""]:
            user_feedback = UserFeedback.EXIT
        else:
            user_feedback = UserFeedback.TEXT
            user_input = console_input

    return user_feedback, user_input, new_cycles_remaining","Point(row=657, column=0)","Point(row=712, column=58)",,classic/original_autogpt/autogpt/app/main.py
print_assistant_thoughts,function,,"def print_assistant_thoughts(
    ai_name: str,
    thoughts: str | ModelWithSummary | AssistantThoughts,
    speak_mode: bool = False,
) -> None:
    logger = logging.getLogger(__name__)

    thoughts_text = remove_ansi_escape(
        thoughts.text
        if isinstance(thoughts, AssistantThoughts)
        else thoughts.summary()
        if isinstance(thoughts, ModelWithSummary)
        else thoughts
    )
    print_attribute(
        f""{ai_name.upper()} THOUGHTS"", thoughts_text, title_color=Fore.YELLOW
    )

    if isinstance(thoughts, AssistantThoughts):
        print_attribute(
            ""REASONING"", remove_ansi_escape(thoughts.reasoning), title_color=Fore.YELLOW
        )
        if assistant_thoughts_plan := remove_ansi_escape(
            ""\n"".join(f""- {p}"" for p in thoughts.plan)
        ):
            print_attribute(""PLAN"", """", title_color=Fore.YELLOW)
            # If it's a list, join it into a string
            if isinstance(assistant_thoughts_plan, list):
                assistant_thoughts_plan = ""\n"".join(assistant_thoughts_plan)
            elif isinstance(assistant_thoughts_plan, dict):
                assistant_thoughts_plan = str(assistant_thoughts_plan)

            # Split the input_string using the newline character and dashes
            lines = assistant_thoughts_plan.split(""\n"")
            for line in lines:
                line = line.lstrip(""- "")
                logger.info(
                    line.strip(), extra={""title"": ""- "", ""title_color"": Fore.GREEN}
                )
        print_attribute(
            ""CRITICISM"",
            remove_ansi_escape(thoughts.self_criticism),
            title_color=Fore.YELLOW,
        )

        # Speak the assistant's thoughts
        if assistant_thoughts_speak := remove_ansi_escape(thoughts.speak):
            if speak_mode:
                speak(assistant_thoughts_speak)
            else:
                print_attribute(
                    ""SPEAK"", assistant_thoughts_speak, title_color=Fore.YELLOW
                )
    else:
        speak(thoughts_text)","Point(row=715, column=0)","Point(row=769, column=28)",,classic/original_autogpt/autogpt/app/main.py
remove_ansi_escape,function,,"def remove_ansi_escape(s: str) -> str:
    return s.replace(""\x1B"", """")","Point(row=772, column=0)","Point(row=773, column=32)",,classic/original_autogpt/autogpt/app/main.py
Spinner,class,A simple spinner class,"class Spinner:
    """"""A simple spinner class""""""

    def __init__(
        self,
        message: str = ""Loading..."",
        delay: float = 0.1,
        plain_output: bool = False,
    ) -> None:
        """"""Initialize the spinner class

        Args:
            message (str): The message to display.
            delay (float): The delay between each spinner update.
            plain_output (bool): Whether to display the spinner or not.
        """"""
        self.plain_output = plain_output
        self.spinner = itertools.cycle([""-"", ""/"", ""|"", ""\\""])
        self.delay = delay
        self.message = message
        self.running = False
        self.spinner_thread = None

    def spin(self) -> None:
        """"""Spin the spinner""""""
        if self.plain_output:
            self.print_message()
            return
        while self.running:
            self.print_message()
            time.sleep(self.delay)

    def print_message(self):
        sys.stdout.write(f""\r{' ' * (len(self.message) + 2)}\r"")
        sys.stdout.write(f""{next(self.spinner)} {self.message}\r"")
        sys.stdout.flush()

    def start(self):
        self.running = True
        self.spinner_thread = threading.Thread(target=self.spin)
        self.spinner_thread.start()

    def stop(self):
        self.running = False
        if self.spinner_thread is not None:
            self.spinner_thread.join()
        sys.stdout.write(f""\r{' ' * (len(self.message) + 2)}\r"")
        sys.stdout.flush()

    def __enter__(self):
        """"""Start the spinner""""""
        self.start()
        return self

    def __exit__(self, exc_type, exc_value, exc_traceback) -> None:
        """"""Stop the spinner

        Args:
            exc_type (Exception): The exception type.
            exc_value (Exception): The exception value.
            exc_traceback (Exception): The exception traceback.
        """"""
        self.stop()","Point(row=7, column=0)","Point(row=69, column=19)",,classic/original_autogpt/autogpt/app/spinner.py
Spinner.__init__,function,"Initialize the spinner class

        Args:
            message (str): The message to display.
            delay (float): The delay between each spinner update.
            plain_output (bool): Whether to display the spinner or not.
","def __init__(
        self,
        message: str = ""Loading..."",
        delay: float = 0.1,
        plain_output: bool = False,
    ) -> None:
        """"""Initialize the spinner class

        Args:
            message (str): The message to display.
            delay (float): The delay between each spinner update.
            plain_output (bool): Whether to display the spinner or not.
        """"""
        self.plain_output = plain_output
        self.spinner = itertools.cycle([""-"", ""/"", ""|"", ""\\""])
        self.delay = delay
        self.message = message
        self.running = False
        self.spinner_thread = None","Point(row=10, column=4)","Point(row=28, column=34)",Spinner,classic/original_autogpt/autogpt/app/spinner.py
Spinner.spin,function,Spin the spinner,"def spin(self) -> None:
        """"""Spin the spinner""""""
        if self.plain_output:
            self.print_message()
            return
        while self.running:
            self.print_message()
            time.sleep(self.delay)","Point(row=30, column=4)","Point(row=37, column=34)",Spinner,classic/original_autogpt/autogpt/app/spinner.py
Spinner.print_message,function,,"def print_message(self):
        sys.stdout.write(f""\r{' ' * (len(self.message) + 2)}\r"")
        sys.stdout.write(f""{next(self.spinner)} {self.message}\r"")
        sys.stdout.flush()","Point(row=39, column=4)","Point(row=42, column=26)",Spinner,classic/original_autogpt/autogpt/app/spinner.py
Spinner.start,function,,"def start(self):
        self.running = True
        self.spinner_thread = threading.Thread(target=self.spin)
        self.spinner_thread.start()","Point(row=44, column=4)","Point(row=47, column=35)",Spinner,classic/original_autogpt/autogpt/app/spinner.py
Spinner.stop,function,,"def stop(self):
        self.running = False
        if self.spinner_thread is not None:
            self.spinner_thread.join()
        sys.stdout.write(f""\r{' ' * (len(self.message) + 2)}\r"")
        sys.stdout.flush()","Point(row=49, column=4)","Point(row=54, column=26)",Spinner,classic/original_autogpt/autogpt/app/spinner.py
Spinner.__enter__,function,Start the spinner,"def __enter__(self):
        """"""Start the spinner""""""
        self.start()
        return self","Point(row=56, column=4)","Point(row=59, column=19)",Spinner,classic/original_autogpt/autogpt/app/spinner.py
Spinner.__exit__,function,"Stop the spinner

        Args:
            exc_type (Exception): The exception type.
            exc_value (Exception): The exception value.
            exc_traceback (Exception): The exception traceback.
","def __exit__(self, exc_type, exc_value, exc_traceback) -> None:
        """"""Stop the spinner

        Args:
            exc_type (Exception): The exception type.
            exc_value (Exception): The exception value.
            exc_traceback (Exception): The exception traceback.
        """"""
        self.stop()","Point(row=61, column=4)","Point(row=69, column=19)",Spinner,classic/original_autogpt/autogpt/app/spinner.py
AgentProtocolServer,class,,"class AgentProtocolServer:
    _task_budgets: dict[str, ModelProviderBudget]

    def __init__(
        self,
        app_config: AppConfig,
        database: AgentDB,
        file_storage: FileStorage,
        llm_provider: MultiProvider,
    ):
        self.app_config = app_config
        self.db = database
        self.file_storage = file_storage
        self.llm_provider = llm_provider
        self.agent_manager = AgentManager(file_storage)
        self._task_budgets = defaultdict(ModelProviderBudget)

    async def start(self, port: int = 8000, router: APIRouter = base_router):
        """"""Start the agent server.""""""
        logger.debug(""Starting the agent server..."")
        if not is_port_free(port):
            logger.error(f""Port {port} is already in use."")
            logger.info(
                ""You can specify a port by either setting the AP_SERVER_PORT ""
                ""environment variable or defining AP_SERVER_PORT in the .env file.""
            )
            return

        config = HypercornConfig()
        config.bind = [f""localhost:{port}""]
        app = FastAPI(
            title=""AutoGPT Server"",
            description=""Forked from AutoGPT Forge; ""
            ""Modified version of The Agent Protocol."",
            version=""v0.4"",
        )

        # Configure CORS middleware
        default_origins = [f""http://localhost:{port}""]  # Default only local access
        configured_origins = [
            origin
            for origin in os.getenv(""AP_SERVER_CORS_ALLOWED_ORIGINS"", """").split("","")
            if origin  # Empty list if not configured
        ]
        origins = configured_origins or default_origins

        app.add_middleware(
            CORSMiddleware,
            allow_origins=origins,
            allow_credentials=True,
            allow_methods=[""*""],
            allow_headers=[""*""],
        )

        app.include_router(router, prefix=""/ap/v1"")
        script_dir = os.path.dirname(os.path.realpath(__file__))
        frontend_path = (
            pathlib.Path(script_dir)
            .joinpath(""../../../classic/frontend/build/web"")
            .resolve()
        )

        if os.path.exists(frontend_path):
            app.mount(""/app"", StaticFiles(directory=frontend_path), name=""app"")

            @app.get(""/"", include_in_schema=False)
            async def root():
                return RedirectResponse(url=""/app/index.html"", status_code=307)

        else:
            logger.warning(
                f""Frontend not found. {frontend_path} does not exist. ""
                ""The frontend will not be available.""
            )

        # Used to access the methods on this class from API route handlers
        app.add_middleware(AgentMiddleware, agent=self)

        config.loglevel = ""ERROR""
        config.bind = [f""0.0.0.0:{port}""]

        logger.info(f""AutoGPT server starting on http://localhost:{port}"")
        await hypercorn_serve(app, config)  # type: ignore

    async def create_task(self, task_request: TaskRequestBody) -> Task:
        """"""
        Create a task for the agent.
        """"""
        if user_id := (task_request.additional_input or {}).get(""user_id""):
            set_user({""id"": user_id})

        task = await self.db.create_task(
            input=task_request.input,
            additional_input=task_request.additional_input,
        )
        # TODO: re-evaluate performance benefit of task-oriented profiles
        # logger.debug(f""Creating agent for task: '{task.input}'"")
        # task_agent = await generate_agent_for_task(
        task_agent = create_agent(
            agent_id=task_agent_id(task.task_id),
            task=task.input,
            app_config=self.app_config,
            file_storage=self.file_storage,
            llm_provider=self._get_task_llm_provider(task),
        )
        await task_agent.file_manager.save_state()

        return task

    async def list_tasks(self, page: int = 1, pageSize: int = 10) -> TaskListResponse:
        """"""
        List all tasks that the agent has created.
        """"""
        logger.debug(""Listing all tasks..."")
        tasks, pagination = await self.db.list_tasks(page, pageSize)
        response = TaskListResponse(tasks=tasks, pagination=pagination)
        return response

    async def get_task(self, task_id: str) -> Task:
        """"""
        Get a task by ID.
        """"""
        logger.debug(f""Getting task with ID: {task_id}..."")
        task = await self.db.get_task(task_id)
        return task

    async def list_steps(
        self, task_id: str, page: int = 1, pageSize: int = 10
    ) -> TaskStepsListResponse:
        """"""
        List the IDs of all steps that the task has created.
        """"""
        logger.debug(f""Listing all steps created by task with ID: {task_id}..."")
        steps, pagination = await self.db.list_steps(task_id, page, pageSize)
        response = TaskStepsListResponse(steps=steps, pagination=pagination)
        return response

    async def execute_step(self, task_id: str, step_request: StepRequestBody) -> Step:
        """"""Create a step for the task.""""""
        logger.debug(f""Creating a step for task with ID: {task_id}..."")

        # Restore Agent instance
        task = await self.get_task(task_id)
        agent = configure_agent_with_state(
            state=self.agent_manager.load_agent_state(task_agent_id(task_id)),
            app_config=self.app_config,
            file_storage=self.file_storage,
            llm_provider=self._get_task_llm_provider(task),
        )

        if user_id := (task.additional_input or {}).get(""user_id""):
            set_user({""id"": user_id})

        # According to the Agent Protocol spec, the first execute_step request contains
        #  the same task input as the parent create_task request.
        # To prevent this from interfering with the agent's process, we ignore the input
        #  of this first step request, and just generate the first step proposal.
        is_init_step = not bool(agent.event_history)
        last_proposal, tool_result = None, None
        execute_approved = False

        # HACK: only for compatibility with AGBenchmark
        if step_request.input == ""y"":
            step_request.input = """"

        user_input = step_request.input if not is_init_step else """"

        if (
            not is_init_step
            and agent.event_history.current_episode
            and not agent.event_history.current_episode.result
        ):
            last_proposal = agent.event_history.current_episode.action
            execute_approved = not user_input

            logger.debug(
                f""Agent proposed command {last_proposal.use_tool}.""
                f"" User input/feedback: {repr(user_input)}""
            )

        # Save step request
        step = await self.db.create_step(
            task_id=task_id,
            input=step_request,
            is_last=(
                last_proposal is not None
                and last_proposal.use_tool.name == FINISH_COMMAND
                and execute_approved
            ),
        )
        agent.llm_provider = self._get_task_llm_provider(task, step.step_id)

        # Execute previously proposed action
        if last_proposal:
            agent.file_manager.workspace.on_write_file = (
                lambda path: self._on_agent_write_file(
                    task=task, step=step, relative_path=path
                )
            )

            if last_proposal.use_tool.name == ASK_COMMAND:
                tool_result = ActionSuccessResult(outputs=user_input)
                agent.event_history.register_result(tool_result)
            elif execute_approved:
                step = await self.db.update_step(
                    task_id=task_id,
                    step_id=step.step_id,
                    status=""running"",
                )

                try:
                    # Execute previously proposed action
                    tool_result = await agent.execute(last_proposal)
                except AgentFinished:
                    additional_output = {}
                    task_total_cost = agent.llm_provider.get_incurred_cost()
                    if task_total_cost > 0:
                        additional_output[""task_total_cost""] = task_total_cost
                        logger.info(
                            f""Total LLM cost for task {task_id}: ""
                            f""${round(task_total_cost, 2)}""
                        )

                    step = await self.db.update_step(
                        task_id=task_id,
                        step_id=step.step_id,
                        output=last_proposal.use_tool.arguments[""reason""],
                        additional_output=additional_output,
                    )
                    await agent.file_manager.save_state()
                    return step
            else:
                assert user_input
                tool_result = await agent.do_not_execute(last_proposal, user_input)

        # Propose next action
        try:
            assistant_response = await agent.propose_action()
            next_tool_to_use = assistant_response.use_tool
            logger.debug(f""AI output: {assistant_response.thoughts}"")
        except Exception as e:
            step = await self.db.update_step(
                task_id=task_id,
                step_id=step.step_id,
                status=""completed"",
                output=f""An error occurred while proposing the next action: {e}"",
            )
            return step

        # Format step output
        output = (
            (
                f""`{last_proposal.use_tool}` returned:""
                + (""\n\n"" if ""\n"" in str(tool_result) else "" "")
                + f""{tool_result}\n\n""
            )
            if last_proposal and last_proposal.use_tool.name != ASK_COMMAND
            else """"
        )
        output += f""{assistant_response.thoughts.speak}\n\n""
        output += (
            f""Next Command: {next_tool_to_use}""
            if next_tool_to_use.name != ASK_COMMAND
            else next_tool_to_use.arguments[""question""]
        )

        additional_output = {
            **(
                {
                    ""last_action"": {
                        ""name"": last_proposal.use_tool.name,
                        ""args"": last_proposal.use_tool.arguments,
                        ""result"": (
                            """"
                            if tool_result is None
                            else (
                                orjson.loads(tool_result.model_dump_json())
                                if not isinstance(tool_result, ActionErrorResult)
                                else {
                                    ""error"": str(tool_result.error),
                                    ""reason"": tool_result.reason,
                                }
                            )
                        ),
                    },
                }
                if last_proposal and tool_result
                else {}
            ),
            **assistant_response.model_dump(),
        }

        task_cumulative_cost = agent.llm_provider.get_incurred_cost()
        if task_cumulative_cost > 0:
            additional_output[""task_cumulative_cost""] = task_cumulative_cost
        logger.debug(
            f""Running total LLM cost for task {task_id}: ""
            f""${round(task_cumulative_cost, 3)}""
        )

        step = await self.db.update_step(
            task_id=task_id,
            step_id=step.step_id,
            status=""completed"",
            output=output,
            additional_output=additional_output,
        )

        await agent.file_manager.save_state()
        return step

    async def _on_agent_write_file(
        self, task: Task, step: Step, relative_path: pathlib.Path
    ) -> None:
        """"""
        Creates an Artifact for the written file, or updates the Artifact if it exists.
        """"""
        if relative_path.is_absolute():
            raise ValueError(f""File path '{relative_path}' is not relative"")
        for a in task.artifacts or []:
            if a.relative_path == str(relative_path):
                logger.debug(f""Updating Artifact after writing to existing file: {a}"")
                if not a.agent_created:
                    await self.db.update_artifact(a.artifact_id, agent_created=True)
                break
        else:
            logger.debug(f""Creating Artifact for new file '{relative_path}'"")
            await self.db.create_artifact(
                task_id=step.task_id,
                step_id=step.step_id,
                file_name=relative_path.parts[-1],
                agent_created=True,
                relative_path=str(relative_path),
            )

    async def get_step(self, task_id: str, step_id: str) -> Step:
        """"""
        Get a step by ID.
        """"""
        step = await self.db.get_step(task_id, step_id)
        return step

    async def list_artifacts(
        self, task_id: str, page: int = 1, pageSize: int = 10
    ) -> TaskArtifactsListResponse:
        """"""
        List the artifacts that the task has created.
        """"""
        artifacts, pagination = await self.db.list_artifacts(task_id, page, pageSize)
        return TaskArtifactsListResponse(artifacts=artifacts, pagination=pagination)

    async def create_artifact(
        self, task_id: str, file: UploadFile, relative_path: str
    ) -> Artifact:
        """"""
        Create an artifact for the task.
        """"""
        file_name = file.filename or str(uuid4())
        data = b""""
        while contents := file.file.read(1024 * 1024):
            data += contents
        # Check if relative path ends with filename
        if relative_path.endswith(file_name):
            file_path = relative_path
        else:
            file_path = os.path.join(relative_path, file_name)

        workspace = self._get_task_agent_file_workspace(task_id)
        await workspace.write_file(file_path, data)

        artifact = await self.db.create_artifact(
            task_id=task_id,
            file_name=file_name,
            relative_path=relative_path,
            agent_created=False,
        )
        return artifact

    async def get_artifact(self, task_id: str, artifact_id: str) -> StreamingResponse:
        """"""
        Download a task artifact by ID.
        """"""
        try:
            workspace = self._get_task_agent_file_workspace(task_id)
            artifact = await self.db.get_artifact(artifact_id)
            if artifact.file_name not in artifact.relative_path:
                file_path = os.path.join(artifact.relative_path, artifact.file_name)
            else:
                file_path = artifact.relative_path
            retrieved_artifact = workspace.read_file(file_path, binary=True)
        except NotFoundError:
            raise
        except FileNotFoundError:
            raise

        return StreamingResponse(
            BytesIO(retrieved_artifact),
            media_type=""application/octet-stream"",
            headers={
                ""Content-Disposition"": f'attachment; filename=""{artifact.file_name}""'
            },
        )

    def _get_task_agent_file_workspace(self, task_id: str | int) -> FileStorage:
        agent_id = task_agent_id(task_id)
        return self.file_storage.clone_with_subroot(f""agents/{agent_id}/workspace"")

    def _get_task_llm_provider(self, task: Task, step_id: str = """") -> MultiProvider:
        """"""
        Configures the LLM provider with headers to link outgoing requests to the task.
        """"""
        task_llm_budget = self._task_budgets[task.task_id]

        task_llm_provider_config = self.llm_provider._configuration.model_copy(
            deep=True
        )
        _extra_request_headers = task_llm_provider_config.extra_request_headers
        _extra_request_headers[""AP-TaskID""] = task.task_id
        if step_id:
            _extra_request_headers[""AP-StepID""] = step_id
        if task.additional_input and (user_id := task.additional_input.get(""user_id"")):
            _extra_request_headers[""AutoGPT-UserID""] = user_id

        settings = self.llm_provider._settings.model_copy()
        settings.budget = task_llm_budget
        settings.configuration = task_llm_provider_config
        task_llm_provider = self.llm_provider.__class__(
            settings=settings,
            logger=logger.getChild(
                f""Task-{task.task_id}_{self.llm_provider.__class__.__name__}""
            ),
        )
        self._task_budgets[task.task_id] = task_llm_provider._budget  # type: ignore

        return task_llm_provider","Point(row=42, column=0)","Point(row=476, column=32)",,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentProtocolServer.__init__,function,,"def __init__(
        self,
        app_config: AppConfig,
        database: AgentDB,
        file_storage: FileStorage,
        llm_provider: MultiProvider,
    ):
        self.app_config = app_config
        self.db = database
        self.file_storage = file_storage
        self.llm_provider = llm_provider
        self.agent_manager = AgentManager(file_storage)
        self._task_budgets = defaultdict(ModelProviderBudget)","Point(row=45, column=4)","Point(row=57, column=61)",AgentProtocolServer,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentProtocolServer.start,function,Start the agent server.,"async def start(self, port: int = 8000, router: APIRouter = base_router):
        """"""Start the agent server.""""""
        logger.debug(""Starting the agent server..."")
        if not is_port_free(port):
            logger.error(f""Port {port} is already in use."")
            logger.info(
                ""You can specify a port by either setting the AP_SERVER_PORT ""
                ""environment variable or defining AP_SERVER_PORT in the .env file.""
            )
            return

        config = HypercornConfig()
        config.bind = [f""localhost:{port}""]
        app = FastAPI(
            title=""AutoGPT Server"",
            description=""Forked from AutoGPT Forge; ""
            ""Modified version of The Agent Protocol."",
            version=""v0.4"",
        )

        # Configure CORS middleware
        default_origins = [f""http://localhost:{port}""]  # Default only local access
        configured_origins = [
            origin
            for origin in os.getenv(""AP_SERVER_CORS_ALLOWED_ORIGINS"", """").split("","")
            if origin  # Empty list if not configured
        ]
        origins = configured_origins or default_origins

        app.add_middleware(
            CORSMiddleware,
            allow_origins=origins,
            allow_credentials=True,
            allow_methods=[""*""],
            allow_headers=[""*""],
        )

        app.include_router(router, prefix=""/ap/v1"")
        script_dir = os.path.dirname(os.path.realpath(__file__))
        frontend_path = (
            pathlib.Path(script_dir)
            .joinpath(""../../../classic/frontend/build/web"")
            .resolve()
        )

        if os.path.exists(frontend_path):
            app.mount(""/app"", StaticFiles(directory=frontend_path), name=""app"")

            @app.get(""/"", include_in_schema=False)
            async def root():
                return RedirectResponse(url=""/app/index.html"", status_code=307)

        else:
            logger.warning(
                f""Frontend not found. {frontend_path} does not exist. ""
                ""The frontend will not be available.""
            )

        # Used to access the methods on this class from API route handlers
        app.add_middleware(AgentMiddleware, agent=self)

        config.loglevel = ""ERROR""
        config.bind = [f""0.0.0.0:{port}""]

        logger.info(f""AutoGPT server starting on http://localhost:{port}"")
        await hypercorn_serve(app, config)  # type: ignore","Point(row=59, column=4)","Point(row=124, column=58)",AgentProtocolServer,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentProtocolServer.start.root,function,,"async def root():
                return RedirectResponse(url=""/app/index.html"", status_code=307)","Point(row=108, column=12)","Point(row=109, column=79)",AgentProtocolServer,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentProtocolServer.create_task,function,"
        Create a task for the agent.
","async def create_task(self, task_request: TaskRequestBody) -> Task:
        """"""
        Create a task for the agent.
        """"""
        if user_id := (task_request.additional_input or {}).get(""user_id""):
            set_user({""id"": user_id})

        task = await self.db.create_task(
            input=task_request.input,
            additional_input=task_request.additional_input,
        )
        # TODO: re-evaluate performance benefit of task-oriented profiles
        # logger.debug(f""Creating agent for task: '{task.input}'"")
        # task_agent = await generate_agent_for_task(
        task_agent = create_agent(
            agent_id=task_agent_id(task.task_id),
            task=task.input,
            app_config=self.app_config,
            file_storage=self.file_storage,
            llm_provider=self._get_task_llm_provider(task),
        )
        await task_agent.file_manager.save_state()

        return task","Point(row=126, column=4)","Point(row=149, column=19)",AgentProtocolServer,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentProtocolServer.list_tasks,function,"
        List all tasks that the agent has created.
","async def list_tasks(self, page: int = 1, pageSize: int = 10) -> TaskListResponse:
        """"""
        List all tasks that the agent has created.
        """"""
        logger.debug(""Listing all tasks..."")
        tasks, pagination = await self.db.list_tasks(page, pageSize)
        response = TaskListResponse(tasks=tasks, pagination=pagination)
        return response","Point(row=151, column=4)","Point(row=158, column=23)",AgentProtocolServer,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentProtocolServer.get_task,function,"
        Get a task by ID.
","async def get_task(self, task_id: str) -> Task:
        """"""
        Get a task by ID.
        """"""
        logger.debug(f""Getting task with ID: {task_id}..."")
        task = await self.db.get_task(task_id)
        return task","Point(row=160, column=4)","Point(row=166, column=19)",AgentProtocolServer,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentProtocolServer.list_steps,function,"
        List the IDs of all steps that the task has created.
","async def list_steps(
        self, task_id: str, page: int = 1, pageSize: int = 10
    ) -> TaskStepsListResponse:
        """"""
        List the IDs of all steps that the task has created.
        """"""
        logger.debug(f""Listing all steps created by task with ID: {task_id}..."")
        steps, pagination = await self.db.list_steps(task_id, page, pageSize)
        response = TaskStepsListResponse(steps=steps, pagination=pagination)
        return response","Point(row=168, column=4)","Point(row=177, column=23)",AgentProtocolServer,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentProtocolServer.execute_step,function,Create a step for the task.,"async def execute_step(self, task_id: str, step_request: StepRequestBody) -> Step:
        """"""Create a step for the task.""""""
        logger.debug(f""Creating a step for task with ID: {task_id}..."")

        # Restore Agent instance
        task = await self.get_task(task_id)
        agent = configure_agent_with_state(
            state=self.agent_manager.load_agent_state(task_agent_id(task_id)),
            app_config=self.app_config,
            file_storage=self.file_storage,
            llm_provider=self._get_task_llm_provider(task),
        )

        if user_id := (task.additional_input or {}).get(""user_id""):
            set_user({""id"": user_id})

        # According to the Agent Protocol spec, the first execute_step request contains
        #  the same task input as the parent create_task request.
        # To prevent this from interfering with the agent's process, we ignore the input
        #  of this first step request, and just generate the first step proposal.
        is_init_step = not bool(agent.event_history)
        last_proposal, tool_result = None, None
        execute_approved = False

        # HACK: only for compatibility with AGBenchmark
        if step_request.input == ""y"":
            step_request.input = """"

        user_input = step_request.input if not is_init_step else """"

        if (
            not is_init_step
            and agent.event_history.current_episode
            and not agent.event_history.current_episode.result
        ):
            last_proposal = agent.event_history.current_episode.action
            execute_approved = not user_input

            logger.debug(
                f""Agent proposed command {last_proposal.use_tool}.""
                f"" User input/feedback: {repr(user_input)}""
            )

        # Save step request
        step = await self.db.create_step(
            task_id=task_id,
            input=step_request,
            is_last=(
                last_proposal is not None
                and last_proposal.use_tool.name == FINISH_COMMAND
                and execute_approved
            ),
        )
        agent.llm_provider = self._get_task_llm_provider(task, step.step_id)

        # Execute previously proposed action
        if last_proposal:
            agent.file_manager.workspace.on_write_file = (
                lambda path: self._on_agent_write_file(
                    task=task, step=step, relative_path=path
                )
            )

            if last_proposal.use_tool.name == ASK_COMMAND:
                tool_result = ActionSuccessResult(outputs=user_input)
                agent.event_history.register_result(tool_result)
            elif execute_approved:
                step = await self.db.update_step(
                    task_id=task_id,
                    step_id=step.step_id,
                    status=""running"",
                )

                try:
                    # Execute previously proposed action
                    tool_result = await agent.execute(last_proposal)
                except AgentFinished:
                    additional_output = {}
                    task_total_cost = agent.llm_provider.get_incurred_cost()
                    if task_total_cost > 0:
                        additional_output[""task_total_cost""] = task_total_cost
                        logger.info(
                            f""Total LLM cost for task {task_id}: ""
                            f""${round(task_total_cost, 2)}""
                        )

                    step = await self.db.update_step(
                        task_id=task_id,
                        step_id=step.step_id,
                        output=last_proposal.use_tool.arguments[""reason""],
                        additional_output=additional_output,
                    )
                    await agent.file_manager.save_state()
                    return step
            else:
                assert user_input
                tool_result = await agent.do_not_execute(last_proposal, user_input)

        # Propose next action
        try:
            assistant_response = await agent.propose_action()
            next_tool_to_use = assistant_response.use_tool
            logger.debug(f""AI output: {assistant_response.thoughts}"")
        except Exception as e:
            step = await self.db.update_step(
                task_id=task_id,
                step_id=step.step_id,
                status=""completed"",
                output=f""An error occurred while proposing the next action: {e}"",
            )
            return step

        # Format step output
        output = (
            (
                f""`{last_proposal.use_tool}` returned:""
                + (""\n\n"" if ""\n"" in str(tool_result) else "" "")
                + f""{tool_result}\n\n""
            )
            if last_proposal and last_proposal.use_tool.name != ASK_COMMAND
            else """"
        )
        output += f""{assistant_response.thoughts.speak}\n\n""
        output += (
            f""Next Command: {next_tool_to_use}""
            if next_tool_to_use.name != ASK_COMMAND
            else next_tool_to_use.arguments[""question""]
        )

        additional_output = {
            **(
                {
                    ""last_action"": {
                        ""name"": last_proposal.use_tool.name,
                        ""args"": last_proposal.use_tool.arguments,
                        ""result"": (
                            """"
                            if tool_result is None
                            else (
                                orjson.loads(tool_result.model_dump_json())
                                if not isinstance(tool_result, ActionErrorResult)
                                else {
                                    ""error"": str(tool_result.error),
                                    ""reason"": tool_result.reason,
                                }
                            )
                        ),
                    },
                }
                if last_proposal and tool_result
                else {}
            ),
            **assistant_response.model_dump(),
        }

        task_cumulative_cost = agent.llm_provider.get_incurred_cost()
        if task_cumulative_cost > 0:
            additional_output[""task_cumulative_cost""] = task_cumulative_cost
        logger.debug(
            f""Running total LLM cost for task {task_id}: ""
            f""${round(task_cumulative_cost, 3)}""
        )

        step = await self.db.update_step(
            task_id=task_id,
            step_id=step.step_id,
            status=""completed"",
            output=output,
            additional_output=additional_output,
        )

        await agent.file_manager.save_state()
        return step","Point(row=179, column=4)","Point(row=351, column=19)",AgentProtocolServer,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentProtocolServer._on_agent_write_file,function,"
        Creates an Artifact for the written file, or updates the Artifact if it exists.
","async def _on_agent_write_file(
        self, task: Task, step: Step, relative_path: pathlib.Path
    ) -> None:
        """"""
        Creates an Artifact for the written file, or updates the Artifact if it exists.
        """"""
        if relative_path.is_absolute():
            raise ValueError(f""File path '{relative_path}' is not relative"")
        for a in task.artifacts or []:
            if a.relative_path == str(relative_path):
                logger.debug(f""Updating Artifact after writing to existing file: {a}"")
                if not a.agent_created:
                    await self.db.update_artifact(a.artifact_id, agent_created=True)
                break
        else:
            logger.debug(f""Creating Artifact for new file '{relative_path}'"")
            await self.db.create_artifact(
                task_id=step.task_id,
                step_id=step.step_id,
                file_name=relative_path.parts[-1],
                agent_created=True,
                relative_path=str(relative_path),
            )","Point(row=353, column=4)","Point(row=375, column=13)",AgentProtocolServer,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentProtocolServer.get_step,function,"
        Get a step by ID.
","async def get_step(self, task_id: str, step_id: str) -> Step:
        """"""
        Get a step by ID.
        """"""
        step = await self.db.get_step(task_id, step_id)
        return step","Point(row=377, column=4)","Point(row=382, column=19)",AgentProtocolServer,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentProtocolServer.list_artifacts,function,"
        List the artifacts that the task has created.
","async def list_artifacts(
        self, task_id: str, page: int = 1, pageSize: int = 10
    ) -> TaskArtifactsListResponse:
        """"""
        List the artifacts that the task has created.
        """"""
        artifacts, pagination = await self.db.list_artifacts(task_id, page, pageSize)
        return TaskArtifactsListResponse(artifacts=artifacts, pagination=pagination)","Point(row=384, column=4)","Point(row=391, column=84)",AgentProtocolServer,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentProtocolServer.create_artifact,function,"
        Create an artifact for the task.
","async def create_artifact(
        self, task_id: str, file: UploadFile, relative_path: str
    ) -> Artifact:
        """"""
        Create an artifact for the task.
        """"""
        file_name = file.filename or str(uuid4())
        data = b""""
        while contents := file.file.read(1024 * 1024):
            data += contents
        # Check if relative path ends with filename
        if relative_path.endswith(file_name):
            file_path = relative_path
        else:
            file_path = os.path.join(relative_path, file_name)

        workspace = self._get_task_agent_file_workspace(task_id)
        await workspace.write_file(file_path, data)

        artifact = await self.db.create_artifact(
            task_id=task_id,
            file_name=file_name,
            relative_path=relative_path,
            agent_created=False,
        )
        return artifact","Point(row=393, column=4)","Point(row=418, column=23)",AgentProtocolServer,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentProtocolServer.get_artifact,function,"
        Download a task artifact by ID.
","async def get_artifact(self, task_id: str, artifact_id: str) -> StreamingResponse:
        """"""
        Download a task artifact by ID.
        """"""
        try:
            workspace = self._get_task_agent_file_workspace(task_id)
            artifact = await self.db.get_artifact(artifact_id)
            if artifact.file_name not in artifact.relative_path:
                file_path = os.path.join(artifact.relative_path, artifact.file_name)
            else:
                file_path = artifact.relative_path
            retrieved_artifact = workspace.read_file(file_path, binary=True)
        except NotFoundError:
            raise
        except FileNotFoundError:
            raise

        return StreamingResponse(
            BytesIO(retrieved_artifact),
            media_type=""application/octet-stream"",
            headers={
                ""Content-Disposition"": f'attachment; filename=""{artifact.file_name}""'
            },
        )","Point(row=420, column=4)","Point(row=443, column=9)",AgentProtocolServer,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentProtocolServer._get_task_agent_file_workspace,function,,"def _get_task_agent_file_workspace(self, task_id: str | int) -> FileStorage:
        agent_id = task_agent_id(task_id)
        return self.file_storage.clone_with_subroot(f""agents/{agent_id}/workspace"")","Point(row=445, column=4)","Point(row=447, column=83)",AgentProtocolServer,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentProtocolServer._get_task_llm_provider,function,"
        Configures the LLM provider with headers to link outgoing requests to the task.
","def _get_task_llm_provider(self, task: Task, step_id: str = """") -> MultiProvider:
        """"""
        Configures the LLM provider with headers to link outgoing requests to the task.
        """"""
        task_llm_budget = self._task_budgets[task.task_id]

        task_llm_provider_config = self.llm_provider._configuration.model_copy(
            deep=True
        )
        _extra_request_headers = task_llm_provider_config.extra_request_headers
        _extra_request_headers[""AP-TaskID""] = task.task_id
        if step_id:
            _extra_request_headers[""AP-StepID""] = step_id
        if task.additional_input and (user_id := task.additional_input.get(""user_id"")):
            _extra_request_headers[""AutoGPT-UserID""] = user_id

        settings = self.llm_provider._settings.model_copy()
        settings.budget = task_llm_budget
        settings.configuration = task_llm_provider_config
        task_llm_provider = self.llm_provider.__class__(
            settings=settings,
            logger=logger.getChild(
                f""Task-{task.task_id}_{self.llm_provider.__class__.__name__}""
            ),
        )
        self._task_budgets[task.task_id] = task_llm_provider._budget  # type: ignore

        return task_llm_provider","Point(row=449, column=4)","Point(row=476, column=32)",AgentProtocolServer,classic/original_autogpt/autogpt/app/agent_protocol_server.py
task_agent_id,function,,"def task_agent_id(task_id: str | int) -> str:
    return f""AutoGPT-{task_id}""","Point(row=479, column=0)","Point(row=480, column=31)",,classic/original_autogpt/autogpt/app/agent_protocol_server.py
AgentManager,class,,"class AgentManager:
    def __init__(self, file_storage: FileStorage):
        self.file_manager = file_storage.clone_with_subroot(""agents"")

    @staticmethod
    def generate_id(agent_name: str) -> str:
        """"""Generate a unique ID for an agent given agent name.""""""
        unique_id = str(uuid.uuid4())[:8]
        return f""{agent_name}-{unique_id}""

    def list_agents(self) -> list[str]:
        """"""Return all agent directories within storage.""""""
        agent_dirs: list[str] = []
        for file_path in self.file_manager.list_files():
            if len(file_path.parts) == 2 and file_path.name == ""state.json"":
                agent_dirs.append(file_path.parent.name)
        return agent_dirs

    def get_agent_dir(self, agent_id: str) -> Path:
        """"""Return the directory of the agent with the given ID.""""""
        assert len(agent_id) > 0
        agent_dir: Path | None = None
        if self.file_manager.exists(agent_id):
            agent_dir = self.file_manager.root / agent_id
        else:
            raise FileNotFoundError(f""No agent with ID '{agent_id}'"")
        return agent_dir

    def load_agent_state(self, agent_id: str) -> AgentSettings:
        """"""Load the state of the agent with the given ID.""""""
        state_file_path = Path(agent_id) / ""state.json""
        if not self.file_manager.exists(state_file_path):
            raise FileNotFoundError(f""Agent with ID '{agent_id}' has no state.json"")

        state = self.file_manager.read_file(state_file_path)
        return AgentSettings.parse_raw(state)","Point(row=10, column=0)","Point(row=45, column=45)",,classic/original_autogpt/autogpt/agents/agent_manager.py
AgentManager.__init__,function,,"def __init__(self, file_storage: FileStorage):
        self.file_manager = file_storage.clone_with_subroot(""agents"")","Point(row=11, column=4)","Point(row=12, column=69)",AgentManager,classic/original_autogpt/autogpt/agents/agent_manager.py
AgentManager.generate_id,function,Generate a unique ID for an agent given agent name.,"def generate_id(agent_name: str) -> str:
        """"""Generate a unique ID for an agent given agent name.""""""
        unique_id = str(uuid.uuid4())[:8]
        return f""{agent_name}-{unique_id}""","Point(row=15, column=4)","Point(row=18, column=42)",AgentManager,classic/original_autogpt/autogpt/agents/agent_manager.py
AgentManager.list_agents,function,Return all agent directories within storage.,"def list_agents(self) -> list[str]:
        """"""Return all agent directories within storage.""""""
        agent_dirs: list[str] = []
        for file_path in self.file_manager.list_files():
            if len(file_path.parts) == 2 and file_path.name == ""state.json"":
                agent_dirs.append(file_path.parent.name)
        return agent_dirs","Point(row=20, column=4)","Point(row=26, column=25)",AgentManager,classic/original_autogpt/autogpt/agents/agent_manager.py
AgentManager.get_agent_dir,function,Return the directory of the agent with the given ID.,"def get_agent_dir(self, agent_id: str) -> Path:
        """"""Return the directory of the agent with the given ID.""""""
        assert len(agent_id) > 0
        agent_dir: Path | None = None
        if self.file_manager.exists(agent_id):
            agent_dir = self.file_manager.root / agent_id
        else:
            raise FileNotFoundError(f""No agent with ID '{agent_id}'"")
        return agent_dir","Point(row=28, column=4)","Point(row=36, column=24)",AgentManager,classic/original_autogpt/autogpt/agents/agent_manager.py
AgentManager.load_agent_state,function,Load the state of the agent with the given ID.,"def load_agent_state(self, agent_id: str) -> AgentSettings:
        """"""Load the state of the agent with the given ID.""""""
        state_file_path = Path(agent_id) / ""state.json""
        if not self.file_manager.exists(state_file_path):
            raise FileNotFoundError(f""Agent with ID '{agent_id}' has no state.json"")

        state = self.file_manager.read_file(state_file_path)
        return AgentSettings.parse_raw(state)","Point(row=38, column=4)","Point(row=45, column=45)",AgentManager,classic/original_autogpt/autogpt/agents/agent_manager.py
AgentConfiguration,class,,"class AgentConfiguration(BaseAgentConfiguration):
    pass","Point(row=69, column=0)","Point(row=70, column=8)",,classic/original_autogpt/autogpt/agents/agent.py
AgentSettings,class,,"class AgentSettings(BaseAgentSettings):
    config: AgentConfiguration = Field(  # type: ignore
        default_factory=AgentConfiguration
    )

    history: EpisodicActionHistory[OneShotAgentActionProposal] = Field(
        default_factory=EpisodicActionHistory[OneShotAgentActionProposal]
    )
    """"""(STATE) The action history of the agent.""""""

    context: AgentContext = Field(default_factory=AgentContext)","Point(row=73, column=0)","Point(row=83, column=63)",,classic/original_autogpt/autogpt/agents/agent.py
Agent,class,,"class Agent(BaseAgent[OneShotAgentActionProposal], Configurable[AgentSettings]):
    default_settings: ClassVar[AgentSettings] = AgentSettings(
        name=""Agent"",
        description=__doc__ if __doc__ else """",
    )

    def __init__(
        self,
        settings: AgentSettings,
        llm_provider: MultiProvider,
        file_storage: FileStorage,
        app_config: AppConfig,
    ):
        super().__init__(settings)

        self.llm_provider = llm_provider
        prompt_config = OneShotAgentPromptStrategy.default_configuration.model_copy(
            deep=True
        )
        prompt_config.use_functions_api = (
            settings.config.use_functions_api
            # Anthropic currently doesn't support tools + prefilling :(
            and self.llm.provider_name != ""anthropic""
        )
        self.prompt_strategy = OneShotAgentPromptStrategy(prompt_config, logger)
        self.commands: list[Command] = []

        # Components
        self.system = SystemComponent()
        self.history = (
            ActionHistoryComponent(
                settings.history,
                lambda x: self.llm_provider.count_tokens(x, self.llm.name),
                llm_provider,
                ActionHistoryConfiguration(
                    llm_name=app_config.fast_llm, max_tokens=self.send_token_limit
                ),
            )
            .run_after(WatchdogComponent)
            .run_after(SystemComponent)
        )
        if not app_config.noninteractive_mode:
            self.user_interaction = UserInteractionComponent()
        self.file_manager = FileManagerComponent(file_storage, settings)
        self.code_executor = CodeExecutorComponent(
            self.file_manager.workspace,
            CodeExecutorConfiguration(
                docker_container_name=f""{settings.agent_id}_sandbox""
            ),
        )
        self.git_ops = GitOperationsComponent()
        self.image_gen = ImageGeneratorComponent(self.file_manager.workspace)
        self.web_search = WebSearchComponent()
        self.web_selenium = WebSeleniumComponent(
            llm_provider,
            app_config.app_data_dir,
        )
        self.context = ContextComponent(self.file_manager.workspace, settings.context)
        self.watchdog = WatchdogComponent(settings.config, settings.history).run_after(
            ContextComponent
        )

        self.event_history = settings.history
        self.app_config = app_config

    async def propose_action(self) -> OneShotAgentActionProposal:
        """"""Proposes the next action to execute, based on the task and current state.

        Returns:
            The command name and arguments, if any, and the agent's thoughts.
        """"""
        self.reset_trace()

        # Get directives
        resources = await self.run_pipeline(DirectiveProvider.get_resources)
        constraints = await self.run_pipeline(DirectiveProvider.get_constraints)
        best_practices = await self.run_pipeline(DirectiveProvider.get_best_practices)

        directives = self.state.directives.model_copy(deep=True)
        directives.resources += resources
        directives.constraints += constraints
        directives.best_practices += best_practices

        # Get commands
        self.commands = await self.run_pipeline(CommandProvider.get_commands)
        self._remove_disabled_commands()

        # Get messages
        messages = await self.run_pipeline(MessageProvider.get_messages)

        include_os_info = (
            self.code_executor.config.execute_local_commands
            if hasattr(self, ""code_executor"")
            else False
        )

        prompt: ChatPrompt = self.prompt_strategy.build_prompt(
            messages=messages,
            task=self.state.task,
            ai_profile=self.state.ai_profile,
            ai_directives=directives,
            commands=function_specs_from_commands(self.commands),
            include_os_info=include_os_info,
        )

        logger.debug(f""Executing prompt:\n{dump_prompt(prompt)}"")
        output = await self.complete_and_parse(prompt)
        self.config.cycle_count += 1

        return output

    async def complete_and_parse(
        self, prompt: ChatPrompt, exception: Optional[Exception] = None
    ) -> OneShotAgentActionProposal:
        if exception:
            prompt.messages.append(ChatMessage.system(f""Error: {exception}""))

        response: ChatModelResponse[
            OneShotAgentActionProposal
        ] = await self.llm_provider.create_chat_completion(
            prompt.messages,
            model_name=self.llm.name,
            completion_parser=self.prompt_strategy.parse_response_content,
            functions=prompt.functions,
            prefill_response=prompt.prefill_response,
        )
        result = response.parsed_result

        await self.run_pipeline(AfterParse.after_parse, result)

        return result

    async def execute(
        self,
        proposal: OneShotAgentActionProposal,
        user_feedback: str = """",
    ) -> ActionResult:
        tool = proposal.use_tool

        # Get commands
        self.commands = await self.run_pipeline(CommandProvider.get_commands)
        self._remove_disabled_commands()

        try:
            return_value = await self._execute_tool(tool)

            result = ActionSuccessResult(outputs=return_value)
        except AgentTerminated:
            raise
        except AgentException as e:
            result = ActionErrorResult.from_exception(e)
            logger.warning(f""{tool} raised an error: {e}"")
            sentry_sdk.capture_exception(e)

        result_tlength = self.llm_provider.count_tokens(str(result), self.llm.name)
        if result_tlength > self.send_token_limit // 3:
            result = ActionErrorResult(
                reason=f""Command {tool.name} returned too much output. ""
                ""Do not execute this command again with the same arguments.""
            )

        await self.run_pipeline(AfterExecute.after_execute, result)

        logger.debug(""\n"".join(self.trace))

        return result

    async def do_not_execute(
        self, denied_proposal: OneShotAgentActionProposal, user_feedback: str
    ) -> ActionResult:
        result = ActionInterruptedByHuman(feedback=user_feedback)

        await self.run_pipeline(AfterExecute.after_execute, result)

        logger.debug(""\n"".join(self.trace))

        return result

    async def _execute_tool(self, tool_call: AssistantFunctionCall) -> Any:
        """"""Execute the command and return the result

        Args:
            tool_call (AssistantFunctionCall): The tool call to execute

        Returns:
            str: The execution result
        """"""
        # Execute a native command with the same name or alias, if it exists
        command = self._get_command(tool_call.name)
        try:
            result = command(**tool_call.arguments)
            if inspect.isawaitable(result):
                return await result
            return result
        except AgentException:
            raise
        except Exception as e:
            raise CommandExecutionError(str(e))

    def _get_command(self, command_name: str) -> Command:
        for command in reversed(self.commands):
            if command_name in command.names:
                return command

        raise UnknownCommandError(
            f""Cannot execute command '{command_name}': unknown command.""
        )

    def _remove_disabled_commands(self) -> None:
        self.commands = [
            command
            for command in self.commands
            if not any(
                name in self.app_config.disabled_commands for name in command.names
            )
        ]

    def find_obscured_commands(self) -> list[Command]:
        seen_names = set()
        obscured_commands = []
        for command in reversed(self.commands):
            # If all of the command's names have been seen, it's obscured
            if seen_names.issuperset(command.names):
                obscured_commands.append(command)
            else:
                seen_names.update(command.names)
        return list(reversed(obscured_commands))","Point(row=86, column=0)","Point(row=312, column=48)",,classic/original_autogpt/autogpt/agents/agent.py
Agent.__init__,function,,"def __init__(
        self,
        settings: AgentSettings,
        llm_provider: MultiProvider,
        file_storage: FileStorage,
        app_config: AppConfig,
    ):
        super().__init__(settings)

        self.llm_provider = llm_provider
        prompt_config = OneShotAgentPromptStrategy.default_configuration.model_copy(
            deep=True
        )
        prompt_config.use_functions_api = (
            settings.config.use_functions_api
            # Anthropic currently doesn't support tools + prefilling :(
            and self.llm.provider_name != ""anthropic""
        )
        self.prompt_strategy = OneShotAgentPromptStrategy(prompt_config, logger)
        self.commands: list[Command] = []

        # Components
        self.system = SystemComponent()
        self.history = (
            ActionHistoryComponent(
                settings.history,
                lambda x: self.llm_provider.count_tokens(x, self.llm.name),
                llm_provider,
                ActionHistoryConfiguration(
                    llm_name=app_config.fast_llm, max_tokens=self.send_token_limit
                ),
            )
            .run_after(WatchdogComponent)
            .run_after(SystemComponent)
        )
        if not app_config.noninteractive_mode:
            self.user_interaction = UserInteractionComponent()
        self.file_manager = FileManagerComponent(file_storage, settings)
        self.code_executor = CodeExecutorComponent(
            self.file_manager.workspace,
            CodeExecutorConfiguration(
                docker_container_name=f""{settings.agent_id}_sandbox""
            ),
        )
        self.git_ops = GitOperationsComponent()
        self.image_gen = ImageGeneratorComponent(self.file_manager.workspace)
        self.web_search = WebSearchComponent()
        self.web_selenium = WebSeleniumComponent(
            llm_provider,
            app_config.app_data_dir,
        )
        self.context = ContextComponent(self.file_manager.workspace, settings.context)
        self.watchdog = WatchdogComponent(settings.config, settings.history).run_after(
            ContextComponent
        )

        self.event_history = settings.history
        self.app_config = app_config","Point(row=92, column=4)","Point(row=149, column=36)",Agent,classic/original_autogpt/autogpt/agents/agent.py
Agent.propose_action,function,"Proposes the next action to execute, based on the task and current state.

        Returns:
            The command name and arguments, if any, and the agent's thoughts.
","async def propose_action(self) -> OneShotAgentActionProposal:
        """"""Proposes the next action to execute, based on the task and current state.

        Returns:
            The command name and arguments, if any, and the agent's thoughts.
        """"""
        self.reset_trace()

        # Get directives
        resources = await self.run_pipeline(DirectiveProvider.get_resources)
        constraints = await self.run_pipeline(DirectiveProvider.get_constraints)
        best_practices = await self.run_pipeline(DirectiveProvider.get_best_practices)

        directives = self.state.directives.model_copy(deep=True)
        directives.resources += resources
        directives.constraints += constraints
        directives.best_practices += best_practices

        # Get commands
        self.commands = await self.run_pipeline(CommandProvider.get_commands)
        self._remove_disabled_commands()

        # Get messages
        messages = await self.run_pipeline(MessageProvider.get_messages)

        include_os_info = (
            self.code_executor.config.execute_local_commands
            if hasattr(self, ""code_executor"")
            else False
        )

        prompt: ChatPrompt = self.prompt_strategy.build_prompt(
            messages=messages,
            task=self.state.task,
            ai_profile=self.state.ai_profile,
            ai_directives=directives,
            commands=function_specs_from_commands(self.commands),
            include_os_info=include_os_info,
        )

        logger.debug(f""Executing prompt:\n{dump_prompt(prompt)}"")
        output = await self.complete_and_parse(prompt)
        self.config.cycle_count += 1

        return output","Point(row=151, column=4)","Point(row=195, column=21)",Agent,classic/original_autogpt/autogpt/agents/agent.py
Agent.complete_and_parse,function,,"async def complete_and_parse(
        self, prompt: ChatPrompt, exception: Optional[Exception] = None
    ) -> OneShotAgentActionProposal:
        if exception:
            prompt.messages.append(ChatMessage.system(f""Error: {exception}""))

        response: ChatModelResponse[
            OneShotAgentActionProposal
        ] = await self.llm_provider.create_chat_completion(
            prompt.messages,
            model_name=self.llm.name,
            completion_parser=self.prompt_strategy.parse_response_content,
            functions=prompt.functions,
            prefill_response=prompt.prefill_response,
        )
        result = response.parsed_result

        await self.run_pipeline(AfterParse.after_parse, result)

        return result","Point(row=197, column=4)","Point(row=216, column=21)",Agent,classic/original_autogpt/autogpt/agents/agent.py
Agent.execute,function,,"async def execute(
        self,
        proposal: OneShotAgentActionProposal,
        user_feedback: str = """",
    ) -> ActionResult:
        tool = proposal.use_tool

        # Get commands
        self.commands = await self.run_pipeline(CommandProvider.get_commands)
        self._remove_disabled_commands()

        try:
            return_value = await self._execute_tool(tool)

            result = ActionSuccessResult(outputs=return_value)
        except AgentTerminated:
            raise
        except AgentException as e:
            result = ActionErrorResult.from_exception(e)
            logger.warning(f""{tool} raised an error: {e}"")
            sentry_sdk.capture_exception(e)

        result_tlength = self.llm_provider.count_tokens(str(result), self.llm.name)
        if result_tlength > self.send_token_limit // 3:
            result = ActionErrorResult(
                reason=f""Command {tool.name} returned too much output. ""
                ""Do not execute this command again with the same arguments.""
            )

        await self.run_pipeline(AfterExecute.after_execute, result)

        logger.debug(""\n"".join(self.trace))

        return result","Point(row=218, column=4)","Point(row=251, column=21)",Agent,classic/original_autogpt/autogpt/agents/agent.py
Agent.do_not_execute,function,,"async def do_not_execute(
        self, denied_proposal: OneShotAgentActionProposal, user_feedback: str
    ) -> ActionResult:
        result = ActionInterruptedByHuman(feedback=user_feedback)

        await self.run_pipeline(AfterExecute.after_execute, result)

        logger.debug(""\n"".join(self.trace))

        return result","Point(row=253, column=4)","Point(row=262, column=21)",Agent,classic/original_autogpt/autogpt/agents/agent.py
Agent._execute_tool,function,"Execute the command and return the result

        Args:
            tool_call (AssistantFunctionCall): The tool call to execute

        Returns:
            str: The execution result
","async def _execute_tool(self, tool_call: AssistantFunctionCall) -> Any:
        """"""Execute the command and return the result

        Args:
            tool_call (AssistantFunctionCall): The tool call to execute

        Returns:
            str: The execution result
        """"""
        # Execute a native command with the same name or alias, if it exists
        command = self._get_command(tool_call.name)
        try:
            result = command(**tool_call.arguments)
            if inspect.isawaitable(result):
                return await result
            return result
        except AgentException:
            raise
        except Exception as e:
            raise CommandExecutionError(str(e))","Point(row=264, column=4)","Point(row=283, column=47)",Agent,classic/original_autogpt/autogpt/agents/agent.py
Agent._get_command,function,,"def _get_command(self, command_name: str) -> Command:
        for command in reversed(self.commands):
            if command_name in command.names:
                return command

        raise UnknownCommandError(
            f""Cannot execute command '{command_name}': unknown command.""
        )","Point(row=285, column=4)","Point(row=292, column=9)",Agent,classic/original_autogpt/autogpt/agents/agent.py
Agent._remove_disabled_commands,function,,"def _remove_disabled_commands(self) -> None:
        self.commands = [
            command
            for command in self.commands
            if not any(
                name in self.app_config.disabled_commands for name in command.names
            )
        ]","Point(row=294, column=4)","Point(row=301, column=9)",Agent,classic/original_autogpt/autogpt/agents/agent.py
Agent.find_obscured_commands,function,,"def find_obscured_commands(self) -> list[Command]:
        seen_names = set()
        obscured_commands = []
        for command in reversed(self.commands):
            # If all of the command's names have been seen, it's obscured
            if seen_names.issuperset(command.names):
                obscured_commands.append(command)
            else:
                seen_names.update(command.names)
        return list(reversed(obscured_commands))","Point(row=303, column=4)","Point(row=312, column=48)",Agent,classic/original_autogpt/autogpt/agents/agent.py
AssistantThoughts,class,,"class AssistantThoughts(ModelWithSummary):
    observations: str = Field(
        description=""Relevant observations from your last action (if any)""
    )
    text: str = Field(description=""Thoughts"")
    reasoning: str = Field(description=""Reasoning behind the thoughts"")
    self_criticism: str = Field(description=""Constructive self-criticism"")
    plan: list[str] = Field(description=""Short list that conveys the long-term plan"")
    speak: str = Field(description=""Summary of thoughts, to say to user"")

    def summary(self) -> str:
        return self.text","Point(row=28, column=0)","Point(row=39, column=24)",,classic/original_autogpt/autogpt/agents/prompt_strategies/one_shot.py
AssistantThoughts.summary,function,,"def summary(self) -> str:
        return self.text","Point(row=38, column=4)","Point(row=39, column=24)",AssistantThoughts,classic/original_autogpt/autogpt/agents/prompt_strategies/one_shot.py
OneShotAgentActionProposal,class,,"class OneShotAgentActionProposal(ActionProposal):
    thoughts: AssistantThoughts  # type: ignore","Point(row=42, column=0)","Point(row=43, column=47)",,classic/original_autogpt/autogpt/agents/prompt_strategies/one_shot.py
OneShotAgentPromptConfiguration,class,,"class OneShotAgentPromptConfiguration(SystemConfiguration):
    DEFAULT_BODY_TEMPLATE: str = (
        ""## Constraints\n""
        ""You operate within the following constraints:\n""
        ""{constraints}\n""
        ""\n""
        ""## Resources\n""
        ""You can leverage access to the following resources:\n""
        ""{resources}\n""
        ""\n""
        ""## Commands\n""
        ""These are the ONLY commands you can use.""
        "" Any action you perform must be possible through one of these commands:\n""
        ""{commands}\n""
        ""\n""
        ""## Best practices\n""
        ""{best_practices}""
    )

    DEFAULT_CHOOSE_ACTION_INSTRUCTION: str = (
        ""Determine exactly one command to use next based on the given goals ""
        ""and the progress you have made so far, ""
        ""and respond using the JSON schema specified previously:""
    )

    body_template: str = UserConfigurable(default=DEFAULT_BODY_TEMPLATE)
    choose_action_instruction: str = UserConfigurable(
        default=DEFAULT_CHOOSE_ACTION_INSTRUCTION
    )
    use_functions_api: bool = UserConfigurable(default=False)

    #########
    # State #
    #########
    # progress_summaries: dict[tuple[int, int], str] = Field(
    #     default_factory=lambda: {(0, 0): """"}
    # )","Point(row=46, column=0)","Point(row=82, column=7)",,classic/original_autogpt/autogpt/agents/prompt_strategies/one_shot.py
OneShotAgentPromptStrategy,class,,"class OneShotAgentPromptStrategy(PromptStrategy):
    default_configuration: OneShotAgentPromptConfiguration = (
        OneShotAgentPromptConfiguration()
    )

    def __init__(
        self,
        configuration: OneShotAgentPromptConfiguration,
        logger: Logger,
    ):
        self.config = configuration
        self.response_schema = JSONSchema.from_dict(
            OneShotAgentActionProposal.model_json_schema()
        )
        self.logger = logger

    @property
    def llm_classification(self) -> LanguageModelClassification:
        return LanguageModelClassification.FAST_MODEL  # FIXME: dynamic switching

    def build_prompt(
        self,
        *,
        messages: list[ChatMessage],
        task: str,
        ai_profile: AIProfile,
        ai_directives: AIDirectives,
        commands: list[CompletionModelFunction],
        include_os_info: bool,
        **extras,
    ) -> ChatPrompt:
        """"""Constructs and returns a prompt with the following structure:
        1. System prompt
        3. `cycle_instruction`
        """"""
        system_prompt, response_prefill = self.build_system_prompt(
            ai_profile=ai_profile,
            ai_directives=ai_directives,
            commands=commands,
            include_os_info=include_os_info,
        )

        final_instruction_msg = ChatMessage.user(self.config.choose_action_instruction)

        return ChatPrompt(
            messages=[
                ChatMessage.system(system_prompt),
                ChatMessage.user(f'""""""{task}""""""'),
                *messages,
                final_instruction_msg,
            ],
            prefill_response=response_prefill,
            functions=commands if self.config.use_functions_api else [],
        )

    def build_system_prompt(
        self,
        ai_profile: AIProfile,
        ai_directives: AIDirectives,
        commands: list[CompletionModelFunction],
        include_os_info: bool,
    ) -> tuple[str, str]:
        """"""
        Builds the system prompt.

        Returns:
            str: The system prompt body
            str: The desired start for the LLM's response; used to steer the output
        """"""
        response_fmt_instruction, response_prefill = self.response_format_instruction(
            self.config.use_functions_api
        )
        system_prompt_parts = (
            self._generate_intro_prompt(ai_profile)
            + (self._generate_os_info() if include_os_info else [])
            + [
                self.config.body_template.format(
                    constraints=format_numbered_list(ai_directives.constraints),
                    resources=format_numbered_list(ai_directives.resources),
                    commands=self._generate_commands_list(commands),
                    best_practices=format_numbered_list(ai_directives.best_practices),
                )
            ]
            + [
                ""## Your Task\n""
                ""The user will specify a task for you to execute, in triple quotes,""
                "" in the next message. Your job is to complete the task while following""
                "" your directives as given above, and terminate when your task is done.""
            ]
            + [""## RESPONSE FORMAT\n"" + response_fmt_instruction]
        )

        # Join non-empty parts together into paragraph format
        return (
            ""\n\n"".join(filter(None, system_prompt_parts)).strip(""\n""),
            response_prefill,
        )

    def response_format_instruction(self, use_functions_api: bool) -> tuple[str, str]:
        response_schema = self.response_schema.model_copy(deep=True)
        assert response_schema.properties
        if use_functions_api and ""use_tool"" in response_schema.properties:
            del response_schema.properties[""use_tool""]

        # Unindent for performance
        response_format = re.sub(
            r""\n\s+"",
            ""\n"",
            response_schema.to_typescript_object_interface(_RESPONSE_INTERFACE_NAME),
        )
        response_prefill = f'{{\n    ""{list(response_schema.properties.keys())[0]}"":'

        return (
            (
                f""YOU MUST ALWAYS RESPOND WITH A JSON OBJECT OF THE FOLLOWING TYPE:\n""
                f""{response_format}""
                + (""\n\nYOU MUST ALSO INVOKE A TOOL!"" if use_functions_api else """")
            ),
            response_prefill,
        )

    def _generate_intro_prompt(self, ai_profile: AIProfile) -> list[str]:
        """"""Generates the introduction part of the prompt.

        Returns:
            list[str]: A list of strings forming the introduction part of the prompt.
        """"""
        return [
            f""You are {ai_profile.ai_name}, {ai_profile.ai_role.rstrip('.')}."",
            ""Your decisions must always be made independently without seeking ""
            ""user assistance. Play to your strengths as an LLM and pursue ""
            ""simple strategies with no legal complications."",
        ]

    def _generate_os_info(self) -> list[str]:
        """"""Generates the OS information part of the prompt.

        Params:
            config (Config): The configuration object.

        Returns:
            str: The OS information part of the prompt.
        """"""
        os_name = platform.system()
        os_info = (
            platform.platform(terse=True)
            if os_name != ""Linux""
            else distro.name(pretty=True)
        )
        return [f""The OS you are running on is: {os_info}""]

    def _generate_commands_list(self, commands: list[CompletionModelFunction]) -> str:
        """"""Lists the commands available to the agent.

        Params:
            agent: The agent for which the commands are being listed.

        Returns:
            str: A string containing a numbered list of commands.
        """"""
        try:
            return format_numbered_list([cmd.fmt_line() for cmd in commands])
        except AttributeError:
            self.logger.warning(f""Formatting commands failed. {commands}"")
            raise

    def parse_response_content(
        self,
        response: AssistantChatMessage,
    ) -> OneShotAgentActionProposal:
        if not response.content:
            raise InvalidAgentResponseError(""Assistant response has no text content"")

        self.logger.debug(
            ""LLM response content:""
            + (
                f""\n{response.content}""
                if ""\n"" in response.content
                else f"" '{response.content}'""
            )
        )
        assistant_reply_dict = extract_dict_from_json(response.content)
        self.logger.debug(
            ""Parsing object extracted from LLM response:\n""
            f""{json.dumps(assistant_reply_dict, indent=4)}""
        )
        if self.config.use_functions_api:
            if not response.tool_calls:
                raise InvalidAgentResponseError(""Assistant did not use a tool"")
            assistant_reply_dict[""use_tool""] = response.tool_calls[0].function

        parsed_response = OneShotAgentActionProposal.model_validate(
            assistant_reply_dict
        )
        parsed_response.raw_message = response.copy()
        return parsed_response","Point(row=85, column=0)","Point(row=280, column=30)",,classic/original_autogpt/autogpt/agents/prompt_strategies/one_shot.py
OneShotAgentPromptStrategy.__init__,function,,"def __init__(
        self,
        configuration: OneShotAgentPromptConfiguration,
        logger: Logger,
    ):
        self.config = configuration
        self.response_schema = JSONSchema.from_dict(
            OneShotAgentActionProposal.model_json_schema()
        )
        self.logger = logger","Point(row=90, column=4)","Point(row=99, column=28)",OneShotAgentPromptStrategy,classic/original_autogpt/autogpt/agents/prompt_strategies/one_shot.py
OneShotAgentPromptStrategy.llm_classification,function,,"def llm_classification(self) -> LanguageModelClassification:
        return LanguageModelClassification.FAST_MODEL  # FIXME: dynamic switching","Point(row=102, column=4)","Point(row=103, column=81)",OneShotAgentPromptStrategy,classic/original_autogpt/autogpt/agents/prompt_strategies/one_shot.py
OneShotAgentPromptStrategy.build_prompt,function,"Constructs and returns a prompt with the following structure:
        1. System prompt
        3. `cycle_instruction`
","def build_prompt(
        self,
        *,
        messages: list[ChatMessage],
        task: str,
        ai_profile: AIProfile,
        ai_directives: AIDirectives,
        commands: list[CompletionModelFunction],
        include_os_info: bool,
        **extras,
    ) -> ChatPrompt:
        """"""Constructs and returns a prompt with the following structure:
        1. System prompt
        3. `cycle_instruction`
        """"""
        system_prompt, response_prefill = self.build_system_prompt(
            ai_profile=ai_profile,
            ai_directives=ai_directives,
            commands=commands,
            include_os_info=include_os_info,
        )

        final_instruction_msg = ChatMessage.user(self.config.choose_action_instruction)

        return ChatPrompt(
            messages=[
                ChatMessage.system(system_prompt),
                ChatMessage.user(f'""""""{task}""""""'),
                *messages,
                final_instruction_msg,
            ],
            prefill_response=response_prefill,
            functions=commands if self.config.use_functions_api else [],
        )","Point(row=105, column=4)","Point(row=138, column=9)",OneShotAgentPromptStrategy,classic/original_autogpt/autogpt/agents/prompt_strategies/one_shot.py
OneShotAgentPromptStrategy.build_system_prompt,function,"
        Builds the system prompt.

        Returns:
            str: The system prompt body
            str: The desired start for the LLM's response; used to steer the output
","def build_system_prompt(
        self,
        ai_profile: AIProfile,
        ai_directives: AIDirectives,
        commands: list[CompletionModelFunction],
        include_os_info: bool,
    ) -> tuple[str, str]:
        """"""
        Builds the system prompt.

        Returns:
            str: The system prompt body
            str: The desired start for the LLM's response; used to steer the output
        """"""
        response_fmt_instruction, response_prefill = self.response_format_instruction(
            self.config.use_functions_api
        )
        system_prompt_parts = (
            self._generate_intro_prompt(ai_profile)
            + (self._generate_os_info() if include_os_info else [])
            + [
                self.config.body_template.format(
                    constraints=format_numbered_list(ai_directives.constraints),
                    resources=format_numbered_list(ai_directives.resources),
                    commands=self._generate_commands_list(commands),
                    best_practices=format_numbered_list(ai_directives.best_practices),
                )
            ]
            + [
                ""## Your Task\n""
                ""The user will specify a task for you to execute, in triple quotes,""
                "" in the next message. Your job is to complete the task while following""
                "" your directives as given above, and terminate when your task is done.""
            ]
            + [""## RESPONSE FORMAT\n"" + response_fmt_instruction]
        )

        # Join non-empty parts together into paragraph format
        return (
            ""\n\n"".join(filter(None, system_prompt_parts)).strip(""\n""),
            response_prefill,
        )","Point(row=140, column=4)","Point(row=181, column=9)",OneShotAgentPromptStrategy,classic/original_autogpt/autogpt/agents/prompt_strategies/one_shot.py
OneShotAgentPromptStrategy.response_format_instruction,function,,"def response_format_instruction(self, use_functions_api: bool) -> tuple[str, str]:
        response_schema = self.response_schema.model_copy(deep=True)
        assert response_schema.properties
        if use_functions_api and ""use_tool"" in response_schema.properties:
            del response_schema.properties[""use_tool""]

        # Unindent for performance
        response_format = re.sub(
            r""\n\s+"",
            ""\n"",
            response_schema.to_typescript_object_interface(_RESPONSE_INTERFACE_NAME),
        )
        response_prefill = f'{{\n    ""{list(response_schema.properties.keys())[0]}"":'

        return (
            (
                f""YOU MUST ALWAYS RESPOND WITH A JSON OBJECT OF THE FOLLOWING TYPE:\n""
                f""{response_format}""
                + (""\n\nYOU MUST ALSO INVOKE A TOOL!"" if use_functions_api else """")
            ),
            response_prefill,
        )","Point(row=183, column=4)","Point(row=204, column=9)",OneShotAgentPromptStrategy,classic/original_autogpt/autogpt/agents/prompt_strategies/one_shot.py
OneShotAgentPromptStrategy._generate_intro_prompt,function,"Generates the introduction part of the prompt.

        Returns:
            list[str]: A list of strings forming the introduction part of the prompt.
","def _generate_intro_prompt(self, ai_profile: AIProfile) -> list[str]:
        """"""Generates the introduction part of the prompt.

        Returns:
            list[str]: A list of strings forming the introduction part of the prompt.
        """"""
        return [
            f""You are {ai_profile.ai_name}, {ai_profile.ai_role.rstrip('.')}."",
            ""Your decisions must always be made independently without seeking ""
            ""user assistance. Play to your strengths as an LLM and pursue ""
            ""simple strategies with no legal complications."",
        ]","Point(row=206, column=4)","Point(row=217, column=9)",OneShotAgentPromptStrategy,classic/original_autogpt/autogpt/agents/prompt_strategies/one_shot.py
OneShotAgentPromptStrategy._generate_os_info,function,"Generates the OS information part of the prompt.

        Params:
            config (Config): The configuration object.

        Returns:
            str: The OS information part of the prompt.
","def _generate_os_info(self) -> list[str]:
        """"""Generates the OS information part of the prompt.

        Params:
            config (Config): The configuration object.

        Returns:
            str: The OS information part of the prompt.
        """"""
        os_name = platform.system()
        os_info = (
            platform.platform(terse=True)
            if os_name != ""Linux""
            else distro.name(pretty=True)
        )
        return [f""The OS you are running on is: {os_info}""]","Point(row=219, column=4)","Point(row=234, column=59)",OneShotAgentPromptStrategy,classic/original_autogpt/autogpt/agents/prompt_strategies/one_shot.py
OneShotAgentPromptStrategy._generate_commands_list,function,"Lists the commands available to the agent.

        Params:
            agent: The agent for which the commands are being listed.

        Returns:
            str: A string containing a numbered list of commands.
","def _generate_commands_list(self, commands: list[CompletionModelFunction]) -> str:
        """"""Lists the commands available to the agent.

        Params:
            agent: The agent for which the commands are being listed.

        Returns:
            str: A string containing a numbered list of commands.
        """"""
        try:
            return format_numbered_list([cmd.fmt_line() for cmd in commands])
        except AttributeError:
            self.logger.warning(f""Formatting commands failed. {commands}"")
            raise","Point(row=236, column=4)","Point(row=249, column=17)",OneShotAgentPromptStrategy,classic/original_autogpt/autogpt/agents/prompt_strategies/one_shot.py
OneShotAgentPromptStrategy.parse_response_content,function,,"def parse_response_content(
        self,
        response: AssistantChatMessage,
    ) -> OneShotAgentActionProposal:
        if not response.content:
            raise InvalidAgentResponseError(""Assistant response has no text content"")

        self.logger.debug(
            ""LLM response content:""
            + (
                f""\n{response.content}""
                if ""\n"" in response.content
                else f"" '{response.content}'""
            )
        )
        assistant_reply_dict = extract_dict_from_json(response.content)
        self.logger.debug(
            ""Parsing object extracted from LLM response:\n""
            f""{json.dumps(assistant_reply_dict, indent=4)}""
        )
        if self.config.use_functions_api:
            if not response.tool_calls:
                raise InvalidAgentResponseError(""Assistant did not use a tool"")
            assistant_reply_dict[""use_tool""] = response.tool_calls[0].function

        parsed_response = OneShotAgentActionProposal.model_validate(
            assistant_reply_dict
        )
        parsed_response.raw_message = response.copy()
        return parsed_response","Point(row=251, column=4)","Point(row=280, column=30)",OneShotAgentPromptStrategy,classic/original_autogpt/autogpt/agents/prompt_strategies/one_shot.py
main,function,,"def main():
    poetry_project = Factory().create_poetry()
    dependency_group = poetry_project.package.dependency_group(""main"")

    missing_packages = []
    for dep in dependency_group.dependencies:
        if dep.is_optional():
            continue
        # Try to verify that the installed version is suitable
        with contextlib.suppress(ModuleNotFoundError):
            installed_version = version(dep.name)  # if this fails -> not installed
            if dep.constraint.allows(Version.parse(installed_version)):
                continue
        # If the above verification fails, mark the package as missing
        missing_packages.append(str(dep))

    if missing_packages:
        print(""Missing packages:"")
        print("", "".join(missing_packages))
        sys.exit(1)","Point(row=14, column=0)","Point(row=33, column=19)",,classic/original_autogpt/scripts/check_requirements.py
generate_release_notes,function,,"async def generate_release_notes(repo_path: Optional[str | Path] = None):
    logger = logging.getLogger(generate_release_notes.name)  # pyright: ignore

    repo = Repo(repo_path, search_parent_directories=True)
    tags = list(repo.tags)
    if not tags:
        click.echo(""No tags found in the repository."")
        return

    click.echo(""Available tags:"")
    for index, tag in enumerate(tags):
        click.echo(f""{index + 1}: {tag.name}"")

    last_release_index = (
        click.prompt(""Enter the number for the last release tag"", type=int) - 1
    )
    if last_release_index >= len(tags) or last_release_index < 0:
        click.echo(""Invalid tag number entered."")
        return
    last_release_tag: TagReference = tags[last_release_index]

    new_release_ref = click.prompt(
        ""Enter the name of the release branch or git ref"",
        default=repo.active_branch.name,
    )
    try:
        new_release_ref = repo.heads[new_release_ref].name
    except IndexError:
        try:
            new_release_ref = repo.tags[new_release_ref].name
        except IndexError:
            new_release_ref = repo.commit(new_release_ref).hexsha
    logger.debug(f""Selected release ref: {new_release_ref}"")

    git_log = repo.git.log(
        f""{last_release_tag.name}...{new_release_ref}"",
        ""classic/original_autogpt/"",
        no_merges=True,
        follow=True,
    )
    logger.debug(f""-------------- GIT LOG --------------\n\n{git_log}\n"")

    model_provider = MultiProvider()
    chat_messages = [
        ChatMessage.system(SYSTEM_PROMPT),
        ChatMessage.user(content=git_log),
    ]
    click.echo(""Writing release notes ..."")
    completion = await model_provider.create_chat_completion(
        model_prompt=chat_messages,
        model_name=AnthropicModelName.CLAUDE3_OPUS_v1,
        # model_name=OpenAIModelName.GPT4_v4,
    )

    click.echo(""-------------- LLM RESPONSE --------------\n"")
    click.echo(completion.response.content)","Point(row=21, column=0)","Point(row=76, column=43)",,classic/original_autogpt/scripts/git_log_to_release_notes.py
main,function,,"def main(
    llamafile: Optional[Path] = None,
    llamafile_url: Optional[str] = None,
    host: Optional[str] = None,
    port: Optional[int] = None,
    force_gpu: bool = False,
):
    print(f""type(llamafile) = {type(llamafile)}"")
    if not llamafile:
        if not llamafile_url:
            llamafile = LLAMAFILE
        else:
            llamafile = Path(llamafile_url.rsplit(""/"", 1)[1])
            if llamafile.suffix != "".llamafile"":
                click.echo(
                    click.style(
                        ""The given URL does not end with '.llamafile' -> ""
                        ""can't get filename from URL. ""
                        ""Specify the filename using --llamafile."",
                        fg=""red"",
                    ),
                    err=True,
                )
                return

    if llamafile == LLAMAFILE and not llamafile_url:
        llamafile_url = LLAMAFILE_URL
    elif llamafile_url != LLAMAFILE_URL:
        if not click.prompt(
            click.style(
                ""You seem to have specified a different URL for the default model ""
                f""({llamafile.name}). Are you sure this is correct? ""
                ""If you want to use a different model, also specify --llamafile."",
                fg=""yellow"",
            ),
            type=bool,
        ):
            return

    # Go to classic/original_autogpt/scripts/llamafile/
    os.chdir(Path(__file__).resolve().parent)

    on_windows = platform.system() == ""Windows""

    if not llamafile.is_file():
        if not llamafile_url:
            click.echo(
                click.style(
                    ""Please use --lamafile_url to specify a download URL for ""
                    f""'{llamafile.name}'. ""
                    ""This will only be necessary once, so we can download the model."",
                    fg=""red"",
                ),
                err=True,
            )
            return

        download_file(llamafile_url, llamafile)

        if not on_windows:
            llamafile.chmod(0o755)
            subprocess.run([llamafile, ""--version""], check=True)

    if not on_windows:
        base_command = [f""./{llamafile}""]
    else:
        # Windows does not allow executables over 4GB, so we have to download a
        # model-less llamafile.exe and run that instead.
        if not LLAMAFILE_EXE.is_file():
            download_file(LLAMAFILE_EXE_URL, LLAMAFILE_EXE)
            LLAMAFILE_EXE.chmod(0o755)
            subprocess.run([f"".\\{LLAMAFILE_EXE}"", ""--version""], check=True)

        base_command = [f"".\\{LLAMAFILE_EXE}"", ""-m"", llamafile]

    if host:
        base_command.extend([""--host"", host])
    if port:
        base_command.extend([""--port"", str(port)])
    if force_gpu:
        base_command.extend([""-ngl"", ""9999""])

    subprocess.run(
        [
            *base_command,
            ""--server"",
            ""--nobrowser"",
            ""--ctx-size"",
            ""0"",
            ""--n-predict"",
            ""1024"",
        ],
        check=True,
    )

    # note: --ctx-size 0 means the prompt context size will be set directly from the
    # underlying model configuration. This may cause slow response times or consume
    # a lot of memory.","Point(row=43, column=0)","Point(row=140, column=22)",,classic/original_autogpt/scripts/llamafile/serve.py
download_file,function,,"def download_file(url: str, to_file: Path) -> None:
    print(f""Downloading {to_file.name}..."")
    import urllib.request

    urllib.request.urlretrieve(url, to_file, reporthook=report_download_progress)
    print()","Point(row=143, column=0)","Point(row=148, column=11)",,classic/original_autogpt/scripts/llamafile/serve.py
report_download_progress,function,,"def report_download_progress(chunk_number: int, chunk_size: int, total_size: int):
    if total_size != -1:
        downloaded_size = chunk_number * chunk_size
        percent = min(1, downloaded_size / total_size)
        bar = ""#"" * int(40 * percent)
        print(
            f""\rDownloading: [{bar:<40}] {percent:.0%}""
            f"" - {downloaded_size/1e6:.1f}/{total_size/1e6:.1f} MB"",
            end="""",
        )","Point(row=151, column=0)","Point(row=160, column=9)",,classic/original_autogpt/scripts/llamafile/serve.py
process_test,function,,"def process_test(test_name: str, test_data: dict):
            result_group = grouped_success_values[f""{label}|{test_name}""]

            if ""tests"" in test_data:
                logger.debug(f""{test_name} is a test suite"")

                # Test suite
                suite_attempted = any(
                    test[""metrics""][""attempted""] for test in test_data[""tests""].values()
                )
                logger.debug(f""suite_attempted: {suite_attempted}"")
                if not suite_attempted:
                    return

                if test_name not in test_names:
                    test_names.append(test_name)

                if test_data[""metrics""][""percentage""] == 0:
                    result_indicator = ""‚ùå""
                else:
                    highest_difficulty = test_data[""metrics""][""highest_difficulty""]
                    result_indicator = {
                        ""interface"": ""üîå"",
                        ""novice"": ""üåë"",
                        ""basic"": ""üåí"",
                        ""intermediate"": ""üåì"",
                        ""advanced"": ""üåî"",
                        ""hard"": ""üåï"",
                    }[highest_difficulty]

                logger.debug(f""result group: {result_group}"")
                logger.debug(f""runs_per_label: {runs_per_label[label]}"")
                if len(result_group) + 1 < runs_per_label[label]:
                    result_group.extend(
                        [""‚ùî""] * (runs_per_label[label] - len(result_group) - 1)
                    )
                result_group.append(result_indicator)
                logger.debug(f""result group (after): {result_group}"")

                if granular:
                    for test_name, test in test_data[""tests""].items():
                        process_test(test_name, test)
                return

            test_metrics = test_data[""metrics""]
            result_indicator = ""‚ùî""

            if ""attempted"" not in test_metrics:
                return
            elif test_metrics[""attempted""]:
                if test_name not in test_names:
                    test_names.append(test_name)

                success_value = test_metrics[""success""]
                result_indicator = {True: ""‚úÖ"", False: ""‚ùå""}[success_value]

            if len(result_group) + 1 < runs_per_label[label]:
                result_group.extend(
                    [""  ""] * (runs_per_label[label] - len(result_group) - 1)
                )
            result_group.append(result_indicator)","Point(row=53, column=8)","Point(row=113, column=49)",,classic/original_autogpt/agbenchmark_config/analyze_reports.py
load_env_vars,function,,"def load_env_vars():
    from dotenv import load_dotenv

    load_dotenv()","Point(row=14, column=0)","Point(row=17, column=17)",,classic/forge/conftest.py
tmp_project_root,function,,"def tmp_project_root(tmp_path: Path) -> Path:
    return tmp_path","Point(row=21, column=0)","Point(row=22, column=19)",,classic/forge/conftest.py
app_data_dir,function,,"def app_data_dir(tmp_project_root: Path) -> Path:
    dir = tmp_project_root / ""data""
    dir.mkdir(parents=True, exist_ok=True)
    return dir","Point(row=26, column=0)","Point(row=29, column=14)",,classic/forge/conftest.py
storage,function,,"def storage(app_data_dir: Path) -> FileStorage:
    storage = LocalFileStorage(
        FileStorageConfiguration(
            root=Path(f""{app_data_dir}/{str(uuid.uuid4())}""), restrict_to_root=False
        )
    )
    storage.initialize()
    return storage","Point(row=33, column=0)","Point(row=40, column=18)",,classic/forge/conftest.py
before_record_request,function,,"def before_record_request(request: Request) -> Request | None:
    if not should_cache_request(request):
        return None

    request = filter_request_headers(request)
    request = freeze_request(request)
    return request","Point(row=37, column=0)","Point(row=43, column=18)",,classic/forge/tests/vcr/vcr_filter.py
should_cache_request,function,,"def should_cache_request(request: Request) -> bool:
    return any(hostname in request.url for hostname in HOSTNAMES_TO_CACHE)","Point(row=46, column=0)","Point(row=47, column=74)",,classic/forge/tests/vcr/vcr_filter.py
filter_request_headers,function,,"def filter_request_headers(request: Request) -> Request:
    for header_name in list(request.headers):
        if any(
            (
                (type(ignore) is str and ignore.lower() == header_name.lower())
                or (isinstance(ignore, re.Pattern) and ignore.match(header_name))
            )
            for ignore in IGNORE_REQUEST_HEADERS
        ):
            del request.headers[header_name]
    return request","Point(row=50, column=0)","Point(row=60, column=18)",,classic/forge/tests/vcr/vcr_filter.py
freeze_request,function,,"def freeze_request(request: Request) -> Request:
    if not request or not request.body:
        return request

    with contextlib.suppress(ValueError):
        request.body = freeze_request_body(
            json.loads(
                request.body.getvalue()
                if isinstance(request.body, BytesIO)
                else request.body
            )
        )

    return request","Point(row=63, column=0)","Point(row=76, column=18)",,classic/forge/tests/vcr/vcr_filter.py
freeze_request_body,function,Remove any dynamic items from the request body,"def freeze_request_body(body: dict) -> bytes:
    """"""Remove any dynamic items from the request body""""""

    if ""messages"" not in body:
        return json.dumps(body, sort_keys=True).encode()

    if ""max_tokens"" in body:
        del body[""max_tokens""]

    for message in body[""messages""]:
        if ""content"" in message and ""role"" in message:
            if message[""role""] == ""system"":
                message[""content""] = replace_message_content(
                    message[""content""], LLM_MESSAGE_REPLACEMENTS
                )

    return json.dumps(body, sort_keys=True).encode()","Point(row=79, column=0)","Point(row=95, column=52)",,classic/forge/tests/vcr/vcr_filter.py
replace_message_content,function,,"def replace_message_content(content: str, replacements: list[dict[str, str]]) -> str:
    for replacement in replacements:
        pattern = re.compile(replacement[""regex""])
        content = pattern.sub(replacement[""replacement""], content)

    return content","Point(row=98, column=0)","Point(row=103, column=18)",,classic/forge/tests/vcr/vcr_filter.py
before_record_response,function,,"def before_record_response(response: dict[str, Any]) -> dict[str, Any]:
    if ""Transfer-Encoding"" in response[""headers""]:
        del response[""headers""][""Transfer-Encoding""]
    return response","Point(row=106, column=0)","Point(row=109, column=19)",,classic/forge/tests/vcr/vcr_filter.py
vcr_config,function,,"def vcr_config(get_base_vcr_config):
    return get_base_vcr_config","Point(row=27, column=0)","Point(row=28, column=30)",,classic/forge/tests/vcr/__init__.py
get_base_vcr_config,function,,"def get_base_vcr_config(request):
    record_mode = request.config.getoption(""--record-mode"", default=""new_episodes"")
    config = BASE_VCR_CONFIG

    if record_mode is None:
        config[""record_mode""] = DEFAULT_RECORD_MODE

    return config","Point(row=32, column=0)","Point(row=39, column=17)",,classic/forge/tests/vcr/__init__.py
vcr_cassette_dir,function,,"def vcr_cassette_dir(request):
    test_name = os.path.splitext(request.node.name)[0]
    return os.path.join(""tests/vcr_cassettes"", test_name)","Point(row=43, column=0)","Point(row=45, column=57)",,classic/forge/tests/vcr/__init__.py
cached_openai_client,function,,"def cached_openai_client(mocker: MockerFixture) -> OpenAI:
    client = OpenAI()
    _prepare_options = client._prepare_options

    def _patched_prepare_options(self, options: FinalRequestOptions):
        _prepare_options(options)

        if not options.json_data:
            return

        headers: dict[str, str | Omit] = (
            {**options.headers} if is_given(options.headers) else {}
        )
        options.headers = headers
        data = cast(dict, options.json_data)

        logging.getLogger(""cached_openai_client"").debug(
            f""Outgoing API request: {headers}\n{data if data else None}""
        )

        # Add hash header for cheap & fast matching on cassette playback
        headers[""X-Content-Hash""] = sha256(
            freeze_request_body(data), usedforsecurity=False
        ).hexdigest()

    mocker.patch.object(
        client,
        ""_prepare_options"",
        new=_patched_prepare_options,
    )

    return client","Point(row=49, column=0)","Point(row=80, column=17)",,classic/forge/tests/vcr/__init__.py
cached_openai_client._patched_prepare_options,function,,"def _patched_prepare_options(self, options: FinalRequestOptions):
        _prepare_options(options)

        if not options.json_data:
            return

        headers: dict[str, str | Omit] = (
            {**options.headers} if is_given(options.headers) else {}
        )
        options.headers = headers
        data = cast(dict, options.json_data)

        logging.getLogger(""cached_openai_client"").debug(
            f""Outgoing API request: {headers}\n{data if data else None}""
        )

        # Add hash header for cheap & fast matching on cassette playback
        headers[""X-Content-Hash""] = sha256(
            freeze_request_body(data), usedforsecurity=False
        ).hexdigest()","Point(row=53, column=4)","Point(row=72, column=21)",,classic/forge/tests/vcr/__init__.py
convert_cassette_file,function,,"def convert_cassette_file(filename: str | Path):
    print(f""{filename} STARTING"")

    with open(filename) as c:
        cassette_content = yaml.load(c, Loader)

    # Iterate over all request+response pairs
    for interaction in cassette_content[""interactions""]:
        request_body: str = interaction[""request""][""body""]
        if request_body is None:
            continue

        with contextlib.suppress(json.decoder.JSONDecodeError):
            request_obj = json.loads(request_body)

            # Strip `max_tokens`, since its value doesn't matter
            #  as long as the request succeeds
            if ""max_tokens"" in request_obj:
                del request_obj[""max_tokens""]

            # Sort the keys of the request body
            request_body = json.dumps(request_obj, sort_keys=True)

        headers = interaction[""request""][""headers""]

        # Calculate hash for the request body, used for VCR lookup
        headers[""X-Content-Hash""] = [
            sha256(request_body.encode(), usedforsecurity=False).hexdigest()
        ]

        # Strip auth headers
        if ""AGENT-MODE"" in headers:
            del headers[""AGENT-MODE""]
        if ""AGENT-TYPE"" in headers:
            del headers[""AGENT-TYPE""]
        if ""OpenAI-Organization"" in headers:
            del headers[""OpenAI-Organization""]

        interaction[""request""][""body""] = request_body

    with open(filename, ""w"") as c:
        c.write(yaml.dump(cassette_content, Dumper=Dumper))

    print(f""{filename} DONE"")","Point(row=11, column=0)","Point(row=54, column=29)",,classic/forge/tests/vcr_cassettes/format_cassettes.py
dump_prompt,function,,"def dump_prompt(prompt: ChatPrompt) -> str:
    def separator(text: str):
        half_sep_len = (SEPARATOR_LENGTH - 2 - len(text)) / 2
        return f""{floor(half_sep_len)*'-'} {text.upper()} {ceil(half_sep_len)*'-'}""

    formatted_messages = ""\n"".join(
        [f""{separator(m.role)}\n{m.content}"" for m in prompt.messages]
    )
    return f""""""
============== {prompt.__class__.__name__} ==============
Length: {len(prompt.messages)} messages
{formatted_messages}
==========================================
""""""","Point(row=8, column=0)","Point(row=21, column=3)",,classic/forge/forge/llm/prompting/utils.py
dump_prompt.separator,function,,"def separator(text: str):
        half_sep_len = (SEPARATOR_LENGTH - 2 - len(text)) / 2
        return f""{floor(half_sep_len)*'-'} {text.upper()} {ceil(half_sep_len)*'-'}""","Point(row=9, column=4)","Point(row=11, column=83)",,classic/forge/forge/llm/prompting/utils.py
format_numbered_list,function,,"def format_numbered_list(items: list[Any], start_at: int = 1) -> str:
    return ""\n"".join(f""{i}. {str(item)}"" for i, item in enumerate(items, start_at))","Point(row=24, column=0)","Point(row=25, column=83)",,classic/forge/forge/llm/prompting/utils.py
indent,function,,"def indent(content: str, indentation: int | str = 4) -> str:
    if type(indentation) is int:
        indentation = "" "" * indentation
    return indentation + content.replace(""\n"", f""\n{indentation}"")  # type: ignore","Point(row=28, column=0)","Point(row=31, column=82)",,classic/forge/forge/llm/prompting/utils.py
to_numbered_list,function,,"def to_numbered_list(
    items: list[str], no_items_response: str = """", **template_args
) -> str:
    if items:
        return ""\n"".join(
            f""{i+1}. {item.format(**template_args)}"" for i, item in enumerate(items)
        )
    else:
        return no_items_response","Point(row=34, column=0)","Point(row=42, column=32)",,classic/forge/forge/llm/prompting/utils.py
PromptStrategy,class,,"class PromptStrategy(abc.ABC):
    @property
    @abc.abstractmethod
    def llm_classification(self) -> LanguageModelClassification:
        ...

    @abc.abstractmethod
    def build_prompt(self, *_, **kwargs) -> ChatPrompt:
        ...

    @abc.abstractmethod
    def parse_response_content(self, response: ""AssistantChatMessage"") -> Any:
        ...","Point(row=9, column=0)","Point(row=21, column=11)",,classic/forge/forge/llm/prompting/base.py
PromptStrategy.llm_classification,function,,"def llm_classification(self) -> LanguageModelClassification:
        ...","Point(row=12, column=4)","Point(row=13, column=11)",PromptStrategy,classic/forge/forge/llm/prompting/base.py
PromptStrategy.build_prompt,function,,"def build_prompt(self, *_, **kwargs) -> ChatPrompt:
        ...","Point(row=16, column=4)","Point(row=17, column=11)",PromptStrategy,classic/forge/forge/llm/prompting/base.py
PromptStrategy.parse_response_content,function,,"def parse_response_content(self, response: ""AssistantChatMessage"") -> Any:
        ...","Point(row=20, column=4)","Point(row=21, column=11)",PromptStrategy,classic/forge/forge/llm/prompting/base.py
LanguageModelClassification,class,"The LanguageModelClassification is a functional description of the model.

    This is used to determine what kind of model to use for a given prompt.
    Sometimes we prefer a faster or cheaper model to accomplish a task when
    possible.
","class LanguageModelClassification(str, enum.Enum):
    """"""The LanguageModelClassification is a functional description of the model.

    This is used to determine what kind of model to use for a given prompt.
    Sometimes we prefer a faster or cheaper model to accomplish a task when
    possible.
    """"""

    FAST_MODEL = ""fast_model""
    SMART_MODEL = ""smart_model""","Point(row=11, column=0)","Point(row=20, column=31)",,classic/forge/forge/llm/prompting/schema.py
ChatPrompt,class,,"class ChatPrompt(BaseModel):
    messages: list[ChatMessage]
    functions: list[CompletionModelFunction] = Field(default_factory=list)
    prefill_response: str = """"

    def raw(self) -> list[ChatMessageDict]:
        return [m.model_dump() for m in self.messages]  # type: ignore

    def __str__(self):
        return ""\n\n"".join(
            f""{m.role.value.upper()}: {m.content}"" for m in self.messages
        )","Point(row=23, column=0)","Point(row=34, column=9)",,classic/forge/forge/llm/prompting/schema.py
ChatPrompt.raw,function,,"def raw(self) -> list[ChatMessageDict]:
        return [m.model_dump() for m in self.messages]  # type: ignore","Point(row=28, column=4)","Point(row=29, column=70)",ChatPrompt,classic/forge/forge/llm/prompting/schema.py
ChatPrompt.__str__,function,,"def __str__(self):
        return ""\n\n"".join(
            f""{m.role.value.upper()}: {m.content}"" for m in self.messages
        )","Point(row=31, column=4)","Point(row=34, column=9)",ChatPrompt,classic/forge/forge/llm/prompting/schema.py
_BaseOpenAIProvider,class,Base class for LLM providers with OpenAI-like APIs,"class _BaseOpenAIProvider(BaseModelProvider[_ModelName, _ModelProviderSettings]):
    """"""Base class for LLM providers with OpenAI-like APIs""""""

    MODELS: ClassVar[
        Mapping[_ModelName, ChatModelInfo[_ModelName] | EmbeddingModelInfo[_ModelName]]  # type: ignore # noqa
    ]

    def __init__(
        self,
        settings: Optional[_ModelProviderSettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        if not getattr(self, ""MODELS"", None):
            raise ValueError(f""{self.__class__.__name__}.MODELS is not set"")

        if not settings:
            settings = self.default_settings.model_copy(deep=True)
        if not settings.credentials:
            settings.credentials = get_args(  # Union[Credentials, None] -> Credentials
                self.default_settings.model_fields[""credentials""].annotation
            )[0].from_env()

        super(_BaseOpenAIProvider, self).__init__(settings=settings, logger=logger)

        if not getattr(self, ""_client"", None):
            from openai import AsyncOpenAI

            self._client = AsyncOpenAI(
                **self._credentials.get_api_access_kwargs()  # type: ignore
            )

    async def get_available_models(
        self,
    ) -> Sequence[ChatModelInfo[_ModelName] | EmbeddingModelInfo[_ModelName]]:
        _models = (await self._client.models.list()).data
        return [
            self.MODELS[cast(_ModelName, m.id)] for m in _models if m.id in self.MODELS
        ]

    def get_token_limit(self, model_name: _ModelName) -> int:
        """"""Get the maximum number of input tokens for a given model""""""
        return self.MODELS[model_name].max_tokens

    def count_tokens(self, text: str, model_name: _ModelName) -> int:
        return len(self.get_tokenizer(model_name).encode(text))

    def _retry_api_request(self, func: Callable[_P, _T]) -> Callable[_P, _T]:
        return tenacity.retry(
            retry=(
                tenacity.retry_if_exception_type(APIConnectionError)
                | tenacity.retry_if_exception(
                    lambda e: isinstance(e, APIStatusError) and e.status_code >= 500
                )
            ),
            wait=tenacity.wait_exponential(),
            stop=tenacity.stop_after_attempt(self._configuration.retries_per_request),
            after=tenacity.after_log(self._logger, logging.DEBUG),
        )(func)

    def __repr__(self):
        return f""{self.__class__.__name__}()""","Point(row=54, column=0)","Point(row=114, column=45)",,classic/forge/forge/llm/providers/_openai_base.py
_BaseOpenAIProvider.__init__,function,,"def __init__(
        self,
        settings: Optional[_ModelProviderSettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        if not getattr(self, ""MODELS"", None):
            raise ValueError(f""{self.__class__.__name__}.MODELS is not set"")

        if not settings:
            settings = self.default_settings.model_copy(deep=True)
        if not settings.credentials:
            settings.credentials = get_args(  # Union[Credentials, None] -> Credentials
                self.default_settings.model_fields[""credentials""].annotation
            )[0].from_env()

        super(_BaseOpenAIProvider, self).__init__(settings=settings, logger=logger)

        if not getattr(self, ""_client"", None):
            from openai import AsyncOpenAI

            self._client = AsyncOpenAI(
                **self._credentials.get_api_access_kwargs()  # type: ignore
            )","Point(row=61, column=4)","Point(row=83, column=13)",_BaseOpenAIProvider,classic/forge/forge/llm/providers/_openai_base.py
_BaseOpenAIProvider.get_available_models,function,,"async def get_available_models(
        self,
    ) -> Sequence[ChatModelInfo[_ModelName] | EmbeddingModelInfo[_ModelName]]:
        _models = (await self._client.models.list()).data
        return [
            self.MODELS[cast(_ModelName, m.id)] for m in _models if m.id in self.MODELS
        ]","Point(row=85, column=4)","Point(row=91, column=9)",_BaseOpenAIProvider,classic/forge/forge/llm/providers/_openai_base.py
_BaseOpenAIProvider.get_token_limit,function,Get the maximum number of input tokens for a given model,"def get_token_limit(self, model_name: _ModelName) -> int:
        """"""Get the maximum number of input tokens for a given model""""""
        return self.MODELS[model_name].max_tokens","Point(row=93, column=4)","Point(row=95, column=49)",_BaseOpenAIProvider,classic/forge/forge/llm/providers/_openai_base.py
_BaseOpenAIProvider.count_tokens,function,,"def count_tokens(self, text: str, model_name: _ModelName) -> int:
        return len(self.get_tokenizer(model_name).encode(text))","Point(row=97, column=4)","Point(row=98, column=63)",_BaseOpenAIProvider,classic/forge/forge/llm/providers/_openai_base.py
_BaseOpenAIProvider._retry_api_request,function,,"def _retry_api_request(self, func: Callable[_P, _T]) -> Callable[_P, _T]:
        return tenacity.retry(
            retry=(
                tenacity.retry_if_exception_type(APIConnectionError)
                | tenacity.retry_if_exception(
                    lambda e: isinstance(e, APIStatusError) and e.status_code >= 500
                )
            ),
            wait=tenacity.wait_exponential(),
            stop=tenacity.stop_after_attempt(self._configuration.retries_per_request),
            after=tenacity.after_log(self._logger, logging.DEBUG),
        )(func)","Point(row=100, column=4)","Point(row=111, column=15)",_BaseOpenAIProvider,classic/forge/forge/llm/providers/_openai_base.py
_BaseOpenAIProvider.__repr__,function,,"def __repr__(self):
        return f""{self.__class__.__name__}()""","Point(row=113, column=4)","Point(row=114, column=45)",_BaseOpenAIProvider,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIChatProvider,class,,"class BaseOpenAIChatProvider(
    _BaseOpenAIProvider[_ModelName, _ModelProviderSettings],
    BaseChatModelProvider[_ModelName, _ModelProviderSettings],
):
    CHAT_MODELS: ClassVar[dict[_ModelName, ChatModelInfo[_ModelName]]]  # type: ignore

    def __init__(
        self,
        settings: Optional[_ModelProviderSettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        if not getattr(self, ""CHAT_MODELS"", None):
            raise ValueError(f""{self.__class__.__name__}.CHAT_MODELS is not set"")

        super(BaseOpenAIChatProvider, self).__init__(settings=settings, logger=logger)

    async def get_available_chat_models(self) -> Sequence[ChatModelInfo[_ModelName]]:
        all_available_models = await self.get_available_models()
        return [
            model
            for model in all_available_models
            if model.service == ModelProviderService.CHAT
        ]

    def count_message_tokens(
        self,
        messages: ChatMessage | list[ChatMessage],
        model_name: _ModelName,
    ) -> int:
        if isinstance(messages, ChatMessage):
            messages = [messages]
        return self.count_tokens(
            ""\n\n"".join(f""{m.role.upper()}: {m.content}"" for m in messages), model_name
        )

    async def create_chat_completion(
        self,
        model_prompt: list[ChatMessage],
        model_name: _ModelName,
        completion_parser: Callable[[AssistantChatMessage], _T] = lambda _: None,
        functions: Optional[list[CompletionModelFunction]] = None,
        max_output_tokens: Optional[int] = None,
        prefill_response: str = """",
        **kwargs,
    ) -> ChatModelResponse[_T]:
        """"""Create a chat completion using the API.""""""

        (
            openai_messages,
            completion_kwargs,
            parse_kwargs,
        ) = self._get_chat_completion_args(
            prompt_messages=model_prompt,
            model=model_name,
            functions=functions,
            max_output_tokens=max_output_tokens,
            **kwargs,
        )

        total_cost = 0.0
        attempts = 0
        while True:
            completion_kwargs[""messages""] = openai_messages
            _response, _cost, t_input, t_output = await self._create_chat_completion(
                model=model_name,
                completion_kwargs=completion_kwargs,
            )
            total_cost += _cost

            # If parsing the response fails, append the error to the prompt, and let the
            # LLM fix its mistake(s).
            attempts += 1
            parse_errors: list[Exception] = []

            _assistant_msg = _response.choices[0].message

            tool_calls, _errors = self._parse_assistant_tool_calls(
                _assistant_msg, **parse_kwargs
            )
            parse_errors += _errors

            # Validate tool calls
            if not parse_errors and tool_calls and functions:
                parse_errors += validate_tool_calls(tool_calls, functions)

            assistant_msg = AssistantChatMessage(
                content=_assistant_msg.content or """",
                tool_calls=tool_calls or None,
            )

            parsed_result: _T = None  # type: ignore
            if not parse_errors:
                try:
                    parsed_result = completion_parser(assistant_msg)
                except Exception as e:
                    parse_errors.append(e)

            if not parse_errors:
                if attempts > 1:
                    self._logger.debug(
                        f""Total cost for {attempts} attempts: ${round(total_cost, 5)}""
                    )

                return ChatModelResponse(
                    response=AssistantChatMessage(
                        content=_assistant_msg.content or """",
                        tool_calls=tool_calls or None,
                    ),
                    parsed_result=parsed_result,
                    llm_info=self.CHAT_MODELS[model_name],
                    prompt_tokens_used=t_input,
                    completion_tokens_used=t_output,
                )

            else:
                self._logger.debug(
                    f""Parsing failed on response: '''{_assistant_msg}'''""
                )
                parse_errors_fmt = ""\n\n"".join(
                    f""{e.__class__.__name__}: {e}"" for e in parse_errors
                )
                self._logger.warning(
                    f""Parsing attempt #{attempts} failed: {parse_errors_fmt}""
                )
                for e in parse_errors:
                    sentry_sdk.capture_exception(
                        error=e,
                        extras={""assistant_msg"": _assistant_msg, ""i_attempt"": attempts},
                    )

                if attempts < self._configuration.fix_failed_parse_tries:
                    openai_messages.append(
                        cast(
                            ChatCompletionAssistantMessageParam,
                            _assistant_msg.model_dump(exclude_none=True),
                        )
                    )
                    openai_messages.append(
                        {
                            ""role"": ""system"",
                            ""content"": (
                                f""ERROR PARSING YOUR RESPONSE:\n\n{parse_errors_fmt}""
                            ),
                        }
                    )
                    continue
                else:
                    raise parse_errors[0]

    def _get_chat_completion_args(
        self,
        prompt_messages: list[ChatMessage],
        model: _ModelName,
        functions: Optional[list[CompletionModelFunction]] = None,
        max_output_tokens: Optional[int] = None,
        **kwargs,
    ) -> tuple[
        list[ChatCompletionMessageParam], CompletionCreateParams, dict[str, Any]
    ]:
        """"""Prepare keyword arguments for a chat completion API call

        Args:
            prompt_messages: List of ChatMessages
            model: The model to use
            functions (optional): List of functions available to the LLM
            max_output_tokens (optional): Maximum number of tokens to generate

        Returns:
            list[ChatCompletionMessageParam]: Prompt messages for the API call
            CompletionCreateParams: Mapping of other kwargs for the API call
            Mapping[str, Any]: Any keyword arguments to pass on to the completion parser
        """"""
        kwargs = cast(CompletionCreateParams, kwargs)

        if max_output_tokens:
            kwargs[""max_tokens""] = max_output_tokens

        if functions:
            kwargs[""tools""] = [  # pyright: ignore - it fails to infer the dict type
                {""type"": ""function"", ""function"": format_function_def_for_openai(f)}
                for f in functions
            ]
            if len(functions) == 1:
                # force the model to call the only specified function
                kwargs[""tool_choice""] = {  # pyright: ignore - type inference failure
                    ""type"": ""function"",
                    ""function"": {""name"": functions[0].name},
                }

        if extra_headers := self._configuration.extra_request_headers:
            # 'extra_headers' is not on CompletionCreateParams, but is on chat.create()
            kwargs[""extra_headers""] = kwargs.get(""extra_headers"", {})  # type: ignore
            kwargs[""extra_headers""].update(extra_headers.copy())  # type: ignore

        prepped_messages: list[ChatCompletionMessageParam] = [
            message.model_dump(  # type: ignore
                include={""role"", ""content"", ""tool_calls"", ""tool_call_id"", ""name""},
                exclude_none=True,
            )
            for message in prompt_messages
        ]

        if ""messages"" in kwargs:
            prepped_messages += kwargs[""messages""]
            del kwargs[""messages""]  # type: ignore - messages are added back later

        return prepped_messages, kwargs, {}

    async def _create_chat_completion(
        self,
        model: _ModelName,
        completion_kwargs: CompletionCreateParams,
    ) -> tuple[ChatCompletion, float, int, int]:
        """"""
        Create a chat completion using an OpenAI-like API with retry handling

        Params:
            model: The model to use for the completion
            completion_kwargs: All other arguments for the completion call

        Returns:
            ChatCompletion: The chat completion response object
            float: The cost ($) of this completion
            int: Number of prompt tokens used
            int: Number of completion tokens used
        """"""
        completion_kwargs[""model""] = completion_kwargs.get(""model"") or model

        @self._retry_api_request
        async def _create_chat_completion_with_retry() -> ChatCompletion:
            return await self._client.chat.completions.create(
                **completion_kwargs,  # type: ignore
            )

        completion = await _create_chat_completion_with_retry()

        if completion.usage:
            prompt_tokens_used = completion.usage.prompt_tokens
            completion_tokens_used = completion.usage.completion_tokens
        else:
            prompt_tokens_used = completion_tokens_used = 0

        if self._budget:
            cost = self._budget.update_usage_and_cost(
                model_info=self.CHAT_MODELS[model],
                input_tokens_used=prompt_tokens_used,
                output_tokens_used=completion_tokens_used,
            )
        else:
            cost = 0

        self._logger.debug(
            f""{model} completion usage: {prompt_tokens_used} input, ""
            f""{completion_tokens_used} output - ${round(cost, 5)}""
        )
        return completion, cost, prompt_tokens_used, completion_tokens_used

    def _parse_assistant_tool_calls(
        self, assistant_message: ChatCompletionMessage, **kwargs
    ) -> tuple[list[AssistantToolCall], list[Exception]]:
        tool_calls: list[AssistantToolCall] = []
        parse_errors: list[Exception] = []

        if assistant_message.tool_calls:
            for _tc in assistant_message.tool_calls:
                try:
                    parsed_arguments = json_loads(_tc.function.arguments)
                except Exception as e:
                    err_message = (
                        f""Decoding arguments for {_tc.function.name} failed: ""
                        + str(e.args[0])
                    )
                    parse_errors.append(
                        type(e)(err_message, *e.args[1:]).with_traceback(
                            e.__traceback__
                        )
                    )
                    continue

                tool_calls.append(
                    AssistantToolCall(
                        id=_tc.id,
                        type=_tc.type,
                        function=AssistantFunctionCall(
                            name=_tc.function.name,
                            arguments=parsed_arguments,
                        ),
                    )
                )

            # If parsing of all tool calls succeeds in the end, we ignore any issues
            if len(tool_calls) == len(assistant_message.tool_calls):
                parse_errors = []

        return tool_calls, parse_errors","Point(row=117, column=0)","Point(row=411, column=39)",,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIChatProvider.__init__,function,,"def __init__(
        self,
        settings: Optional[_ModelProviderSettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        if not getattr(self, ""CHAT_MODELS"", None):
            raise ValueError(f""{self.__class__.__name__}.CHAT_MODELS is not set"")

        super(BaseOpenAIChatProvider, self).__init__(settings=settings, logger=logger)","Point(row=123, column=4)","Point(row=131, column=86)",BaseOpenAIChatProvider,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIChatProvider.get_available_chat_models,function,,"async def get_available_chat_models(self) -> Sequence[ChatModelInfo[_ModelName]]:
        all_available_models = await self.get_available_models()
        return [
            model
            for model in all_available_models
            if model.service == ModelProviderService.CHAT
        ]","Point(row=133, column=4)","Point(row=139, column=9)",BaseOpenAIChatProvider,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIChatProvider.count_message_tokens,function,,"def count_message_tokens(
        self,
        messages: ChatMessage | list[ChatMessage],
        model_name: _ModelName,
    ) -> int:
        if isinstance(messages, ChatMessage):
            messages = [messages]
        return self.count_tokens(
            ""\n\n"".join(f""{m.role.upper()}: {m.content}"" for m in messages), model_name
        )","Point(row=141, column=4)","Point(row=150, column=9)",BaseOpenAIChatProvider,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIChatProvider.create_chat_completion,function,Create a chat completion using the API.,"async def create_chat_completion(
        self,
        model_prompt: list[ChatMessage],
        model_name: _ModelName,
        completion_parser: Callable[[AssistantChatMessage], _T] = lambda _: None,
        functions: Optional[list[CompletionModelFunction]] = None,
        max_output_tokens: Optional[int] = None,
        prefill_response: str = """",
        **kwargs,
    ) -> ChatModelResponse[_T]:
        """"""Create a chat completion using the API.""""""

        (
            openai_messages,
            completion_kwargs,
            parse_kwargs,
        ) = self._get_chat_completion_args(
            prompt_messages=model_prompt,
            model=model_name,
            functions=functions,
            max_output_tokens=max_output_tokens,
            **kwargs,
        )

        total_cost = 0.0
        attempts = 0
        while True:
            completion_kwargs[""messages""] = openai_messages
            _response, _cost, t_input, t_output = await self._create_chat_completion(
                model=model_name,
                completion_kwargs=completion_kwargs,
            )
            total_cost += _cost

            # If parsing the response fails, append the error to the prompt, and let the
            # LLM fix its mistake(s).
            attempts += 1
            parse_errors: list[Exception] = []

            _assistant_msg = _response.choices[0].message

            tool_calls, _errors = self._parse_assistant_tool_calls(
                _assistant_msg, **parse_kwargs
            )
            parse_errors += _errors

            # Validate tool calls
            if not parse_errors and tool_calls and functions:
                parse_errors += validate_tool_calls(tool_calls, functions)

            assistant_msg = AssistantChatMessage(
                content=_assistant_msg.content or """",
                tool_calls=tool_calls or None,
            )

            parsed_result: _T = None  # type: ignore
            if not parse_errors:
                try:
                    parsed_result = completion_parser(assistant_msg)
                except Exception as e:
                    parse_errors.append(e)

            if not parse_errors:
                if attempts > 1:
                    self._logger.debug(
                        f""Total cost for {attempts} attempts: ${round(total_cost, 5)}""
                    )

                return ChatModelResponse(
                    response=AssistantChatMessage(
                        content=_assistant_msg.content or """",
                        tool_calls=tool_calls or None,
                    ),
                    parsed_result=parsed_result,
                    llm_info=self.CHAT_MODELS[model_name],
                    prompt_tokens_used=t_input,
                    completion_tokens_used=t_output,
                )

            else:
                self._logger.debug(
                    f""Parsing failed on response: '''{_assistant_msg}'''""
                )
                parse_errors_fmt = ""\n\n"".join(
                    f""{e.__class__.__name__}: {e}"" for e in parse_errors
                )
                self._logger.warning(
                    f""Parsing attempt #{attempts} failed: {parse_errors_fmt}""
                )
                for e in parse_errors:
                    sentry_sdk.capture_exception(
                        error=e,
                        extras={""assistant_msg"": _assistant_msg, ""i_attempt"": attempts},
                    )

                if attempts < self._configuration.fix_failed_parse_tries:
                    openai_messages.append(
                        cast(
                            ChatCompletionAssistantMessageParam,
                            _assistant_msg.model_dump(exclude_none=True),
                        )
                    )
                    openai_messages.append(
                        {
                            ""role"": ""system"",
                            ""content"": (
                                f""ERROR PARSING YOUR RESPONSE:\n\n{parse_errors_fmt}""
                            ),
                        }
                    )
                    continue
                else:
                    raise parse_errors[0]","Point(row=152, column=4)","Point(row=264, column=41)",BaseOpenAIChatProvider,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIChatProvider._get_chat_completion_args,function,"Prepare keyword arguments for a chat completion API call

        Args:
            prompt_messages: List of ChatMessages
            model: The model to use
            functions (optional): List of functions available to the LLM
            max_output_tokens (optional): Maximum number of tokens to generate

        Returns:
            list[ChatCompletionMessageParam]: Prompt messages for the API call
            CompletionCreateParams: Mapping of other kwargs for the API call
            Mapping[str, Any]: Any keyword arguments to pass on to the completion parser
","def _get_chat_completion_args(
        self,
        prompt_messages: list[ChatMessage],
        model: _ModelName,
        functions: Optional[list[CompletionModelFunction]] = None,
        max_output_tokens: Optional[int] = None,
        **kwargs,
    ) -> tuple[
        list[ChatCompletionMessageParam], CompletionCreateParams, dict[str, Any]
    ]:
        """"""Prepare keyword arguments for a chat completion API call

        Args:
            prompt_messages: List of ChatMessages
            model: The model to use
            functions (optional): List of functions available to the LLM
            max_output_tokens (optional): Maximum number of tokens to generate

        Returns:
            list[ChatCompletionMessageParam]: Prompt messages for the API call
            CompletionCreateParams: Mapping of other kwargs for the API call
            Mapping[str, Any]: Any keyword arguments to pass on to the completion parser
        """"""
        kwargs = cast(CompletionCreateParams, kwargs)

        if max_output_tokens:
            kwargs[""max_tokens""] = max_output_tokens

        if functions:
            kwargs[""tools""] = [  # pyright: ignore - it fails to infer the dict type
                {""type"": ""function"", ""function"": format_function_def_for_openai(f)}
                for f in functions
            ]
            if len(functions) == 1:
                # force the model to call the only specified function
                kwargs[""tool_choice""] = {  # pyright: ignore - type inference failure
                    ""type"": ""function"",
                    ""function"": {""name"": functions[0].name},
                }

        if extra_headers := self._configuration.extra_request_headers:
            # 'extra_headers' is not on CompletionCreateParams, but is on chat.create()
            kwargs[""extra_headers""] = kwargs.get(""extra_headers"", {})  # type: ignore
            kwargs[""extra_headers""].update(extra_headers.copy())  # type: ignore

        prepped_messages: list[ChatCompletionMessageParam] = [
            message.model_dump(  # type: ignore
                include={""role"", ""content"", ""tool_calls"", ""tool_call_id"", ""name""},
                exclude_none=True,
            )
            for message in prompt_messages
        ]

        if ""messages"" in kwargs:
            prepped_messages += kwargs[""messages""]
            del kwargs[""messages""]  # type: ignore - messages are added back later

        return prepped_messages, kwargs, {}","Point(row=266, column=4)","Point(row=323, column=43)",BaseOpenAIChatProvider,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIChatProvider._create_chat_completion,function,"
        Create a chat completion using an OpenAI-like API with retry handling

        Params:
            model: The model to use for the completion
            completion_kwargs: All other arguments for the completion call

        Returns:
            ChatCompletion: The chat completion response object
            float: The cost ($) of this completion
            int: Number of prompt tokens used
            int: Number of completion tokens used
","async def _create_chat_completion(
        self,
        model: _ModelName,
        completion_kwargs: CompletionCreateParams,
    ) -> tuple[ChatCompletion, float, int, int]:
        """"""
        Create a chat completion using an OpenAI-like API with retry handling

        Params:
            model: The model to use for the completion
            completion_kwargs: All other arguments for the completion call

        Returns:
            ChatCompletion: The chat completion response object
            float: The cost ($) of this completion
            int: Number of prompt tokens used
            int: Number of completion tokens used
        """"""
        completion_kwargs[""model""] = completion_kwargs.get(""model"") or model

        @self._retry_api_request
        async def _create_chat_completion_with_retry() -> ChatCompletion:
            return await self._client.chat.completions.create(
                **completion_kwargs,  # type: ignore
            )

        completion = await _create_chat_completion_with_retry()

        if completion.usage:
            prompt_tokens_used = completion.usage.prompt_tokens
            completion_tokens_used = completion.usage.completion_tokens
        else:
            prompt_tokens_used = completion_tokens_used = 0

        if self._budget:
            cost = self._budget.update_usage_and_cost(
                model_info=self.CHAT_MODELS[model],
                input_tokens_used=prompt_tokens_used,
                output_tokens_used=completion_tokens_used,
            )
        else:
            cost = 0

        self._logger.debug(
            f""{model} completion usage: {prompt_tokens_used} input, ""
            f""{completion_tokens_used} output - ${round(cost, 5)}""
        )
        return completion, cost, prompt_tokens_used, completion_tokens_used","Point(row=325, column=4)","Point(row=372, column=75)",BaseOpenAIChatProvider,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIChatProvider._create_chat_completion._create_chat_completion_with_retry,function,,"async def _create_chat_completion_with_retry() -> ChatCompletion:
            return await self._client.chat.completions.create(
                **completion_kwargs,  # type: ignore
            )","Point(row=346, column=8)","Point(row=349, column=13)",BaseOpenAIChatProvider,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIChatProvider._parse_assistant_tool_calls,function,,"def _parse_assistant_tool_calls(
        self, assistant_message: ChatCompletionMessage, **kwargs
    ) -> tuple[list[AssistantToolCall], list[Exception]]:
        tool_calls: list[AssistantToolCall] = []
        parse_errors: list[Exception] = []

        if assistant_message.tool_calls:
            for _tc in assistant_message.tool_calls:
                try:
                    parsed_arguments = json_loads(_tc.function.arguments)
                except Exception as e:
                    err_message = (
                        f""Decoding arguments for {_tc.function.name} failed: ""
                        + str(e.args[0])
                    )
                    parse_errors.append(
                        type(e)(err_message, *e.args[1:]).with_traceback(
                            e.__traceback__
                        )
                    )
                    continue

                tool_calls.append(
                    AssistantToolCall(
                        id=_tc.id,
                        type=_tc.type,
                        function=AssistantFunctionCall(
                            name=_tc.function.name,
                            arguments=parsed_arguments,
                        ),
                    )
                )

            # If parsing of all tool calls succeeds in the end, we ignore any issues
            if len(tool_calls) == len(assistant_message.tool_calls):
                parse_errors = []

        return tool_calls, parse_errors","Point(row=374, column=4)","Point(row=411, column=39)",BaseOpenAIChatProvider,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIEmbeddingProvider,class,,"class BaseOpenAIEmbeddingProvider(
    _BaseOpenAIProvider[_ModelName, _ModelProviderSettings],
    BaseEmbeddingModelProvider[_ModelName, _ModelProviderSettings],
):
    EMBEDDING_MODELS: ClassVar[
        dict[_ModelName, EmbeddingModelInfo[_ModelName]]  # type: ignore
    ]

    def __init__(
        self,
        settings: Optional[_ModelProviderSettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        if not getattr(self, ""EMBEDDING_MODELS"", None):
            raise ValueError(f""{self.__class__.__name__}.EMBEDDING_MODELS is not set"")

        super(BaseOpenAIEmbeddingProvider, self).__init__(
            settings=settings, logger=logger
        )

    async def get_available_embedding_models(
        self,
    ) -> Sequence[EmbeddingModelInfo[_ModelName]]:
        all_available_models = await self.get_available_models()
        return [
            model
            for model in all_available_models
            if model.service == ModelProviderService.EMBEDDING
        ]

    async def create_embedding(
        self,
        text: str,
        model_name: _ModelName,
        embedding_parser: Callable[[Embedding], Embedding],
        **kwargs,
    ) -> EmbeddingModelResponse:
        """"""Create an embedding using an OpenAI-like API""""""
        embedding_kwargs = self._get_embedding_kwargs(
            input=text, model=model_name, **kwargs
        )
        response = await self._create_embedding(embedding_kwargs)

        return EmbeddingModelResponse(
            embedding=embedding_parser(response.data[0].embedding),
            llm_info=self.EMBEDDING_MODELS[model_name],
            prompt_tokens_used=response.usage.prompt_tokens,
        )

    def _get_embedding_kwargs(
        self, input: str | list[str], model: _ModelName, **kwargs
    ) -> EmbeddingCreateParams:
        """"""Get kwargs for an embedding API call

        Params:
            input: Text body or list of text bodies to create embedding(s) from
            model: Embedding model to use

        Returns:
            The kwargs for the embedding API call
        """"""
        kwargs = cast(EmbeddingCreateParams, kwargs)

        kwargs[""input""] = input
        kwargs[""model""] = model

        if extra_headers := self._configuration.extra_request_headers:
            # 'extra_headers' is not on CompletionCreateParams, but is on embedding.create()  # noqa
            kwargs[""extra_headers""] = kwargs.get(""extra_headers"", {})  # type: ignore
            kwargs[""extra_headers""].update(extra_headers.copy())  # type: ignore

        return kwargs

    def _create_embedding(
        self, embedding_kwargs: EmbeddingCreateParams
    ) -> Awaitable[CreateEmbeddingResponse]:
        """"""Create an embedding using an OpenAI-like API with retry handling.""""""

        @self._retry_api_request
        async def _create_embedding_with_retry() -> CreateEmbeddingResponse:
            return await self._client.embeddings.create(**embedding_kwargs)

        return _create_embedding_with_retry()","Point(row=414, column=0)","Point(row=496, column=45)",,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIEmbeddingProvider.__init__,function,,"def __init__(
        self,
        settings: Optional[_ModelProviderSettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        if not getattr(self, ""EMBEDDING_MODELS"", None):
            raise ValueError(f""{self.__class__.__name__}.EMBEDDING_MODELS is not set"")

        super(BaseOpenAIEmbeddingProvider, self).__init__(
            settings=settings, logger=logger
        )","Point(row=422, column=4)","Point(row=432, column=9)",BaseOpenAIEmbeddingProvider,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIEmbeddingProvider.get_available_embedding_models,function,,"async def get_available_embedding_models(
        self,
    ) -> Sequence[EmbeddingModelInfo[_ModelName]]:
        all_available_models = await self.get_available_models()
        return [
            model
            for model in all_available_models
            if model.service == ModelProviderService.EMBEDDING
        ]","Point(row=434, column=4)","Point(row=442, column=9)",BaseOpenAIEmbeddingProvider,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIEmbeddingProvider.create_embedding,function,Create an embedding using an OpenAI-like API,"async def create_embedding(
        self,
        text: str,
        model_name: _ModelName,
        embedding_parser: Callable[[Embedding], Embedding],
        **kwargs,
    ) -> EmbeddingModelResponse:
        """"""Create an embedding using an OpenAI-like API""""""
        embedding_kwargs = self._get_embedding_kwargs(
            input=text, model=model_name, **kwargs
        )
        response = await self._create_embedding(embedding_kwargs)

        return EmbeddingModelResponse(
            embedding=embedding_parser(response.data[0].embedding),
            llm_info=self.EMBEDDING_MODELS[model_name],
            prompt_tokens_used=response.usage.prompt_tokens,
        )","Point(row=444, column=4)","Point(row=461, column=9)",BaseOpenAIEmbeddingProvider,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIEmbeddingProvider._get_embedding_kwargs,function,"Get kwargs for an embedding API call

        Params:
            input: Text body or list of text bodies to create embedding(s) from
            model: Embedding model to use

        Returns:
            The kwargs for the embedding API call
","def _get_embedding_kwargs(
        self, input: str | list[str], model: _ModelName, **kwargs
    ) -> EmbeddingCreateParams:
        """"""Get kwargs for an embedding API call

        Params:
            input: Text body or list of text bodies to create embedding(s) from
            model: Embedding model to use

        Returns:
            The kwargs for the embedding API call
        """"""
        kwargs = cast(EmbeddingCreateParams, kwargs)

        kwargs[""input""] = input
        kwargs[""model""] = model

        if extra_headers := self._configuration.extra_request_headers:
            # 'extra_headers' is not on CompletionCreateParams, but is on embedding.create()  # noqa
            kwargs[""extra_headers""] = kwargs.get(""extra_headers"", {})  # type: ignore
            kwargs[""extra_headers""].update(extra_headers.copy())  # type: ignore

        return kwargs","Point(row=463, column=4)","Point(row=485, column=21)",BaseOpenAIEmbeddingProvider,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIEmbeddingProvider._create_embedding,function,Create an embedding using an OpenAI-like API with retry handling.,"def _create_embedding(
        self, embedding_kwargs: EmbeddingCreateParams
    ) -> Awaitable[CreateEmbeddingResponse]:
        """"""Create an embedding using an OpenAI-like API with retry handling.""""""

        @self._retry_api_request
        async def _create_embedding_with_retry() -> CreateEmbeddingResponse:
            return await self._client.embeddings.create(**embedding_kwargs)

        return _create_embedding_with_retry()","Point(row=487, column=4)","Point(row=496, column=45)",BaseOpenAIEmbeddingProvider,classic/forge/forge/llm/providers/_openai_base.py
BaseOpenAIEmbeddingProvider._create_embedding._create_embedding_with_retry,function,,"async def _create_embedding_with_retry() -> CreateEmbeddingResponse:
            return await self._client.embeddings.create(**embedding_kwargs)","Point(row=493, column=8)","Point(row=494, column=75)",BaseOpenAIEmbeddingProvider,classic/forge/forge/llm/providers/_openai_base.py
format_function_def_for_openai,function,Returns an OpenAI-consumable function definition,"def format_function_def_for_openai(self: CompletionModelFunction) -> FunctionDefinition:
    """"""Returns an OpenAI-consumable function definition""""""

    return {
        ""name"": self.name,
        ""description"": self.description,
        ""parameters"": {
            ""type"": ""object"",
            ""properties"": {
                name: param.to_dict() for name, param in self.parameters.items()
            },
            ""required"": [
                name for name, param in self.parameters.items() if param.required
            ],
        },
    }","Point(row=499, column=0)","Point(row=514, column=5)",,classic/forge/forge/llm/providers/_openai_base.py
MultiProvider,class,,"class MultiProvider(BaseChatModelProvider[ModelName, ModelProviderSettings]):
    default_settings = ModelProviderSettings(
        name=""multi_provider"",
        description=(
            ""Provides access to all of the available models, regardless of provider.""
        ),
        configuration=ModelProviderConfiguration(
            retries_per_request=7,
        ),
        budget=ModelProviderBudget(),
    )

    _budget: ModelProviderBudget

    _provider_instances: dict[ModelProviderName, ChatModelProvider]

    def __init__(
        self,
        settings: Optional[ModelProviderSettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        super(MultiProvider, self).__init__(settings=settings, logger=logger)
        self._budget = self._settings.budget or ModelProviderBudget()

        self._provider_instances = {}

    async def get_available_models(self) -> Sequence[ChatModelInfo[ModelName]]:
        # TODO: support embeddings
        return await self.get_available_chat_models()

    async def get_available_chat_models(self) -> Sequence[ChatModelInfo[ModelName]]:
        models = []
        async for provider in self.get_available_providers():
            models.extend(await provider.get_available_chat_models())
        return models

    def get_token_limit(self, model_name: ModelName) -> int:
        """"""Get the token limit for a given model.""""""
        return self.get_model_provider(model_name).get_token_limit(
            model_name  # type: ignore
        )

    def get_tokenizer(self, model_name: ModelName) -> ModelTokenizer[Any]:
        return self.get_model_provider(model_name).get_tokenizer(
            model_name  # type: ignore
        )

    def count_tokens(self, text: str, model_name: ModelName) -> int:
        return self.get_model_provider(model_name).count_tokens(
            text=text, model_name=model_name  # type: ignore
        )

    def count_message_tokens(
        self, messages: ChatMessage | list[ChatMessage], model_name: ModelName
    ) -> int:
        return self.get_model_provider(model_name).count_message_tokens(
            messages=messages, model_name=model_name  # type: ignore
        )

    async def create_chat_completion(
        self,
        model_prompt: list[ChatMessage],
        model_name: ModelName,
        completion_parser: Callable[[AssistantChatMessage], _T] = lambda _: None,
        functions: Optional[list[CompletionModelFunction]] = None,
        max_output_tokens: Optional[int] = None,
        prefill_response: str = """",
        **kwargs,
    ) -> ChatModelResponse[_T]:
        """"""Create a completion using the Anthropic API.""""""
        return await self.get_model_provider(model_name).create_chat_completion(
            model_prompt=model_prompt,
            model_name=model_name,  # type: ignore
            completion_parser=completion_parser,
            functions=functions,
            max_output_tokens=max_output_tokens,
            prefill_response=prefill_response,
            **kwargs,
        )

    def get_model_provider(self, model: ModelName) -> ChatModelProvider:
        model_info = CHAT_MODELS[model]
        return self._get_provider(model_info.provider_name)

    async def get_available_providers(self) -> AsyncIterator[ChatModelProvider]:
        for provider_name in ModelProviderName:
            self._logger.debug(f""Checking if provider {provider_name} is available..."")
            try:
                provider = self._get_provider(provider_name)
                await provider.get_available_models()  # check connection
                yield provider
                self._logger.debug(f""Provider '{provider_name}' is available!"")
            except ValueError:
                pass
            except Exception as e:
                self._logger.debug(f""Provider '{provider_name}' is failing: {e}"")

    def _get_provider(self, provider_name: ModelProviderName) -> ChatModelProvider:
        _provider = self._provider_instances.get(provider_name)
        if not _provider:
            Provider = self._get_provider_class(provider_name)
            self._logger.debug(
                f""{Provider.__name__} not yet in cache, trying to init...""
            )

            settings = Provider.default_settings.model_copy(deep=True)
            settings.budget = self._budget
            settings.configuration.extra_request_headers.update(
                self._settings.configuration.extra_request_headers
            )
            if settings.credentials is None:
                credentials_field = settings.model_fields[""credentials""]
                Credentials = get_args(  # Union[Credentials, None] -> Credentials
                    credentials_field.annotation
                )[0]
                self._logger.debug(f""Loading {Credentials.__name__}..."")
                try:
                    settings.credentials = Credentials.from_env()
                except ValidationError as e:
                    if credentials_field.is_required():
                        self._logger.debug(
                            f""Could not load (required) {Credentials.__name__}""
                        )
                        raise ValueError(
                            f""{Provider.__name__} is unavailable: ""
                            ""can't load credentials""
                        ) from e
                    self._logger.debug(
                        f""Could not load {Credentials.__name__}, continuing without...""
                    )

            self._provider_instances[provider_name] = _provider = Provider(
                settings=settings, logger=self._logger  # type: ignore
            )
            _provider._budget = self._budget  # Object binding not preserved by Pydantic
            self._logger.debug(f""Initialized {Provider.__name__}!"")
        return _provider

    @classmethod
    def _get_provider_class(
        cls, provider_name: ModelProviderName
    ) -> type[AnthropicProvider | GroqProvider | OpenAIProvider]:
        try:
            return {
                ModelProviderName.ANTHROPIC: AnthropicProvider,
                ModelProviderName.GROQ: GroqProvider,
                ModelProviderName.LLAMAFILE: LlamafileProvider,
                ModelProviderName.OPENAI: OpenAIProvider,
            }[provider_name]
        except KeyError:
            raise ValueError(f""{provider_name} is not a known provider"") from None

    def __repr__(self):
        return f""{self.__class__.__name__}()""","Point(row=38, column=0)","Point(row=191, column=45)",,classic/forge/forge/llm/providers/multi.py
MultiProvider.__init__,function,,"def __init__(
        self,
        settings: Optional[ModelProviderSettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        super(MultiProvider, self).__init__(settings=settings, logger=logger)
        self._budget = self._settings.budget or ModelProviderBudget()

        self._provider_instances = {}","Point(row=54, column=4)","Point(row=62, column=37)",MultiProvider,classic/forge/forge/llm/providers/multi.py
MultiProvider.get_available_models,function,,"async def get_available_models(self) -> Sequence[ChatModelInfo[ModelName]]:
        # TODO: support embeddings
        return await self.get_available_chat_models()","Point(row=64, column=4)","Point(row=66, column=53)",MultiProvider,classic/forge/forge/llm/providers/multi.py
MultiProvider.get_available_chat_models,function,,"async def get_available_chat_models(self) -> Sequence[ChatModelInfo[ModelName]]:
        models = []
        async for provider in self.get_available_providers():
            models.extend(await provider.get_available_chat_models())
        return models","Point(row=68, column=4)","Point(row=72, column=21)",MultiProvider,classic/forge/forge/llm/providers/multi.py
MultiProvider.get_token_limit,function,Get the token limit for a given model.,"def get_token_limit(self, model_name: ModelName) -> int:
        """"""Get the token limit for a given model.""""""
        return self.get_model_provider(model_name).get_token_limit(
            model_name  # type: ignore
        )","Point(row=74, column=4)","Point(row=78, column=9)",MultiProvider,classic/forge/forge/llm/providers/multi.py
MultiProvider.get_tokenizer,function,,"def get_tokenizer(self, model_name: ModelName) -> ModelTokenizer[Any]:
        return self.get_model_provider(model_name).get_tokenizer(
            model_name  # type: ignore
        )","Point(row=80, column=4)","Point(row=83, column=9)",MultiProvider,classic/forge/forge/llm/providers/multi.py
MultiProvider.count_tokens,function,,"def count_tokens(self, text: str, model_name: ModelName) -> int:
        return self.get_model_provider(model_name).count_tokens(
            text=text, model_name=model_name  # type: ignore
        )","Point(row=85, column=4)","Point(row=88, column=9)",MultiProvider,classic/forge/forge/llm/providers/multi.py
MultiProvider.count_message_tokens,function,,"def count_message_tokens(
        self, messages: ChatMessage | list[ChatMessage], model_name: ModelName
    ) -> int:
        return self.get_model_provider(model_name).count_message_tokens(
            messages=messages, model_name=model_name  # type: ignore
        )","Point(row=90, column=4)","Point(row=95, column=9)",MultiProvider,classic/forge/forge/llm/providers/multi.py
MultiProvider.create_chat_completion,function,Create a completion using the Anthropic API.,"async def create_chat_completion(
        self,
        model_prompt: list[ChatMessage],
        model_name: ModelName,
        completion_parser: Callable[[AssistantChatMessage], _T] = lambda _: None,
        functions: Optional[list[CompletionModelFunction]] = None,
        max_output_tokens: Optional[int] = None,
        prefill_response: str = """",
        **kwargs,
    ) -> ChatModelResponse[_T]:
        """"""Create a completion using the Anthropic API.""""""
        return await self.get_model_provider(model_name).create_chat_completion(
            model_prompt=model_prompt,
            model_name=model_name,  # type: ignore
            completion_parser=completion_parser,
            functions=functions,
            max_output_tokens=max_output_tokens,
            prefill_response=prefill_response,
            **kwargs,
        )","Point(row=97, column=4)","Point(row=116, column=9)",MultiProvider,classic/forge/forge/llm/providers/multi.py
MultiProvider.get_model_provider,function,,"def get_model_provider(self, model: ModelName) -> ChatModelProvider:
        model_info = CHAT_MODELS[model]
        return self._get_provider(model_info.provider_name)","Point(row=118, column=4)","Point(row=120, column=59)",MultiProvider,classic/forge/forge/llm/providers/multi.py
MultiProvider.get_available_providers,function,,"async def get_available_providers(self) -> AsyncIterator[ChatModelProvider]:
        for provider_name in ModelProviderName:
            self._logger.debug(f""Checking if provider {provider_name} is available..."")
            try:
                provider = self._get_provider(provider_name)
                await provider.get_available_models()  # check connection
                yield provider
                self._logger.debug(f""Provider '{provider_name}' is available!"")
            except ValueError:
                pass
            except Exception as e:
                self._logger.debug(f""Provider '{provider_name}' is failing: {e}"")","Point(row=122, column=4)","Point(row=133, column=81)",MultiProvider,classic/forge/forge/llm/providers/multi.py
MultiProvider._get_provider,function,,"def _get_provider(self, provider_name: ModelProviderName) -> ChatModelProvider:
        _provider = self._provider_instances.get(provider_name)
        if not _provider:
            Provider = self._get_provider_class(provider_name)
            self._logger.debug(
                f""{Provider.__name__} not yet in cache, trying to init...""
            )

            settings = Provider.default_settings.model_copy(deep=True)
            settings.budget = self._budget
            settings.configuration.extra_request_headers.update(
                self._settings.configuration.extra_request_headers
            )
            if settings.credentials is None:
                credentials_field = settings.model_fields[""credentials""]
                Credentials = get_args(  # Union[Credentials, None] -> Credentials
                    credentials_field.annotation
                )[0]
                self._logger.debug(f""Loading {Credentials.__name__}..."")
                try:
                    settings.credentials = Credentials.from_env()
                except ValidationError as e:
                    if credentials_field.is_required():
                        self._logger.debug(
                            f""Could not load (required) {Credentials.__name__}""
                        )
                        raise ValueError(
                            f""{Provider.__name__} is unavailable: ""
                            ""can't load credentials""
                        ) from e
                    self._logger.debug(
                        f""Could not load {Credentials.__name__}, continuing without...""
                    )

            self._provider_instances[provider_name] = _provider = Provider(
                settings=settings, logger=self._logger  # type: ignore
            )
            _provider._budget = self._budget  # Object binding not preserved by Pydantic
            self._logger.debug(f""Initialized {Provider.__name__}!"")
        return _provider","Point(row=135, column=4)","Point(row=174, column=24)",MultiProvider,classic/forge/forge/llm/providers/multi.py
MultiProvider._get_provider_class,function,,"def _get_provider_class(
        cls, provider_name: ModelProviderName
    ) -> type[AnthropicProvider | GroqProvider | OpenAIProvider]:
        try:
            return {
                ModelProviderName.ANTHROPIC: AnthropicProvider,
                ModelProviderName.GROQ: GroqProvider,
                ModelProviderName.LLAMAFILE: LlamafileProvider,
                ModelProviderName.OPENAI: OpenAIProvider,
            }[provider_name]
        except KeyError:
            raise ValueError(f""{provider_name} is not a known provider"") from None","Point(row=177, column=4)","Point(row=188, column=82)",MultiProvider,classic/forge/forge/llm/providers/multi.py
MultiProvider.__repr__,function,,"def __repr__(self):
        return f""{self.__class__.__name__}()""","Point(row=190, column=4)","Point(row=191, column=45)",MultiProvider,classic/forge/forge/llm/providers/multi.py
GroqModelName,class,,"class GroqModelName(str, enum.Enum):
    LLAMA3_8B = ""llama3-8b-8192""
    LLAMA3_70B = ""llama3-70b-8192""
    MIXTRAL_8X7B = ""mixtral-8x7b-32768""
    GEMMA_7B = ""gemma-7b-it""","Point(row=23, column=0)","Point(row=27, column=28)",,classic/forge/forge/llm/providers/groq.py
GroqCredentials,class,Credentials for Groq.,"class GroqCredentials(ModelProviderCredentials):
    """"""Credentials for Groq.""""""

    api_key: SecretStr = UserConfigurable(from_env=""GROQ_API_KEY"")  # type: ignore
    api_base: Optional[SecretStr] = UserConfigurable(
        default=None, from_env=""GROQ_API_BASE_URL""
    )

    def get_api_access_kwargs(self) -> dict[str, str]:
        return {
            k: v.get_secret_value()
            for k, v in {
                ""api_key"": self.api_key,
                ""base_url"": self.api_base,
            }.items()
            if v is not None
        }","Point(row=69, column=0)","Point(row=85, column=9)",,classic/forge/forge/llm/providers/groq.py
GroqCredentials.get_api_access_kwargs,function,,"def get_api_access_kwargs(self) -> dict[str, str]:
        return {
            k: v.get_secret_value()
            for k, v in {
                ""api_key"": self.api_key,
                ""base_url"": self.api_base,
            }.items()
            if v is not None
        }","Point(row=77, column=4)","Point(row=85, column=9)",GroqCredentials,classic/forge/forge/llm/providers/groq.py
GroqSettings,class,,"class GroqSettings(ModelProviderSettings):
    credentials: Optional[GroqCredentials]  # type: ignore
    budget: ModelProviderBudget  # type: ignore","Point(row=88, column=0)","Point(row=90, column=47)",,classic/forge/forge/llm/providers/groq.py
GroqProvider,class,,"class GroqProvider(BaseOpenAIChatProvider[GroqModelName, GroqSettings]):
    CHAT_MODELS = GROQ_CHAT_MODELS
    MODELS = CHAT_MODELS

    default_settings = GroqSettings(
        name=""groq_provider"",
        description=""Provides access to Groq's API."",
        configuration=ModelProviderConfiguration(),
        credentials=None,
        budget=ModelProviderBudget(),
    )

    _settings: GroqSettings
    _configuration: ModelProviderConfiguration
    _credentials: GroqCredentials
    _budget: ModelProviderBudget

    def __init__(
        self,
        settings: Optional[GroqSettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        super(GroqProvider, self).__init__(settings=settings, logger=logger)

        from groq import AsyncGroq

        self._client = AsyncGroq(
            **self._credentials.get_api_access_kwargs()  # type: ignore
        )

    def get_tokenizer(self, model_name: GroqModelName) -> ModelTokenizer[Any]:
        # HACK: No official tokenizer is available for Groq
        return tiktoken.encoding_for_model(""gpt-3.5-turbo"")","Point(row=93, column=0)","Point(row=125, column=59)",,classic/forge/forge/llm/providers/groq.py
GroqProvider.__init__,function,,"def __init__(
        self,
        settings: Optional[GroqSettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        super(GroqProvider, self).__init__(settings=settings, logger=logger)

        from groq import AsyncGroq

        self._client = AsyncGroq(
            **self._credentials.get_api_access_kwargs()  # type: ignore
        )","Point(row=110, column=4)","Point(row=121, column=9)",GroqProvider,classic/forge/forge/llm/providers/groq.py
GroqProvider.get_tokenizer,function,,"def get_tokenizer(self, model_name: GroqModelName) -> ModelTokenizer[Any]:
        # HACK: No official tokenizer is available for Groq
        return tiktoken.encoding_for_model(""gpt-3.5-turbo"")","Point(row=123, column=4)","Point(row=125, column=59)",GroqProvider,classic/forge/forge/llm/providers/groq.py
OpenAIModelName,class,,"class OpenAIModelName(str, enum.Enum):
    EMBEDDING_v2 = ""text-embedding-ada-002""
    EMBEDDING_v3_S = ""text-embedding-3-small""
    EMBEDDING_v3_L = ""text-embedding-3-large""

    GPT3_v1 = ""gpt-3.5-turbo-0301""
    GPT3_v2 = ""gpt-3.5-turbo-0613""
    GPT3_v2_16k = ""gpt-3.5-turbo-16k-0613""
    GPT3_v3 = ""gpt-3.5-turbo-1106""
    GPT3_v4 = ""gpt-3.5-turbo-0125""
    GPT3_ROLLING = ""gpt-3.5-turbo""
    GPT3_ROLLING_16k = ""gpt-3.5-turbo-16k""
    GPT3 = GPT3_ROLLING
    GPT3_16k = GPT3_ROLLING_16k

    GPT4_v1 = ""gpt-4-0314""
    GPT4_v1_32k = ""gpt-4-32k-0314""
    GPT4_v2 = ""gpt-4-0613""
    GPT4_v2_32k = ""gpt-4-32k-0613""
    GPT4_v3 = ""gpt-4-1106-preview""
    GPT4_v3_VISION = ""gpt-4-1106-vision-preview""
    GPT4_v4 = ""gpt-4-0125-preview""
    GPT4_v5 = ""gpt-4-turbo-2024-04-09""
    GPT4_ROLLING = ""gpt-4""
    GPT4_ROLLING_32k = ""gpt-4-32k""
    GPT4_TURBO = ""gpt-4-turbo""
    GPT4_TURBO_PREVIEW = ""gpt-4-turbo-preview""
    GPT4_VISION = ""gpt-4-vision-preview""
    GPT4_O_v1 = ""gpt-4o-2024-05-13""
    GPT4_O_ROLLING = ""gpt-4o""
    GPT4 = GPT4_ROLLING
    GPT4_32k = GPT4_ROLLING_32k
    GPT4_O = GPT4_O_ROLLING","Point(row=45, column=0)","Point(row=77, column=27)",,classic/forge/forge/llm/providers/openai.py
OpenAICredentials,class,Credentials for OpenAI.,"class OpenAICredentials(ModelProviderCredentials):
    """"""Credentials for OpenAI.""""""

    api_key: SecretStr = UserConfigurable(from_env=""OPENAI_API_KEY"")  # type: ignore
    api_base: Optional[SecretStr] = UserConfigurable(
        default=None, from_env=""OPENAI_API_BASE_URL""
    )
    organization: Optional[SecretStr] = UserConfigurable(
        default=None, from_env=""OPENAI_ORGANIZATION""
    )

    api_type: Optional[SecretStr] = UserConfigurable(
        default=None,
        from_env=lambda: cast(
            SecretStr | None,
            (
                ""azure""
                if os.getenv(""USE_AZURE"") == ""True""
                else os.getenv(""OPENAI_API_TYPE"")
            ),
        ),
    )
    api_version: Optional[SecretStr] = UserConfigurable(
        default=None, from_env=""OPENAI_API_VERSION""
    )
    azure_endpoint: Optional[SecretStr] = None
    azure_model_to_deploy_id_map: Optional[dict[str, str]] = None

    def get_api_access_kwargs(self) -> dict[str, str]:
        kwargs = {
            k: v.get_secret_value()
            for k, v in {
                ""api_key"": self.api_key,
                ""base_url"": self.api_base,
                ""organization"": self.organization,
                ""api_version"": self.api_version,
            }.items()
            if v is not None
        }
        if self.api_type == SecretStr(""azure""):
            assert self.azure_endpoint, ""Azure endpoint not configured""
            kwargs[""azure_endpoint""] = self.azure_endpoint.get_secret_value()
        return kwargs

    def get_model_access_kwargs(self, model: str) -> dict[str, str]:
        kwargs = {""model"": model}
        if self.api_type == SecretStr(""azure"") and model:
            azure_kwargs = self._get_azure_access_kwargs(model)
            kwargs.update(azure_kwargs)
        return kwargs

    def load_azure_config(self, config_file: Path) -> None:
        with open(config_file) as file:
            config_params = yaml.load(file, Loader=yaml.SafeLoader) or {}

        try:
            assert config_params.get(
                ""azure_model_map"", {}
            ), ""Azure model->deployment_id map is empty""
        except AssertionError as e:
            raise ValueError(*e.args)

        self.api_type = config_params.get(""azure_api_type"", ""azure"")
        self.api_version = config_params.get(""azure_api_version"", None)
        self.azure_endpoint = config_params.get(""azure_endpoint"")
        self.azure_model_to_deploy_id_map = config_params.get(""azure_model_map"")

    def _get_azure_access_kwargs(self, model: str) -> dict[str, str]:
        """"""Get the kwargs for the Azure API.""""""

        if not self.azure_model_to_deploy_id_map:
            raise ValueError(""Azure model deployment map not configured"")

        if model not in self.azure_model_to_deploy_id_map:
            raise ValueError(f""No Azure deployment ID configured for model '{model}'"")
        deployment_id = self.azure_model_to_deploy_id_map[model]

        return {""model"": deployment_id}","Point(row=214, column=0)","Point(row=291, column=39)",,classic/forge/forge/llm/providers/openai.py
OpenAICredentials.get_api_access_kwargs,function,,"def get_api_access_kwargs(self) -> dict[str, str]:
        kwargs = {
            k: v.get_secret_value()
            for k, v in {
                ""api_key"": self.api_key,
                ""base_url"": self.api_base,
                ""organization"": self.organization,
                ""api_version"": self.api_version,
            }.items()
            if v is not None
        }
        if self.api_type == SecretStr(""azure""):
            assert self.azure_endpoint, ""Azure endpoint not configured""
            kwargs[""azure_endpoint""] = self.azure_endpoint.get_secret_value()
        return kwargs","Point(row=242, column=4)","Point(row=256, column=21)",OpenAICredentials,classic/forge/forge/llm/providers/openai.py
OpenAICredentials.get_model_access_kwargs,function,,"def get_model_access_kwargs(self, model: str) -> dict[str, str]:
        kwargs = {""model"": model}
        if self.api_type == SecretStr(""azure"") and model:
            azure_kwargs = self._get_azure_access_kwargs(model)
            kwargs.update(azure_kwargs)
        return kwargs","Point(row=258, column=4)","Point(row=263, column=21)",OpenAICredentials,classic/forge/forge/llm/providers/openai.py
OpenAICredentials.load_azure_config,function,,"def load_azure_config(self, config_file: Path) -> None:
        with open(config_file) as file:
            config_params = yaml.load(file, Loader=yaml.SafeLoader) or {}

        try:
            assert config_params.get(
                ""azure_model_map"", {}
            ), ""Azure model->deployment_id map is empty""
        except AssertionError as e:
            raise ValueError(*e.args)

        self.api_type = config_params.get(""azure_api_type"", ""azure"")
        self.api_version = config_params.get(""azure_api_version"", None)
        self.azure_endpoint = config_params.get(""azure_endpoint"")
        self.azure_model_to_deploy_id_map = config_params.get(""azure_model_map"")","Point(row=265, column=4)","Point(row=279, column=80)",OpenAICredentials,classic/forge/forge/llm/providers/openai.py
OpenAICredentials._get_azure_access_kwargs,function,Get the kwargs for the Azure API.,"def _get_azure_access_kwargs(self, model: str) -> dict[str, str]:
        """"""Get the kwargs for the Azure API.""""""

        if not self.azure_model_to_deploy_id_map:
            raise ValueError(""Azure model deployment map not configured"")

        if model not in self.azure_model_to_deploy_id_map:
            raise ValueError(f""No Azure deployment ID configured for model '{model}'"")
        deployment_id = self.azure_model_to_deploy_id_map[model]

        return {""model"": deployment_id}","Point(row=281, column=4)","Point(row=291, column=39)",OpenAICredentials,classic/forge/forge/llm/providers/openai.py
OpenAISettings,class,,"class OpenAISettings(ModelProviderSettings):
    credentials: Optional[OpenAICredentials]  # type: ignore
    budget: ModelProviderBudget  # type: ignore","Point(row=294, column=0)","Point(row=296, column=47)",,classic/forge/forge/llm/providers/openai.py
OpenAIProvider,class,,"class OpenAIProvider(
    BaseOpenAIChatProvider[OpenAIModelName, OpenAISettings],
    BaseOpenAIEmbeddingProvider[OpenAIModelName, OpenAISettings],
):
    MODELS = OPEN_AI_MODELS
    CHAT_MODELS = OPEN_AI_CHAT_MODELS
    EMBEDDING_MODELS = OPEN_AI_EMBEDDING_MODELS

    default_settings = OpenAISettings(
        name=""openai_provider"",
        description=""Provides access to OpenAI's API."",
        configuration=ModelProviderConfiguration(),
        credentials=None,
        budget=ModelProviderBudget(),
    )

    _settings: OpenAISettings
    _configuration: ModelProviderConfiguration
    _credentials: OpenAICredentials
    _budget: ModelProviderBudget

    def __init__(
        self,
        settings: Optional[OpenAISettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        super(OpenAIProvider, self).__init__(settings=settings, logger=logger)

        if self._credentials.api_type == SecretStr(""azure""):
            from openai import AsyncAzureOpenAI

            # API key and org (if configured) are passed, the rest of the required
            # credentials is loaded from the environment by the AzureOpenAI client.
            self._client = AsyncAzureOpenAI(
                **self._credentials.get_api_access_kwargs()  # type: ignore
            )
        else:
            from openai import AsyncOpenAI

            self._client = AsyncOpenAI(
                **self._credentials.get_api_access_kwargs()  # type: ignore
            )

    def get_tokenizer(self, model_name: OpenAIModelName) -> ModelTokenizer[int]:
        return tiktoken.encoding_for_model(model_name)

    def count_message_tokens(
        self,
        messages: ChatMessage | list[ChatMessage],
        model_name: OpenAIModelName,
    ) -> int:
        if isinstance(messages, ChatMessage):
            messages = [messages]

        if model_name.startswith(""gpt-3.5-turbo""):
            tokens_per_message = (
                4  # every message follows <|start|>{role/name}\n{content}<|end|>\n
            )
            tokens_per_name = -1  # if there's a name, the role is omitted
        # TODO: check if this is still valid for gpt-4o
        elif model_name.startswith(""gpt-4""):
            tokens_per_message = 3
            tokens_per_name = 1
        else:
            raise NotImplementedError(
                f""count_message_tokens() is not implemented for model {model_name}.\n""
                ""See https://github.com/openai/openai-python/blob/120d225b91a8453e15240a49fb1c6794d8119326/chatml.md ""  # noqa
                ""for information on how messages are converted to tokens.""
            )
        tokenizer = self.get_tokenizer(model_name)

        num_tokens = 0
        for message in messages:
            num_tokens += tokens_per_message
            for key, value in message.model_dump().items():
                num_tokens += len(tokenizer.encode(value))
                if key == ""name"":
                    num_tokens += tokens_per_name
        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>
        return num_tokens

    def _get_chat_completion_args(
        self,
        prompt_messages: list[ChatMessage],
        model: OpenAIModelName,
        functions: Optional[list[CompletionModelFunction]] = None,
        max_output_tokens: Optional[int] = None,
        **kwargs,
    ) -> tuple[
        list[ChatCompletionMessageParam], CompletionCreateParams, dict[str, Any]
    ]:
        """"""Prepare keyword arguments for an OpenAI chat completion call

        Args:
            prompt_messages: List of ChatMessages
            model: The model to use
            functions (optional): List of functions available to the LLM
            max_output_tokens (optional): Maximum number of tokens to generate

        Returns:
            list[ChatCompletionMessageParam]: Prompt messages for the OpenAI call
            CompletionCreateParams: Mapping of other kwargs for the OpenAI call
            Mapping[str, Any]: Any keyword arguments to pass on to the completion parser
        """"""
        tools_compat_mode = False
        if functions:
            if not OPEN_AI_CHAT_MODELS[model].has_function_call_api:
                # Provide compatibility with older models
                _functions_compat_fix_kwargs(functions, prompt_messages)
                tools_compat_mode = True
                functions = None

        openai_messages, kwargs, parse_kwargs = super()._get_chat_completion_args(
            prompt_messages=prompt_messages,
            model=model,
            functions=functions,
            max_output_tokens=max_output_tokens,
            **kwargs,
        )
        kwargs.update(self._credentials.get_model_access_kwargs(model))  # type: ignore

        if tools_compat_mode:
            parse_kwargs[""compat_mode""] = True

        return openai_messages, kwargs, parse_kwargs

    def _parse_assistant_tool_calls(
        self,
        assistant_message: ChatCompletionMessage,
        compat_mode: bool = False,
        **kwargs,
    ) -> tuple[list[AssistantToolCall], list[Exception]]:
        tool_calls: list[AssistantToolCall] = []
        parse_errors: list[Exception] = []

        if not compat_mode:
            return super()._parse_assistant_tool_calls(
                assistant_message=assistant_message, compat_mode=compat_mode, **kwargs
            )
        elif assistant_message.content:
            try:
                tool_calls = list(
                    _tool_calls_compat_extract_calls(assistant_message.content)
                )
            except Exception as e:
                parse_errors.append(e)

        return tool_calls, parse_errors

    def _get_embedding_kwargs(
        self, input: str | list[str], model: OpenAIModelName, **kwargs
    ) -> EmbeddingCreateParams:
        kwargs = super()._get_embedding_kwargs(input=input, model=model, **kwargs)
        kwargs.update(self._credentials.get_model_access_kwargs(model))  # type: ignore
        return kwargs

    _get_embedding_kwargs.__doc__ = (
        BaseOpenAIEmbeddingProvider._get_embedding_kwargs.__doc__
    )

    def _retry_api_request(self, func: Callable[_P, _T]) -> Callable[_P, _T]:
        _log_retry_debug_message = tenacity.after_log(self._logger, logging.DEBUG)

        def _log_on_fail(retry_state: tenacity.RetryCallState) -> None:
            _log_retry_debug_message(retry_state)

            if (
                retry_state.attempt_number == 0
                and retry_state.outcome
                and isinstance(retry_state.outcome.exception(), RateLimitError)
            ):
                self._logger.warning(
                    ""Please double check that you have setup a PAID OpenAI API Account.""
                    "" You can read more here: ""
                    ""https://docs.agpt.co/setup/#getting-an-openai-api-key""
                )

        return tenacity.retry(
            retry=(
                tenacity.retry_if_exception_type(RateLimitError)
                | tenacity.retry_if_exception(
                    lambda e: isinstance(e, APIStatusError) and e.status_code == 502
                )
            ),
            wait=tenacity.wait_exponential(),
            stop=tenacity.stop_after_attempt(self._configuration.retries_per_request),
            after=_log_on_fail,
        )(func)

    def __repr__(self):
        return ""OpenAIProvider()""","Point(row=299, column=0)","Point(row=489, column=33)",,classic/forge/forge/llm/providers/openai.py
OpenAIProvider.__init__,function,,"def __init__(
        self,
        settings: Optional[OpenAISettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        super(OpenAIProvider, self).__init__(settings=settings, logger=logger)

        if self._credentials.api_type == SecretStr(""azure""):
            from openai import AsyncAzureOpenAI

            # API key and org (if configured) are passed, the rest of the required
            # credentials is loaded from the environment by the AzureOpenAI client.
            self._client = AsyncAzureOpenAI(
                **self._credentials.get_api_access_kwargs()  # type: ignore
            )
        else:
            from openai import AsyncOpenAI

            self._client = AsyncOpenAI(
                **self._credentials.get_api_access_kwargs()  # type: ignore
            )","Point(row=320, column=4)","Point(row=340, column=13)",OpenAIProvider,classic/forge/forge/llm/providers/openai.py
OpenAIProvider.get_tokenizer,function,,"def get_tokenizer(self, model_name: OpenAIModelName) -> ModelTokenizer[int]:
        return tiktoken.encoding_for_model(model_name)","Point(row=342, column=4)","Point(row=343, column=54)",OpenAIProvider,classic/forge/forge/llm/providers/openai.py
OpenAIProvider.count_message_tokens,function,,"def count_message_tokens(
        self,
        messages: ChatMessage | list[ChatMessage],
        model_name: OpenAIModelName,
    ) -> int:
        if isinstance(messages, ChatMessage):
            messages = [messages]

        if model_name.startswith(""gpt-3.5-turbo""):
            tokens_per_message = (
                4  # every message follows <|start|>{role/name}\n{content}<|end|>\n
            )
            tokens_per_name = -1  # if there's a name, the role is omitted
        # TODO: check if this is still valid for gpt-4o
        elif model_name.startswith(""gpt-4""):
            tokens_per_message = 3
            tokens_per_name = 1
        else:
            raise NotImplementedError(
                f""count_message_tokens() is not implemented for model {model_name}.\n""
                ""See https://github.com/openai/openai-python/blob/120d225b91a8453e15240a49fb1c6794d8119326/chatml.md ""  # noqa
                ""for information on how messages are converted to tokens.""
            )
        tokenizer = self.get_tokenizer(model_name)

        num_tokens = 0
        for message in messages:
            num_tokens += tokens_per_message
            for key, value in message.model_dump().items():
                num_tokens += len(tokenizer.encode(value))
                if key == ""name"":
                    num_tokens += tokens_per_name
        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>
        return num_tokens","Point(row=345, column=4)","Point(row=378, column=25)",OpenAIProvider,classic/forge/forge/llm/providers/openai.py
OpenAIProvider._get_chat_completion_args,function,"Prepare keyword arguments for an OpenAI chat completion call

        Args:
            prompt_messages: List of ChatMessages
            model: The model to use
            functions (optional): List of functions available to the LLM
            max_output_tokens (optional): Maximum number of tokens to generate

        Returns:
            list[ChatCompletionMessageParam]: Prompt messages for the OpenAI call
            CompletionCreateParams: Mapping of other kwargs for the OpenAI call
            Mapping[str, Any]: Any keyword arguments to pass on to the completion parser
","def _get_chat_completion_args(
        self,
        prompt_messages: list[ChatMessage],
        model: OpenAIModelName,
        functions: Optional[list[CompletionModelFunction]] = None,
        max_output_tokens: Optional[int] = None,
        **kwargs,
    ) -> tuple[
        list[ChatCompletionMessageParam], CompletionCreateParams, dict[str, Any]
    ]:
        """"""Prepare keyword arguments for an OpenAI chat completion call

        Args:
            prompt_messages: List of ChatMessages
            model: The model to use
            functions (optional): List of functions available to the LLM
            max_output_tokens (optional): Maximum number of tokens to generate

        Returns:
            list[ChatCompletionMessageParam]: Prompt messages for the OpenAI call
            CompletionCreateParams: Mapping of other kwargs for the OpenAI call
            Mapping[str, Any]: Any keyword arguments to pass on to the completion parser
        """"""
        tools_compat_mode = False
        if functions:
            if not OPEN_AI_CHAT_MODELS[model].has_function_call_api:
                # Provide compatibility with older models
                _functions_compat_fix_kwargs(functions, prompt_messages)
                tools_compat_mode = True
                functions = None

        openai_messages, kwargs, parse_kwargs = super()._get_chat_completion_args(
            prompt_messages=prompt_messages,
            model=model,
            functions=functions,
            max_output_tokens=max_output_tokens,
            **kwargs,
        )
        kwargs.update(self._credentials.get_model_access_kwargs(model))  # type: ignore

        if tools_compat_mode:
            parse_kwargs[""compat_mode""] = True

        return openai_messages, kwargs, parse_kwargs","Point(row=380, column=4)","Point(row=423, column=52)",OpenAIProvider,classic/forge/forge/llm/providers/openai.py
OpenAIProvider._parse_assistant_tool_calls,function,,"def _parse_assistant_tool_calls(
        self,
        assistant_message: ChatCompletionMessage,
        compat_mode: bool = False,
        **kwargs,
    ) -> tuple[list[AssistantToolCall], list[Exception]]:
        tool_calls: list[AssistantToolCall] = []
        parse_errors: list[Exception] = []

        if not compat_mode:
            return super()._parse_assistant_tool_calls(
                assistant_message=assistant_message, compat_mode=compat_mode, **kwargs
            )
        elif assistant_message.content:
            try:
                tool_calls = list(
                    _tool_calls_compat_extract_calls(assistant_message.content)
                )
            except Exception as e:
                parse_errors.append(e)

        return tool_calls, parse_errors","Point(row=425, column=4)","Point(row=446, column=39)",OpenAIProvider,classic/forge/forge/llm/providers/openai.py
OpenAIProvider._get_embedding_kwargs,function,,"def _get_embedding_kwargs(
        self, input: str | list[str], model: OpenAIModelName, **kwargs
    ) -> EmbeddingCreateParams:
        kwargs = super()._get_embedding_kwargs(input=input, model=model, **kwargs)
        kwargs.update(self._credentials.get_model_access_kwargs(model))  # type: ignore
        return kwargs","Point(row=448, column=4)","Point(row=453, column=21)",OpenAIProvider,classic/forge/forge/llm/providers/openai.py
OpenAIProvider._retry_api_request,function,,"def _retry_api_request(self, func: Callable[_P, _T]) -> Callable[_P, _T]:
        _log_retry_debug_message = tenacity.after_log(self._logger, logging.DEBUG)

        def _log_on_fail(retry_state: tenacity.RetryCallState) -> None:
            _log_retry_debug_message(retry_state)

            if (
                retry_state.attempt_number == 0
                and retry_state.outcome
                and isinstance(retry_state.outcome.exception(), RateLimitError)
            ):
                self._logger.warning(
                    ""Please double check that you have setup a PAID OpenAI API Account.""
                    "" You can read more here: ""
                    ""https://docs.agpt.co/setup/#getting-an-openai-api-key""
                )

        return tenacity.retry(
            retry=(
                tenacity.retry_if_exception_type(RateLimitError)
                | tenacity.retry_if_exception(
                    lambda e: isinstance(e, APIStatusError) and e.status_code == 502
                )
            ),
            wait=tenacity.wait_exponential(),
            stop=tenacity.stop_after_attempt(self._configuration.retries_per_request),
            after=_log_on_fail,
        )(func)","Point(row=459, column=4)","Point(row=486, column=15)",OpenAIProvider,classic/forge/forge/llm/providers/openai.py
OpenAIProvider._retry_api_request._log_on_fail,function,,"def _log_on_fail(retry_state: tenacity.RetryCallState) -> None:
            _log_retry_debug_message(retry_state)

            if (
                retry_state.attempt_number == 0
                and retry_state.outcome
                and isinstance(retry_state.outcome.exception(), RateLimitError)
            ):
                self._logger.warning(
                    ""Please double check that you have setup a PAID OpenAI API Account.""
                    "" You can read more here: ""
                    ""https://docs.agpt.co/setup/#getting-an-openai-api-key""
                )","Point(row=462, column=8)","Point(row=474, column=17)",OpenAIProvider,classic/forge/forge/llm/providers/openai.py
OpenAIProvider.__repr__,function,,"def __repr__(self):
        return ""OpenAIProvider()""","Point(row=488, column=4)","Point(row=489, column=33)",OpenAIProvider,classic/forge/forge/llm/providers/openai.py
format_function_specs_as_typescript_ns,function,"Returns a function signature block in the format used by OpenAI internally:
    https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/18

    For use with `count_tokens` to determine token usage of provided functions.

    Example:
    ```ts
    namespace functions {

    // Get the current weather in a given location
    type get_current_weather = (_: {
    // The city and state, e.g. San Francisco, CA
    location: string,
    unit?: ""celsius"" | ""fahrenheit"",
    }) => any;

    } // namespace functions
    ```
","def format_function_specs_as_typescript_ns(
    functions: list[CompletionModelFunction],
) -> str:
    """"""Returns a function signature block in the format used by OpenAI internally:
    https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/18

    For use with `count_tokens` to determine token usage of provided functions.

    Example:
    ```ts
    namespace functions {

    // Get the current weather in a given location
    type get_current_weather = (_: {
    // The city and state, e.g. San Francisco, CA
    location: string,
    unit?: ""celsius"" | ""fahrenheit"",
    }) => any;

    } // namespace functions
    ```
    """"""

    return (
        ""namespace functions {\n\n""
        + ""\n\n"".join(format_openai_function_for_prompt(f) for f in functions)
        + ""\n\n} // namespace functions""
    )","Point(row=492, column=0)","Point(row=519, column=5)",,classic/forge/forge/llm/providers/openai.py
format_openai_function_for_prompt,function,"Returns the function formatted similarly to the way OpenAI does it internally:
    https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/18

    Example:
    ```ts
    // Get the current weather in a given location
    type get_current_weather = (_: {
    // The city and state, e.g. San Francisco, CA
    location: string,
    unit?: ""celsius"" | ""fahrenheit"",
    }) => any;
    ```
","def format_openai_function_for_prompt(func: CompletionModelFunction) -> str:
    """"""Returns the function formatted similarly to the way OpenAI does it internally:
    https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/18

    Example:
    ```ts
    // Get the current weather in a given location
    type get_current_weather = (_: {
    // The city and state, e.g. San Francisco, CA
    location: string,
    unit?: ""celsius"" | ""fahrenheit"",
    }) => any;
    ```
    """"""

    def param_signature(name: str, spec: JSONSchema) -> str:
        return (
            f""// {spec.description}\n"" if spec.description else """"
        ) + f""{name}{'' if spec.required else '?'}: {spec.typescript_type},""

    return ""\n"".join(
        [
            f""// {func.description}"",
            f""type {func.name} = (_ :{{"",
            *[param_signature(name, p) for name, p in func.parameters.items()],
            ""}) => any;"",
        ]
    )","Point(row=522, column=0)","Point(row=549, column=5)",,classic/forge/forge/llm/providers/openai.py
format_openai_function_for_prompt.param_signature,function,,"def param_signature(name: str, spec: JSONSchema) -> str:
        return (
            f""// {spec.description}\n"" if spec.description else """"
        ) + f""{name}{'' if spec.required else '?'}: {spec.typescript_type},""","Point(row=537, column=4)","Point(row=540, column=76)",,classic/forge/forge/llm/providers/openai.py
count_openai_functions_tokens,function,"Returns the number of tokens taken up by a set of function definitions

    Reference: https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/18  # noqa: E501
","def count_openai_functions_tokens(
    functions: list[CompletionModelFunction], count_tokens: Callable[[str], int]
) -> int:
    """"""Returns the number of tokens taken up by a set of function definitions

    Reference: https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/18  # noqa: E501
    """"""
    return count_tokens(
        ""# Tools\n\n""
        ""## functions\n\n""
        f""{format_function_specs_as_typescript_ns(functions)}""
    )","Point(row=552, column=0)","Point(row=563, column=5)",,classic/forge/forge/llm/providers/openai.py
_functions_compat_fix_kwargs,function,,"def _functions_compat_fix_kwargs(
    functions: list[CompletionModelFunction],
    prompt_messages: list[ChatMessage],
):
    function_definitions = format_function_specs_as_typescript_ns(functions)
    function_call_schema = JSONSchema(
        type=JSONSchema.Type.OBJECT,
        properties={
            ""name"": JSONSchema(
                description=""The name of the function to call"",
                enum=[f.name for f in functions],
                required=True,
            ),
            ""arguments"": JSONSchema(
                description=""The arguments for the function call"",
                type=JSONSchema.Type.OBJECT,
                required=True,
            ),
        },
    )
    tool_calls_schema = JSONSchema(
        type=JSONSchema.Type.ARRAY,
        items=JSONSchema(
            type=JSONSchema.Type.OBJECT,
            properties={
                ""type"": JSONSchema(
                    type=JSONSchema.Type.STRING,
                    enum=[""function""],
                ),
                ""function"": function_call_schema,
            },
        ),
    )
    prompt_messages.append(
        ChatMessage.system(
            ""# tool usage instructions\n\n""
            ""Specify a '```tool_calls' block in your response,""
            "" with a valid JSON object that adheres to the following schema:\n\n""
            f""{tool_calls_schema.to_dict()}\n\n""
            ""Specify any tools that you need to use through this JSON object.\n\n""
            ""Put the tool_calls block at the end of your response""
            "" and include its fences if it is not the only content.\n\n""
            ""## functions\n\n""
            ""For the function call itself, use one of the following""
            f"" functions:\n\n{function_definitions}""
        ),
    )","Point(row=566, column=0)","Point(row=612, column=5)",,classic/forge/forge/llm/providers/openai.py
_tool_calls_compat_extract_calls,function,,"def _tool_calls_compat_extract_calls(response: str) -> Iterator[AssistantToolCall]:
    import re
    import uuid

    logging.debug(f""Trying to extract tool calls from response:\n{response}"")

    if response[0] == ""["":
        tool_calls: list[AssistantToolCallDict] = json_loads(response)
    else:
        block = re.search(r""```(?:tool_calls)?\n(.*)\n```\s*$"", response, re.DOTALL)
        if not block:
            raise ValueError(""Could not find tool_calls block in response"")
        tool_calls: list[AssistantToolCallDict] = json_loads(block.group(1))

    for t in tool_calls:
        t[""id""] = str(uuid.uuid4())
        yield AssistantToolCall.model_validate(t)","Point(row=615, column=0)","Point(row=631, column=49)",,classic/forge/forge/llm/providers/openai.py
InvalidFunctionCallError,class,,"class InvalidFunctionCallError(Exception):
    def __init__(self, name: str, arguments: dict[str, Any], message: str):
        self.message = message
        self.name = name
        self.arguments = arguments
        super().__init__(message)

    def __str__(self) -> str:
        return f""Invalid function call for {self.name}: {self.message}""","Point(row=8, column=0)","Point(row=16, column=71)",,classic/forge/forge/llm/providers/utils.py
InvalidFunctionCallError.__init__,function,,"def __init__(self, name: str, arguments: dict[str, Any], message: str):
        self.message = message
        self.name = name
        self.arguments = arguments
        super().__init__(message)","Point(row=9, column=4)","Point(row=13, column=33)",InvalidFunctionCallError,classic/forge/forge/llm/providers/utils.py
InvalidFunctionCallError.__str__,function,,"def __str__(self) -> str:
        return f""Invalid function call for {self.name}: {self.message}""","Point(row=15, column=4)","Point(row=16, column=71)",InvalidFunctionCallError,classic/forge/forge/llm/providers/utils.py
validate_tool_calls,function,"
    Validates a list of tool calls against a list of functions.

    1. Tries to find a function matching each tool call
    2. If a matching function is found, validates the tool call's arguments,
    reporting any resulting errors
    2. If no matching function is found, an error ""Unknown function X"" is reported
    3. A list of all errors encountered during validation is returned

    Params:
        tool_calls: A list of tool calls to validate.
        functions: A list of functions to validate against.

    Returns:
        list[InvalidFunctionCallError]: All errors encountered during validation.
","def validate_tool_calls(
    tool_calls: list[AssistantToolCall], functions: list[CompletionModelFunction]
) -> list[InvalidFunctionCallError]:
    """"""
    Validates a list of tool calls against a list of functions.

    1. Tries to find a function matching each tool call
    2. If a matching function is found, validates the tool call's arguments,
    reporting any resulting errors
    2. If no matching function is found, an error ""Unknown function X"" is reported
    3. A list of all errors encountered during validation is returned

    Params:
        tool_calls: A list of tool calls to validate.
        functions: A list of functions to validate against.

    Returns:
        list[InvalidFunctionCallError]: All errors encountered during validation.
    """"""
    errors: list[InvalidFunctionCallError] = []
    for tool_call in tool_calls:
        function_call = tool_call.function

        if function := next(
            (f for f in functions if f.name == function_call.name),
            None,
        ):
            is_valid, validation_errors = function.validate_call(function_call)
            if not is_valid:
                fmt_errors = [
                    f""{'.'.join(str(p) for p in f.path)}: {f.message}""
                    if f.path
                    else f.message
                    for f in validation_errors
                ]
                errors.append(
                    InvalidFunctionCallError(
                        name=function_call.name,
                        arguments=function_call.arguments,
                        message=(
                            ""The set of arguments supplied is invalid:\n""
                            + ""\n"".join(fmt_errors)
                        ),
                    )
                )
        else:
            errors.append(
                InvalidFunctionCallError(
                    name=function_call.name,
                    arguments=function_call.arguments,
                    message=f""Unknown function {function_call.name}"",
                )
            )

    return errors","Point(row=19, column=0)","Point(row=73, column=17)",,classic/forge/forge/llm/providers/utils.py
function_specs_from_commands,function,Get LLM-consumable function specs for the agent's available commands.,"def function_specs_from_commands(
    commands: Iterable[""Command""],
) -> list[CompletionModelFunction]:
    """"""Get LLM-consumable function specs for the agent's available commands.""""""
    return [
        CompletionModelFunction(
            name=command.names[0],
            description=command.description,
            parameters={param.name: param.spec for param in command.parameters},
        )
        for command in commands
    ]","Point(row=76, column=0)","Point(row=87, column=5)",,classic/forge/forge/llm/providers/utils.py
AnthropicModelName,class,,"class AnthropicModelName(str, enum.Enum):
    CLAUDE3_OPUS_v1 = ""claude-3-opus-20240229""
    CLAUDE3_SONNET_v1 = ""claude-3-sonnet-20240229""
    CLAUDE3_5_SONNET_v1 = ""claude-3-5-sonnet-20240620""
    CLAUDE3_HAIKU_v1 = ""claude-3-haiku-20240307""","Point(row=42, column=0)","Point(row=46, column=48)",,classic/forge/forge/llm/providers/anthropic.py
AnthropicCredentials,class,Credentials for Anthropic.,"class AnthropicCredentials(ModelProviderCredentials):
    """"""Credentials for Anthropic.""""""

    api_key: SecretStr = UserConfigurable(from_env=""ANTHROPIC_API_KEY"")  # type: ignore
    api_base: Optional[SecretStr] = UserConfigurable(
        default=None, from_env=""ANTHROPIC_API_BASE_URL""
    )

    def get_api_access_kwargs(self) -> dict[str, str]:
        return {
            k: v.get_secret_value()
            for k, v in {
                ""api_key"": self.api_key,
                ""base_url"": self.api_base,
            }.items()
            if v is not None
        }","Point(row=88, column=0)","Point(row=104, column=9)",,classic/forge/forge/llm/providers/anthropic.py
AnthropicCredentials.get_api_access_kwargs,function,,"def get_api_access_kwargs(self) -> dict[str, str]:
        return {
            k: v.get_secret_value()
            for k, v in {
                ""api_key"": self.api_key,
                ""base_url"": self.api_base,
            }.items()
            if v is not None
        }","Point(row=96, column=4)","Point(row=104, column=9)",AnthropicCredentials,classic/forge/forge/llm/providers/anthropic.py
AnthropicSettings,class,,"class AnthropicSettings(ModelProviderSettings):
    credentials: Optional[AnthropicCredentials]  # type: ignore
    budget: ModelProviderBudget  # type: ignore","Point(row=107, column=0)","Point(row=109, column=47)",,classic/forge/forge/llm/providers/anthropic.py
AnthropicProvider,class,,"class AnthropicProvider(BaseChatModelProvider[AnthropicModelName, AnthropicSettings]):
    default_settings = AnthropicSettings(
        name=""anthropic_provider"",
        description=""Provides access to Anthropic's API."",
        configuration=ModelProviderConfiguration(),
        credentials=None,
        budget=ModelProviderBudget(),
    )

    _settings: AnthropicSettings
    _credentials: AnthropicCredentials
    _budget: ModelProviderBudget

    def __init__(
        self,
        settings: Optional[AnthropicSettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        if not settings:
            settings = self.default_settings.model_copy(deep=True)
        if not settings.credentials:
            settings.credentials = AnthropicCredentials.from_env()

        super(AnthropicProvider, self).__init__(settings=settings, logger=logger)

        from anthropic import AsyncAnthropic

        self._client = AsyncAnthropic(
            **self._credentials.get_api_access_kwargs()  # type: ignore
        )

    async def get_available_models(self) -> Sequence[ChatModelInfo[AnthropicModelName]]:
        return await self.get_available_chat_models()

    async def get_available_chat_models(
        self,
    ) -> Sequence[ChatModelInfo[AnthropicModelName]]:
        return list(ANTHROPIC_CHAT_MODELS.values())

    def get_token_limit(self, model_name: AnthropicModelName) -> int:
        """"""Get the token limit for a given model.""""""
        return ANTHROPIC_CHAT_MODELS[model_name].max_tokens

    def get_tokenizer(self, model_name: AnthropicModelName) -> ModelTokenizer[Any]:
        # HACK: No official tokenizer is available for Claude 3
        return tiktoken.encoding_for_model(model_name)

    def count_tokens(self, text: str, model_name: AnthropicModelName) -> int:
        return 0  # HACK: No official tokenizer is available for Claude 3

    def count_message_tokens(
        self,
        messages: ChatMessage | list[ChatMessage],
        model_name: AnthropicModelName,
    ) -> int:
        return 0  # HACK: No official tokenizer is available for Claude 3

    async def create_chat_completion(
        self,
        model_prompt: list[ChatMessage],
        model_name: AnthropicModelName,
        completion_parser: Callable[[AssistantChatMessage], _T] = lambda _: None,
        functions: Optional[list[CompletionModelFunction]] = None,
        max_output_tokens: Optional[int] = None,
        prefill_response: str = """",
        **kwargs,
    ) -> ChatModelResponse[_T]:
        """"""Create a completion using the Anthropic API.""""""
        anthropic_messages, completion_kwargs = self._get_chat_completion_args(
            prompt_messages=model_prompt,
            model=model_name,
            functions=functions,
            max_output_tokens=max_output_tokens,
            **kwargs,
        )

        total_cost = 0.0
        attempts = 0
        while True:
            completion_kwargs[""messages""] = anthropic_messages.copy()
            if prefill_response:
                completion_kwargs[""messages""].append(
                    {""role"": ""assistant"", ""content"": prefill_response}
                )

            (
                _assistant_msg,
                cost,
                t_input,
                t_output,
            ) = await self._create_chat_completion(model_name, completion_kwargs)
            total_cost += cost
            self._logger.debug(
                f""Completion usage: {t_input} input, {t_output} output ""
                f""- ${round(cost, 5)}""
            )

            # Merge prefill into generated response
            if prefill_response:
                first_text_block = next(
                    b for b in _assistant_msg.content if b.type == ""text""
                )
                first_text_block.text = prefill_response + first_text_block.text

            assistant_msg = AssistantChatMessage(
                content=""\n\n"".join(
                    b.text for b in _assistant_msg.content if b.type == ""text""
                ),
                tool_calls=self._parse_assistant_tool_calls(_assistant_msg),
            )

            # If parsing the response fails, append the error to the prompt, and let the
            # LLM fix its mistake(s).
            attempts += 1
            tool_call_errors = []
            try:
                # Validate tool calls
                if assistant_msg.tool_calls and functions:
                    tool_call_errors = validate_tool_calls(
                        assistant_msg.tool_calls, functions
                    )
                    if tool_call_errors:
                        raise ValueError(
                            ""Invalid tool use(s):\n""
                            + ""\n"".join(str(e) for e in tool_call_errors)
                        )

                parsed_result = completion_parser(assistant_msg)
                break
            except Exception as e:
                self._logger.debug(
                    f""Parsing failed on response: '''{_assistant_msg}'''""
                )
                self._logger.warning(f""Parsing attempt #{attempts} failed: {e}"")
                sentry_sdk.capture_exception(
                    error=e,
                    extras={""assistant_msg"": _assistant_msg, ""i_attempt"": attempts},
                )
                if attempts < self._configuration.fix_failed_parse_tries:
                    anthropic_messages.append(
                        _assistant_msg.model_dump(include={""role"", ""content""})  # type: ignore # noqa
                    )
                    anthropic_messages.append(
                        {
                            ""role"": ""user"",
                            ""content"": [
                                *(
                                    # tool_result is required if last assistant message
                                    # had tool_use block(s)
                                    {
                                        ""type"": ""tool_result"",
                                        ""tool_use_id"": tc.id,
                                        ""is_error"": True,
                                        ""content"": [
                                            {
                                                ""type"": ""text"",
                                                ""text"": ""Not executed because parsing ""
                                                ""of your last message failed""
                                                if not tool_call_errors
                                                else str(e)
                                                if (
                                                    e := next(
                                                        (
                                                            tce
                                                            for tce in tool_call_errors
                                                            if tce.name
                                                            == tc.function.name
                                                        ),
                                                        None,
                                                    )
                                                )
                                                else ""Not executed because validation ""
                                                ""of tool input failed"",
                                            }
                                        ],
                                    }
                                    for tc in assistant_msg.tool_calls or []
                                ),
                                {
                                    ""type"": ""text"",
                                    ""text"": (
                                        ""ERROR PARSING YOUR RESPONSE:\n\n""
                                        f""{e.__class__.__name__}: {e}""
                                    ),
                                },
                            ],
                        }
                    )
                else:
                    raise

        if attempts > 1:
            self._logger.debug(
                f""Total cost for {attempts} attempts: ${round(total_cost, 5)}""
            )

        return ChatModelResponse(
            response=assistant_msg,
            parsed_result=parsed_result,
            llm_info=ANTHROPIC_CHAT_MODELS[model_name],
            prompt_tokens_used=t_input,
            completion_tokens_used=t_output,
        )

    def _get_chat_completion_args(
        self,
        prompt_messages: list[ChatMessage],
        functions: Optional[list[CompletionModelFunction]] = None,
        max_output_tokens: Optional[int] = None,
        **kwargs,
    ) -> tuple[list[MessageParam], MessageCreateParams]:
        """"""Prepare arguments for message completion API call.

        Args:
            prompt_messages: List of ChatMessages.
            functions: Optional list of functions available to the LLM.
            kwargs: Additional keyword arguments.

        Returns:
            list[MessageParam]: Prompt messages for the Anthropic call
            dict[str, Any]: Any other kwargs for the Anthropic call
        """"""
        if functions:
            kwargs[""tools""] = [
                {
                    ""name"": f.name,
                    ""description"": f.description,
                    ""input_schema"": {
                        ""type"": ""object"",
                        ""properties"": {
                            name: param.to_dict()
                            for name, param in f.parameters.items()
                        },
                        ""required"": [
                            name
                            for name, param in f.parameters.items()
                            if param.required
                        ],
                    },
                }
                for f in functions
            ]

        kwargs[""max_tokens""] = max_output_tokens or 4096

        if extra_headers := self._configuration.extra_request_headers:
            kwargs[""extra_headers""] = kwargs.get(""extra_headers"", {})
            kwargs[""extra_headers""].update(extra_headers.copy())

        system_messages = [
            m for m in prompt_messages if m.role == ChatMessage.Role.SYSTEM
        ]
        if (_n := len(system_messages)) > 1:
            self._logger.warning(
                f""Prompt has {_n} system messages; Anthropic supports only 1. ""
                ""They will be merged, and removed from the rest of the prompt.""
            )
        kwargs[""system""] = ""\n\n"".join(sm.content for sm in system_messages)

        messages: list[MessageParam] = []
        for message in prompt_messages:
            if message.role == ChatMessage.Role.SYSTEM:
                continue
            elif message.role == ChatMessage.Role.USER:
                # Merge subsequent user messages
                if messages and (prev_msg := messages[-1])[""role""] == ""user"":
                    if isinstance(prev_msg[""content""], str):
                        prev_msg[""content""] += f""\n\n{message.content}""
                    else:
                        assert isinstance(prev_msg[""content""], list)
                        prev_msg[""content""].append(
                            {""type"": ""text"", ""text"": message.content}
                        )
                else:
                    messages.append({""role"": ""user"", ""content"": message.content})
                # TODO: add support for image blocks
            elif message.role == ChatMessage.Role.ASSISTANT:
                if isinstance(message, AssistantChatMessage) and message.tool_calls:
                    messages.append(
                        {
                            ""role"": ""assistant"",
                            ""content"": [
                                *(
                                    [{""type"": ""text"", ""text"": message.content}]
                                    if message.content
                                    else []
                                ),
                                *(
                                    {
                                        ""type"": ""tool_use"",
                                        ""id"": tc.id,
                                        ""name"": tc.function.name,
                                        ""input"": tc.function.arguments,
                                    }
                                    for tc in message.tool_calls
                                ),
                            ],
                        }
                    )
                elif message.content:
                    messages.append(
                        {
                            ""role"": ""assistant"",
                            ""content"": message.content,
                        }
                    )
            elif isinstance(message, ToolResultMessage):
                messages.append(
                    {
                        ""role"": ""user"",
                        ""content"": [
                            {
                                ""type"": ""tool_result"",
                                ""tool_use_id"": message.tool_call_id,
                                ""content"": [{""type"": ""text"", ""text"": message.content}],
                                ""is_error"": message.is_error,
                            }
                        ],
                    }
                )

        return messages, kwargs  # type: ignore

    async def _create_chat_completion(
        self, model: AnthropicModelName, completion_kwargs: MessageCreateParams
    ) -> tuple[Message, float, int, int]:
        """"""
        Create a chat completion using the Anthropic API with retry handling.

        Params:
            completion_kwargs: Keyword arguments for an Anthropic Messages API call

        Returns:
            Message: The message completion object
            float: The cost ($) of this completion
            int: Number of input tokens used
            int: Number of output tokens used
        """"""

        @self._retry_api_request
        async def _create_chat_completion_with_retry() -> Message:
            return await self._client.beta.tools.messages.create(
                model=model, **completion_kwargs  # type: ignore
            )

        response = await _create_chat_completion_with_retry()

        cost = self._budget.update_usage_and_cost(
            model_info=ANTHROPIC_CHAT_MODELS[model],
            input_tokens_used=response.usage.input_tokens,
            output_tokens_used=response.usage.output_tokens,
        )
        return response, cost, response.usage.input_tokens, response.usage.output_tokens

    def _parse_assistant_tool_calls(
        self, assistant_message: Message
    ) -> list[AssistantToolCall]:
        return [
            AssistantToolCall(
                id=c.id,
                type=""function"",
                function=AssistantFunctionCall(
                    name=c.name,
                    arguments=c.input,  # type: ignore
                ),
            )
            for c in assistant_message.content
            if c.type == ""tool_use""
        ]

    def _retry_api_request(self, func: Callable[_P, _T]) -> Callable[_P, _T]:
        return tenacity.retry(
            retry=(
                tenacity.retry_if_exception_type(APIConnectionError)
                | tenacity.retry_if_exception(
                    lambda e: isinstance(e, APIStatusError) and e.status_code >= 500
                )
            ),
            wait=tenacity.wait_exponential(),
            stop=tenacity.stop_after_attempt(self._configuration.retries_per_request),
            after=tenacity.after_log(self._logger, logging.DEBUG),
        )(func)

    def __repr__(self):
        return ""AnthropicProvider()""","Point(row=112, column=0)","Point(row=496, column=36)",,classic/forge/forge/llm/providers/anthropic.py
AnthropicProvider.__init__,function,,"def __init__(
        self,
        settings: Optional[AnthropicSettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        if not settings:
            settings = self.default_settings.model_copy(deep=True)
        if not settings.credentials:
            settings.credentials = AnthropicCredentials.from_env()

        super(AnthropicProvider, self).__init__(settings=settings, logger=logger)

        from anthropic import AsyncAnthropic

        self._client = AsyncAnthropic(
            **self._credentials.get_api_access_kwargs()  # type: ignore
        )","Point(row=125, column=4)","Point(row=141, column=9)",AnthropicProvider,classic/forge/forge/llm/providers/anthropic.py
AnthropicProvider.get_available_models,function,,"async def get_available_models(self) -> Sequence[ChatModelInfo[AnthropicModelName]]:
        return await self.get_available_chat_models()","Point(row=143, column=4)","Point(row=144, column=53)",AnthropicProvider,classic/forge/forge/llm/providers/anthropic.py
AnthropicProvider.get_available_chat_models,function,,"async def get_available_chat_models(
        self,
    ) -> Sequence[ChatModelInfo[AnthropicModelName]]:
        return list(ANTHROPIC_CHAT_MODELS.values())","Point(row=146, column=4)","Point(row=149, column=51)",AnthropicProvider,classic/forge/forge/llm/providers/anthropic.py
AnthropicProvider.get_token_limit,function,Get the token limit for a given model.,"def get_token_limit(self, model_name: AnthropicModelName) -> int:
        """"""Get the token limit for a given model.""""""
        return ANTHROPIC_CHAT_MODELS[model_name].max_tokens","Point(row=151, column=4)","Point(row=153, column=59)",AnthropicProvider,classic/forge/forge/llm/providers/anthropic.py
AnthropicProvider.get_tokenizer,function,,"def get_tokenizer(self, model_name: AnthropicModelName) -> ModelTokenizer[Any]:
        # HACK: No official tokenizer is available for Claude 3
        return tiktoken.encoding_for_model(model_name)","Point(row=155, column=4)","Point(row=157, column=54)",AnthropicProvider,classic/forge/forge/llm/providers/anthropic.py
AnthropicProvider.count_tokens,function,,"def count_tokens(self, text: str, model_name: AnthropicModelName) -> int:
        return 0  # HACK: No official tokenizer is available for Claude 3","Point(row=159, column=4)","Point(row=160, column=73)",AnthropicProvider,classic/forge/forge/llm/providers/anthropic.py
AnthropicProvider.count_message_tokens,function,,"def count_message_tokens(
        self,
        messages: ChatMessage | list[ChatMessage],
        model_name: AnthropicModelName,
    ) -> int:
        return 0  # HACK: No official tokenizer is available for Claude 3","Point(row=162, column=4)","Point(row=167, column=73)",AnthropicProvider,classic/forge/forge/llm/providers/anthropic.py
AnthropicProvider.create_chat_completion,function,Create a completion using the Anthropic API.,"async def create_chat_completion(
        self,
        model_prompt: list[ChatMessage],
        model_name: AnthropicModelName,
        completion_parser: Callable[[AssistantChatMessage], _T] = lambda _: None,
        functions: Optional[list[CompletionModelFunction]] = None,
        max_output_tokens: Optional[int] = None,
        prefill_response: str = """",
        **kwargs,
    ) -> ChatModelResponse[_T]:
        """"""Create a completion using the Anthropic API.""""""
        anthropic_messages, completion_kwargs = self._get_chat_completion_args(
            prompt_messages=model_prompt,
            model=model_name,
            functions=functions,
            max_output_tokens=max_output_tokens,
            **kwargs,
        )

        total_cost = 0.0
        attempts = 0
        while True:
            completion_kwargs[""messages""] = anthropic_messages.copy()
            if prefill_response:
                completion_kwargs[""messages""].append(
                    {""role"": ""assistant"", ""content"": prefill_response}
                )

            (
                _assistant_msg,
                cost,
                t_input,
                t_output,
            ) = await self._create_chat_completion(model_name, completion_kwargs)
            total_cost += cost
            self._logger.debug(
                f""Completion usage: {t_input} input, {t_output} output ""
                f""- ${round(cost, 5)}""
            )

            # Merge prefill into generated response
            if prefill_response:
                first_text_block = next(
                    b for b in _assistant_msg.content if b.type == ""text""
                )
                first_text_block.text = prefill_response + first_text_block.text

            assistant_msg = AssistantChatMessage(
                content=""\n\n"".join(
                    b.text for b in _assistant_msg.content if b.type == ""text""
                ),
                tool_calls=self._parse_assistant_tool_calls(_assistant_msg),
            )

            # If parsing the response fails, append the error to the prompt, and let the
            # LLM fix its mistake(s).
            attempts += 1
            tool_call_errors = []
            try:
                # Validate tool calls
                if assistant_msg.tool_calls and functions:
                    tool_call_errors = validate_tool_calls(
                        assistant_msg.tool_calls, functions
                    )
                    if tool_call_errors:
                        raise ValueError(
                            ""Invalid tool use(s):\n""
                            + ""\n"".join(str(e) for e in tool_call_errors)
                        )

                parsed_result = completion_parser(assistant_msg)
                break
            except Exception as e:
                self._logger.debug(
                    f""Parsing failed on response: '''{_assistant_msg}'''""
                )
                self._logger.warning(f""Parsing attempt #{attempts} failed: {e}"")
                sentry_sdk.capture_exception(
                    error=e,
                    extras={""assistant_msg"": _assistant_msg, ""i_attempt"": attempts},
                )
                if attempts < self._configuration.fix_failed_parse_tries:
                    anthropic_messages.append(
                        _assistant_msg.model_dump(include={""role"", ""content""})  # type: ignore # noqa
                    )
                    anthropic_messages.append(
                        {
                            ""role"": ""user"",
                            ""content"": [
                                *(
                                    # tool_result is required if last assistant message
                                    # had tool_use block(s)
                                    {
                                        ""type"": ""tool_result"",
                                        ""tool_use_id"": tc.id,
                                        ""is_error"": True,
                                        ""content"": [
                                            {
                                                ""type"": ""text"",
                                                ""text"": ""Not executed because parsing ""
                                                ""of your last message failed""
                                                if not tool_call_errors
                                                else str(e)
                                                if (
                                                    e := next(
                                                        (
                                                            tce
                                                            for tce in tool_call_errors
                                                            if tce.name
                                                            == tc.function.name
                                                        ),
                                                        None,
                                                    )
                                                )
                                                else ""Not executed because validation ""
                                                ""of tool input failed"",
                                            }
                                        ],
                                    }
                                    for tc in assistant_msg.tool_calls or []
                                ),
                                {
                                    ""type"": ""text"",
                                    ""text"": (
                                        ""ERROR PARSING YOUR RESPONSE:\n\n""
                                        f""{e.__class__.__name__}: {e}""
                                    ),
                                },
                            ],
                        }
                    )
                else:
                    raise

        if attempts > 1:
            self._logger.debug(
                f""Total cost for {attempts} attempts: ${round(total_cost, 5)}""
            )

        return ChatModelResponse(
            response=assistant_msg,
            parsed_result=parsed_result,
            llm_info=ANTHROPIC_CHAT_MODELS[model_name],
            prompt_tokens_used=t_input,
            completion_tokens_used=t_output,
        )","Point(row=169, column=4)","Point(row=314, column=9)",AnthropicProvider,classic/forge/forge/llm/providers/anthropic.py
AnthropicProvider._get_chat_completion_args,function,"Prepare arguments for message completion API call.

        Args:
            prompt_messages: List of ChatMessages.
            functions: Optional list of functions available to the LLM.
            kwargs: Additional keyword arguments.

        Returns:
            list[MessageParam]: Prompt messages for the Anthropic call
            dict[str, Any]: Any other kwargs for the Anthropic call
","def _get_chat_completion_args(
        self,
        prompt_messages: list[ChatMessage],
        functions: Optional[list[CompletionModelFunction]] = None,
        max_output_tokens: Optional[int] = None,
        **kwargs,
    ) -> tuple[list[MessageParam], MessageCreateParams]:
        """"""Prepare arguments for message completion API call.

        Args:
            prompt_messages: List of ChatMessages.
            functions: Optional list of functions available to the LLM.
            kwargs: Additional keyword arguments.

        Returns:
            list[MessageParam]: Prompt messages for the Anthropic call
            dict[str, Any]: Any other kwargs for the Anthropic call
        """"""
        if functions:
            kwargs[""tools""] = [
                {
                    ""name"": f.name,
                    ""description"": f.description,
                    ""input_schema"": {
                        ""type"": ""object"",
                        ""properties"": {
                            name: param.to_dict()
                            for name, param in f.parameters.items()
                        },
                        ""required"": [
                            name
                            for name, param in f.parameters.items()
                            if param.required
                        ],
                    },
                }
                for f in functions
            ]

        kwargs[""max_tokens""] = max_output_tokens or 4096

        if extra_headers := self._configuration.extra_request_headers:
            kwargs[""extra_headers""] = kwargs.get(""extra_headers"", {})
            kwargs[""extra_headers""].update(extra_headers.copy())

        system_messages = [
            m for m in prompt_messages if m.role == ChatMessage.Role.SYSTEM
        ]
        if (_n := len(system_messages)) > 1:
            self._logger.warning(
                f""Prompt has {_n} system messages; Anthropic supports only 1. ""
                ""They will be merged, and removed from the rest of the prompt.""
            )
        kwargs[""system""] = ""\n\n"".join(sm.content for sm in system_messages)

        messages: list[MessageParam] = []
        for message in prompt_messages:
            if message.role == ChatMessage.Role.SYSTEM:
                continue
            elif message.role == ChatMessage.Role.USER:
                # Merge subsequent user messages
                if messages and (prev_msg := messages[-1])[""role""] == ""user"":
                    if isinstance(prev_msg[""content""], str):
                        prev_msg[""content""] += f""\n\n{message.content}""
                    else:
                        assert isinstance(prev_msg[""content""], list)
                        prev_msg[""content""].append(
                            {""type"": ""text"", ""text"": message.content}
                        )
                else:
                    messages.append({""role"": ""user"", ""content"": message.content})
                # TODO: add support for image blocks
            elif message.role == ChatMessage.Role.ASSISTANT:
                if isinstance(message, AssistantChatMessage) and message.tool_calls:
                    messages.append(
                        {
                            ""role"": ""assistant"",
                            ""content"": [
                                *(
                                    [{""type"": ""text"", ""text"": message.content}]
                                    if message.content
                                    else []
                                ),
                                *(
                                    {
                                        ""type"": ""tool_use"",
                                        ""id"": tc.id,
                                        ""name"": tc.function.name,
                                        ""input"": tc.function.arguments,
                                    }
                                    for tc in message.tool_calls
                                ),
                            ],
                        }
                    )
                elif message.content:
                    messages.append(
                        {
                            ""role"": ""assistant"",
                            ""content"": message.content,
                        }
                    )
            elif isinstance(message, ToolResultMessage):
                messages.append(
                    {
                        ""role"": ""user"",
                        ""content"": [
                            {
                                ""type"": ""tool_result"",
                                ""tool_use_id"": message.tool_call_id,
                                ""content"": [{""type"": ""text"", ""text"": message.content}],
                                ""is_error"": message.is_error,
                            }
                        ],
                    }
                )

        return messages, kwargs  # type: ignore","Point(row=316, column=4)","Point(row=433, column=47)",AnthropicProvider,classic/forge/forge/llm/providers/anthropic.py
AnthropicProvider._create_chat_completion,function,"
        Create a chat completion using the Anthropic API with retry handling.

        Params:
            completion_kwargs: Keyword arguments for an Anthropic Messages API call

        Returns:
            Message: The message completion object
            float: The cost ($) of this completion
            int: Number of input tokens used
            int: Number of output tokens used
","async def _create_chat_completion(
        self, model: AnthropicModelName, completion_kwargs: MessageCreateParams
    ) -> tuple[Message, float, int, int]:
        """"""
        Create a chat completion using the Anthropic API with retry handling.

        Params:
            completion_kwargs: Keyword arguments for an Anthropic Messages API call

        Returns:
            Message: The message completion object
            float: The cost ($) of this completion
            int: Number of input tokens used
            int: Number of output tokens used
        """"""

        @self._retry_api_request
        async def _create_chat_completion_with_retry() -> Message:
            return await self._client.beta.tools.messages.create(
                model=model, **completion_kwargs  # type: ignore
            )

        response = await _create_chat_completion_with_retry()

        cost = self._budget.update_usage_and_cost(
            model_info=ANTHROPIC_CHAT_MODELS[model],
            input_tokens_used=response.usage.input_tokens,
            output_tokens_used=response.usage.output_tokens,
        )
        return response, cost, response.usage.input_tokens, response.usage.output_tokens","Point(row=435, column=4)","Point(row=464, column=88)",AnthropicProvider,classic/forge/forge/llm/providers/anthropic.py
AnthropicProvider._create_chat_completion._create_chat_completion_with_retry,function,,"async def _create_chat_completion_with_retry() -> Message:
            return await self._client.beta.tools.messages.create(
                model=model, **completion_kwargs  # type: ignore
            )","Point(row=452, column=8)","Point(row=455, column=13)",AnthropicProvider,classic/forge/forge/llm/providers/anthropic.py
AnthropicProvider._parse_assistant_tool_calls,function,,"def _parse_assistant_tool_calls(
        self, assistant_message: Message
    ) -> list[AssistantToolCall]:
        return [
            AssistantToolCall(
                id=c.id,
                type=""function"",
                function=AssistantFunctionCall(
                    name=c.name,
                    arguments=c.input,  # type: ignore
                ),
            )
            for c in assistant_message.content
            if c.type == ""tool_use""
        ]","Point(row=466, column=4)","Point(row=480, column=9)",AnthropicProvider,classic/forge/forge/llm/providers/anthropic.py
AnthropicProvider._retry_api_request,function,,"def _retry_api_request(self, func: Callable[_P, _T]) -> Callable[_P, _T]:
        return tenacity.retry(
            retry=(
                tenacity.retry_if_exception_type(APIConnectionError)
                | tenacity.retry_if_exception(
                    lambda e: isinstance(e, APIStatusError) and e.status_code >= 500
                )
            ),
            wait=tenacity.wait_exponential(),
            stop=tenacity.stop_after_attempt(self._configuration.retries_per_request),
            after=tenacity.after_log(self._logger, logging.DEBUG),
        )(func)","Point(row=482, column=4)","Point(row=493, column=15)",AnthropicProvider,classic/forge/forge/llm/providers/anthropic.py
AnthropicProvider.__repr__,function,,"def __repr__(self):
        return ""AnthropicProvider()""","Point(row=495, column=4)","Point(row=496, column=36)",AnthropicProvider,classic/forge/forge/llm/providers/anthropic.py
ModelProviderService,class,A ModelService describes what kind of service the model provides.,"class ModelProviderService(str, enum.Enum):
    """"""A ModelService describes what kind of service the model provides.""""""

    EMBEDDING = ""embedding""
    CHAT = ""chat_completion""
    TEXT = ""text_completion""","Point(row=45, column=0)","Point(row=50, column=28)",,classic/forge/forge/llm/providers/schema.py
ModelProviderName,class,,"class ModelProviderName(str, enum.Enum):
    OPENAI = ""openai""
    ANTHROPIC = ""anthropic""
    GROQ = ""groq""
    LLAMAFILE = ""llamafile""","Point(row=53, column=0)","Point(row=57, column=27)",,classic/forge/forge/llm/providers/schema.py
ChatMessage,class,,"class ChatMessage(BaseModel):
    class Role(str, enum.Enum):
        USER = ""user""
        SYSTEM = ""system""
        ASSISTANT = ""assistant""

        TOOL = ""tool""
        """"""May be used for the result of tool calls""""""
        FUNCTION = ""function""
        """"""May be used for the return value of function calls""""""

    role: Role
    content: str

    @staticmethod
    def user(content: str) -> ""ChatMessage"":
        return ChatMessage(role=ChatMessage.Role.USER, content=content)

    @staticmethod
    def system(content: str) -> ""ChatMessage"":
        return ChatMessage(role=ChatMessage.Role.SYSTEM, content=content)","Point(row=60, column=0)","Point(row=80, column=73)",,classic/forge/forge/llm/providers/schema.py
Role,class,,"class Role(str, enum.Enum):
        USER = ""user""
        SYSTEM = ""system""
        ASSISTANT = ""assistant""

        TOOL = ""tool""
        """"""May be used for the result of tool calls""""""
        FUNCTION = ""function""
        """"""May be used for the return value of function calls""""""","Point(row=61, column=4)","Point(row=69, column=64)",,classic/forge/forge/llm/providers/schema.py
ChatMessage.user,function,,"def user(content: str) -> ""ChatMessage"":
        return ChatMessage(role=ChatMessage.Role.USER, content=content)","Point(row=75, column=4)","Point(row=76, column=71)",ChatMessage,classic/forge/forge/llm/providers/schema.py
ChatMessage.system,function,,"def system(content: str) -> ""ChatMessage"":
        return ChatMessage(role=ChatMessage.Role.SYSTEM, content=content)","Point(row=79, column=4)","Point(row=80, column=73)",ChatMessage,classic/forge/forge/llm/providers/schema.py
ChatMessageDict,class,,"class ChatMessageDict(TypedDict):
    role: str
    content: str","Point(row=83, column=0)","Point(row=85, column=16)",,classic/forge/forge/llm/providers/schema.py
AssistantFunctionCall,class,,"class AssistantFunctionCall(BaseModel):
    name: str
    arguments: dict[str, Any]

    def __str__(self) -> str:
        return f""{self.name}({fmt_kwargs(self.arguments)})""","Point(row=88, column=0)","Point(row=93, column=59)",,classic/forge/forge/llm/providers/schema.py
AssistantFunctionCall.__str__,function,,"def __str__(self) -> str:
        return f""{self.name}({fmt_kwargs(self.arguments)})""","Point(row=92, column=4)","Point(row=93, column=59)",AssistantFunctionCall,classic/forge/forge/llm/providers/schema.py
AssistantFunctionCallDict,class,,"class AssistantFunctionCallDict(TypedDict):
    name: str
    arguments: dict[str, Any]","Point(row=96, column=0)","Point(row=98, column=29)",,classic/forge/forge/llm/providers/schema.py
AssistantToolCall,class,,"class AssistantToolCall(BaseModel):
    id: str
    type: Literal[""function""]
    function: AssistantFunctionCall","Point(row=101, column=0)","Point(row=104, column=35)",,classic/forge/forge/llm/providers/schema.py
AssistantToolCallDict,class,,"class AssistantToolCallDict(TypedDict):
    id: str
    type: Literal[""function""]
    function: AssistantFunctionCallDict","Point(row=107, column=0)","Point(row=110, column=39)",,classic/forge/forge/llm/providers/schema.py
AssistantChatMessage,class,,"class AssistantChatMessage(ChatMessage):
    role: Literal[ChatMessage.Role.ASSISTANT] = ChatMessage.Role.ASSISTANT  # type: ignore # noqa
    content: str = """"
    tool_calls: Optional[list[AssistantToolCall]] = None","Point(row=113, column=0)","Point(row=116, column=56)",,classic/forge/forge/llm/providers/schema.py
ToolResultMessage,class,,"class ToolResultMessage(ChatMessage):
    role: Literal[ChatMessage.Role.TOOL] = ChatMessage.Role.TOOL  # type: ignore
    is_error: bool = False
    tool_call_id: str","Point(row=119, column=0)","Point(row=122, column=21)",,classic/forge/forge/llm/providers/schema.py
AssistantChatMessageDict,class,,"class AssistantChatMessageDict(TypedDict, total=False):
    role: str
    content: str
    tool_calls: list[AssistantToolCallDict]","Point(row=125, column=0)","Point(row=128, column=43)",,classic/forge/forge/llm/providers/schema.py
CompletionModelFunction,class,General representation object for LLM-callable functions.,"class CompletionModelFunction(BaseModel):
    """"""General representation object for LLM-callable functions.""""""

    name: str
    description: str
    parameters: dict[str, ""JSONSchema""]

    def fmt_line(self) -> str:
        params = "", "".join(
            f""{name}{'?' if not p.required else ''}: "" f""{p.typescript_type}""
            for name, p in self.parameters.items()
        )
        return f""{self.name}: {self.description}. Params: ({params})""

    def validate_call(
        self, function_call: AssistantFunctionCall
    ) -> tuple[bool, list[""ValidationError""]]:
        """"""
        Validates the given function call against the function's parameter specs

        Returns:
            bool: Whether the given set of arguments is valid for this command
            list[ValidationError]: Issues with the set of arguments (if any)

        Raises:
            ValueError: If the function_call doesn't call this function
        """"""
        if function_call.name != self.name:
            raise ValueError(
                f""Can't validate {function_call.name} call using {self.name} spec""
            )

        params_schema = JSONSchema(
            type=JSONSchema.Type.OBJECT,
            properties={name: spec for name, spec in self.parameters.items()},
        )
        return params_schema.validate_object(function_call.arguments)","Point(row=131, column=0)","Point(row=167, column=69)",,classic/forge/forge/llm/providers/schema.py
CompletionModelFunction.fmt_line,function,,"def fmt_line(self) -> str:
        params = "", "".join(
            f""{name}{'?' if not p.required else ''}: "" f""{p.typescript_type}""
            for name, p in self.parameters.items()
        )
        return f""{self.name}: {self.description}. Params: ({params})""","Point(row=138, column=4)","Point(row=143, column=69)",CompletionModelFunction,classic/forge/forge/llm/providers/schema.py
CompletionModelFunction.validate_call,function,"
        Validates the given function call against the function's parameter specs

        Returns:
            bool: Whether the given set of arguments is valid for this command
            list[ValidationError]: Issues with the set of arguments (if any)

        Raises:
            ValueError: If the function_call doesn't call this function
","def validate_call(
        self, function_call: AssistantFunctionCall
    ) -> tuple[bool, list[""ValidationError""]]:
        """"""
        Validates the given function call against the function's parameter specs

        Returns:
            bool: Whether the given set of arguments is valid for this command
            list[ValidationError]: Issues with the set of arguments (if any)

        Raises:
            ValueError: If the function_call doesn't call this function
        """"""
        if function_call.name != self.name:
            raise ValueError(
                f""Can't validate {function_call.name} call using {self.name} spec""
            )

        params_schema = JSONSchema(
            type=JSONSchema.Type.OBJECT,
            properties={name: spec for name, spec in self.parameters.items()},
        )
        return params_schema.validate_object(function_call.arguments)","Point(row=145, column=4)","Point(row=167, column=69)",CompletionModelFunction,classic/forge/forge/llm/providers/schema.py
ModelInfo,class,"Struct for model information.

    Would be lovely to eventually get this directly from APIs, but needs to be
    scraped from websites for now.
","class ModelInfo(BaseModel, Generic[_ModelName]):
    """"""Struct for model information.

    Would be lovely to eventually get this directly from APIs, but needs to be
    scraped from websites for now.
    """"""

    name: _ModelName
    service: ClassVar[ModelProviderService]
    provider_name: ModelProviderName
    prompt_token_cost: float = 0.0
    completion_token_cost: float = 0.0","Point(row=170, column=0)","Point(row=181, column=38)",,classic/forge/forge/llm/providers/schema.py
ModelResponse,class,Standard response struct for a response from a model.,"class ModelResponse(BaseModel):
    """"""Standard response struct for a response from a model.""""""

    prompt_tokens_used: int
    completion_tokens_used: int
    llm_info: ModelInfo","Point(row=184, column=0)","Point(row=189, column=23)",,classic/forge/forge/llm/providers/schema.py
ModelProviderConfiguration,class,,"class ModelProviderConfiguration(SystemConfiguration):
    retries_per_request: int = UserConfigurable(7)
    fix_failed_parse_tries: int = UserConfigurable(3)
    extra_request_headers: dict[str, str] = Field(default_factory=dict)","Point(row=192, column=0)","Point(row=195, column=71)",,classic/forge/forge/llm/providers/schema.py
ModelProviderCredentials,class,Credentials for a model provider.,"class ModelProviderCredentials(ProviderCredentials):
    """"""Credentials for a model provider.""""""

    api_key: SecretStr | None = UserConfigurable(default=None)
    api_type: SecretStr | None = UserConfigurable(default=None)
    api_base: SecretStr | None = UserConfigurable(default=None)
    api_version: SecretStr | None = UserConfigurable(default=None)
    deployment_id: SecretStr | None = UserConfigurable(default=None)

    model_config = ConfigDict(extra=""ignore"")","Point(row=198, column=0)","Point(row=207, column=45)",,classic/forge/forge/llm/providers/schema.py
ModelProviderUsage,class,Usage for a particular model from a model provider.,"class ModelProviderUsage(BaseModel):
    """"""Usage for a particular model from a model provider.""""""

    class ModelUsage(BaseModel):
        completion_tokens: int = 0
        prompt_tokens: int = 0

    usage_per_model: dict[str, ModelUsage] = defaultdict(ModelUsage)

    @property
    def completion_tokens(self) -> int:
        return sum(model.completion_tokens for model in self.usage_per_model.values())

    @property
    def prompt_tokens(self) -> int:
        return sum(model.prompt_tokens for model in self.usage_per_model.values())

    def update_usage(
        self,
        model: str,
        input_tokens_used: int,
        output_tokens_used: int = 0,
    ) -> None:
        self.usage_per_model[model].prompt_tokens += input_tokens_used
        self.usage_per_model[model].completion_tokens += output_tokens_used","Point(row=210, column=0)","Point(row=234, column=75)",,classic/forge/forge/llm/providers/schema.py
ModelUsage,class,,"class ModelUsage(BaseModel):
        completion_tokens: int = 0
        prompt_tokens: int = 0","Point(row=213, column=4)","Point(row=215, column=30)",,classic/forge/forge/llm/providers/schema.py
ModelProviderUsage.completion_tokens,function,,"def completion_tokens(self) -> int:
        return sum(model.completion_tokens for model in self.usage_per_model.values())","Point(row=220, column=4)","Point(row=221, column=86)",ModelProviderUsage,classic/forge/forge/llm/providers/schema.py
ModelProviderUsage.prompt_tokens,function,,"def prompt_tokens(self) -> int:
        return sum(model.prompt_tokens for model in self.usage_per_model.values())","Point(row=224, column=4)","Point(row=225, column=82)",ModelProviderUsage,classic/forge/forge/llm/providers/schema.py
ModelProviderUsage.update_usage,function,,"def update_usage(
        self,
        model: str,
        input_tokens_used: int,
        output_tokens_used: int = 0,
    ) -> None:
        self.usage_per_model[model].prompt_tokens += input_tokens_used
        self.usage_per_model[model].completion_tokens += output_tokens_used","Point(row=227, column=4)","Point(row=234, column=75)",ModelProviderUsage,classic/forge/forge/llm/providers/schema.py
ModelProviderBudget,class,,"class ModelProviderBudget(ProviderBudget[ModelProviderUsage]):
    usage: ModelProviderUsage = Field(default_factory=ModelProviderUsage)

    def update_usage_and_cost(
        self,
        model_info: ModelInfo,
        input_tokens_used: int,
        output_tokens_used: int = 0,
    ) -> float:
        """"""Update the usage and cost of the provider.

        Returns:
            float: The (calculated) cost of the given model response.
        """"""
        self.usage.update_usage(model_info.name, input_tokens_used, output_tokens_used)
        incurred_cost = (
            output_tokens_used * model_info.completion_token_cost
            + input_tokens_used * model_info.prompt_token_cost
        )
        self.total_cost += incurred_cost
        self.remaining_budget -= incurred_cost
        return incurred_cost","Point(row=237, column=0)","Point(row=258, column=28)",,classic/forge/forge/llm/providers/schema.py
ModelProviderBudget.update_usage_and_cost,function,"Update the usage and cost of the provider.

        Returns:
            float: The (calculated) cost of the given model response.
","def update_usage_and_cost(
        self,
        model_info: ModelInfo,
        input_tokens_used: int,
        output_tokens_used: int = 0,
    ) -> float:
        """"""Update the usage and cost of the provider.

        Returns:
            float: The (calculated) cost of the given model response.
        """"""
        self.usage.update_usage(model_info.name, input_tokens_used, output_tokens_used)
        incurred_cost = (
            output_tokens_used * model_info.completion_token_cost
            + input_tokens_used * model_info.prompt_token_cost
        )
        self.total_cost += incurred_cost
        self.remaining_budget -= incurred_cost
        return incurred_cost","Point(row=240, column=4)","Point(row=258, column=28)",ModelProviderBudget,classic/forge/forge/llm/providers/schema.py
ModelProviderSettings,class,,"class ModelProviderSettings(SystemSettings):
    resource_type: ClassVar[ResourceType] = ResourceType.MODEL
    configuration: ModelProviderConfiguration
    credentials: Optional[ModelProviderCredentials] = None
    budget: Optional[ModelProviderBudget] = None","Point(row=261, column=0)","Point(row=265, column=48)",,classic/forge/forge/llm/providers/schema.py
BaseModelProvider,class,A ModelProvider abstracts the details of a particular provider of models.,"class BaseModelProvider(
    abc.ABC,
    Generic[_ModelName, _ModelProviderSettings],
    Configurable[_ModelProviderSettings],
):
    """"""A ModelProvider abstracts the details of a particular provider of models.""""""

    default_settings: ClassVar[_ModelProviderSettings]  # type: ignore

    _settings: _ModelProviderSettings
    _logger: logging.Logger

    def __init__(
        self,
        settings: Optional[_ModelProviderSettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        if not settings:
            settings = self.default_settings.model_copy(deep=True)

        self._settings = settings
        self._configuration = settings.configuration
        self._credentials = settings.credentials
        self._budget = settings.budget

        self._logger = logger or logging.getLogger(self.__module__)

    @abc.abstractmethod
    async def get_available_models(
        self,
    ) -> Sequence[""ChatModelInfo[_ModelName] | EmbeddingModelInfo[_ModelName]""]:
        ...

    @abc.abstractmethod
    def count_tokens(self, text: str, model_name: _ModelName) -> int:
        ...

    @abc.abstractmethod
    def get_tokenizer(self, model_name: _ModelName) -> ""ModelTokenizer[Any]"":
        ...

    @abc.abstractmethod
    def get_token_limit(self, model_name: _ModelName) -> int:
        ...

    def get_incurred_cost(self) -> float:
        if self._budget:
            return self._budget.total_cost
        return 0

    def get_remaining_budget(self) -> float:
        if self._budget:
            return self._budget.remaining_budget
        return math.inf","Point(row=273, column=0)","Point(row=326, column=23)",,classic/forge/forge/llm/providers/schema.py
BaseModelProvider.__init__,function,,"def __init__(
        self,
        settings: Optional[_ModelProviderSettings] = None,
        logger: Optional[logging.Logger] = None,
    ):
        if not settings:
            settings = self.default_settings.model_copy(deep=True)

        self._settings = settings
        self._configuration = settings.configuration
        self._credentials = settings.credentials
        self._budget = settings.budget

        self._logger = logger or logging.getLogger(self.__module__)","Point(row=285, column=4)","Point(row=298, column=67)",BaseModelProvider,classic/forge/forge/llm/providers/schema.py
BaseModelProvider.get_available_models,function,,"async def get_available_models(
        self,
    ) -> Sequence[""ChatModelInfo[_ModelName] | EmbeddingModelInfo[_ModelName]""]:
        ...","Point(row=301, column=4)","Point(row=304, column=11)",BaseModelProvider,classic/forge/forge/llm/providers/schema.py
BaseModelProvider.count_tokens,function,,"def count_tokens(self, text: str, model_name: _ModelName) -> int:
        ...","Point(row=307, column=4)","Point(row=308, column=11)",BaseModelProvider,classic/forge/forge/llm/providers/schema.py
BaseModelProvider.get_tokenizer,function,,"def get_tokenizer(self, model_name: _ModelName) -> ""ModelTokenizer[Any]"":
        ...","Point(row=311, column=4)","Point(row=312, column=11)",BaseModelProvider,classic/forge/forge/llm/providers/schema.py
BaseModelProvider.get_token_limit,function,,"def get_token_limit(self, model_name: _ModelName) -> int:
        ...","Point(row=315, column=4)","Point(row=316, column=11)",BaseModelProvider,classic/forge/forge/llm/providers/schema.py
BaseModelProvider.get_incurred_cost,function,,"def get_incurred_cost(self) -> float:
        if self._budget:
            return self._budget.total_cost
        return 0","Point(row=318, column=4)","Point(row=321, column=16)",BaseModelProvider,classic/forge/forge/llm/providers/schema.py
BaseModelProvider.get_remaining_budget,function,,"def get_remaining_budget(self) -> float:
        if self._budget:
            return self._budget.remaining_budget
        return math.inf","Point(row=323, column=4)","Point(row=326, column=23)",BaseModelProvider,classic/forge/forge/llm/providers/schema.py
ModelTokenizer,class,A ModelTokenizer provides tokenization specific to a model.,"class ModelTokenizer(Protocol, Generic[_T]):
    """"""A ModelTokenizer provides tokenization specific to a model.""""""

    @abc.abstractmethod
    def encode(self, text: str) -> list[_T]:
        ...

    @abc.abstractmethod
    def decode(self, tokens: list[_T]) -> str:
        ...","Point(row=329, column=0)","Point(row=338, column=11)",,classic/forge/forge/llm/providers/schema.py
ModelTokenizer.encode,function,,"def encode(self, text: str) -> list[_T]:
        ...","Point(row=333, column=4)","Point(row=334, column=11)",ModelTokenizer,classic/forge/forge/llm/providers/schema.py
ModelTokenizer.decode,function,,"def decode(self, tokens: list[_T]) -> str:
        ...","Point(row=337, column=4)","Point(row=338, column=11)",ModelTokenizer,classic/forge/forge/llm/providers/schema.py
EmbeddingModelInfo,class,Struct for embedding model information.,"class EmbeddingModelInfo(ModelInfo[_ModelName]):
    """"""Struct for embedding model information.""""""

    service: Literal[ModelProviderService.EMBEDDING] = ModelProviderService.EMBEDDING  # type: ignore # noqa
    max_tokens: int
    embedding_dimensions: int","Point(row=346, column=0)","Point(row=351, column=29)",,classic/forge/forge/llm/providers/schema.py
EmbeddingModelResponse,class,Standard response struct for a response from an embedding model.,"class EmbeddingModelResponse(ModelResponse):
    """"""Standard response struct for a response from an embedding model.""""""

    embedding: Embedding = Field(default_factory=list)
    completion_tokens_used: int = Field(default=0, frozen=True)","Point(row=354, column=0)","Point(row=358, column=63)",,classic/forge/forge/llm/providers/schema.py
BaseEmbeddingModelProvider,class,,"class BaseEmbeddingModelProvider(BaseModelProvider[_ModelName, _ModelProviderSettings]):
    @abc.abstractmethod
    async def get_available_embedding_models(
        self,
    ) -> Sequence[EmbeddingModelInfo[_ModelName]]:
        ...

    @abc.abstractmethod
    async def create_embedding(
        self,
        text: str,
        model_name: _ModelName,
        embedding_parser: Callable[[Embedding], Embedding],
        **kwargs,
    ) -> EmbeddingModelResponse:
        ...","Point(row=361, column=0)","Point(row=376, column=11)",,classic/forge/forge/llm/providers/schema.py
BaseEmbeddingModelProvider.get_available_embedding_models,function,,"async def get_available_embedding_models(
        self,
    ) -> Sequence[EmbeddingModelInfo[_ModelName]]:
        ...","Point(row=363, column=4)","Point(row=366, column=11)",BaseEmbeddingModelProvider,classic/forge/forge/llm/providers/schema.py
BaseEmbeddingModelProvider.create_embedding,function,,"async def create_embedding(
        self,
        text: str,
        model_name: _ModelName,
        embedding_parser: Callable[[Embedding], Embedding],
        **kwargs,
    ) -> EmbeddingModelResponse:
        ...","Point(row=369, column=4)","Point(row=376, column=11)",BaseEmbeddingModelProvider,classic/forge/forge/llm/providers/schema.py
ChatModelInfo,class,Struct for language model information.,"class ChatModelInfo(ModelInfo[_ModelName]):
    """"""Struct for language model information.""""""

    service: Literal[ModelProviderService.CHAT] = ModelProviderService.CHAT  # type: ignore # noqa
    max_tokens: int
    has_function_call_api: bool = False","Point(row=384, column=0)","Point(row=389, column=39)",,classic/forge/forge/llm/providers/schema.py
ChatModelResponse,class,Standard response struct for a response from a language model.,"class ChatModelResponse(ModelResponse, Generic[_T]):
    """"""Standard response struct for a response from a language model.""""""

    response: AssistantChatMessage
    parsed_result: _T","Point(row=392, column=0)","Point(row=396, column=21)",,classic/forge/forge/llm/providers/schema.py
BaseChatModelProvider,class,,"class BaseChatModelProvider(BaseModelProvider[_ModelName, _ModelProviderSettings]):
    @abc.abstractmethod
    async def get_available_chat_models(self) -> Sequence[ChatModelInfo[_ModelName]]:
        ...

    @abc.abstractmethod
    def count_message_tokens(
        self,
        messages: ChatMessage | list[ChatMessage],
        model_name: _ModelName,
    ) -> int:
        ...

    @abc.abstractmethod
    async def create_chat_completion(
        self,
        model_prompt: list[ChatMessage],
        model_name: _ModelName,
        completion_parser: Callable[[AssistantChatMessage], _T] = lambda _: None,
        functions: Optional[list[CompletionModelFunction]] = None,
        max_output_tokens: Optional[int] = None,
        prefill_response: str = """",
        **kwargs,
    ) -> ChatModelResponse[_T]:
        ...","Point(row=399, column=0)","Point(row=423, column=11)",,classic/forge/forge/llm/providers/schema.py
BaseChatModelProvider.get_available_chat_models,function,,"async def get_available_chat_models(self) -> Sequence[ChatModelInfo[_ModelName]]:
        ...","Point(row=401, column=4)","Point(row=402, column=11)",BaseChatModelProvider,classic/forge/forge/llm/providers/schema.py
BaseChatModelProvider.count_message_tokens,function,,"def count_message_tokens(
        self,
        messages: ChatMessage | list[ChatMessage],
        model_name: _ModelName,
    ) -> int:
        ...","Point(row=405, column=4)","Point(row=410, column=11)",BaseChatModelProvider,classic/forge/forge/llm/providers/schema.py
BaseChatModelProvider.create_chat_completion,function,,"async def create_chat_completion(
        self,
        model_prompt: list[ChatMessage],
        model_name: _ModelName,
        completion_parser: Callable[[AssistantChatMessage], _T] = lambda _: None,
        functions: Optional[list[CompletionModelFunction]] = None,
        max_output_tokens: Optional[int] = None,
        prefill_response: str = """",
        **kwargs,
    ) -> ChatModelResponse[_T]:
        ...","Point(row=413, column=4)","Point(row=423, column=11)",BaseChatModelProvider,classic/forge/forge/llm/providers/schema.py
LlamafileModelName,class,,"class LlamafileModelName(str, enum.Enum):
    MISTRAL_7B_INSTRUCT = ""mistral-7b-instruct-v0.2""","Point(row=32, column=0)","Point(row=33, column=52)",,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileConfiguration,class,,"class LlamafileConfiguration(ModelProviderConfiguration):
    # TODO: implement 'seed' across forge.llm.providers
    seed: Optional[int] = None","Point(row=53, column=0)","Point(row=55, column=30)",,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileCredentials,class,,"class LlamafileCredentials(ModelProviderCredentials):
    api_key: Optional[SecretStr] = SecretStr(""sk-no-key-required"")
    api_base: SecretStr = UserConfigurable(  # type: ignore
        default=SecretStr(""http://localhost:8080/v1""), from_env=""LLAMAFILE_API_BASE""
    )

    def get_api_access_kwargs(self) -> dict[str, str]:
        return {
            k: v.get_secret_value()
            for k, v in {
                ""api_key"": self.api_key,
                ""base_url"": self.api_base,
            }.items()
            if v is not None
        }","Point(row=58, column=0)","Point(row=72, column=9)",,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileCredentials.get_api_access_kwargs,function,,"def get_api_access_kwargs(self) -> dict[str, str]:
        return {
            k: v.get_secret_value()
            for k, v in {
                ""api_key"": self.api_key,
                ""base_url"": self.api_base,
            }.items()
            if v is not None
        }","Point(row=64, column=4)","Point(row=72, column=9)",LlamafileCredentials,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileSettings,class,,"class LlamafileSettings(ModelProviderSettings):
    configuration: LlamafileConfiguration  # type: ignore
    credentials: Optional[LlamafileCredentials] = None  # type: ignore","Point(row=75, column=0)","Point(row=77, column=70)",,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileTokenizer,class,,"class LlamafileTokenizer(ModelTokenizer[int]):
    def __init__(self, credentials: LlamafileCredentials):
        self._credentials = credentials

    @property
    def _tokenizer_base_url(self):
        # The OpenAI-chat-compatible base url should look something like
        # 'http://localhost:8080/v1' but the tokenizer endpoint is
        # 'http://localhost:8080/tokenize'. So here we just strip off the '/v1'.
        api_base = self._credentials.api_base.get_secret_value()
        return api_base.strip(""/v1"")

    def encode(self, text: str) -> list[int]:
        response = requests.post(
            url=f""{self._tokenizer_base_url}/tokenize"", json={""content"": text}
        )
        response.raise_for_status()
        return response.json()[""tokens""]

    def decode(self, tokens: list[int]) -> str:
        response = requests.post(
            url=f""{self._tokenizer_base_url}/detokenize"", json={""tokens"": tokens}
        )
        response.raise_for_status()
        return response.json()[""content""]","Point(row=80, column=0)","Point(row=104, column=41)",,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileTokenizer.__init__,function,,"def __init__(self, credentials: LlamafileCredentials):
        self._credentials = credentials","Point(row=81, column=4)","Point(row=82, column=39)",LlamafileTokenizer,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileTokenizer._tokenizer_base_url,function,,"def _tokenizer_base_url(self):
        # The OpenAI-chat-compatible base url should look something like
        # 'http://localhost:8080/v1' but the tokenizer endpoint is
        # 'http://localhost:8080/tokenize'. So here we just strip off the '/v1'.
        api_base = self._credentials.api_base.get_secret_value()
        return api_base.strip(""/v1"")","Point(row=85, column=4)","Point(row=90, column=36)",LlamafileTokenizer,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileTokenizer.encode,function,,"def encode(self, text: str) -> list[int]:
        response = requests.post(
            url=f""{self._tokenizer_base_url}/tokenize"", json={""content"": text}
        )
        response.raise_for_status()
        return response.json()[""tokens""]","Point(row=92, column=4)","Point(row=97, column=40)",LlamafileTokenizer,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileTokenizer.decode,function,,"def decode(self, tokens: list[int]) -> str:
        response = requests.post(
            url=f""{self._tokenizer_base_url}/detokenize"", json={""tokens"": tokens}
        )
        response.raise_for_status()
        return response.json()[""content""]","Point(row=99, column=4)","Point(row=104, column=41)",LlamafileTokenizer,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileProvider,class,,"class LlamafileProvider(
    BaseOpenAIChatProvider[LlamafileModelName, LlamafileSettings],
    # TODO: add and test support for embedding models
    # BaseOpenAIEmbeddingProvider[LlamafileModelName, LlamafileSettings],
):
    EMBEDDING_MODELS = LLAMAFILE_EMBEDDING_MODELS
    CHAT_MODELS = LLAMAFILE_CHAT_MODELS
    MODELS = {**CHAT_MODELS, **EMBEDDING_MODELS}

    default_settings = LlamafileSettings(
        name=""llamafile_provider"",
        description=(
            ""Provides chat completion and embedding services ""
            ""through a llamafile instance""
        ),
        configuration=LlamafileConfiguration(),
    )

    _settings: LlamafileSettings
    _credentials: LlamafileCredentials
    _configuration: LlamafileConfiguration

    async def get_available_models(self) -> Sequence[ChatModelInfo[LlamafileModelName]]:
        _models = (await self._client.models.list()).data
        # note: at the moment, llamafile only serves one model at a time (so this
        # list will only ever have one value). however, in the future, llamafile
        # may support multiple models, so leaving this method as-is for now.
        self._logger.debug(f""Retrieved llamafile models: {_models}"")

        clean_model_ids = [clean_model_name(m.id) for m in _models]
        self._logger.debug(f""Cleaned llamafile model IDs: {clean_model_ids}"")

        return [
            LLAMAFILE_CHAT_MODELS[id]
            for id in clean_model_ids
            if id in LLAMAFILE_CHAT_MODELS
        ]

    def get_tokenizer(self, model_name: LlamafileModelName) -> LlamafileTokenizer:
        return LlamafileTokenizer(self._credentials)

    def count_message_tokens(
        self,
        messages: ChatMessage | list[ChatMessage],
        model_name: LlamafileModelName,
    ) -> int:
        if isinstance(messages, ChatMessage):
            messages = [messages]

        if model_name == LlamafileModelName.MISTRAL_7B_INSTRUCT:
            # For mistral-instruct, num added tokens depends on if the message
            # is a prompt/instruction or an assistant-generated message.
            # - prompt gets [INST], [/INST] added and the first instruction
            # begins with '<s>' ('beginning-of-sentence' token).
            # - assistant-generated messages get '</s>' added
            # see: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
            #
            prompt_added = 1  # one for '<s>' token
            assistant_num_added = 0
            ntokens = 0
            for message in messages:
                if (
                    message.role == ChatMessage.Role.USER
                    # note that 'system' messages will get converted
                    # to 'user' messages before being sent to the model
                    or message.role == ChatMessage.Role.SYSTEM
                ):
                    # 5 tokens for [INST], [/INST], which actually get
                    # tokenized into ""[, INST, ]"" and ""[, /, INST, ]""
                    # by the mistral tokenizer
                    prompt_added += 5
                elif message.role == ChatMessage.Role.ASSISTANT:
                    assistant_num_added += 1  # for </s>
                else:
                    raise ValueError(
                        f""{model_name} does not support role: {message.role}""
                    )

                ntokens += self.count_tokens(message.content, model_name)

            total_token_count = prompt_added + assistant_num_added + ntokens
            return total_token_count

        else:
            raise NotImplementedError(
                f""count_message_tokens not implemented for model {model_name}""
            )

    def _get_chat_completion_args(
        self,
        prompt_messages: list[ChatMessage],
        model: LlamafileModelName,
        functions: list[CompletionModelFunction] | None = None,
        max_output_tokens: int | None = None,
        **kwargs,
    ) -> tuple[
        list[ChatCompletionMessageParam], CompletionCreateParams, dict[str, Any]
    ]:
        messages, completion_kwargs, parse_kwargs = super()._get_chat_completion_args(
            prompt_messages, model, functions, max_output_tokens, **kwargs
        )

        if model == LlamafileModelName.MISTRAL_7B_INSTRUCT:
            messages = self._adapt_chat_messages_for_mistral_instruct(messages)

        if ""seed"" not in kwargs and self._configuration.seed is not None:
            completion_kwargs[""seed""] = self._configuration.seed

        # Convert all messages with content blocks to simple text messages
        for message in messages:
            if isinstance(content := message.get(""content""), list):
                message[""content""] = ""\n\n"".join(
                    b[""text""]
                    for b in content
                    if b[""type""] == ""text""
                    # FIXME: add support for images through image_data completion kwarg
                )

        return messages, completion_kwargs, parse_kwargs

    def _adapt_chat_messages_for_mistral_instruct(
        self, messages: list[ChatCompletionMessageParam]
    ) -> list[ChatCompletionMessageParam]:
        """"""
        Munge the messages to be compatible with the mistral-7b-instruct chat
        template, which:
        - only supports 'user' and 'assistant' roles.
        - expects messages to alternate between user/assistant roles.

        See details here:
        https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2#instruction-format
        """"""
        adapted_messages: list[ChatCompletionMessageParam] = []
        for message in messages:
            # convert 'system' role to 'user' role as mistral-7b-instruct does
            # not support 'system'
            if message[""role""] == ChatMessage.Role.SYSTEM:
                message[""role""] = ChatMessage.Role.USER

            if (
                len(adapted_messages) == 0
                or message[""role""] != (last_message := adapted_messages[-1])[""role""]
            ):
                adapted_messages.append(message)
            else:
                if not message.get(""content""):
                    continue

                # if the curr message has the same role as the previous one,
                # concat the current message content to the prev message
                if message[""role""] == ""user"" and last_message[""role""] == ""user"":
                    # user messages can contain other types of content blocks
                    if not isinstance(last_message[""content""], list):
                        last_message[""content""] = [
                            {""type"": ""text"", ""text"": last_message[""content""]}
                        ]

                    last_message[""content""].extend(
                        message[""content""]
                        if isinstance(message[""content""], list)
                        else [{""type"": ""text"", ""text"": message[""content""]}]
                    )
                elif message[""role""] != ""user"" and last_message[""role""] != ""user"":
                    last_message[""content""] = (
                        (last_message.get(""content"") or """")
                        + ""\n\n""
                        + (message.get(""content"") or """")
                    ).strip()

        return adapted_messages

    def _parse_assistant_tool_calls(
        self,
        assistant_message: ChatCompletionMessage,
        compat_mode: bool = False,
        **kwargs,
    ):
        tool_calls: list[AssistantToolCall] = []
        parse_errors: list[Exception] = []

        if compat_mode and assistant_message.content:
            try:
                tool_calls = list(
                    _tool_calls_compat_extract_calls(assistant_message.content)
                )
            except Exception as e:
                parse_errors.append(e)

        return tool_calls, parse_errors","Point(row=107, column=0)","Point(row=295, column=39)",,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileProvider.get_available_models,function,,"async def get_available_models(self) -> Sequence[ChatModelInfo[LlamafileModelName]]:
        _models = (await self._client.models.list()).data
        # note: at the moment, llamafile only serves one model at a time (so this
        # list will only ever have one value). however, in the future, llamafile
        # may support multiple models, so leaving this method as-is for now.
        self._logger.debug(f""Retrieved llamafile models: {_models}"")

        clean_model_ids = [clean_model_name(m.id) for m in _models]
        self._logger.debug(f""Cleaned llamafile model IDs: {clean_model_ids}"")

        return [
            LLAMAFILE_CHAT_MODELS[id]
            for id in clean_model_ids
            if id in LLAMAFILE_CHAT_MODELS
        ]","Point(row=129, column=4)","Point(row=143, column=9)",LlamafileProvider,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileProvider.get_tokenizer,function,,"def get_tokenizer(self, model_name: LlamafileModelName) -> LlamafileTokenizer:
        return LlamafileTokenizer(self._credentials)","Point(row=145, column=4)","Point(row=146, column=52)",LlamafileProvider,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileProvider.count_message_tokens,function,,"def count_message_tokens(
        self,
        messages: ChatMessage | list[ChatMessage],
        model_name: LlamafileModelName,
    ) -> int:
        if isinstance(messages, ChatMessage):
            messages = [messages]

        if model_name == LlamafileModelName.MISTRAL_7B_INSTRUCT:
            # For mistral-instruct, num added tokens depends on if the message
            # is a prompt/instruction or an assistant-generated message.
            # - prompt gets [INST], [/INST] added and the first instruction
            # begins with '<s>' ('beginning-of-sentence' token).
            # - assistant-generated messages get '</s>' added
            # see: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
            #
            prompt_added = 1  # one for '<s>' token
            assistant_num_added = 0
            ntokens = 0
            for message in messages:
                if (
                    message.role == ChatMessage.Role.USER
                    # note that 'system' messages will get converted
                    # to 'user' messages before being sent to the model
                    or message.role == ChatMessage.Role.SYSTEM
                ):
                    # 5 tokens for [INST], [/INST], which actually get
                    # tokenized into ""[, INST, ]"" and ""[, /, INST, ]""
                    # by the mistral tokenizer
                    prompt_added += 5
                elif message.role == ChatMessage.Role.ASSISTANT:
                    assistant_num_added += 1  # for </s>
                else:
                    raise ValueError(
                        f""{model_name} does not support role: {message.role}""
                    )

                ntokens += self.count_tokens(message.content, model_name)

            total_token_count = prompt_added + assistant_num_added + ntokens
            return total_token_count

        else:
            raise NotImplementedError(
                f""count_message_tokens not implemented for model {model_name}""
            )","Point(row=148, column=4)","Point(row=193, column=13)",LlamafileProvider,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileProvider._get_chat_completion_args,function,,"def _get_chat_completion_args(
        self,
        prompt_messages: list[ChatMessage],
        model: LlamafileModelName,
        functions: list[CompletionModelFunction] | None = None,
        max_output_tokens: int | None = None,
        **kwargs,
    ) -> tuple[
        list[ChatCompletionMessageParam], CompletionCreateParams, dict[str, Any]
    ]:
        messages, completion_kwargs, parse_kwargs = super()._get_chat_completion_args(
            prompt_messages, model, functions, max_output_tokens, **kwargs
        )

        if model == LlamafileModelName.MISTRAL_7B_INSTRUCT:
            messages = self._adapt_chat_messages_for_mistral_instruct(messages)

        if ""seed"" not in kwargs and self._configuration.seed is not None:
            completion_kwargs[""seed""] = self._configuration.seed

        # Convert all messages with content blocks to simple text messages
        for message in messages:
            if isinstance(content := message.get(""content""), list):
                message[""content""] = ""\n\n"".join(
                    b[""text""]
                    for b in content
                    if b[""type""] == ""text""
                    # FIXME: add support for images through image_data completion kwarg
                )

        return messages, completion_kwargs, parse_kwargs","Point(row=195, column=4)","Point(row=225, column=56)",LlamafileProvider,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileProvider._adapt_chat_messages_for_mistral_instruct,function,"
        Munge the messages to be compatible with the mistral-7b-instruct chat
        template, which:
        - only supports 'user' and 'assistant' roles.
        - expects messages to alternate between user/assistant roles.

        See details here:
        https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2#instruction-format
","def _adapt_chat_messages_for_mistral_instruct(
        self, messages: list[ChatCompletionMessageParam]
    ) -> list[ChatCompletionMessageParam]:
        """"""
        Munge the messages to be compatible with the mistral-7b-instruct chat
        template, which:
        - only supports 'user' and 'assistant' roles.
        - expects messages to alternate between user/assistant roles.

        See details here:
        https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2#instruction-format
        """"""
        adapted_messages: list[ChatCompletionMessageParam] = []
        for message in messages:
            # convert 'system' role to 'user' role as mistral-7b-instruct does
            # not support 'system'
            if message[""role""] == ChatMessage.Role.SYSTEM:
                message[""role""] = ChatMessage.Role.USER

            if (
                len(adapted_messages) == 0
                or message[""role""] != (last_message := adapted_messages[-1])[""role""]
            ):
                adapted_messages.append(message)
            else:
                if not message.get(""content""):
                    continue

                # if the curr message has the same role as the previous one,
                # concat the current message content to the prev message
                if message[""role""] == ""user"" and last_message[""role""] == ""user"":
                    # user messages can contain other types of content blocks
                    if not isinstance(last_message[""content""], list):
                        last_message[""content""] = [
                            {""type"": ""text"", ""text"": last_message[""content""]}
                        ]

                    last_message[""content""].extend(
                        message[""content""]
                        if isinstance(message[""content""], list)
                        else [{""type"": ""text"", ""text"": message[""content""]}]
                    )
                elif message[""role""] != ""user"" and last_message[""role""] != ""user"":
                    last_message[""content""] = (
                        (last_message.get(""content"") or """")
                        + ""\n\n""
                        + (message.get(""content"") or """")
                    ).strip()

        return adapted_messages","Point(row=227, column=4)","Point(row=276, column=31)",LlamafileProvider,classic/forge/forge/llm/providers/llamafile/llamafile.py
LlamafileProvider._parse_assistant_tool_calls,function,,"def _parse_assistant_tool_calls(
        self,
        assistant_message: ChatCompletionMessage,
        compat_mode: bool = False,
        **kwargs,
    ):
        tool_calls: list[AssistantToolCall] = []
        parse_errors: list[Exception] = []

        if compat_mode and assistant_message.content:
            try:
                tool_calls = list(
                    _tool_calls_compat_extract_calls(assistant_message.content)
                )
            except Exception as e:
                parse_errors.append(e)

        return tool_calls, parse_errors","Point(row=278, column=4)","Point(row=295, column=39)",LlamafileProvider,classic/forge/forge/llm/providers/llamafile/llamafile.py
clean_model_name,function,"
    Clean up model names:
    1. Remove file extension
    2. Remove quantization info

    Examples:
    ```
    raw:   'mistral-7b-instruct-v0.2.Q5_K_M.gguf'
    clean: 'mistral-7b-instruct-v0.2'

    raw: '/Users/kate/models/mistral-7b-instruct-v0.2.Q5_K_M.gguf'
    clean:                  'mistral-7b-instruct-v0.2'

    raw:   'llava-v1.5-7b-q4.gguf'
    clean: 'llava-v1.5-7b'
    ```
","def clean_model_name(model_file: str) -> str:
    """"""
    Clean up model names:
    1. Remove file extension
    2. Remove quantization info

    Examples:
    ```
    raw:   'mistral-7b-instruct-v0.2.Q5_K_M.gguf'
    clean: 'mistral-7b-instruct-v0.2'

    raw: '/Users/kate/models/mistral-7b-instruct-v0.2.Q5_K_M.gguf'
    clean:                  'mistral-7b-instruct-v0.2'

    raw:   'llava-v1.5-7b-q4.gguf'
    clean: 'llava-v1.5-7b'
    ```
    """"""
    name_without_ext = Path(model_file).name.rsplit(""."", 1)[0]
    name_without_Q = re.match(
        r""^[a-zA-Z0-9]+([.\-](?!([qQ]|B?F)\d{1,2})[a-zA-Z0-9]+)*"",
        name_without_ext,
    )
    return name_without_Q.group() if name_without_Q else name_without_ext","Point(row=298, column=0)","Point(row=321, column=73)",,classic/forge/forge/llm/providers/llamafile/llamafile.py
_tool_calls_compat_extract_calls,function,,"def _tool_calls_compat_extract_calls(response: str) -> Iterator[AssistantToolCall]:
    import re
    import uuid

    logging.debug(f""Trying to extract tool calls from response:\n{response}"")

    response = response.strip()  # strip off any leading/trailing whitespace
    if response.startswith(""```""):
        # attempt to remove any extraneous markdown artifacts like ""```json""
        response = response.strip(""```"")
        if response.startswith(""json""):
            response = response.strip(""json"")
        response = response.strip()  # any remaining whitespace

    if response[0] == ""["":
        tool_calls: list[AssistantToolCallDict] = json_loads(response)
    else:
        block = re.search(r""```(?:tool_calls)?\n(.*)\n```\s*$"", response, re.DOTALL)
        if not block:
            raise ValueError(""Could not find tool_calls block in response"")
        tool_calls: list[AssistantToolCallDict] = json_loads(block.group(1))

    for t in tool_calls:
        t[""id""] = str(uuid.uuid4())
        # t[""function""][""arguments""] = str(t[""function""][""arguments""])  # HACK

        yield AssistantToolCall.parse_obj(t)","Point(row=324, column=0)","Point(row=350, column=44)",,classic/forge/forge/llm/providers/llamafile/llamafile.py
AIProfile,class,"
    Object to hold the AI's personality.

    Attributes:
        ai_name (str): The name of the AI.
        ai_role (str): The description of the AI's role.
        ai_goals (list): The list of objectives the AI is supposed to complete.
        api_budget (float): The maximum dollar value for API calls (0.0 means infinite)
","class AIProfile(BaseModel):
    """"""
    Object to hold the AI's personality.

    Attributes:
        ai_name (str): The name of the AI.
        ai_role (str): The description of the AI's role.
        ai_goals (list): The list of objectives the AI is supposed to complete.
        api_budget (float): The maximum dollar value for API calls (0.0 means infinite)
    """"""

    ai_name: str = DEFAULT_AI_NAME
    ai_role: str = DEFAULT_AI_ROLE
    """"""`ai_role` should fit in the following format: `You are {ai_name}, {ai_role}`""""""
    ai_goals: list[str] = Field(default_factory=list[str])","Point(row=12, column=0)","Point(row=26, column=58)",,classic/forge/forge/config/ai_profile.py
AIDirectives,class,"An object that contains the basic directives for the AI prompt.

    Attributes:
        constraints (list): A list of constraints that the AI should adhere to.
        resources (list): A list of resources that the AI can utilize.
        best_practices (list): A list of best practices that the AI should follow.
","class AIDirectives(BaseModel):
    """"""An object that contains the basic directives for the AI prompt.

    Attributes:
        constraints (list): A list of constraints that the AI should adhere to.
        resources (list): A list of resources that the AI can utilize.
        best_practices (list): A list of best practices that the AI should follow.
    """"""

    resources: list[str] = Field(default_factory=list)
    constraints: list[str] = Field(default_factory=list)
    best_practices: list[str] = Field(default_factory=list)

    def __add__(self, other: AIDirectives) -> AIDirectives:
        return AIDirectives(
            resources=self.resources + other.resources,
            constraints=self.constraints + other.constraints,
            best_practices=self.best_practices + other.best_practices,
        ).model_copy(deep=True)","Point(row=9, column=0)","Point(row=27, column=31)",,classic/forge/forge/config/ai_directives.py
AIDirectives.__add__,function,,"def __add__(self, other: AIDirectives) -> AIDirectives:
        return AIDirectives(
            resources=self.resources + other.resources,
            constraints=self.constraints + other.constraints,
            best_practices=self.best_practices + other.best_practices,
        ).model_copy(deep=True)","Point(row=22, column=4)","Point(row=27, column=31)",AIDirectives,classic/forge/forge/config/ai_directives.py
BaseConfig,class,,"class BaseConfig(SystemSettings):
    name: str = ""Base configuration""
    description: str = ""Default configuration for forge agent.""

    # TTS configuration
    tts_config: TTSConfig = TTSConfig()

    # File storage
    file_storage_backend: FileStorageBackendName = UserConfigurable(
        default=FileStorageBackendName.LOCAL, from_env=""FILE_STORAGE_BACKEND""
    )","Point(row=5, column=0)","Point(row=15, column=5)",,classic/forge/forge/config/base.py
GCSFileStorageConfiguration,class,,"class GCSFileStorageConfiguration(FileStorageConfiguration):
    bucket: str = UserConfigurable(""autogpt"", from_env=""STORAGE_BUCKET"")","Point(row=24, column=0)","Point(row=25, column=72)",,classic/forge/forge/file_storage/gcs.py
GCSFileStorage,class,A class that represents a Google Cloud Storage.,"class GCSFileStorage(FileStorage):
    """"""A class that represents a Google Cloud Storage.""""""

    _bucket: storage.Bucket

    def __init__(self, config: GCSFileStorageConfiguration):
        self._bucket_name = config.bucket
        self._root = config.root
        # Add / at the beginning of the root path
        if not self._root.is_absolute():
            self._root = Path(""/"").joinpath(self._root)

        self._gcs = storage.Client()
        super().__init__()

    @property
    def root(self) -> Path:
        """"""The root directory of the file storage.""""""
        return self._root

    @property
    def restrict_to_root(self) -> bool:
        """"""Whether to restrict generated paths to the root.""""""
        return True

    @property
    def is_local(self) -> bool:
        """"""Whether the storage is local (i.e. on the same machine, not cloud-based).""""""
        return False

    def initialize(self) -> None:
        logger.debug(f""Initializing {repr(self)}..."")
        try:
            self._bucket = self._gcs.get_bucket(self._bucket_name)
        except NotFound:
            logger.info(f""Bucket '{self._bucket_name}' does not exist; creating it..."")
            self._bucket = self._gcs.create_bucket(self._bucket_name)

    def get_path(self, relative_path: str | Path) -> Path:
        # We set GCS root with ""/"" at the beginning
        # but relative_to(""/"") will remove it
        # because we don't actually want it in the storage filenames
        return super().get_path(relative_path).relative_to(""/"")

    def _get_blob(self, path: str | Path) -> storage.Blob:
        path = self.get_path(path)
        return self._bucket.blob(str(path))

    @overload
    def open_file(
        self,
        path: str | Path,
        mode: Literal[""r"", ""w""] = ""r"",
        binary: Literal[False] = False,
    ) -> TextIOWrapper:
        ...

    @overload
    def open_file(
        self, path: str | Path, mode: Literal[""r""], binary: Literal[True]
    ) -> BlobReader:
        ...

    @overload
    def open_file(
        self, path: str | Path, mode: Literal[""w""], binary: Literal[True]
    ) -> BlobWriter:
        ...

    @overload
    def open_file(
        self, path: str | Path, mode: Literal[""r"", ""w""], binary: Literal[True]
    ) -> BlobWriter | BlobReader:
        ...

    @overload
    def open_file(self, path: str | Path, *, binary: Literal[True]) -> BlobReader:
        ...

    @overload
    def open_file(
        self, path: str | Path, mode: Literal[""r"", ""w""] = ""r"", binary: bool = False
    ) -> BlobReader | BlobWriter | TextIOWrapper:
        ...

    # https://github.com/microsoft/pyright/issues/8007
    def open_file(  # pyright: ignore[reportIncompatibleMethodOverride]
        self, path: str | Path, mode: Literal[""r"", ""w""] = ""r"", binary: bool = False
    ) -> BlobReader | BlobWriter | TextIOWrapper:
        """"""Open a file in the storage.""""""
        blob = self._get_blob(path)
        blob.reload()  # pin revision number to prevent version mixing while reading
        return blob.open(f""{mode}b"" if binary else mode)

    @overload
    def read_file(self, path: str | Path, binary: Literal[False] = False) -> str:
        """"""Read a file in the storage as text.""""""
        ...

    @overload
    def read_file(self, path: str | Path, binary: Literal[True]) -> bytes:
        """"""Read a file in the storage as binary.""""""
        ...

    @overload
    def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:
        """"""Read a file in the storage.""""""
        ...

    def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:
        """"""Read a file in the storage.""""""
        return self.open_file(path, ""r"", binary).read()

    async def write_file(self, path: str | Path, content: str | bytes) -> None:
        """"""Write to a file in the storage.""""""
        blob = self._get_blob(path)

        blob.upload_from_string(
            data=content,
            content_type=(
                ""text/plain""
                if type(content) is str
                # TODO: get MIME type from file extension or binary content
                else ""application/octet-stream""
            ),
        )

        if self.on_write_file:
            path = Path(path)
            if path.is_absolute():
                path = path.relative_to(self.root)
            res = self.on_write_file(path)
            if inspect.isawaitable(res):
                await res

    def list_files(self, path: str | Path = ""."") -> list[Path]:
        """"""List all files (recursively) in a directory in the storage.""""""
        path = self.get_path(path)
        return [
            Path(blob.name).relative_to(path)
            for blob in self._bucket.list_blobs(
                prefix=f""{path}/"" if path != Path(""."") else None
            )
        ]

    def list_folders(
        self, path: str | Path = ""."", recursive: bool = False
    ) -> list[Path]:
        """"""List 'directories' directly in a given path or recursively in the storage.""""""
        path = self.get_path(path)
        folder_names = set()

        # List objects with the specified prefix and delimiter
        for blob in self._bucket.list_blobs(prefix=path):
            # Remove path prefix and the object name (last part)
            folder = Path(blob.name).relative_to(path).parent
            if not folder or folder == Path("".""):
                continue
            # For non-recursive, only add the first level of folders
            if not recursive:
                folder_names.add(folder.parts[0])
            else:
                # For recursive, need to add all nested folders
                for i in range(len(folder.parts)):
                    folder_names.add(""/"".join(folder.parts[: i + 1]))

        return [Path(f) for f in folder_names]

    def delete_file(self, path: str | Path) -> None:
        """"""Delete a file in the storage.""""""
        path = self.get_path(path)
        blob = self._bucket.blob(str(path))
        blob.delete()

    def delete_dir(self, path: str | Path) -> None:
        """"""Delete an empty folder in the storage.""""""
        # Since GCS does not have directories, we don't need to do anything
        pass

    def exists(self, path: str | Path) -> bool:
        """"""Check if a file or folder exists in GCS storage.""""""
        path = self.get_path(path)
        # Check for exact blob match (file)
        blob = self._bucket.blob(str(path))
        if blob.exists():
            return True
        # Check for any blobs with prefix (folder)
        prefix = f""{str(path).rstrip('/')}/""
        blobs = self._bucket.list_blobs(prefix=prefix, max_results=1)
        return next(blobs, None) is not None

    def make_dir(self, path: str | Path) -> None:
        """"""Create a directory in the storage if doesn't exist.""""""
        # GCS does not have directories, so we don't need to do anything
        pass

    def rename(self, old_path: str | Path, new_path: str | Path) -> None:
        """"""Rename a file or folder in the storage.""""""
        old_path = self.get_path(old_path)
        new_path = self.get_path(new_path)
        blob = self._bucket.blob(str(old_path))
        # If the blob with exact name exists, rename it
        if blob.exists():
            self._bucket.rename_blob(blob, new_name=str(new_path))
            return
        # Otherwise, rename all blobs with the prefix (folder)
        for blob in self._bucket.list_blobs(prefix=f""{old_path}/""):
            new_name = str(blob.name).replace(str(old_path), str(new_path), 1)
            self._bucket.rename_blob(blob, new_name=new_name)

    def copy(self, source: str | Path, destination: str | Path) -> None:
        """"""Copy a file or folder with all contents in the storage.""""""
        source = self.get_path(source)
        destination = self.get_path(destination)
        # If the source is a file, copy it
        if self._bucket.blob(str(source)).exists():
            self._bucket.copy_blob(
                self._bucket.blob(str(source)), self._bucket, str(destination)
            )
            return
        # Otherwise, copy all blobs with the prefix (folder)
        for blob in self._bucket.list_blobs(prefix=f""{source}/""):
            new_name = str(blob.name).replace(str(source), str(destination), 1)
            self._bucket.copy_blob(blob, self._bucket, new_name)

    def clone_with_subroot(self, subroot: str | Path) -> GCSFileStorage:
        """"""Create a new GCSFileStorage with a subroot of the current storage.""""""
        file_storage = GCSFileStorage(
            GCSFileStorageConfiguration(
                root=Path(""/"").joinpath(self.get_path(subroot)),
                bucket=self._bucket_name,
            )
        )
        file_storage._gcs = self._gcs
        file_storage._bucket = self._bucket
        return file_storage

    def __repr__(self) -> str:
        return f""{__class__.__name__}(bucket='{self._bucket_name}', root={self._root})""","Point(row=28, column=0)","Point(row=266, column=87)",,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.__init__,function,,"def __init__(self, config: GCSFileStorageConfiguration):
        self._bucket_name = config.bucket
        self._root = config.root
        # Add / at the beginning of the root path
        if not self._root.is_absolute():
            self._root = Path(""/"").joinpath(self._root)

        self._gcs = storage.Client()
        super().__init__()","Point(row=33, column=4)","Point(row=41, column=26)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.root,function,The root directory of the file storage.,"def root(self) -> Path:
        """"""The root directory of the file storage.""""""
        return self._root","Point(row=44, column=4)","Point(row=46, column=25)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.restrict_to_root,function,Whether to restrict generated paths to the root.,"def restrict_to_root(self) -> bool:
        """"""Whether to restrict generated paths to the root.""""""
        return True","Point(row=49, column=4)","Point(row=51, column=19)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.is_local,function,"Whether the storage is local (i.e. on the same machine, not cloud-based).","def is_local(self) -> bool:
        """"""Whether the storage is local (i.e. on the same machine, not cloud-based).""""""
        return False","Point(row=54, column=4)","Point(row=56, column=20)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.initialize,function,,"def initialize(self) -> None:
        logger.debug(f""Initializing {repr(self)}..."")
        try:
            self._bucket = self._gcs.get_bucket(self._bucket_name)
        except NotFound:
            logger.info(f""Bucket '{self._bucket_name}' does not exist; creating it..."")
            self._bucket = self._gcs.create_bucket(self._bucket_name)","Point(row=58, column=4)","Point(row=64, column=69)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.get_path,function,,"def get_path(self, relative_path: str | Path) -> Path:
        # We set GCS root with ""/"" at the beginning
        # but relative_to(""/"") will remove it
        # because we don't actually want it in the storage filenames
        return super().get_path(relative_path).relative_to(""/"")","Point(row=66, column=4)","Point(row=70, column=63)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage._get_blob,function,,"def _get_blob(self, path: str | Path) -> storage.Blob:
        path = self.get_path(path)
        return self._bucket.blob(str(path))","Point(row=72, column=4)","Point(row=74, column=43)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.open_file,function,,"def open_file(
        self,
        path: str | Path,
        mode: Literal[""r"", ""w""] = ""r"",
        binary: Literal[False] = False,
    ) -> TextIOWrapper:
        ...","Point(row=77, column=4)","Point(row=83, column=11)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.open_file,function,,"def open_file(
        self, path: str | Path, mode: Literal[""r""], binary: Literal[True]
    ) -> BlobReader:
        ...","Point(row=86, column=4)","Point(row=89, column=11)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.open_file,function,,"def open_file(
        self, path: str | Path, mode: Literal[""w""], binary: Literal[True]
    ) -> BlobWriter:
        ...","Point(row=92, column=4)","Point(row=95, column=11)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.open_file,function,,"def open_file(
        self, path: str | Path, mode: Literal[""r"", ""w""], binary: Literal[True]
    ) -> BlobWriter | BlobReader:
        ...","Point(row=98, column=4)","Point(row=101, column=11)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.open_file,function,,"def open_file(self, path: str | Path, *, binary: Literal[True]) -> BlobReader:
        ...","Point(row=104, column=4)","Point(row=105, column=11)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.open_file,function,,"def open_file(
        self, path: str | Path, mode: Literal[""r"", ""w""] = ""r"", binary: bool = False
    ) -> BlobReader | BlobWriter | TextIOWrapper:
        ...","Point(row=108, column=4)","Point(row=111, column=11)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.open_file,function,Open a file in the storage.,"def open_file(  # pyright: ignore[reportIncompatibleMethodOverride]
        self, path: str | Path, mode: Literal[""r"", ""w""] = ""r"", binary: bool = False
    ) -> BlobReader | BlobWriter | TextIOWrapper:
        """"""Open a file in the storage.""""""
        blob = self._get_blob(path)
        blob.reload()  # pin revision number to prevent version mixing while reading
        return blob.open(f""{mode}b"" if binary else mode)","Point(row=114, column=4)","Point(row=120, column=56)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.read_file,function,Read a file in the storage as text.,"def read_file(self, path: str | Path, binary: Literal[False] = False) -> str:
        """"""Read a file in the storage as text.""""""
        ...","Point(row=123, column=4)","Point(row=125, column=11)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.read_file,function,Read a file in the storage as binary.,"def read_file(self, path: str | Path, binary: Literal[True]) -> bytes:
        """"""Read a file in the storage as binary.""""""
        ...","Point(row=128, column=4)","Point(row=130, column=11)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.read_file,function,Read a file in the storage.,"def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:
        """"""Read a file in the storage.""""""
        ...","Point(row=133, column=4)","Point(row=135, column=11)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.read_file,function,Read a file in the storage.,"def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:
        """"""Read a file in the storage.""""""
        return self.open_file(path, ""r"", binary).read()","Point(row=137, column=4)","Point(row=139, column=55)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.write_file,function,Write to a file in the storage.,"async def write_file(self, path: str | Path, content: str | bytes) -> None:
        """"""Write to a file in the storage.""""""
        blob = self._get_blob(path)

        blob.upload_from_string(
            data=content,
            content_type=(
                ""text/plain""
                if type(content) is str
                # TODO: get MIME type from file extension or binary content
                else ""application/octet-stream""
            ),
        )

        if self.on_write_file:
            path = Path(path)
            if path.is_absolute():
                path = path.relative_to(self.root)
            res = self.on_write_file(path)
            if inspect.isawaitable(res):
                await res","Point(row=141, column=4)","Point(row=161, column=25)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.list_files,function,List all files (recursively) in a directory in the storage.,"def list_files(self, path: str | Path = ""."") -> list[Path]:
        """"""List all files (recursively) in a directory in the storage.""""""
        path = self.get_path(path)
        return [
            Path(blob.name).relative_to(path)
            for blob in self._bucket.list_blobs(
                prefix=f""{path}/"" if path != Path(""."") else None
            )
        ]","Point(row=163, column=4)","Point(row=171, column=9)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.list_folders,function,List 'directories' directly in a given path or recursively in the storage.,"def list_folders(
        self, path: str | Path = ""."", recursive: bool = False
    ) -> list[Path]:
        """"""List 'directories' directly in a given path or recursively in the storage.""""""
        path = self.get_path(path)
        folder_names = set()

        # List objects with the specified prefix and delimiter
        for blob in self._bucket.list_blobs(prefix=path):
            # Remove path prefix and the object name (last part)
            folder = Path(blob.name).relative_to(path).parent
            if not folder or folder == Path("".""):
                continue
            # For non-recursive, only add the first level of folders
            if not recursive:
                folder_names.add(folder.parts[0])
            else:
                # For recursive, need to add all nested folders
                for i in range(len(folder.parts)):
                    folder_names.add(""/"".join(folder.parts[: i + 1]))

        return [Path(f) for f in folder_names]","Point(row=173, column=4)","Point(row=194, column=46)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.delete_file,function,Delete a file in the storage.,"def delete_file(self, path: str | Path) -> None:
        """"""Delete a file in the storage.""""""
        path = self.get_path(path)
        blob = self._bucket.blob(str(path))
        blob.delete()","Point(row=196, column=4)","Point(row=200, column=21)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.delete_dir,function,Delete an empty folder in the storage.,"def delete_dir(self, path: str | Path) -> None:
        """"""Delete an empty folder in the storage.""""""
        # Since GCS does not have directories, we don't need to do anything
        pass","Point(row=202, column=4)","Point(row=205, column=12)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.exists,function,Check if a file or folder exists in GCS storage.,"def exists(self, path: str | Path) -> bool:
        """"""Check if a file or folder exists in GCS storage.""""""
        path = self.get_path(path)
        # Check for exact blob match (file)
        blob = self._bucket.blob(str(path))
        if blob.exists():
            return True
        # Check for any blobs with prefix (folder)
        prefix = f""{str(path).rstrip('/')}/""
        blobs = self._bucket.list_blobs(prefix=prefix, max_results=1)
        return next(blobs, None) is not None","Point(row=207, column=4)","Point(row=217, column=44)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.make_dir,function,Create a directory in the storage if doesn't exist.,"def make_dir(self, path: str | Path) -> None:
        """"""Create a directory in the storage if doesn't exist.""""""
        # GCS does not have directories, so we don't need to do anything
        pass","Point(row=219, column=4)","Point(row=222, column=12)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.rename,function,Rename a file or folder in the storage.,"def rename(self, old_path: str | Path, new_path: str | Path) -> None:
        """"""Rename a file or folder in the storage.""""""
        old_path = self.get_path(old_path)
        new_path = self.get_path(new_path)
        blob = self._bucket.blob(str(old_path))
        # If the blob with exact name exists, rename it
        if blob.exists():
            self._bucket.rename_blob(blob, new_name=str(new_path))
            return
        # Otherwise, rename all blobs with the prefix (folder)
        for blob in self._bucket.list_blobs(prefix=f""{old_path}/""):
            new_name = str(blob.name).replace(str(old_path), str(new_path), 1)
            self._bucket.rename_blob(blob, new_name=new_name)","Point(row=224, column=4)","Point(row=236, column=61)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.copy,function,Copy a file or folder with all contents in the storage.,"def copy(self, source: str | Path, destination: str | Path) -> None:
        """"""Copy a file or folder with all contents in the storage.""""""
        source = self.get_path(source)
        destination = self.get_path(destination)
        # If the source is a file, copy it
        if self._bucket.blob(str(source)).exists():
            self._bucket.copy_blob(
                self._bucket.blob(str(source)), self._bucket, str(destination)
            )
            return
        # Otherwise, copy all blobs with the prefix (folder)
        for blob in self._bucket.list_blobs(prefix=f""{source}/""):
            new_name = str(blob.name).replace(str(source), str(destination), 1)
            self._bucket.copy_blob(blob, self._bucket, new_name)","Point(row=238, column=4)","Point(row=251, column=64)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.clone_with_subroot,function,Create a new GCSFileStorage with a subroot of the current storage.,"def clone_with_subroot(self, subroot: str | Path) -> GCSFileStorage:
        """"""Create a new GCSFileStorage with a subroot of the current storage.""""""
        file_storage = GCSFileStorage(
            GCSFileStorageConfiguration(
                root=Path(""/"").joinpath(self.get_path(subroot)),
                bucket=self._bucket_name,
            )
        )
        file_storage._gcs = self._gcs
        file_storage._bucket = self._bucket
        return file_storage","Point(row=253, column=4)","Point(row=263, column=27)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
GCSFileStorage.__repr__,function,,"def __repr__(self) -> str:
        return f""{__class__.__name__}(bucket='{self._bucket_name}', root={self._root})""","Point(row=265, column=4)","Point(row=266, column=87)",GCSFileStorage,classic/forge/forge/file_storage/gcs.py
LocalFileStorage,class,A class that represents a file storage.,"class LocalFileStorage(FileStorage):
    """"""A class that represents a file storage.""""""

    def __init__(self, config: FileStorageConfiguration):
        self._root = config.root.resolve()
        self._restrict_to_root = config.restrict_to_root
        self.make_dir(self.root)
        super().__init__()

    @property
    def root(self) -> Path:
        """"""The root directory of the file storage.""""""
        return self._root

    @property
    def restrict_to_root(self) -> bool:
        """"""Whether to restrict generated paths to the root.""""""
        return self._restrict_to_root

    @property
    def is_local(self) -> bool:
        """"""Whether the storage is local (i.e. on the same machine, not cloud-based).""""""
        return True

    def initialize(self) -> None:
        self.root.mkdir(exist_ok=True, parents=True)

    @overload
    def open_file(
        self,
        path: str | Path,
        mode: Literal[""w"", ""r""] = ""r"",
        binary: Literal[False] = False,
    ) -> TextIO:
        ...

    @overload
    def open_file(
        self, path: str | Path, mode: Literal[""w"", ""r""], binary: Literal[True]
    ) -> BinaryIO:
        ...

    @overload
    def open_file(self, path: str | Path, *, binary: Literal[True]) -> BinaryIO:
        ...

    @overload
    def open_file(
        self, path: str | Path, mode: Literal[""w"", ""r""] = ""r"", binary: bool = False
    ) -> TextIO | BinaryIO:
        ...

    def open_file(
        self, path: str | Path, mode: Literal[""w"", ""r""] = ""r"", binary: bool = False
    ) -> TextIO | BinaryIO:
        """"""Open a file in the storage.""""""
        return self._open_file(path, f""{mode}b"" if binary else mode)

    def _open_file(self, path: str | Path, mode: str) -> TextIO | BinaryIO:
        full_path = self.get_path(path)
        if any(m in mode for m in (""w"", ""a"", ""x"")):
            full_path.parent.mkdir(parents=True, exist_ok=True)
        return open(full_path, mode)  # type: ignore

    @overload
    def read_file(self, path: str | Path, binary: Literal[False] = False) -> str:
        """"""Read a file in the storage as text.""""""
        ...

    @overload
    def read_file(self, path: str | Path, binary: Literal[True]) -> bytes:
        """"""Read a file in the storage as binary.""""""
        ...

    @overload
    def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:
        """"""Read a file in the storage.""""""
        ...

    def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:
        """"""Read a file in the storage.""""""
        with self._open_file(path, ""rb"" if binary else ""r"") as file:
            return file.read()

    async def write_file(self, path: str | Path, content: str | bytes) -> None:
        """"""Write to a file in the storage.""""""
        with self._open_file(path, ""wb"" if type(content) is bytes else ""w"") as file:
            file.write(content)  # type: ignore

        if self.on_write_file:
            path = Path(path)
            if path.is_absolute():
                path = path.relative_to(self.root)
            res = self.on_write_file(path)
            if inspect.isawaitable(res):
                await res

    def list_files(self, path: str | Path = ""."") -> list[Path]:
        """"""List all files (recursively) in a directory in the storage.""""""
        path = self.get_path(path)
        return [file.relative_to(path) for file in path.rglob(""*"") if file.is_file()]

    def list_folders(
        self, path: str | Path = ""."", recursive: bool = False
    ) -> list[Path]:
        """"""List directories directly in a given path or recursively.""""""
        path = self.get_path(path)
        if recursive:
            return [
                folder.relative_to(path)
                for folder in path.rglob(""*"")
                if folder.is_dir()
            ]
        else:
            return [
                folder.relative_to(path) for folder in path.iterdir() if folder.is_dir()
            ]

    def delete_file(self, path: str | Path) -> None:
        """"""Delete a file in the storage.""""""
        full_path = self.get_path(path)
        full_path.unlink()

    def delete_dir(self, path: str | Path) -> None:
        """"""Delete an empty folder in the storage.""""""
        full_path = self.get_path(path)
        full_path.rmdir()

    def exists(self, path: str | Path) -> bool:
        """"""Check if a file or folder exists in the storage.""""""
        return self.get_path(path).exists()

    def make_dir(self, path: str | Path) -> None:
        """"""Create a directory in the storage if doesn't exist.""""""
        full_path = self.get_path(path)
        full_path.mkdir(exist_ok=True, parents=True)

    def rename(self, old_path: str | Path, new_path: str | Path) -> None:
        """"""Rename a file or folder in the storage.""""""
        old_path = self.get_path(old_path)
        new_path = self.get_path(new_path)
        old_path.rename(new_path)

    def copy(self, source: str | Path, destination: str | Path) -> None:
        """"""Copy a file or folder with all contents in the storage.""""""
        source = self.get_path(source)
        destination = self.get_path(destination)
        if source.is_file():
            destination.write_bytes(source.read_bytes())
        else:
            destination.mkdir(exist_ok=True, parents=True)
            for file in source.rglob(""*""):
                if file.is_file():
                    target = destination / file.relative_to(source)
                    target.parent.mkdir(exist_ok=True, parents=True)
                    target.write_bytes(file.read_bytes())

    def clone_with_subroot(self, subroot: str | Path) -> FileStorage:
        """"""Create a new LocalFileStorage with a subroot of the current storage.""""""
        return LocalFileStorage(
            FileStorageConfiguration(
                root=self.get_path(subroot),
                restrict_to_root=self.restrict_to_root,
            )
        )

    @contextmanager
    def mount(self, path: str | Path = ""."") -> Generator[Path, Any, None]:
        """"""Mount the file storage and provide a local path.""""""
        # No need to do anything for local storage
        yield Path(self.get_path(""."")).absolute()","Point(row=17, column=0)","Point(row=187, column=49)",,classic/forge/forge/file_storage/local.py
LocalFileStorage.__init__,function,,"def __init__(self, config: FileStorageConfiguration):
        self._root = config.root.resolve()
        self._restrict_to_root = config.restrict_to_root
        self.make_dir(self.root)
        super().__init__()","Point(row=20, column=4)","Point(row=24, column=26)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.root,function,The root directory of the file storage.,"def root(self) -> Path:
        """"""The root directory of the file storage.""""""
        return self._root","Point(row=27, column=4)","Point(row=29, column=25)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.restrict_to_root,function,Whether to restrict generated paths to the root.,"def restrict_to_root(self) -> bool:
        """"""Whether to restrict generated paths to the root.""""""
        return self._restrict_to_root","Point(row=32, column=4)","Point(row=34, column=37)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.is_local,function,"Whether the storage is local (i.e. on the same machine, not cloud-based).","def is_local(self) -> bool:
        """"""Whether the storage is local (i.e. on the same machine, not cloud-based).""""""
        return True","Point(row=37, column=4)","Point(row=39, column=19)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.initialize,function,,"def initialize(self) -> None:
        self.root.mkdir(exist_ok=True, parents=True)","Point(row=41, column=4)","Point(row=42, column=52)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.open_file,function,,"def open_file(
        self,
        path: str | Path,
        mode: Literal[""w"", ""r""] = ""r"",
        binary: Literal[False] = False,
    ) -> TextIO:
        ...","Point(row=45, column=4)","Point(row=51, column=11)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.open_file,function,,"def open_file(
        self, path: str | Path, mode: Literal[""w"", ""r""], binary: Literal[True]
    ) -> BinaryIO:
        ...","Point(row=54, column=4)","Point(row=57, column=11)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.open_file,function,,"def open_file(self, path: str | Path, *, binary: Literal[True]) -> BinaryIO:
        ...","Point(row=60, column=4)","Point(row=61, column=11)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.open_file,function,,"def open_file(
        self, path: str | Path, mode: Literal[""w"", ""r""] = ""r"", binary: bool = False
    ) -> TextIO | BinaryIO:
        ...","Point(row=64, column=4)","Point(row=67, column=11)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.open_file,function,Open a file in the storage.,"def open_file(
        self, path: str | Path, mode: Literal[""w"", ""r""] = ""r"", binary: bool = False
    ) -> TextIO | BinaryIO:
        """"""Open a file in the storage.""""""
        return self._open_file(path, f""{mode}b"" if binary else mode)","Point(row=69, column=4)","Point(row=73, column=68)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage._open_file,function,,"def _open_file(self, path: str | Path, mode: str) -> TextIO | BinaryIO:
        full_path = self.get_path(path)
        if any(m in mode for m in (""w"", ""a"", ""x"")):
            full_path.parent.mkdir(parents=True, exist_ok=True)
        return open(full_path, mode)  # type: ignore","Point(row=75, column=4)","Point(row=79, column=52)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.read_file,function,Read a file in the storage as text.,"def read_file(self, path: str | Path, binary: Literal[False] = False) -> str:
        """"""Read a file in the storage as text.""""""
        ...","Point(row=82, column=4)","Point(row=84, column=11)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.read_file,function,Read a file in the storage as binary.,"def read_file(self, path: str | Path, binary: Literal[True]) -> bytes:
        """"""Read a file in the storage as binary.""""""
        ...","Point(row=87, column=4)","Point(row=89, column=11)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.read_file,function,Read a file in the storage.,"def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:
        """"""Read a file in the storage.""""""
        ...","Point(row=92, column=4)","Point(row=94, column=11)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.read_file,function,Read a file in the storage.,"def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:
        """"""Read a file in the storage.""""""
        with self._open_file(path, ""rb"" if binary else ""r"") as file:
            return file.read()","Point(row=96, column=4)","Point(row=99, column=30)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.write_file,function,Write to a file in the storage.,"async def write_file(self, path: str | Path, content: str | bytes) -> None:
        """"""Write to a file in the storage.""""""
        with self._open_file(path, ""wb"" if type(content) is bytes else ""w"") as file:
            file.write(content)  # type: ignore

        if self.on_write_file:
            path = Path(path)
            if path.is_absolute():
                path = path.relative_to(self.root)
            res = self.on_write_file(path)
            if inspect.isawaitable(res):
                await res","Point(row=101, column=4)","Point(row=112, column=25)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.list_files,function,List all files (recursively) in a directory in the storage.,"def list_files(self, path: str | Path = ""."") -> list[Path]:
        """"""List all files (recursively) in a directory in the storage.""""""
        path = self.get_path(path)
        return [file.relative_to(path) for file in path.rglob(""*"") if file.is_file()]","Point(row=114, column=4)","Point(row=117, column=85)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.list_folders,function,List directories directly in a given path or recursively.,"def list_folders(
        self, path: str | Path = ""."", recursive: bool = False
    ) -> list[Path]:
        """"""List directories directly in a given path or recursively.""""""
        path = self.get_path(path)
        if recursive:
            return [
                folder.relative_to(path)
                for folder in path.rglob(""*"")
                if folder.is_dir()
            ]
        else:
            return [
                folder.relative_to(path) for folder in path.iterdir() if folder.is_dir()
            ]","Point(row=119, column=4)","Point(row=133, column=13)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.delete_file,function,Delete a file in the storage.,"def delete_file(self, path: str | Path) -> None:
        """"""Delete a file in the storage.""""""
        full_path = self.get_path(path)
        full_path.unlink()","Point(row=135, column=4)","Point(row=138, column=26)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.delete_dir,function,Delete an empty folder in the storage.,"def delete_dir(self, path: str | Path) -> None:
        """"""Delete an empty folder in the storage.""""""
        full_path = self.get_path(path)
        full_path.rmdir()","Point(row=140, column=4)","Point(row=143, column=25)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.exists,function,Check if a file or folder exists in the storage.,"def exists(self, path: str | Path) -> bool:
        """"""Check if a file or folder exists in the storage.""""""
        return self.get_path(path).exists()","Point(row=145, column=4)","Point(row=147, column=43)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.make_dir,function,Create a directory in the storage if doesn't exist.,"def make_dir(self, path: str | Path) -> None:
        """"""Create a directory in the storage if doesn't exist.""""""
        full_path = self.get_path(path)
        full_path.mkdir(exist_ok=True, parents=True)","Point(row=149, column=4)","Point(row=152, column=52)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.rename,function,Rename a file or folder in the storage.,"def rename(self, old_path: str | Path, new_path: str | Path) -> None:
        """"""Rename a file or folder in the storage.""""""
        old_path = self.get_path(old_path)
        new_path = self.get_path(new_path)
        old_path.rename(new_path)","Point(row=154, column=4)","Point(row=158, column=33)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.copy,function,Copy a file or folder with all contents in the storage.,"def copy(self, source: str | Path, destination: str | Path) -> None:
        """"""Copy a file or folder with all contents in the storage.""""""
        source = self.get_path(source)
        destination = self.get_path(destination)
        if source.is_file():
            destination.write_bytes(source.read_bytes())
        else:
            destination.mkdir(exist_ok=True, parents=True)
            for file in source.rglob(""*""):
                if file.is_file():
                    target = destination / file.relative_to(source)
                    target.parent.mkdir(exist_ok=True, parents=True)
                    target.write_bytes(file.read_bytes())","Point(row=160, column=4)","Point(row=172, column=57)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.clone_with_subroot,function,Create a new LocalFileStorage with a subroot of the current storage.,"def clone_with_subroot(self, subroot: str | Path) -> FileStorage:
        """"""Create a new LocalFileStorage with a subroot of the current storage.""""""
        return LocalFileStorage(
            FileStorageConfiguration(
                root=self.get_path(subroot),
                restrict_to_root=self.restrict_to_root,
            )
        )","Point(row=174, column=4)","Point(row=181, column=9)",LocalFileStorage,classic/forge/forge/file_storage/local.py
LocalFileStorage.mount,function,Mount the file storage and provide a local path.,"def mount(self, path: str | Path = ""."") -> Generator[Path, Any, None]:
        """"""Mount the file storage and provide a local path.""""""
        # No need to do anything for local storage
        yield Path(self.get_path(""."")).absolute()","Point(row=184, column=4)","Point(row=187, column=49)",LocalFileStorage,classic/forge/forge/file_storage/local.py
gcs_bucket_name,function,,"def gcs_bucket_name() -> str:
    return f""test-bucket-{str(uuid.uuid4())[:8]}""","Point(row=21, column=0)","Point(row=22, column=49)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
gcs_root,function,,"def gcs_root() -> Path:
    return Path(""/workspaces/AutoGPT-some-unique-task-id"")","Point(row=26, column=0)","Point(row=27, column=58)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
gcs_storage_uninitialized,function,,"def gcs_storage_uninitialized(gcs_bucket_name: str, gcs_root: Path):
    os.environ[""STORAGE_BUCKET""] = gcs_bucket_name
    storage_config = GCSFileStorageConfiguration.from_env()
    storage_config.root = gcs_root
    storage = GCSFileStorage(storage_config)
    yield storage  # type: ignore
    del os.environ[""STORAGE_BUCKET""]","Point(row=31, column=0)","Point(row=37, column=36)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
test_initialize,function,,"def test_initialize(gcs_bucket_name: str, gcs_storage_uninitialized: GCSFileStorage):
    gcs = gcs_storage_uninitialized._gcs

    # test that the bucket doesn't exist yet
    with pytest.raises(NotFound):
        gcs.get_bucket(gcs_bucket_name)

    gcs_storage_uninitialized.initialize()

    # test that the bucket has been created
    bucket = gcs.get_bucket(gcs_bucket_name)

    # clean up
    bucket.delete(force=True)","Point(row=40, column=0)","Point(row=53, column=29)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
gcs_storage,function,,"def gcs_storage(gcs_storage_uninitialized: GCSFileStorage):
    (gcs_storage := gcs_storage_uninitialized).initialize()
    yield gcs_storage  # type: ignore

    # Empty & delete the test bucket
    gcs_storage._bucket.delete(force=True)","Point(row=57, column=0)","Point(row=62, column=42)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
test_workspace_bucket_name,function,,"def test_workspace_bucket_name(
    gcs_storage: GCSFileStorage,
    gcs_bucket_name: str,
):
    assert gcs_storage._bucket.name == gcs_bucket_name","Point(row=65, column=0)","Point(row=69, column=54)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
gcs_storage_with_files,function,,"async def gcs_storage_with_files(gcs_storage: GCSFileStorage):
    for file_name, file_content in TEST_FILES:
        gcs_storage._bucket.blob(
            str(gcs_storage.get_path(file_name))
        ).upload_from_string(file_content)
    yield gcs_storage  # type: ignore","Point(row=82, column=0)","Point(row=87, column=37)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
test_read_file,function,,"async def test_read_file(gcs_storage_with_files: GCSFileStorage):
    for file_name, file_content in TEST_FILES:
        content = gcs_storage_with_files.read_file(file_name)
        assert content == file_content

    with pytest.raises(NotFound):
        gcs_storage_with_files.read_file(""non_existent_file"")","Point(row=91, column=0)","Point(row=97, column=61)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
test_list_files,function,,"def test_list_files(gcs_storage_with_files: GCSFileStorage):
    # List at root level
    assert (
        files := gcs_storage_with_files.list_files()
    ) == gcs_storage_with_files.list_files()
    assert len(files) > 0
    assert set(files) == set(Path(file_name) for file_name, _ in TEST_FILES)

    # List at nested path
    assert (
        nested_files := gcs_storage_with_files.list_files(NESTED_DIR)
    ) == gcs_storage_with_files.list_files(NESTED_DIR)
    assert len(nested_files) > 0
    assert set(nested_files) == set(
        p.relative_to(NESTED_DIR)
        for file_name, _ in TEST_FILES
        if (p := Path(file_name)).is_relative_to(NESTED_DIR)
    )","Point(row=100, column=0)","Point(row=117, column=5)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
test_list_folders,function,,"def test_list_folders(gcs_storage_with_files: GCSFileStorage):
    # List recursive
    folders = gcs_storage_with_files.list_folders(recursive=True)
    assert len(folders) > 0
    assert set(folders) == {
        Path(""existing""),
        Path(""existing/test""),
        Path(""existing/test/dir""),
    }
    # List non-recursive
    folders = gcs_storage_with_files.list_folders(recursive=False)
    assert len(folders) > 0
    assert set(folders) == {Path(""existing"")}","Point(row=120, column=0)","Point(row=132, column=45)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
test_write_read_file,function,,"async def test_write_read_file(gcs_storage: GCSFileStorage):
    await gcs_storage.write_file(""test_file"", ""test_content"")
    assert gcs_storage.read_file(""test_file"") == ""test_content""","Point(row=136, column=0)","Point(row=138, column=63)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
test_overwrite_file,function,,"async def test_overwrite_file(gcs_storage_with_files: GCSFileStorage):
    for file_name, _ in TEST_FILES:
        await gcs_storage_with_files.write_file(file_name, ""new content"")
        assert gcs_storage_with_files.read_file(file_name) == ""new content""","Point(row=142, column=0)","Point(row=145, column=75)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
test_delete_file,function,,"def test_delete_file(gcs_storage_with_files: GCSFileStorage):
    for file_to_delete, _ in TEST_FILES:
        gcs_storage_with_files.delete_file(file_to_delete)
        assert not gcs_storage_with_files.exists(file_to_delete)","Point(row=148, column=0)","Point(row=151, column=64)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
test_exists,function,,"def test_exists(gcs_storage_with_files: GCSFileStorage):
    for file_name, _ in TEST_FILES:
        assert gcs_storage_with_files.exists(file_name)

    assert not gcs_storage_with_files.exists(""non_existent_file"")","Point(row=154, column=0)","Point(row=158, column=65)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
test_rename_file,function,,"def test_rename_file(gcs_storage_with_files: GCSFileStorage):
    for file_name, _ in TEST_FILES:
        new_name = str(file_name) + ""_renamed""
        gcs_storage_with_files.rename(file_name, new_name)
        assert gcs_storage_with_files.exists(new_name)
        assert not gcs_storage_with_files.exists(file_name)","Point(row=161, column=0)","Point(row=166, column=59)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
test_rename_dir,function,,"def test_rename_dir(gcs_storage_with_files: GCSFileStorage):
    gcs_storage_with_files.rename(NESTED_DIR, ""existing/test/dir_renamed"")
    assert gcs_storage_with_files.exists(""existing/test/dir_renamed"")
    assert not gcs_storage_with_files.exists(NESTED_DIR)","Point(row=169, column=0)","Point(row=172, column=56)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
test_clone,function,,"def test_clone(gcs_storage_with_files: GCSFileStorage, gcs_root: Path):
    cloned = gcs_storage_with_files.clone_with_subroot(""existing/test"")
    assert cloned.root == gcs_root / Path(""existing/test"")
    assert cloned._bucket.name == gcs_storage_with_files._bucket.name
    assert cloned.exists(""dir"")
    assert cloned.exists(""dir/test_file_4"")","Point(row=175, column=0)","Point(row=180, column=43)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
test_copy_file,function,,"async def test_copy_file(storage: GCSFileStorage):
    await storage.write_file(""test_file.txt"", ""test content"")
    storage.copy(""test_file.txt"", ""test_file_copy.txt"")
    storage.make_dir(""dir"")
    storage.copy(""test_file.txt"", ""dir/test_file_copy.txt"")
    assert storage.read_file(""test_file_copy.txt"") == ""test content""
    assert storage.read_file(""dir/test_file_copy.txt"") == ""test content""","Point(row=184, column=0)","Point(row=190, column=72)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
test_copy_dir,function,,"async def test_copy_dir(storage: GCSFileStorage):
    storage.make_dir(""dir"")
    storage.make_dir(""dir/sub_dir"")
    await storage.write_file(""dir/test_file.txt"", ""test content"")
    await storage.write_file(""dir/sub_dir/test_file.txt"", ""test content"")
    storage.copy(""dir"", ""dir_copy"")
    assert storage.read_file(""dir_copy/test_file.txt"") == ""test content""
    assert storage.read_file(""dir_copy/sub_dir/test_file.txt"") == ""test content""","Point(row=194, column=0)","Point(row=201, column=80)",,classic/forge/forge/file_storage/test_gcs_file_storage.py
FileStorageBackendName,class,,"class FileStorageBackendName(str, enum.Enum):
    LOCAL = ""local""
    GCS = ""gcs""
    S3 = ""s3""","Point(row=6, column=0)","Point(row=9, column=13)",,classic/forge/forge/file_storage/__init__.py
get_storage,function,,"def get_storage(
    backend: FileStorageBackendName,
    root_path: Path = Path("".""),
    restrict_to_root: bool = True,
) -> FileStorage:
    match backend:
        case FileStorageBackendName.LOCAL:
            from .local import FileStorageConfiguration, LocalFileStorage

            config = FileStorageConfiguration.from_env()
            config.root = root_path
            config.restrict_to_root = restrict_to_root
            return LocalFileStorage(config)
        case FileStorageBackendName.S3:
            from .s3 import S3FileStorage, S3FileStorageConfiguration

            config = S3FileStorageConfiguration.from_env()
            config.root = root_path
            return S3FileStorage(config)
        case FileStorageBackendName.GCS:
            from .gcs import GCSFileStorage, GCSFileStorageConfiguration

            config = GCSFileStorageConfiguration.from_env()
            config.root = root_path
            return GCSFileStorage(config)","Point(row=12, column=0)","Point(row=36, column=41)",,classic/forge/forge/file_storage/__init__.py
s3_bucket_name,function,,"def s3_bucket_name() -> str:
    return f""test-bucket-{str(uuid.uuid4())[:8]}""","Point(row=15, column=0)","Point(row=16, column=49)",,classic/forge/forge/file_storage/test_s3_file_storage.py
s3_root,function,,"def s3_root() -> Path:
    return Path(""/workspaces/AutoGPT-some-unique-task-id"")","Point(row=20, column=0)","Point(row=21, column=58)",,classic/forge/forge/file_storage/test_s3_file_storage.py
s3_storage_uninitialized,function,,"def s3_storage_uninitialized(s3_bucket_name: str, s3_root: Path):
    os.environ[""STORAGE_BUCKET""] = s3_bucket_name
    storage_config = S3FileStorageConfiguration.from_env()
    storage_config.root = s3_root
    storage = S3FileStorage(storage_config)
    yield storage  # type: ignore
    del os.environ[""STORAGE_BUCKET""]","Point(row=25, column=0)","Point(row=31, column=36)",,classic/forge/forge/file_storage/test_s3_file_storage.py
test_initialize,function,,"def test_initialize(s3_bucket_name: str, s3_storage_uninitialized: S3FileStorage):
    s3 = s3_storage_uninitialized._s3

    # test that the bucket doesn't exist yet
    with pytest.raises(ClientError):
        s3.meta.client.head_bucket(Bucket=s3_bucket_name)  # pyright: ignore

    s3_storage_uninitialized.initialize()

    # test that the bucket has been created
    s3.meta.client.head_bucket(Bucket=s3_bucket_name)  # pyright: ignore
    # FIXME: remove the ""pyright: ignore"" comments after moving this test file to forge","Point(row=34, column=0)","Point(row=45, column=87)",,classic/forge/forge/file_storage/test_s3_file_storage.py
test_workspace_bucket_name,function,,"def test_workspace_bucket_name(
    s3_storage: S3FileStorage,
    s3_bucket_name: str,
):
    assert s3_storage._bucket.name == s3_bucket_name","Point(row=48, column=0)","Point(row=52, column=52)",,classic/forge/forge/file_storage/test_s3_file_storage.py
s3_storage,function,,"def s3_storage(s3_storage_uninitialized: S3FileStorage):
    (s3_storage := s3_storage_uninitialized).initialize()
    yield s3_storage  # type: ignore

    # Empty & delete the test bucket
    s3_storage._bucket.objects.all().delete()
    s3_storage._bucket.delete()","Point(row=56, column=0)","Point(row=62, column=31)",,classic/forge/forge/file_storage/test_s3_file_storage.py
s3_storage_with_files,function,,"async def s3_storage_with_files(s3_storage: S3FileStorage):
    for file_name, file_content in TEST_FILES:
        s3_storage._bucket.Object(str(s3_storage.get_path(file_name))).put(
            Body=file_content
        )
    yield s3_storage  # type: ignore","Point(row=75, column=0)","Point(row=80, column=36)",,classic/forge/forge/file_storage/test_s3_file_storage.py
test_read_file,function,,"async def test_read_file(s3_storage_with_files: S3FileStorage):
    for file_name, file_content in TEST_FILES:
        content = s3_storage_with_files.read_file(file_name)
        assert content == file_content

    with pytest.raises(ClientError):
        s3_storage_with_files.read_file(""non_existent_file"")","Point(row=84, column=0)","Point(row=90, column=60)",,classic/forge/forge/file_storage/test_s3_file_storage.py
test_list_files,function,,"def test_list_files(s3_storage_with_files: S3FileStorage):
    # List at root level
    assert (
        files := s3_storage_with_files.list_files()
    ) == s3_storage_with_files.list_files()
    assert len(files) > 0
    assert set(files) == set(Path(file_name) for file_name, _ in TEST_FILES)

    # List at nested path
    assert (
        nested_files := s3_storage_with_files.list_files(NESTED_DIR)
    ) == s3_storage_with_files.list_files(NESTED_DIR)
    assert len(nested_files) > 0
    assert set(nested_files) == set(
        p.relative_to(NESTED_DIR)
        for file_name, _ in TEST_FILES
        if (p := Path(file_name)).is_relative_to(NESTED_DIR)
    )","Point(row=93, column=0)","Point(row=110, column=5)",,classic/forge/forge/file_storage/test_s3_file_storage.py
test_list_folders,function,,"def test_list_folders(s3_storage_with_files: S3FileStorage):
    # List recursive
    folders = s3_storage_with_files.list_folders(recursive=True)
    assert len(folders) > 0
    assert set(folders) == {
        Path(""existing""),
        Path(""existing/test""),
        Path(""existing/test/dir""),
    }
    # List non-recursive
    folders = s3_storage_with_files.list_folders(recursive=False)
    assert len(folders) > 0
    assert set(folders) == {Path(""existing"")}","Point(row=113, column=0)","Point(row=125, column=45)",,classic/forge/forge/file_storage/test_s3_file_storage.py
test_write_read_file,function,,"async def test_write_read_file(s3_storage: S3FileStorage):
    await s3_storage.write_file(""test_file"", ""test_content"")
    assert s3_storage.read_file(""test_file"") == ""test_content""","Point(row=129, column=0)","Point(row=131, column=62)",,classic/forge/forge/file_storage/test_s3_file_storage.py
test_overwrite_file,function,,"async def test_overwrite_file(s3_storage_with_files: S3FileStorage):
    for file_name, _ in TEST_FILES:
        await s3_storage_with_files.write_file(file_name, ""new content"")
        assert s3_storage_with_files.read_file(file_name) == ""new content""","Point(row=135, column=0)","Point(row=138, column=74)",,classic/forge/forge/file_storage/test_s3_file_storage.py
test_delete_file,function,,"def test_delete_file(s3_storage_with_files: S3FileStorage):
    for file_to_delete, _ in TEST_FILES:
        s3_storage_with_files.delete_file(file_to_delete)
        with pytest.raises(ClientError):
            s3_storage_with_files.read_file(file_to_delete)","Point(row=141, column=0)","Point(row=145, column=59)",,classic/forge/forge/file_storage/test_s3_file_storage.py
test_exists,function,,"def test_exists(s3_storage_with_files: S3FileStorage):
    for file_name, _ in TEST_FILES:
        assert s3_storage_with_files.exists(file_name)

    assert not s3_storage_with_files.exists(""non_existent_file"")","Point(row=148, column=0)","Point(row=152, column=64)",,classic/forge/forge/file_storage/test_s3_file_storage.py
test_rename_file,function,,"def test_rename_file(s3_storage_with_files: S3FileStorage):
    for file_name, _ in TEST_FILES:
        new_name = str(file_name) + ""_renamed""
        s3_storage_with_files.rename(file_name, new_name)
        assert s3_storage_with_files.exists(new_name)
        assert not s3_storage_with_files.exists(file_name)","Point(row=155, column=0)","Point(row=160, column=58)",,classic/forge/forge/file_storage/test_s3_file_storage.py
test_rename_dir,function,,"def test_rename_dir(s3_storage_with_files: S3FileStorage):
    s3_storage_with_files.rename(NESTED_DIR, ""existing/test/dir_renamed"")
    assert s3_storage_with_files.exists(""existing/test/dir_renamed"")
    assert not s3_storage_with_files.exists(NESTED_DIR)","Point(row=163, column=0)","Point(row=166, column=55)",,classic/forge/forge/file_storage/test_s3_file_storage.py
test_clone,function,,"def test_clone(s3_storage_with_files: S3FileStorage, s3_root: Path):
    cloned = s3_storage_with_files.clone_with_subroot(""existing/test"")
    assert cloned.root == s3_root / Path(""existing/test"")
    assert cloned._bucket.name == s3_storage_with_files._bucket.name
    assert cloned.exists(""dir"")
    assert cloned.exists(""dir/test_file_4"")","Point(row=169, column=0)","Point(row=174, column=43)",,classic/forge/forge/file_storage/test_s3_file_storage.py
test_copy_file,function,,"async def test_copy_file(storage: S3FileStorage):
    await storage.write_file(""test_file.txt"", ""test content"")
    storage.copy(""test_file.txt"", ""test_file_copy.txt"")
    storage.make_dir(""dir"")
    storage.copy(""test_file.txt"", ""dir/test_file_copy.txt"")
    assert storage.read_file(""test_file_copy.txt"") == ""test content""
    assert storage.read_file(""dir/test_file_copy.txt"") == ""test content""","Point(row=178, column=0)","Point(row=184, column=72)",,classic/forge/forge/file_storage/test_s3_file_storage.py
test_copy_dir,function,,"async def test_copy_dir(storage: S3FileStorage):
    storage.make_dir(""dir"")
    storage.make_dir(""dir/sub_dir"")
    await storage.write_file(""dir/test_file.txt"", ""test content"")
    await storage.write_file(""dir/sub_dir/test_file.txt"", ""test content"")
    storage.copy(""dir"", ""dir_copy"")
    assert storage.read_file(""dir_copy/test_file.txt"") == ""test content""
    assert storage.read_file(""dir_copy/sub_dir/test_file.txt"") == ""test content""","Point(row=188, column=0)","Point(row=195, column=80)",,classic/forge/forge/file_storage/test_s3_file_storage.py
storage_root,function,,"def storage_root(tmp_path):
    return tmp_path / ""data""","Point(row=58, column=0)","Point(row=59, column=28)",,classic/forge/forge/file_storage/test_local_file_storage.py
storage,function,,"def storage(storage_root):
    return LocalFileStorage(
        FileStorageConfiguration(root=storage_root, restrict_to_root=True)
    )","Point(row=63, column=0)","Point(row=66, column=5)",,classic/forge/forge/file_storage/test_local_file_storage.py
content,function,,"def content():
    return ""test content""","Point(row=70, column=0)","Point(row=71, column=25)",,classic/forge/forge/file_storage/test_local_file_storage.py
accessible_path,function,,"def accessible_path(request):
    return request.param","Point(row=75, column=0)","Point(row=76, column=24)",,classic/forge/forge/file_storage/test_local_file_storage.py
inaccessible_path,function,,"def inaccessible_path(request):
    return request.param","Point(row=80, column=0)","Point(row=81, column=24)",,classic/forge/forge/file_storage/test_local_file_storage.py
file_path,function,,"def file_path(request):
    return request.param","Point(row=85, column=0)","Point(row=86, column=24)",,classic/forge/forge/file_storage/test_local_file_storage.py
test_open_file,function,,"async def test_open_file(file_path: Path, content: str, storage: LocalFileStorage):
    if file_path.parent:
        storage.make_dir(file_path.parent)
    await storage.write_file(file_path, content)
    file = storage.open_file(file_path)
    assert file.read() == content
    file.close()
    storage.delete_file(file_path)","Point(row=90, column=0)","Point(row=97, column=34)",,classic/forge/forge/file_storage/test_local_file_storage.py
test_write_read_file,function,,"async def test_write_read_file(content: str, storage: LocalFileStorage):
    await storage.write_file(""test_file.txt"", content)
    assert storage.read_file(""test_file.txt"") == content","Point(row=101, column=0)","Point(row=103, column=56)",,classic/forge/forge/file_storage/test_local_file_storage.py
test_list_files,function,,"async def test_list_files(content: str, storage: LocalFileStorage):
    storage.make_dir(""dir"")
    storage.make_dir(""dir/sub_dir"")
    await storage.write_file(""test_file.txt"", content)
    await storage.write_file(""dir/test_file.txt"", content)
    await storage.write_file(""dir/test_file2.txt"", content)
    await storage.write_file(""dir/sub_dir/test_file.txt"", content)
    files = storage.list_files()
    assert Path(""test_file.txt"") in files
    assert Path(""dir/test_file.txt"") in files
    assert Path(""dir/test_file2.txt"") in files
    assert Path(""dir/sub_dir/test_file.txt"") in files
    storage.delete_file(""test_file.txt"")
    storage.delete_file(""dir/test_file.txt"")
    storage.delete_file(""dir/test_file2.txt"")
    storage.delete_file(""dir/sub_dir/test_file.txt"")
    storage.delete_dir(""dir/sub_dir"")
    storage.delete_dir(""dir"")","Point(row=107, column=0)","Point(row=124, column=29)",,classic/forge/forge/file_storage/test_local_file_storage.py
test_list_folders,function,,"async def test_list_folders(content: str, storage: LocalFileStorage):
    storage.make_dir(""dir"")
    storage.make_dir(""dir/sub_dir"")
    await storage.write_file(""dir/test_file.txt"", content)
    await storage.write_file(""dir/sub_dir/test_file.txt"", content)
    folders = storage.list_folders(recursive=False)
    folders_recursive = storage.list_folders(recursive=True)
    assert Path(""dir"") in folders
    assert Path(""dir/sub_dir"") not in folders
    assert Path(""dir"") in folders_recursive
    assert Path(""dir/sub_dir"") in folders_recursive
    storage.delete_file(""dir/test_file.txt"")
    storage.delete_file(""dir/sub_dir/test_file.txt"")
    storage.delete_dir(""dir/sub_dir"")
    storage.delete_dir(""dir"")","Point(row=128, column=0)","Point(row=142, column=29)",,classic/forge/forge/file_storage/test_local_file_storage.py
test_exists_delete_file,function,,"async def test_exists_delete_file(
    file_path: Path, content: str, storage: LocalFileStorage
):
    if file_path.parent:
        storage.make_dir(file_path.parent)
    await storage.write_file(file_path, content)
    assert storage.exists(file_path)
    storage.delete_file(file_path)
    assert not storage.exists(file_path)","Point(row=146, column=0)","Point(row=154, column=40)",,classic/forge/forge/file_storage/test_local_file_storage.py
test_make_delete_dir,function,,"def test_make_delete_dir(request, storage: LocalFileStorage):
    storage.make_dir(request)
    assert storage.exists(request)
    storage.delete_dir(request)
    assert not storage.exists(request)","Point(row=158, column=0)","Point(row=162, column=38)",,classic/forge/forge/file_storage/test_local_file_storage.py
test_rename,function,,"async def test_rename(file_path: Path, content: str, storage: LocalFileStorage):
    if file_path.parent:
        storage.make_dir(file_path.parent)
    await storage.write_file(file_path, content)
    assert storage.exists(file_path)
    storage.rename(file_path, Path(str(file_path) + ""_renamed""))
    assert not storage.exists(file_path)
    assert storage.exists(Path(str(file_path) + ""_renamed""))","Point(row=166, column=0)","Point(row=173, column=60)",,classic/forge/forge/file_storage/test_local_file_storage.py
test_clone_with_subroot,function,,"def test_clone_with_subroot(storage: LocalFileStorage):
    subroot = storage.clone_with_subroot(""dir"")
    assert subroot.root == storage.root / ""dir""","Point(row=176, column=0)","Point(row=178, column=47)",,classic/forge/forge/file_storage/test_local_file_storage.py
test_get_path_accessible,function,,"def test_get_path_accessible(accessible_path: Path, storage: LocalFileStorage):
    full_path = storage.get_path(accessible_path)
    assert full_path.is_absolute()
    assert full_path.is_relative_to(storage.root)","Point(row=181, column=0)","Point(row=184, column=49)",,classic/forge/forge/file_storage/test_local_file_storage.py
test_get_path_inaccessible,function,,"def test_get_path_inaccessible(inaccessible_path: Path, storage: LocalFileStorage):
    with pytest.raises(ValueError):
        storage.get_path(inaccessible_path)","Point(row=187, column=0)","Point(row=189, column=43)",,classic/forge/forge/file_storage/test_local_file_storage.py
test_copy_file,function,,"async def test_copy_file(storage: LocalFileStorage):
    await storage.write_file(""test_file.txt"", ""test content"")
    storage.copy(""test_file.txt"", ""test_file_copy.txt"")
    storage.make_dir(""dir"")
    storage.copy(""test_file.txt"", ""dir/test_file_copy.txt"")
    assert storage.read_file(""test_file_copy.txt"") == ""test content""
    assert storage.read_file(""dir/test_file_copy.txt"") == ""test content""","Point(row=193, column=0)","Point(row=199, column=72)",,classic/forge/forge/file_storage/test_local_file_storage.py
test_copy_dir,function,,"async def test_copy_dir(storage: LocalFileStorage):
    storage.make_dir(""dir"")
    storage.make_dir(""dir/sub_dir"")
    await storage.write_file(""dir/test_file.txt"", ""test content"")
    await storage.write_file(""dir/sub_dir/test_file.txt"", ""test content"")
    storage.copy(""dir"", ""dir_copy"")
    assert storage.read_file(""dir_copy/test_file.txt"") == ""test content""
    assert storage.read_file(""dir_copy/sub_dir/test_file.txt"") == ""test content""","Point(row=203, column=0)","Point(row=210, column=80)",,classic/forge/forge/file_storage/test_local_file_storage.py
S3FileStorageConfiguration,class,,"class S3FileStorageConfiguration(FileStorageConfiguration):
    bucket: str = UserConfigurable(""autogpt"", from_env=""STORAGE_BUCKET"")
    s3_endpoint_url: Optional[SecretStr] = UserConfigurable(from_env=""S3_ENDPOINT_URL"")","Point(row=29, column=0)","Point(row=31, column=87)",,classic/forge/forge/file_storage/s3.py
S3FileStorage,class,A class that represents an S3 storage.,"class S3FileStorage(FileStorage):
    """"""A class that represents an S3 storage.""""""

    _bucket: mypy_boto3_s3.service_resource.Bucket

    def __init__(self, config: S3FileStorageConfiguration):
        self._bucket_name = config.bucket
        self._root = config.root
        # Add / at the beginning of the root path
        if not self._root.is_absolute():
            self._root = Path(""/"").joinpath(self._root)

        # https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html
        self._s3 = boto3.resource(
            ""s3"",
            endpoint_url=(
                config.s3_endpoint_url.get_secret_value()
                if config.s3_endpoint_url
                else None
            ),
        )

        super().__init__()

    @property
    def root(self) -> Path:
        """"""The root directory of the file storage.""""""
        return self._root

    @property
    def restrict_to_root(self):
        """"""Whether to restrict generated paths to the root.""""""
        return True

    @property
    def is_local(self) -> bool:
        """"""Whether the storage is local (i.e. on the same machine, not cloud-based).""""""
        return False

    def initialize(self) -> None:
        logger.debug(f""Initializing {repr(self)}..."")
        try:
            self._s3.meta.client.head_bucket(Bucket=self._bucket_name)
            self._bucket = self._s3.Bucket(self._bucket_name)
        except botocore.exceptions.ClientError as e:
            if ""(404)"" not in str(e):
                raise
            logger.info(f""Bucket '{self._bucket_name}' does not exist; creating it..."")
            self._bucket = self._s3.create_bucket(Bucket=self._bucket_name)

    def get_path(self, relative_path: str | Path) -> Path:
        # We set S3 root with ""/"" at the beginning
        # but relative_to(""/"") will remove it
        # because we don't actually want it in the storage filenames
        return super().get_path(relative_path).relative_to(""/"")

    def _get_obj(self, path: str | Path) -> mypy_boto3_s3.service_resource.Object:
        """"""Get an S3 object.""""""
        obj = self._bucket.Object(str(path))
        with contextlib.suppress(botocore.exceptions.ClientError):
            obj.load()
        return obj

    @overload
    def open_file(
        self,
        path: str | Path,
        mode: Literal[""r"", ""w""] = ""r"",
        binary: Literal[False] = False,
    ) -> TextIOWrapper:
        ...

    @overload
    def open_file(
        self, path: str | Path, mode: Literal[""r"", ""w""], binary: Literal[True]
    ) -> S3BinaryIOWrapper:
        ...

    @overload
    def open_file(
        self, path: str | Path, *, binary: Literal[True]
    ) -> S3BinaryIOWrapper:
        ...

    @overload
    def open_file(
        self, path: str | Path, mode: Literal[""r"", ""w""] = ""r"", binary: bool = False
    ) -> S3BinaryIOWrapper | TextIOWrapper:
        ...

    def open_file(
        self, path: str | Path, mode: Literal[""r"", ""w""] = ""r"", binary: bool = False
    ) -> TextIOWrapper | S3BinaryIOWrapper:
        """"""Open a file in the storage.""""""
        path = self.get_path(path)
        body = S3BinaryIOWrapper(self._get_obj(path).get()[""Body""], str(path))
        return body if binary else TextIOWrapper(body)

    @overload
    def read_file(self, path: str | Path, binary: Literal[False] = False) -> str:
        """"""Read a file in the storage as text.""""""
        ...

    @overload
    def read_file(self, path: str | Path, binary: Literal[True]) -> bytes:
        """"""Read a file in the storage as binary.""""""
        ...

    @overload
    def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:
        """"""Read a file in the storage.""""""
        ...

    def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:
        """"""Read a file in the storage.""""""
        return self.open_file(path, binary=binary).read()

    async def write_file(self, path: str | Path, content: str | bytes) -> None:
        """"""Write to a file in the storage.""""""
        obj = self._get_obj(self.get_path(path))
        obj.put(Body=content)

        if self.on_write_file:
            path = Path(path)
            if path.is_absolute():
                path = path.relative_to(self.root)
            res = self.on_write_file(path)
            if inspect.isawaitable(res):
                await res

    def list_files(self, path: str | Path = ""."") -> list[Path]:
        """"""List all files (recursively) in a directory in the storage.""""""
        path = self.get_path(path)
        if path == Path("".""):  # root level of bucket
            return [Path(obj.key) for obj in self._bucket.objects.all()]
        else:
            return [
                Path(obj.key).relative_to(path)
                for obj in self._bucket.objects.filter(Prefix=f""{path}/"")
            ]

    def list_folders(
        self, path: str | Path = ""."", recursive: bool = False
    ) -> list[Path]:
        """"""List 'directories' directly in a given path or recursively in the storage.""""""
        path = self.get_path(path)
        folder_names = set()

        # List objects with the specified prefix and delimiter
        for obj_summary in self._bucket.objects.filter(Prefix=str(path)):
            # Remove path prefix and the object name (last part)
            folder = Path(obj_summary.key).relative_to(path).parent
            if not folder or folder == Path("".""):
                continue
            # For non-recursive, only add the first level of folders
            if not recursive:
                folder_names.add(folder.parts[0])
            else:
                # For recursive, need to add all nested folders
                for i in range(len(folder.parts)):
                    folder_names.add(""/"".join(folder.parts[: i + 1]))

        return [Path(f) for f in folder_names]

    def delete_file(self, path: str | Path) -> None:
        """"""Delete a file in the storage.""""""
        path = self.get_path(path)
        obj = self._s3.Object(self._bucket_name, str(path))
        obj.delete()

    def delete_dir(self, path: str | Path) -> None:
        """"""Delete an empty folder in the storage.""""""
        # S3 does not have directories, so we don't need to do anything
        pass

    def exists(self, path: str | Path) -> bool:
        """"""Check if a file or folder exists in S3 storage.""""""
        path = self.get_path(path)
        try:
            # Check for exact object match (file)
            self._s3.meta.client.head_object(Bucket=self._bucket_name, Key=str(path))
            return True
        except botocore.exceptions.ClientError as e:
            if e.response.get(""ResponseMetadata"", {}).get(""HTTPStatusCode"") == 404:
                # If the object does not exist,
                # check for objects with the prefix (folder)
                prefix = f""{str(path).rstrip('/')}/""
                objs = list(self._bucket.objects.filter(Prefix=prefix, MaxKeys=1))
                return len(objs) > 0  # True if any objects exist with the prefix
            else:
                raise  # Re-raise for any other client errors

    def make_dir(self, path: str | Path) -> None:
        """"""Create a directory in the storage if doesn't exist.""""""
        # S3 does not have directories, so we don't need to do anything
        pass

    def rename(self, old_path: str | Path, new_path: str | Path) -> None:
        """"""Rename a file or folder in the storage.""""""
        old_path = str(self.get_path(old_path))
        new_path = str(self.get_path(new_path))

        try:
            # If file exists, rename it
            self._s3.meta.client.head_object(Bucket=self._bucket_name, Key=old_path)
            self._s3.meta.client.copy_object(
                CopySource={""Bucket"": self._bucket_name, ""Key"": old_path},
                Bucket=self._bucket_name,
                Key=new_path,
            )
            self._s3.meta.client.delete_object(Bucket=self._bucket_name, Key=old_path)
        except botocore.exceptions.ClientError as e:
            if e.response.get(""ResponseMetadata"", {}).get(""HTTPStatusCode"") == 404:
                # If the object does not exist,
                # it may be a folder
                prefix = f""{old_path.rstrip('/')}/""
                objs = list(self._bucket.objects.filter(Prefix=prefix))
                for obj in objs:
                    new_key = new_path + obj.key[len(old_path) :]
                    self._s3.meta.client.copy_object(
                        CopySource={""Bucket"": self._bucket_name, ""Key"": obj.key},
                        Bucket=self._bucket_name,
                        Key=new_key,
                    )
                    self._s3.meta.client.delete_object(
                        Bucket=self._bucket_name, Key=obj.key
                    )
            else:
                raise  # Re-raise for any other client errors

    def copy(self, source: str | Path, destination: str | Path) -> None:
        """"""Copy a file or folder with all contents in the storage.""""""
        source = str(self.get_path(source))
        destination = str(self.get_path(destination))

        try:
            # If source is a file, copy it
            self._s3.meta.client.head_object(Bucket=self._bucket_name, Key=source)
            self._s3.meta.client.copy_object(
                CopySource={""Bucket"": self._bucket_name, ""Key"": source},
                Bucket=self._bucket_name,
                Key=destination,
            )
        except botocore.exceptions.ClientError as e:
            if e.response.get(""ResponseMetadata"", {}).get(""HTTPStatusCode"") == 404:
                # If the object does not exist,
                # it may be a folder
                prefix = f""{source.rstrip('/')}/""
                objs = list(self._bucket.objects.filter(Prefix=prefix))
                for obj in objs:
                    new_key = destination + obj.key[len(source) :]
                    self._s3.meta.client.copy_object(
                        CopySource={""Bucket"": self._bucket_name, ""Key"": obj.key},
                        Bucket=self._bucket_name,
                        Key=new_key,
                    )
            else:
                raise

    def clone_with_subroot(self, subroot: str | Path) -> S3FileStorage:
        """"""Create a new S3FileStorage with a subroot of the current storage.""""""
        file_storage = S3FileStorage(
            S3FileStorageConfiguration(
                bucket=self._bucket_name,
                root=Path(""/"").joinpath(self.get_path(subroot)),
                s3_endpoint_url=SecretStr(self._s3.meta.client.meta.endpoint_url),
            )
        )
        file_storage._s3 = self._s3
        file_storage._bucket = self._bucket
        return file_storage

    def __repr__(self) -> str:
        return f""{__class__.__name__}(bucket='{self._bucket_name}', root={self._root})""","Point(row=34, column=0)","Point(row=307, column=87)",,classic/forge/forge/file_storage/s3.py
S3FileStorage.__init__,function,,"def __init__(self, config: S3FileStorageConfiguration):
        self._bucket_name = config.bucket
        self._root = config.root
        # Add / at the beginning of the root path
        if not self._root.is_absolute():
            self._root = Path(""/"").joinpath(self._root)

        # https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html
        self._s3 = boto3.resource(
            ""s3"",
            endpoint_url=(
                config.s3_endpoint_url.get_secret_value()
                if config.s3_endpoint_url
                else None
            ),
        )

        super().__init__()","Point(row=39, column=4)","Point(row=56, column=26)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.root,function,The root directory of the file storage.,"def root(self) -> Path:
        """"""The root directory of the file storage.""""""
        return self._root","Point(row=59, column=4)","Point(row=61, column=25)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.restrict_to_root,function,Whether to restrict generated paths to the root.,"def restrict_to_root(self):
        """"""Whether to restrict generated paths to the root.""""""
        return True","Point(row=64, column=4)","Point(row=66, column=19)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.is_local,function,"Whether the storage is local (i.e. on the same machine, not cloud-based).","def is_local(self) -> bool:
        """"""Whether the storage is local (i.e. on the same machine, not cloud-based).""""""
        return False","Point(row=69, column=4)","Point(row=71, column=20)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.initialize,function,,"def initialize(self) -> None:
        logger.debug(f""Initializing {repr(self)}..."")
        try:
            self._s3.meta.client.head_bucket(Bucket=self._bucket_name)
            self._bucket = self._s3.Bucket(self._bucket_name)
        except botocore.exceptions.ClientError as e:
            if ""(404)"" not in str(e):
                raise
            logger.info(f""Bucket '{self._bucket_name}' does not exist; creating it..."")
            self._bucket = self._s3.create_bucket(Bucket=self._bucket_name)","Point(row=73, column=4)","Point(row=82, column=75)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.get_path,function,,"def get_path(self, relative_path: str | Path) -> Path:
        # We set S3 root with ""/"" at the beginning
        # but relative_to(""/"") will remove it
        # because we don't actually want it in the storage filenames
        return super().get_path(relative_path).relative_to(""/"")","Point(row=84, column=4)","Point(row=88, column=63)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage._get_obj,function,Get an S3 object.,"def _get_obj(self, path: str | Path) -> mypy_boto3_s3.service_resource.Object:
        """"""Get an S3 object.""""""
        obj = self._bucket.Object(str(path))
        with contextlib.suppress(botocore.exceptions.ClientError):
            obj.load()
        return obj","Point(row=90, column=4)","Point(row=95, column=18)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.open_file,function,,"def open_file(
        self,
        path: str | Path,
        mode: Literal[""r"", ""w""] = ""r"",
        binary: Literal[False] = False,
    ) -> TextIOWrapper:
        ...","Point(row=98, column=4)","Point(row=104, column=11)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.open_file,function,,"def open_file(
        self, path: str | Path, mode: Literal[""r"", ""w""], binary: Literal[True]
    ) -> S3BinaryIOWrapper:
        ...","Point(row=107, column=4)","Point(row=110, column=11)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.open_file,function,,"def open_file(
        self, path: str | Path, *, binary: Literal[True]
    ) -> S3BinaryIOWrapper:
        ...","Point(row=113, column=4)","Point(row=116, column=11)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.open_file,function,,"def open_file(
        self, path: str | Path, mode: Literal[""r"", ""w""] = ""r"", binary: bool = False
    ) -> S3BinaryIOWrapper | TextIOWrapper:
        ...","Point(row=119, column=4)","Point(row=122, column=11)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.open_file,function,Open a file in the storage.,"def open_file(
        self, path: str | Path, mode: Literal[""r"", ""w""] = ""r"", binary: bool = False
    ) -> TextIOWrapper | S3BinaryIOWrapper:
        """"""Open a file in the storage.""""""
        path = self.get_path(path)
        body = S3BinaryIOWrapper(self._get_obj(path).get()[""Body""], str(path))
        return body if binary else TextIOWrapper(body)","Point(row=124, column=4)","Point(row=130, column=54)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.read_file,function,Read a file in the storage as text.,"def read_file(self, path: str | Path, binary: Literal[False] = False) -> str:
        """"""Read a file in the storage as text.""""""
        ...","Point(row=133, column=4)","Point(row=135, column=11)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.read_file,function,Read a file in the storage as binary.,"def read_file(self, path: str | Path, binary: Literal[True]) -> bytes:
        """"""Read a file in the storage as binary.""""""
        ...","Point(row=138, column=4)","Point(row=140, column=11)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.read_file,function,Read a file in the storage.,"def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:
        """"""Read a file in the storage.""""""
        ...","Point(row=143, column=4)","Point(row=145, column=11)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.read_file,function,Read a file in the storage.,"def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:
        """"""Read a file in the storage.""""""
        return self.open_file(path, binary=binary).read()","Point(row=147, column=4)","Point(row=149, column=57)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.write_file,function,Write to a file in the storage.,"async def write_file(self, path: str | Path, content: str | bytes) -> None:
        """"""Write to a file in the storage.""""""
        obj = self._get_obj(self.get_path(path))
        obj.put(Body=content)

        if self.on_write_file:
            path = Path(path)
            if path.is_absolute():
                path = path.relative_to(self.root)
            res = self.on_write_file(path)
            if inspect.isawaitable(res):
                await res","Point(row=151, column=4)","Point(row=162, column=25)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.list_files,function,List all files (recursively) in a directory in the storage.,"def list_files(self, path: str | Path = ""."") -> list[Path]:
        """"""List all files (recursively) in a directory in the storage.""""""
        path = self.get_path(path)
        if path == Path("".""):  # root level of bucket
            return [Path(obj.key) for obj in self._bucket.objects.all()]
        else:
            return [
                Path(obj.key).relative_to(path)
                for obj in self._bucket.objects.filter(Prefix=f""{path}/"")
            ]","Point(row=164, column=4)","Point(row=173, column=13)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.list_folders,function,List 'directories' directly in a given path or recursively in the storage.,"def list_folders(
        self, path: str | Path = ""."", recursive: bool = False
    ) -> list[Path]:
        """"""List 'directories' directly in a given path or recursively in the storage.""""""
        path = self.get_path(path)
        folder_names = set()

        # List objects with the specified prefix and delimiter
        for obj_summary in self._bucket.objects.filter(Prefix=str(path)):
            # Remove path prefix and the object name (last part)
            folder = Path(obj_summary.key).relative_to(path).parent
            if not folder or folder == Path("".""):
                continue
            # For non-recursive, only add the first level of folders
            if not recursive:
                folder_names.add(folder.parts[0])
            else:
                # For recursive, need to add all nested folders
                for i in range(len(folder.parts)):
                    folder_names.add(""/"".join(folder.parts[: i + 1]))

        return [Path(f) for f in folder_names]","Point(row=175, column=4)","Point(row=196, column=46)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.delete_file,function,Delete a file in the storage.,"def delete_file(self, path: str | Path) -> None:
        """"""Delete a file in the storage.""""""
        path = self.get_path(path)
        obj = self._s3.Object(self._bucket_name, str(path))
        obj.delete()","Point(row=198, column=4)","Point(row=202, column=20)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.delete_dir,function,Delete an empty folder in the storage.,"def delete_dir(self, path: str | Path) -> None:
        """"""Delete an empty folder in the storage.""""""
        # S3 does not have directories, so we don't need to do anything
        pass","Point(row=204, column=4)","Point(row=207, column=12)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.exists,function,Check if a file or folder exists in S3 storage.,"def exists(self, path: str | Path) -> bool:
        """"""Check if a file or folder exists in S3 storage.""""""
        path = self.get_path(path)
        try:
            # Check for exact object match (file)
            self._s3.meta.client.head_object(Bucket=self._bucket_name, Key=str(path))
            return True
        except botocore.exceptions.ClientError as e:
            if e.response.get(""ResponseMetadata"", {}).get(""HTTPStatusCode"") == 404:
                # If the object does not exist,
                # check for objects with the prefix (folder)
                prefix = f""{str(path).rstrip('/')}/""
                objs = list(self._bucket.objects.filter(Prefix=prefix, MaxKeys=1))
                return len(objs) > 0  # True if any objects exist with the prefix
            else:
                raise  # Re-raise for any other client errors","Point(row=209, column=4)","Point(row=224, column=61)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.make_dir,function,Create a directory in the storage if doesn't exist.,"def make_dir(self, path: str | Path) -> None:
        """"""Create a directory in the storage if doesn't exist.""""""
        # S3 does not have directories, so we don't need to do anything
        pass","Point(row=226, column=4)","Point(row=229, column=12)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.rename,function,Rename a file or folder in the storage.,"def rename(self, old_path: str | Path, new_path: str | Path) -> None:
        """"""Rename a file or folder in the storage.""""""
        old_path = str(self.get_path(old_path))
        new_path = str(self.get_path(new_path))

        try:
            # If file exists, rename it
            self._s3.meta.client.head_object(Bucket=self._bucket_name, Key=old_path)
            self._s3.meta.client.copy_object(
                CopySource={""Bucket"": self._bucket_name, ""Key"": old_path},
                Bucket=self._bucket_name,
                Key=new_path,
            )
            self._s3.meta.client.delete_object(Bucket=self._bucket_name, Key=old_path)
        except botocore.exceptions.ClientError as e:
            if e.response.get(""ResponseMetadata"", {}).get(""HTTPStatusCode"") == 404:
                # If the object does not exist,
                # it may be a folder
                prefix = f""{old_path.rstrip('/')}/""
                objs = list(self._bucket.objects.filter(Prefix=prefix))
                for obj in objs:
                    new_key = new_path + obj.key[len(old_path) :]
                    self._s3.meta.client.copy_object(
                        CopySource={""Bucket"": self._bucket_name, ""Key"": obj.key},
                        Bucket=self._bucket_name,
                        Key=new_key,
                    )
                    self._s3.meta.client.delete_object(
                        Bucket=self._bucket_name, Key=obj.key
                    )
            else:
                raise  # Re-raise for any other client errors","Point(row=231, column=4)","Point(row=262, column=61)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.copy,function,Copy a file or folder with all contents in the storage.,"def copy(self, source: str | Path, destination: str | Path) -> None:
        """"""Copy a file or folder with all contents in the storage.""""""
        source = str(self.get_path(source))
        destination = str(self.get_path(destination))

        try:
            # If source is a file, copy it
            self._s3.meta.client.head_object(Bucket=self._bucket_name, Key=source)
            self._s3.meta.client.copy_object(
                CopySource={""Bucket"": self._bucket_name, ""Key"": source},
                Bucket=self._bucket_name,
                Key=destination,
            )
        except botocore.exceptions.ClientError as e:
            if e.response.get(""ResponseMetadata"", {}).get(""HTTPStatusCode"") == 404:
                # If the object does not exist,
                # it may be a folder
                prefix = f""{source.rstrip('/')}/""
                objs = list(self._bucket.objects.filter(Prefix=prefix))
                for obj in objs:
                    new_key = destination + obj.key[len(source) :]
                    self._s3.meta.client.copy_object(
                        CopySource={""Bucket"": self._bucket_name, ""Key"": obj.key},
                        Bucket=self._bucket_name,
                        Key=new_key,
                    )
            else:
                raise","Point(row=264, column=4)","Point(row=291, column=21)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.clone_with_subroot,function,Create a new S3FileStorage with a subroot of the current storage.,"def clone_with_subroot(self, subroot: str | Path) -> S3FileStorage:
        """"""Create a new S3FileStorage with a subroot of the current storage.""""""
        file_storage = S3FileStorage(
            S3FileStorageConfiguration(
                bucket=self._bucket_name,
                root=Path(""/"").joinpath(self.get_path(subroot)),
                s3_endpoint_url=SecretStr(self._s3.meta.client.meta.endpoint_url),
            )
        )
        file_storage._s3 = self._s3
        file_storage._bucket = self._bucket
        return file_storage","Point(row=293, column=4)","Point(row=304, column=27)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3FileStorage.__repr__,function,,"def __repr__(self) -> str:
        return f""{__class__.__name__}(bucket='{self._bucket_name}', root={self._root})""","Point(row=306, column=4)","Point(row=307, column=87)",S3FileStorage,classic/forge/forge/file_storage/s3.py
S3BinaryIOWrapper,class,,"class S3BinaryIOWrapper(BinaryIO):
    def __init__(self, body: StreamingBody, name: str):
        self.body = body
        self._name = name

    @property
    def name(self) -> str:
        return self._name

    def read(self, size: int = -1) -> bytes:
        return self.body.read(size if size > 0 else None)

    def readinto(self, b: bytearray) -> int:
        data = self.read(len(b))
        b[: len(data)] = data
        return len(data)

    def close(self) -> None:
        self.body.close()

    def fileno(self) -> int:
        return self.body.fileno()

    def flush(self) -> None:
        self.body.flush()

    def isatty(self) -> bool:
        return self.body.isatty()

    def readable(self) -> bool:
        return self.body.readable()

    def seekable(self) -> bool:
        return self.body.seekable()

    def writable(self) -> bool:
        return False

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.body.close()","Point(row=310, column=0)","Point(row=352, column=25)",,classic/forge/forge/file_storage/s3.py
S3BinaryIOWrapper.__init__,function,,"def __init__(self, body: StreamingBody, name: str):
        self.body = body
        self._name = name","Point(row=311, column=4)","Point(row=313, column=25)",S3BinaryIOWrapper,classic/forge/forge/file_storage/s3.py
S3BinaryIOWrapper.name,function,,"def name(self) -> str:
        return self._name","Point(row=316, column=4)","Point(row=317, column=25)",S3BinaryIOWrapper,classic/forge/forge/file_storage/s3.py
S3BinaryIOWrapper.read,function,,"def read(self, size: int = -1) -> bytes:
        return self.body.read(size if size > 0 else None)","Point(row=319, column=4)","Point(row=320, column=57)",S3BinaryIOWrapper,classic/forge/forge/file_storage/s3.py
S3BinaryIOWrapper.readinto,function,,"def readinto(self, b: bytearray) -> int:
        data = self.read(len(b))
        b[: len(data)] = data
        return len(data)","Point(row=322, column=4)","Point(row=325, column=24)",S3BinaryIOWrapper,classic/forge/forge/file_storage/s3.py
S3BinaryIOWrapper.close,function,,"def close(self) -> None:
        self.body.close()","Point(row=327, column=4)","Point(row=328, column=25)",S3BinaryIOWrapper,classic/forge/forge/file_storage/s3.py
S3BinaryIOWrapper.fileno,function,,"def fileno(self) -> int:
        return self.body.fileno()","Point(row=330, column=4)","Point(row=331, column=33)",S3BinaryIOWrapper,classic/forge/forge/file_storage/s3.py
S3BinaryIOWrapper.flush,function,,"def flush(self) -> None:
        self.body.flush()","Point(row=333, column=4)","Point(row=334, column=25)",S3BinaryIOWrapper,classic/forge/forge/file_storage/s3.py
S3BinaryIOWrapper.isatty,function,,"def isatty(self) -> bool:
        return self.body.isatty()","Point(row=336, column=4)","Point(row=337, column=33)",S3BinaryIOWrapper,classic/forge/forge/file_storage/s3.py
S3BinaryIOWrapper.readable,function,,"def readable(self) -> bool:
        return self.body.readable()","Point(row=339, column=4)","Point(row=340, column=35)",S3BinaryIOWrapper,classic/forge/forge/file_storage/s3.py
S3BinaryIOWrapper.seekable,function,,"def seekable(self) -> bool:
        return self.body.seekable()","Point(row=342, column=4)","Point(row=343, column=35)",S3BinaryIOWrapper,classic/forge/forge/file_storage/s3.py
S3BinaryIOWrapper.writable,function,,"def writable(self) -> bool:
        return False","Point(row=345, column=4)","Point(row=346, column=20)",S3BinaryIOWrapper,classic/forge/forge/file_storage/s3.py
S3BinaryIOWrapper.__enter__,function,,"def __enter__(self):
        return self","Point(row=348, column=4)","Point(row=349, column=19)",S3BinaryIOWrapper,classic/forge/forge/file_storage/s3.py
S3BinaryIOWrapper.__exit__,function,,"def __exit__(self, exc_type, exc_val, exc_tb):
        self.body.close()","Point(row=351, column=4)","Point(row=352, column=25)",S3BinaryIOWrapper,classic/forge/forge/file_storage/s3.py
FileStorageConfiguration,class,,"class FileStorageConfiguration(SystemConfiguration):
    restrict_to_root: bool = True
    root: Path = Path(""/"")","Point(row=24, column=0)","Point(row=26, column=26)",,classic/forge/forge/file_storage/base.py
FileStorage,class,A class that represents a file storage.,"class FileStorage(ABC):
    """"""A class that represents a file storage.""""""

    on_write_file: Callable[[Path], Any] | None = None
    """"""
    Event hook, executed after writing a file.

    Params:
        Path: The path of the file that was written, relative to the storage root.
    """"""

    @property
    @abstractmethod
    def root(self) -> Path:
        """"""The root path of the file storage.""""""

    @property
    @abstractmethod
    def restrict_to_root(self) -> bool:
        """"""Whether to restrict file access to within the storage's root path.""""""

    @property
    @abstractmethod
    def is_local(self) -> bool:
        """"""Whether the storage is local (i.e. on the same machine, not cloud-based).""""""

    @abstractmethod
    def initialize(self) -> None:
        """"""
        Calling `initialize()` should bring the storage to a ready-to-use state.
        For example, it can create the resource in which files will be stored, if it
        doesn't exist yet. E.g. a folder on disk, or an S3 Bucket.
        """"""

    @overload
    @abstractmethod
    def open_file(
        self,
        path: str | Path,
        mode: Literal[""r"", ""w""] = ""r"",
        binary: Literal[False] = False,
    ) -> TextIO:
        """"""Returns a readable text file-like object representing the file.""""""

    @overload
    @abstractmethod
    def open_file(
        self, path: str | Path, mode: Literal[""r"", ""w""], binary: Literal[True]
    ) -> BinaryIO:
        """"""Returns a binary file-like object representing the file.""""""

    @overload
    @abstractmethod
    def open_file(self, path: str | Path, *, binary: Literal[True]) -> BinaryIO:
        """"""Returns a readable binary file-like object representing the file.""""""

    @overload
    @abstractmethod
    def open_file(
        self, path: str | Path, mode: Literal[""r"", ""w""] = ""r"", binary: bool = False
    ) -> TextIO | BinaryIO:
        """"""Returns a file-like object representing the file.""""""

    @overload
    @abstractmethod
    def read_file(self, path: str | Path, binary: Literal[False] = False) -> str:
        """"""Read a file in the storage as text.""""""
        ...

    @overload
    @abstractmethod
    def read_file(self, path: str | Path, binary: Literal[True]) -> bytes:
        """"""Read a file in the storage as binary.""""""
        ...

    @overload
    @abstractmethod
    def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:
        """"""Read a file in the storage.""""""
        ...

    @abstractmethod
    async def write_file(self, path: str | Path, content: str | bytes) -> None:
        """"""Write to a file in the storage.""""""

    @abstractmethod
    def list_files(self, path: str | Path = ""."") -> list[Path]:
        """"""List all files (recursively) in a directory in the storage.""""""

    @abstractmethod
    def list_folders(
        self, path: str | Path = ""."", recursive: bool = False
    ) -> list[Path]:
        """"""List all folders in a directory in the storage.""""""

    @abstractmethod
    def delete_file(self, path: str | Path) -> None:
        """"""Delete a file in the storage.""""""

    @abstractmethod
    def delete_dir(self, path: str | Path) -> None:
        """"""Delete an empty folder in the storage.""""""

    @abstractmethod
    def exists(self, path: str | Path) -> bool:
        """"""Check if a file or folder exists in the storage.""""""

    @abstractmethod
    def rename(self, old_path: str | Path, new_path: str | Path) -> None:
        """"""Rename a file or folder in the storage.""""""

    @abstractmethod
    def copy(self, source: str | Path, destination: str | Path) -> None:
        """"""Copy a file or folder with all contents in the storage.""""""

    @abstractmethod
    def make_dir(self, path: str | Path) -> None:
        """"""Create a directory in the storage if doesn't exist.""""""

    @abstractmethod
    def clone_with_subroot(self, subroot: str | Path) -> FileStorage:
        """"""Create a new FileStorage with a subroot of the current storage.""""""

    def get_path(self, relative_path: str | Path) -> Path:
        """"""Get the full path for an item in the storage.

        Parameters:
            relative_path: The relative path to resolve in the storage.

        Returns:
            Path: The resolved path relative to the storage.
        """"""
        return self._sanitize_path(relative_path)

    @contextmanager
    def mount(self, path: str | Path = ""."") -> Generator[Path, Any, None]:
        """"""Mount the file storage and provide a local path.""""""
        local_path = tempfile.mkdtemp(dir=path)

        observer = Observer()
        try:
            # Copy all files to the local directory
            files = self.list_files()
            for file in files:
                file_path = local_path / file
                file_path.parent.mkdir(parents=True, exist_ok=True)
                content = self.read_file(file, binary=True)
                file_path.write_bytes(content)

            # Sync changes
            event_handler = FileSyncHandler(self, local_path)
            observer.schedule(event_handler, local_path, recursive=True)
            observer.start()

            yield Path(local_path)
        finally:
            observer.stop()
            observer.join()
            shutil.rmtree(local_path)

    def _sanitize_path(
        self,
        path: str | Path,
    ) -> Path:
        """"""Resolve the relative path within the given root if possible.

        Parameters:
            relative_path: The relative path to resolve.

        Returns:
            Path: The resolved path.

        Raises:
            ValueError: If the path is absolute and a root is provided.
            ValueError: If the path is outside the root and the root is restricted.
        """"""

        # Posix systems disallow null bytes in paths. Windows is agnostic about it.
        # Do an explicit check here for all sorts of null byte representations.
        if ""\0"" in str(path):
            raise ValueError(""Embedded null byte"")

        logger.debug(f""Resolving path '{path}' in storage '{self.root}'"")

        relative_path = Path(path)

        # Allow absolute paths if they are contained in the storage.
        if (
            relative_path.is_absolute()
            and self.restrict_to_root
            and not relative_path.is_relative_to(self.root)
        ):
            raise ValueError(
                f""Attempted to access absolute path '{relative_path}' ""
                f""in storage '{self.root}'""
            )

        full_path = self.root / relative_path
        if self.is_local:
            full_path = full_path.resolve()
        else:
            full_path = Path(os.path.normpath(full_path))

        logger.debug(f""Joined paths as '{full_path}'"")

        if self.restrict_to_root and not full_path.is_relative_to(self.root):
            raise ValueError(
                f""Attempted to access path '{full_path}' ""
                f""outside of storage '{self.root}'.""
            )

        return full_path","Point(row=29, column=0)","Point(row=240, column=24)",,classic/forge/forge/file_storage/base.py
FileStorage.root,function,The root path of the file storage.,"def root(self) -> Path:
        """"""The root path of the file storage.""""""","Point(row=42, column=4)","Point(row=43, column=48)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.restrict_to_root,function,Whether to restrict file access to within the storage's root path.,"def restrict_to_root(self) -> bool:
        """"""Whether to restrict file access to within the storage's root path.""""""","Point(row=47, column=4)","Point(row=48, column=80)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.is_local,function,"Whether the storage is local (i.e. on the same machine, not cloud-based).","def is_local(self) -> bool:
        """"""Whether the storage is local (i.e. on the same machine, not cloud-based).""""""","Point(row=52, column=4)","Point(row=53, column=87)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.initialize,function,"
        Calling `initialize()` should bring the storage to a ready-to-use state.
        For example, it can create the resource in which files will be stored, if it
        doesn't exist yet. E.g. a folder on disk, or an S3 Bucket.
","def initialize(self) -> None:
        """"""
        Calling `initialize()` should bring the storage to a ready-to-use state.
        For example, it can create the resource in which files will be stored, if it
        doesn't exist yet. E.g. a folder on disk, or an S3 Bucket.
        """"""","Point(row=56, column=4)","Point(row=61, column=11)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.open_file,function,Returns a readable text file-like object representing the file.,"def open_file(
        self,
        path: str | Path,
        mode: Literal[""r"", ""w""] = ""r"",
        binary: Literal[False] = False,
    ) -> TextIO:
        """"""Returns a readable text file-like object representing the file.""""""","Point(row=65, column=4)","Point(row=71, column=77)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.open_file,function,Returns a binary file-like object representing the file.,"def open_file(
        self, path: str | Path, mode: Literal[""r"", ""w""], binary: Literal[True]
    ) -> BinaryIO:
        """"""Returns a binary file-like object representing the file.""""""","Point(row=75, column=4)","Point(row=78, column=70)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.open_file,function,Returns a readable binary file-like object representing the file.,"def open_file(self, path: str | Path, *, binary: Literal[True]) -> BinaryIO:
        """"""Returns a readable binary file-like object representing the file.""""""","Point(row=82, column=4)","Point(row=83, column=79)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.open_file,function,Returns a file-like object representing the file.,"def open_file(
        self, path: str | Path, mode: Literal[""r"", ""w""] = ""r"", binary: bool = False
    ) -> TextIO | BinaryIO:
        """"""Returns a file-like object representing the file.""""""","Point(row=87, column=4)","Point(row=90, column=63)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.read_file,function,Read a file in the storage as text.,"def read_file(self, path: str | Path, binary: Literal[False] = False) -> str:
        """"""Read a file in the storage as text.""""""
        ...","Point(row=94, column=4)","Point(row=96, column=11)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.read_file,function,Read a file in the storage as binary.,"def read_file(self, path: str | Path, binary: Literal[True]) -> bytes:
        """"""Read a file in the storage as binary.""""""
        ...","Point(row=100, column=4)","Point(row=102, column=11)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.read_file,function,Read a file in the storage.,"def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:
        """"""Read a file in the storage.""""""
        ...","Point(row=106, column=4)","Point(row=108, column=11)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.write_file,function,Write to a file in the storage.,"async def write_file(self, path: str | Path, content: str | bytes) -> None:
        """"""Write to a file in the storage.""""""","Point(row=111, column=4)","Point(row=112, column=45)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.list_files,function,List all files (recursively) in a directory in the storage.,"def list_files(self, path: str | Path = ""."") -> list[Path]:
        """"""List all files (recursively) in a directory in the storage.""""""","Point(row=115, column=4)","Point(row=116, column=73)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.list_folders,function,List all folders in a directory in the storage.,"def list_folders(
        self, path: str | Path = ""."", recursive: bool = False
    ) -> list[Path]:
        """"""List all folders in a directory in the storage.""""""","Point(row=119, column=4)","Point(row=122, column=61)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.delete_file,function,Delete a file in the storage.,"def delete_file(self, path: str | Path) -> None:
        """"""Delete a file in the storage.""""""","Point(row=125, column=4)","Point(row=126, column=43)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.delete_dir,function,Delete an empty folder in the storage.,"def delete_dir(self, path: str | Path) -> None:
        """"""Delete an empty folder in the storage.""""""","Point(row=129, column=4)","Point(row=130, column=52)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.exists,function,Check if a file or folder exists in the storage.,"def exists(self, path: str | Path) -> bool:
        """"""Check if a file or folder exists in the storage.""""""","Point(row=133, column=4)","Point(row=134, column=62)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.rename,function,Rename a file or folder in the storage.,"def rename(self, old_path: str | Path, new_path: str | Path) -> None:
        """"""Rename a file or folder in the storage.""""""","Point(row=137, column=4)","Point(row=138, column=53)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.copy,function,Copy a file or folder with all contents in the storage.,"def copy(self, source: str | Path, destination: str | Path) -> None:
        """"""Copy a file or folder with all contents in the storage.""""""","Point(row=141, column=4)","Point(row=142, column=69)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.make_dir,function,Create a directory in the storage if doesn't exist.,"def make_dir(self, path: str | Path) -> None:
        """"""Create a directory in the storage if doesn't exist.""""""","Point(row=145, column=4)","Point(row=146, column=65)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.clone_with_subroot,function,Create a new FileStorage with a subroot of the current storage.,"def clone_with_subroot(self, subroot: str | Path) -> FileStorage:
        """"""Create a new FileStorage with a subroot of the current storage.""""""","Point(row=149, column=4)","Point(row=150, column=77)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.get_path,function,"Get the full path for an item in the storage.

        Parameters:
            relative_path: The relative path to resolve in the storage.

        Returns:
            Path: The resolved path relative to the storage.
","def get_path(self, relative_path: str | Path) -> Path:
        """"""Get the full path for an item in the storage.

        Parameters:
            relative_path: The relative path to resolve in the storage.

        Returns:
            Path: The resolved path relative to the storage.
        """"""
        return self._sanitize_path(relative_path)","Point(row=152, column=4)","Point(row=161, column=49)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage.mount,function,Mount the file storage and provide a local path.,"def mount(self, path: str | Path = ""."") -> Generator[Path, Any, None]:
        """"""Mount the file storage and provide a local path.""""""
        local_path = tempfile.mkdtemp(dir=path)

        observer = Observer()
        try:
            # Copy all files to the local directory
            files = self.list_files()
            for file in files:
                file_path = local_path / file
                file_path.parent.mkdir(parents=True, exist_ok=True)
                content = self.read_file(file, binary=True)
                file_path.write_bytes(content)

            # Sync changes
            event_handler = FileSyncHandler(self, local_path)
            observer.schedule(event_handler, local_path, recursive=True)
            observer.start()

            yield Path(local_path)
        finally:
            observer.stop()
            observer.join()
            shutil.rmtree(local_path)","Point(row=164, column=4)","Point(row=187, column=37)",FileStorage,classic/forge/forge/file_storage/base.py
FileStorage._sanitize_path,function,"Resolve the relative path within the given root if possible.

        Parameters:
            relative_path: The relative path to resolve.

        Returns:
            Path: The resolved path.

        Raises:
            ValueError: If the path is absolute and a root is provided.
            ValueError: If the path is outside the root and the root is restricted.
","def _sanitize_path(
        self,
        path: str | Path,
    ) -> Path:
        """"""Resolve the relative path within the given root if possible.

        Parameters:
            relative_path: The relative path to resolve.

        Returns:
            Path: The resolved path.

        Raises:
            ValueError: If the path is absolute and a root is provided.
            ValueError: If the path is outside the root and the root is restricted.
        """"""

        # Posix systems disallow null bytes in paths. Windows is agnostic about it.
        # Do an explicit check here for all sorts of null byte representations.
        if ""\0"" in str(path):
            raise ValueError(""Embedded null byte"")

        logger.debug(f""Resolving path '{path}' in storage '{self.root}'"")

        relative_path = Path(path)

        # Allow absolute paths if they are contained in the storage.
        if (
            relative_path.is_absolute()
            and self.restrict_to_root
            and not relative_path.is_relative_to(self.root)
        ):
            raise ValueError(
                f""Attempted to access absolute path '{relative_path}' ""
                f""in storage '{self.root}'""
            )

        full_path = self.root / relative_path
        if self.is_local:
            full_path = full_path.resolve()
        else:
            full_path = Path(os.path.normpath(full_path))

        logger.debug(f""Joined paths as '{full_path}'"")

        if self.restrict_to_root and not full_path.is_relative_to(self.root):
            raise ValueError(
                f""Attempted to access path '{full_path}' ""
                f""outside of storage '{self.root}'.""
            )

        return full_path","Point(row=189, column=4)","Point(row=240, column=24)",FileStorage,classic/forge/forge/file_storage/base.py
FileSyncHandler,class,,"class FileSyncHandler(FileSystemEventHandler):
    def __init__(self, storage: FileStorage, path: str | Path = "".""):
        self.storage = storage
        self.path = Path(path)

    def on_modified(self, event: FileSystemEvent):
        if event.is_directory:
            return

        file_path = Path(event.src_path).relative_to(self.path)
        content = file_path.read_bytes()
        # Must execute write_file synchronously because the hook is synchronous
        # TODO: Schedule write operation using asyncio.create_task (non-blocking)
        asyncio.get_event_loop().run_until_complete(
            self.storage.write_file(file_path, content)
        )

    def on_created(self, event: FileSystemEvent):
        if event.is_directory:
            self.storage.make_dir(event.src_path)
            return

        file_path = Path(event.src_path).relative_to(self.path)
        content = file_path.read_bytes()
        # Must execute write_file synchronously because the hook is synchronous
        # TODO: Schedule write operation using asyncio.create_task (non-blocking)
        asyncio.get_event_loop().run_until_complete(
            self.storage.write_file(file_path, content)
        )

    def on_deleted(self, event: FileSystemEvent):
        if event.is_directory:
            self.storage.delete_dir(event.src_path)
            return

        file_path = event.src_path
        self.storage.delete_file(file_path)

    def on_moved(self, event: FileSystemEvent):
        self.storage.rename(event.src_path, event.dest_path)","Point(row=243, column=0)","Point(row=282, column=60)",,classic/forge/forge/file_storage/base.py
FileSyncHandler.__init__,function,,"def __init__(self, storage: FileStorage, path: str | Path = "".""):
        self.storage = storage
        self.path = Path(path)","Point(row=244, column=4)","Point(row=246, column=30)",FileSyncHandler,classic/forge/forge/file_storage/base.py
FileSyncHandler.on_modified,function,,"def on_modified(self, event: FileSystemEvent):
        if event.is_directory:
            return

        file_path = Path(event.src_path).relative_to(self.path)
        content = file_path.read_bytes()
        # Must execute write_file synchronously because the hook is synchronous
        # TODO: Schedule write operation using asyncio.create_task (non-blocking)
        asyncio.get_event_loop().run_until_complete(
            self.storage.write_file(file_path, content)
        )","Point(row=248, column=4)","Point(row=258, column=9)",FileSyncHandler,classic/forge/forge/file_storage/base.py
FileSyncHandler.on_created,function,,"def on_created(self, event: FileSystemEvent):
        if event.is_directory:
            self.storage.make_dir(event.src_path)
            return

        file_path = Path(event.src_path).relative_to(self.path)
        content = file_path.read_bytes()
        # Must execute write_file synchronously because the hook is synchronous
        # TODO: Schedule write operation using asyncio.create_task (non-blocking)
        asyncio.get_event_loop().run_until_complete(
            self.storage.write_file(file_path, content)
        )","Point(row=260, column=4)","Point(row=271, column=9)",FileSyncHandler,classic/forge/forge/file_storage/base.py
FileSyncHandler.on_deleted,function,,"def on_deleted(self, event: FileSystemEvent):
        if event.is_directory:
            self.storage.delete_dir(event.src_path)
            return

        file_path = event.src_path
        self.storage.delete_file(file_path)","Point(row=273, column=4)","Point(row=279, column=43)",FileSyncHandler,classic/forge/forge/file_storage/base.py
FileSyncHandler.on_moved,function,,"def on_moved(self, event: FileSystemEvent):
        self.storage.rename(event.src_path, event.dest_path)","Point(row=281, column=4)","Point(row=282, column=60)",FileSyncHandler,classic/forge/forge/file_storage/base.py
mock_text_file,function,,"def mock_text_file():
    with tempfile.NamedTemporaryFile(mode=""w"", delete=False, suffix="".txt"") as f:
        f.write(plain_text_str)
    return f.name","Point(row=19, column=0)","Point(row=22, column=17)",,classic/forge/forge/utils/test_file_operations.py
mock_csv_file,function,,"def mock_csv_file():
    with tempfile.NamedTemporaryFile(mode=""w"", delete=False, suffix="".csv"") as f:
        f.write(plain_text_str)
    return f.name","Point(row=25, column=0)","Point(row=28, column=17)",,classic/forge/forge/utils/test_file_operations.py
mock_pdf_file,function,,"def mock_pdf_file():
    with tempfile.NamedTemporaryFile(mode=""wb"", delete=False, suffix="".pdf"") as f:
        # Create a new PDF and add a page with the text plain_text_str
        # Write the PDF header
        f.write(b""%PDF-1.7\n"")
        # Write the document catalog
        f.write(b""1 0 obj\n"")
        f.write(b""<< /Type /Catalog /Pages 2 0 R >>\n"")
        f.write(b""endobj\n"")
        # Write the page object
        f.write(b""2 0 obj\n"")
        f.write(
            b""<< /Type /Page /Parent 1 0 R /Resources << /Font << /F1 3 0 R >> >> ""
            b""/MediaBox [0 0 612 792] /Contents 4 0 R >>\n""
        )
        f.write(b""endobj\n"")
        # Write the font object
        f.write(b""3 0 obj\n"")
        f.write(
            b""<< /Type /Font /Subtype /Type1 /Name /F1 /BaseFont /Helvetica-Bold >>\n""
        )
        f.write(b""endobj\n"")
        # Write the page contents object
        f.write(b""4 0 obj\n"")
        f.write(b""<< /Length 25 >>\n"")
        f.write(b""stream\n"")
        f.write(b""BT\n/F1 12 Tf\n72 720 Td\n(Hello, world!) Tj\nET\n"")
        f.write(b""endstream\n"")
        f.write(b""endobj\n"")
        # Write the cross-reference table
        f.write(b""xref\n"")
        f.write(b""0 5\n"")
        f.write(b""0000000000 65535 f \n"")
        f.write(b""0000000017 00000 n \n"")
        f.write(b""0000000073 00000 n \n"")
        f.write(b""0000000123 00000 n \n"")
        f.write(b""0000000271 00000 n \n"")
        f.write(b""trailer\n"")
        f.write(b""<< /Size 5 /Root 1 0 R >>\n"")
        f.write(b""startxref\n"")
        f.write(b""380\n"")
        f.write(b""%%EOF\n"")
        f.write(b""\x00"")
    return f.name","Point(row=31, column=0)","Point(row=74, column=17)",,classic/forge/forge/utils/test_file_operations.py
mock_docx_file,function,,"def mock_docx_file():
    with tempfile.NamedTemporaryFile(mode=""wb"", delete=False, suffix="".docx"") as f:
        document = docx.Document()
        document.add_paragraph(plain_text_str)
        document.save(f.name)
    return f.name","Point(row=77, column=0)","Point(row=82, column=17)",,classic/forge/forge/utils/test_file_operations.py
mock_json_file,function,,"def mock_json_file():
    with tempfile.NamedTemporaryFile(mode=""w"", delete=False, suffix="".json"") as f:
        json.dump({""text"": plain_text_str}, f)
    return f.name","Point(row=85, column=0)","Point(row=88, column=17)",,classic/forge/forge/utils/test_file_operations.py
mock_xml_file,function,,"def mock_xml_file():
    root = ElementTree.Element(""text"")
    root.text = plain_text_str
    tree = ElementTree.ElementTree(root)
    with tempfile.NamedTemporaryFile(mode=""wb"", delete=False, suffix="".xml"") as f:
        tree.write(f)
    return f.name","Point(row=91, column=0)","Point(row=97, column=17)",,classic/forge/forge/utils/test_file_operations.py
mock_yaml_file,function,,"def mock_yaml_file():
    with tempfile.NamedTemporaryFile(mode=""w"", delete=False, suffix="".yaml"") as f:
        yaml.dump({""text"": plain_text_str}, f)
    return f.name","Point(row=100, column=0)","Point(row=103, column=17)",,classic/forge/forge/utils/test_file_operations.py
mock_html_file,function,,"def mock_html_file():
    html = BeautifulSoup(
        ""<html>""
        ""<head><title>This is a test</title></head>""
        f""<body><p>{plain_text_str}</p></body>""
        ""</html>"",
        ""html.parser"",
    )
    with tempfile.NamedTemporaryFile(mode=""w"", delete=False, suffix="".html"") as f:
        f.write(str(html))
    return f.name","Point(row=106, column=0)","Point(row=116, column=17)",,classic/forge/forge/utils/test_file_operations.py
mock_md_file,function,,"def mock_md_file():
    with tempfile.NamedTemporaryFile(mode=""w"", delete=False, suffix="".md"") as f:
        f.write(f""# {plain_text_str}!\n"")
    return f.name","Point(row=119, column=0)","Point(row=122, column=17)",,classic/forge/forge/utils/test_file_operations.py
mock_latex_file,function,,"def mock_latex_file():
    with tempfile.NamedTemporaryFile(mode=""w"", delete=False, suffix="".tex"") as f:
        latex_str = (
            r""\documentclass{article}""
            r""\begin{document}""
            f""{plain_text_str}""
            r""\end{document}""
        )
        f.write(latex_str)
    return f.name","Point(row=125, column=0)","Point(row=134, column=17)",,classic/forge/forge/utils/test_file_operations.py
test_parsers,function,,"def test_parsers(file_extension, c_file_creator):
    created_file_path = Path(c_file_creator())
    with open(created_file_path, ""rb"") as file:
        loaded_text = decode_textual_file(file, os.path.splitext(file.name)[1], logger)

        assert plain_text_str in loaded_text

        should_be_binary = file_extension in binary_files_extensions
        assert should_be_binary == is_file_binary_fn(file)

    created_file_path.unlink()  # cleanup","Point(row=156, column=0)","Point(row=166, column=41)",,classic/forge/forge/utils/test_file_operations.py
get_exception_message,function,Get current exception type and message.,"def get_exception_message():
    """"""Get current exception type and message.""""""
    exc_type, exc_value, _ = sys.exc_info()
    exception_message = f""{exc_type.__name__}: {exc_value}"" if exc_type else exc_value
    return exception_message","Point(row=6, column=0)","Point(row=10, column=28)",,classic/forge/forge/utils/exceptions.py
get_detailed_traceback,function,Get current exception traceback with local variables.,"def get_detailed_traceback():
    """"""Get current exception traceback with local variables.""""""
    _, _, exc_tb = sys.exc_info()
    detailed_traceback = ""Traceback (most recent call last):\n""
    formatted_tb = traceback.format_tb(exc_tb)
    detailed_traceback += """".join(formatted_tb)

    # Optionally add local variables to the traceback information
    detailed_traceback += ""\nLocal variables by frame, innermost last:\n""
    while exc_tb:
        frame = exc_tb.tb_frame
        lineno = exc_tb.tb_lineno
        function_name = frame.f_code.co_name

        # Format frame information
        detailed_traceback += (
            f""  Frame {function_name} in {frame.f_code.co_filename} at line {lineno}\n""
        )

        # Get local variables for the frame
        local_vars = inspect.getargvalues(frame).locals
        for var_name, value in local_vars.items():
            detailed_traceback += f""    {var_name} = {value}\n""

        exc_tb = exc_tb.tb_next

    return detailed_traceback","Point(row=13, column=0)","Point(row=39, column=29)",,classic/forge/forge/utils/exceptions.py
NotFoundError,class,,"class NotFoundError(Exception):
    pass","Point(row=42, column=0)","Point(row=43, column=8)",,classic/forge/forge/utils/exceptions.py
AgentException,class,Base class for specific exceptions relevant in the execution of Agents,"class AgentException(Exception):
    """"""Base class for specific exceptions relevant in the execution of Agents""""""

    message: str

    hint: Optional[str] = None
    """"""A hint which can be passed to the LLM to reduce reoccurrence of this error""""""

    def __init__(self, message: str, *args):
        self.message = message
        super().__init__(message, *args)","Point(row=46, column=0)","Point(row=56, column=40)",,classic/forge/forge/utils/exceptions.py
AgentException.__init__,function,,"def __init__(self, message: str, *args):
        self.message = message
        super().__init__(message, *args)","Point(row=54, column=4)","Point(row=56, column=40)",AgentException,classic/forge/forge/utils/exceptions.py
AgentTerminated,class,The agent terminated or was terminated,"class AgentTerminated(AgentException):
    """"""The agent terminated or was terminated""""""","Point(row=59, column=0)","Point(row=60, column=48)",,classic/forge/forge/utils/exceptions.py
AgentFinished,class,The agent self-terminated,"class AgentFinished(AgentTerminated):
    """"""The agent self-terminated""""""","Point(row=63, column=0)","Point(row=64, column=35)",,classic/forge/forge/utils/exceptions.py
ConfigurationError,class,"Error caused by invalid, incompatible or otherwise incorrect configuration","class ConfigurationError(AgentException):
    """"""Error caused by invalid, incompatible or otherwise incorrect configuration""""""","Point(row=67, column=0)","Point(row=68, column=84)",,classic/forge/forge/utils/exceptions.py
InvalidAgentResponseError,class,The LLM deviated from the prescribed response format,"class InvalidAgentResponseError(AgentException):
    """"""The LLM deviated from the prescribed response format""""""","Point(row=71, column=0)","Point(row=72, column=62)",,classic/forge/forge/utils/exceptions.py
UnknownCommandError,class,The AI tried to use an unknown command,"class UnknownCommandError(AgentException):
    """"""The AI tried to use an unknown command""""""

    hint = ""Do not try to use this command again.""","Point(row=75, column=0)","Point(row=78, column=50)",,classic/forge/forge/utils/exceptions.py
CommandExecutionError,class,An error occurred when trying to execute the command,"class CommandExecutionError(AgentException):
    """"""An error occurred when trying to execute the command""""""","Point(row=81, column=0)","Point(row=82, column=62)",,classic/forge/forge/utils/exceptions.py
InvalidArgumentError,class,The command received an invalid argument,"class InvalidArgumentError(CommandExecutionError):
    """"""The command received an invalid argument""""""","Point(row=85, column=0)","Point(row=86, column=50)",,classic/forge/forge/utils/exceptions.py
OperationNotAllowedError,class,The agent is not allowed to execute the proposed operation,"class OperationNotAllowedError(CommandExecutionError):
    """"""The agent is not allowed to execute the proposed operation""""""","Point(row=89, column=0)","Point(row=90, column=68)",,classic/forge/forge/utils/exceptions.py
TooMuchOutputError,class,The operation generated more output than what the Agent can process,"class TooMuchOutputError(CommandExecutionError):
    """"""The operation generated more output than what the Agent can process""""""","Point(row=93, column=0)","Point(row=94, column=77)",,classic/forge/forge/utils/exceptions.py
dummy_method,function,,"def dummy_method(url):
    return url","Point(row=7, column=0)","Point(row=8, column=14)",,classic/forge/forge/utils/test_url_validator.py
test_url_validation_succeeds,function,,"def test_url_validation_succeeds(url):
    assert dummy_method(url) == url","Point(row=20, column=0)","Point(row=21, column=35)",,classic/forge/forge/utils/test_url_validator.py
test_url_validation_fails_invalid_url,function,,"def test_url_validation_fails_invalid_url(url, expected_error):
    with raises(ValueError, match=expected_error):
        dummy_method(url)","Point(row=33, column=0)","Point(row=35, column=25)",,classic/forge/forge/utils/test_url_validator.py
test_url_validation_fails_local_path,function,,"def test_url_validation_fails_local_path(url):
    with raises(ValueError):
        dummy_method(url)","Point(row=47, column=0)","Point(row=49, column=25)",,classic/forge/forge/utils/test_url_validator.py
test_happy_path_valid_url,function,"
    Test that the function successfully validates a valid URL with `http://` or
    `https://` prefix.
","def test_happy_path_valid_url():
    """"""
    Test that the function successfully validates a valid URL with `http://` or
    `https://` prefix.
    """"""

    @validate_url
    def test_func(url):
        return url

    assert test_func(""https://www.google.com"") == ""https://www.google.com""
    assert test_func(""http://www.google.com"") == ""http://www.google.com""","Point(row=52, column=0)","Point(row=63, column=72)",,classic/forge/forge/utils/test_url_validator.py
test_happy_path_valid_url.test_func,function,,"def test_func(url):
        return url","Point(row=59, column=4)","Point(row=60, column=18)",,classic/forge/forge/utils/test_url_validator.py
test_general_behavior_additional_path_parameters_query_string,function,"
    Test that the function successfully validates a valid URL with additional path,
    parameters, and query string.
","def test_general_behavior_additional_path_parameters_query_string():
    """"""
    Test that the function successfully validates a valid URL with additional path,
    parameters, and query string.
    """"""

    @validate_url
    def test_func(url):
        return url

    assert (
        test_func(""https://www.google.com/search?q=python"")
        == ""https://www.google.com/search?q=python""
    )","Point(row=66, column=0)","Point(row=79, column=5)",,classic/forge/forge/utils/test_url_validator.py
test_general_behavior_additional_path_parameters_query_string.test_func,function,,"def test_func(url):
        return url","Point(row=73, column=4)","Point(row=74, column=18)",,classic/forge/forge/utils/test_url_validator.py
test_edge_case_missing_scheme_or_network_location,function,"
    Test that the function raises a ValueError if the URL is missing scheme or
    network location.
","def test_edge_case_missing_scheme_or_network_location():
    """"""
    Test that the function raises a ValueError if the URL is missing scheme or
    network location.
    """"""

    @validate_url
    def test_func(url):
        return url

    with pytest.raises(ValueError):
        test_func(""www.google.com"")","Point(row=82, column=0)","Point(row=93, column=35)",,classic/forge/forge/utils/test_url_validator.py
test_edge_case_missing_scheme_or_network_location.test_func,function,,"def test_func(url):
        return url","Point(row=89, column=4)","Point(row=90, column=18)",,classic/forge/forge/utils/test_url_validator.py
test_edge_case_local_file_access,function,Test that the function raises a ValueError if the URL has local file access,"def test_edge_case_local_file_access():
    """"""Test that the function raises a ValueError if the URL has local file access""""""

    @validate_url
    def test_func(url):
        return url

    with pytest.raises(ValueError):
        test_func(""file:///etc/passwd"")","Point(row=96, column=0)","Point(row=104, column=39)",,classic/forge/forge/utils/test_url_validator.py
test_edge_case_local_file_access.test_func,function,,"def test_func(url):
        return url","Point(row=100, column=4)","Point(row=101, column=18)",,classic/forge/forge/utils/test_url_validator.py
test_general_behavior_sanitizes_url,function,Test that the function sanitizes the URL by removing unnecessary components,"def test_general_behavior_sanitizes_url():
    """"""Test that the function sanitizes the URL by removing unnecessary components""""""

    @validate_url
    def test_func(url):
        return url

    assert (
        test_func(""https://www.google.com/search?q=python#top"")
        == ""https://www.google.com/search?q=python""
    )","Point(row=107, column=0)","Point(row=117, column=5)",,classic/forge/forge/utils/test_url_validator.py
test_general_behavior_sanitizes_url.test_func,function,,"def test_func(url):
        return url","Point(row=111, column=4)","Point(row=112, column=18)",,classic/forge/forge/utils/test_url_validator.py
test_general_behavior_invalid_url_format,function,"
    Test that the function raises a ValueError if the URL has an invalid format
    (e.g. missing slashes)
","def test_general_behavior_invalid_url_format():
    """"""
    Test that the function raises a ValueError if the URL has an invalid format
    (e.g. missing slashes)
    """"""

    @validate_url
    def test_func(url):
        return url

    with pytest.raises(ValueError):
        test_func(""https:www.google.com"")","Point(row=120, column=0)","Point(row=131, column=41)",,classic/forge/forge/utils/test_url_validator.py
test_general_behavior_invalid_url_format.test_func,function,,"def test_func(url):
        return url","Point(row=127, column=4)","Point(row=128, column=18)",,classic/forge/forge/utils/test_url_validator.py
test_url_with_special_chars,function,"
    Tests that the function can handle URLs that contain unusual but valid characters.
","def test_url_with_special_chars():
    """"""
    Tests that the function can handle URLs that contain unusual but valid characters.
    """"""
    url = ""https://example.com/path%20with%20spaces""
    assert dummy_method(url) == url","Point(row=134, column=0)","Point(row=139, column=35)",,classic/forge/forge/utils/test_url_validator.py
test_extremely_long_url,function,"
    Tests that the function raises a ValueError if the URL is over 2000 characters.
","def test_extremely_long_url():
    """"""
    Tests that the function raises a ValueError if the URL is over 2000 characters.
    """"""
    url = ""http://example.com/"" + ""a"" * 2000
    with raises(ValueError, match=""URL is too long""):
        dummy_method(url)","Point(row=142, column=0)","Point(row=148, column=25)",,classic/forge/forge/utils/test_url_validator.py
test_internationalized_url,function,"
    Tests that the function can handle internationalized URLs with non-ASCII characters.
","def test_internationalized_url():
    """"""
    Tests that the function can handle internationalized URLs with non-ASCII characters.
    """"""
    url = ""http://‰æãÂ≠ê.ÊµãËØï""
    assert dummy_method(url) == url","Point(row=151, column=0)","Point(row=156, column=35)",,classic/forge/forge/utils/test_url_validator.py
ParserStrategy,class,,"class ParserStrategy(ABC):
    @abstractmethod
    def read(self, file: BinaryIO) -> str:
        ...","Point(row=15, column=0)","Point(row=18, column=11)",,classic/forge/forge/utils/file_operations.py
ParserStrategy.read,function,,"def read(self, file: BinaryIO) -> str:
        ...","Point(row=17, column=4)","Point(row=18, column=11)",ParserStrategy,classic/forge/forge/utils/file_operations.py
TXTParser,class,,"class TXTParser(ParserStrategy):
    def read(self, file: BinaryIO) -> str:
        charset_match = charset_normalizer.from_bytes(file.read()).best()
        logger.debug(
            f""Reading {getattr(file, 'name', 'file')} ""
            f""with encoding '{charset_match.encoding if charset_match else None}'""
        )
        return str(charset_match)","Point(row=22, column=0)","Point(row=29, column=33)",,classic/forge/forge/utils/file_operations.py
TXTParser.read,function,,"def read(self, file: BinaryIO) -> str:
        charset_match = charset_normalizer.from_bytes(file.read()).best()
        logger.debug(
            f""Reading {getattr(file, 'name', 'file')} ""
            f""with encoding '{charset_match.encoding if charset_match else None}'""
        )
        return str(charset_match)","Point(row=23, column=4)","Point(row=29, column=33)",TXTParser,classic/forge/forge/utils/file_operations.py
PDFParser,class,,"class PDFParser(ParserStrategy):
    def read(self, file: BinaryIO) -> str:
        parser = pypdf.PdfReader(file)
        text = """"
        for page_idx in range(len(parser.pages)):
            text += parser.pages[page_idx].extract_text()
        return text","Point(row=33, column=0)","Point(row=39, column=19)",,classic/forge/forge/utils/file_operations.py
PDFParser.read,function,,"def read(self, file: BinaryIO) -> str:
        parser = pypdf.PdfReader(file)
        text = """"
        for page_idx in range(len(parser.pages)):
            text += parser.pages[page_idx].extract_text()
        return text","Point(row=34, column=4)","Point(row=39, column=19)",PDFParser,classic/forge/forge/utils/file_operations.py
DOCXParser,class,,"class DOCXParser(ParserStrategy):
    def read(self, file: BinaryIO) -> str:
        doc_file = docx.Document(file)
        text = """"
        for para in doc_file.paragraphs:
            text += para.text
        return text","Point(row=43, column=0)","Point(row=49, column=19)",,classic/forge/forge/utils/file_operations.py
DOCXParser.read,function,,"def read(self, file: BinaryIO) -> str:
        doc_file = docx.Document(file)
        text = """"
        for para in doc_file.paragraphs:
            text += para.text
        return text","Point(row=44, column=4)","Point(row=49, column=19)",DOCXParser,classic/forge/forge/utils/file_operations.py
JSONParser,class,,"class JSONParser(ParserStrategy):
    def read(self, file: BinaryIO) -> str:
        data = json.load(file)
        text = str(data)
        return text","Point(row=53, column=0)","Point(row=57, column=19)",,classic/forge/forge/utils/file_operations.py
JSONParser.read,function,,"def read(self, file: BinaryIO) -> str:
        data = json.load(file)
        text = str(data)
        return text","Point(row=54, column=4)","Point(row=57, column=19)",JSONParser,classic/forge/forge/utils/file_operations.py
XMLParser,class,,"class XMLParser(ParserStrategy):
    def read(self, file: BinaryIO) -> str:
        soup = BeautifulSoup(file, ""xml"")
        text = soup.get_text()
        return text","Point(row=60, column=0)","Point(row=64, column=19)",,classic/forge/forge/utils/file_operations.py
XMLParser.read,function,,"def read(self, file: BinaryIO) -> str:
        soup = BeautifulSoup(file, ""xml"")
        text = soup.get_text()
        return text","Point(row=61, column=4)","Point(row=64, column=19)",XMLParser,classic/forge/forge/utils/file_operations.py
YAMLParser,class,,"class YAMLParser(ParserStrategy):
    def read(self, file: BinaryIO) -> str:
        data = yaml.load(file, Loader=yaml.SafeLoader)
        text = str(data)
        return text","Point(row=68, column=0)","Point(row=72, column=19)",,classic/forge/forge/utils/file_operations.py
YAMLParser.read,function,,"def read(self, file: BinaryIO) -> str:
        data = yaml.load(file, Loader=yaml.SafeLoader)
        text = str(data)
        return text","Point(row=69, column=4)","Point(row=72, column=19)",YAMLParser,classic/forge/forge/utils/file_operations.py
HTMLParser,class,,"class HTMLParser(ParserStrategy):
    def read(self, file: BinaryIO) -> str:
        soup = BeautifulSoup(file, ""html.parser"")
        text = soup.get_text()
        return text","Point(row=75, column=0)","Point(row=79, column=19)",,classic/forge/forge/utils/file_operations.py
HTMLParser.read,function,,"def read(self, file: BinaryIO) -> str:
        soup = BeautifulSoup(file, ""html.parser"")
        text = soup.get_text()
        return text","Point(row=76, column=4)","Point(row=79, column=19)",HTMLParser,classic/forge/forge/utils/file_operations.py
LaTeXParser,class,,"class LaTeXParser(ParserStrategy):
    def read(self, file: BinaryIO) -> str:
        latex = file.read().decode()
        text = LatexNodes2Text().latex_to_text(latex)
        return text","Point(row=82, column=0)","Point(row=86, column=19)",,classic/forge/forge/utils/file_operations.py
LaTeXParser.read,function,,"def read(self, file: BinaryIO) -> str:
        latex = file.read().decode()
        text = LatexNodes2Text().latex_to_text(latex)
        return text","Point(row=83, column=4)","Point(row=86, column=19)",LaTeXParser,classic/forge/forge/utils/file_operations.py
FileContext,class,,"class FileContext:
    def __init__(self, parser: ParserStrategy, logger: logging.Logger):
        self.parser = parser
        self.logger = logger

    def set_parser(self, parser: ParserStrategy) -> None:
        self.logger.debug(f""Setting Context Parser to {parser}"")
        self.parser = parser

    def decode_file(self, file: BinaryIO) -> str:
        self.logger.debug(
            f""Reading {getattr(file, 'name', 'file')} with parser {self.parser}""
        )
        return self.parser.read(file)","Point(row=89, column=0)","Point(row=102, column=37)",,classic/forge/forge/utils/file_operations.py
FileContext.__init__,function,,"def __init__(self, parser: ParserStrategy, logger: logging.Logger):
        self.parser = parser
        self.logger = logger","Point(row=90, column=4)","Point(row=92, column=28)",FileContext,classic/forge/forge/utils/file_operations.py
FileContext.set_parser,function,,"def set_parser(self, parser: ParserStrategy) -> None:
        self.logger.debug(f""Setting Context Parser to {parser}"")
        self.parser = parser","Point(row=94, column=4)","Point(row=96, column=28)",FileContext,classic/forge/forge/utils/file_operations.py
FileContext.decode_file,function,,"def decode_file(self, file: BinaryIO) -> str:
        self.logger.debug(
            f""Reading {getattr(file, 'name', 'file')} with parser {self.parser}""
        )
        return self.parser.read(file)","Point(row=98, column=4)","Point(row=102, column=37)",FileContext,classic/forge/forge/utils/file_operations.py
is_file_binary_fn,function,"Given a file path load all its content and checks if the null bytes is present

    Args:
        file (_type_): _description_

    Returns:
        bool: is_binary
","def is_file_binary_fn(file: BinaryIO):
    """"""Given a file path load all its content and checks if the null bytes is present

    Args:
        file (_type_): _description_

    Returns:
        bool: is_binary
    """"""
    file_data = file.read()
    file.seek(0)
    if b""\x00"" in file_data:
        return True
    return False","Point(row=123, column=0)","Point(row=136, column=16)",,classic/forge/forge/utils/file_operations.py
decode_textual_file,function,,"def decode_textual_file(file: BinaryIO, ext: str, logger: logging.Logger) -> str:
    if not file.readable():
        raise ValueError(f""{repr(file)} is not readable"")

    parser = extension_to_parser.get(ext.lower())
    if not parser:
        if is_file_binary_fn(file):
            raise ValueError(f""Unsupported binary file format: {ext}"")
        # fallback to txt file parser (to support script and code files loading)
        parser = TXTParser()
    file_context = FileContext(parser, logger)
    return file_context.decode_file(file)","Point(row=139, column=0)","Point(row=150, column=41)",,classic/forge/forge/utils/file_operations.py
validate_url,function,"
    The method decorator validate_url is used to validate urls for any command that
    requires a url as an argument.
","def validate_url(func: Callable[P, T]) -> Callable[P, T]:
    """"""
    The method decorator validate_url is used to validate urls for any command that
    requires a url as an argument.
    """"""

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        sig = signature(func)
        bound_args = sig.bind(*args, **kwargs)
        bound_args.apply_defaults()

        url = bound_args.arguments.get(""url"")
        if url is None:
            raise ValueError(""URL is required for this function"")

        if not re.match(r""^https?://"", url):
            raise ValueError(
                ""Invalid URL format: URL must start with http:// or https://""
            )
        if not is_valid_url(url):
            raise ValueError(""Missing Scheme or Network location"")
        if check_local_file_access(url):
            raise ValueError(""Access to local files is restricted"")
        if len(url) > 2000:
            raise ValueError(""URL is too long"")

        bound_args.arguments[""url""] = sanitize_url(url)

        return func(*bound_args.args, **bound_args.kwargs)

    return wrapper  # type: ignore","Point(row=10, column=0)","Point(row=41, column=34)",,classic/forge/forge/utils/url_validator.py
validate_url.wrapper,function,,"def wrapper(*args, **kwargs):
        sig = signature(func)
        bound_args = sig.bind(*args, **kwargs)
        bound_args.apply_defaults()

        url = bound_args.arguments.get(""url"")
        if url is None:
            raise ValueError(""URL is required for this function"")

        if not re.match(r""^https?://"", url):
            raise ValueError(
                ""Invalid URL format: URL must start with http:// or https://""
            )
        if not is_valid_url(url):
            raise ValueError(""Missing Scheme or Network location"")
        if check_local_file_access(url):
            raise ValueError(""Access to local files is restricted"")
        if len(url) > 2000:
            raise ValueError(""URL is too long"")

        bound_args.arguments[""url""] = sanitize_url(url)

        return func(*bound_args.args, **bound_args.kwargs)","Point(row=17, column=4)","Point(row=39, column=58)",,classic/forge/forge/utils/url_validator.py
is_valid_url,function,"Check if the URL is valid

    Args:
        url (str): The URL to check

    Returns:
        bool: True if the URL is valid, False otherwise
","def is_valid_url(url: str) -> bool:
    """"""Check if the URL is valid

    Args:
        url (str): The URL to check

    Returns:
        bool: True if the URL is valid, False otherwise
    """"""
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except ValueError:
        return False","Point(row=44, column=0)","Point(row=57, column=20)",,classic/forge/forge/utils/url_validator.py
sanitize_url,function,"Sanitize the URL

    Args:
        url (str): The URL to sanitize

    Returns:
        str: The sanitized URL
","def sanitize_url(url: str) -> str:
    """"""Sanitize the URL

    Args:
        url (str): The URL to sanitize

    Returns:
        str: The sanitized URL
    """"""
    parsed_url = urlparse(url)
    reconstructed_url = f""{parsed_url.path}{parsed_url.params}?{parsed_url.query}""
    return urljoin(url, reconstructed_url)","Point(row=60, column=0)","Point(row=71, column=42)",,classic/forge/forge/utils/url_validator.py
check_local_file_access,function,"Check if the URL is a local file

    Args:
        url (str): The URL to check

    Returns:
        bool: True if the URL is a local file, False otherwise
","def check_local_file_access(url: str) -> bool:
    """"""Check if the URL is a local file

    Args:
        url (str): The URL to check

    Returns:
        bool: True if the URL is a local file, False otherwise
    """"""
    # List of local file prefixes
    local_file_prefixes = [
        ""file:///"",
        ""file://localhost"",
    ]

    return any(url.startswith(prefix) for prefix in local_file_prefixes)","Point(row=74, column=0)","Point(row=89, column=72)",,classic/forge/forge/utils/url_validator.py
DirectiveProvider,class,,"class DirectiveProvider(AgentComponent):
    def get_constraints(self) -> Iterator[str]:
        return iter([])

    def get_resources(self) -> Iterator[str]:
        return iter([])

    def get_best_practices(self) -> Iterator[str]:
        return iter([])","Point(row=12, column=0)","Point(row=20, column=23)",,classic/forge/forge/agent/protocols.py
DirectiveProvider.get_constraints,function,,"def get_constraints(self) -> Iterator[str]:
        return iter([])","Point(row=13, column=4)","Point(row=14, column=23)",DirectiveProvider,classic/forge/forge/agent/protocols.py
DirectiveProvider.get_resources,function,,"def get_resources(self) -> Iterator[str]:
        return iter([])","Point(row=16, column=4)","Point(row=17, column=23)",DirectiveProvider,classic/forge/forge/agent/protocols.py
DirectiveProvider.get_best_practices,function,,"def get_best_practices(self) -> Iterator[str]:
        return iter([])","Point(row=19, column=4)","Point(row=20, column=23)",DirectiveProvider,classic/forge/forge/agent/protocols.py
CommandProvider,class,,"class CommandProvider(AgentComponent):
    @abstractmethod
    def get_commands(self) -> Iterator[""Command""]:
        ...","Point(row=23, column=0)","Point(row=26, column=11)",,classic/forge/forge/agent/protocols.py
CommandProvider.get_commands,function,,"def get_commands(self) -> Iterator[""Command""]:
        ...","Point(row=25, column=4)","Point(row=26, column=11)",CommandProvider,classic/forge/forge/agent/protocols.py
MessageProvider,class,,"class MessageProvider(AgentComponent):
    @abstractmethod
    def get_messages(self) -> Iterator[""ChatMessage""]:
        ...","Point(row=29, column=0)","Point(row=32, column=11)",,classic/forge/forge/agent/protocols.py
MessageProvider.get_messages,function,,"def get_messages(self) -> Iterator[""ChatMessage""]:
        ...","Point(row=31, column=4)","Point(row=32, column=11)",MessageProvider,classic/forge/forge/agent/protocols.py
AfterParse,class,,"class AfterParse(AgentComponent, Generic[AnyProposal]):
    @abstractmethod
    def after_parse(self, result: AnyProposal) -> None | Awaitable[None]:
        ...","Point(row=35, column=0)","Point(row=38, column=11)",,classic/forge/forge/agent/protocols.py
AfterParse.after_parse,function,,"def after_parse(self, result: AnyProposal) -> None | Awaitable[None]:
        ...","Point(row=37, column=4)","Point(row=38, column=11)",AfterParse,classic/forge/forge/agent/protocols.py
ExecutionFailure,class,,"class ExecutionFailure(AgentComponent):
    @abstractmethod
    def execution_failure(self, error: Exception) -> None | Awaitable[None]:
        ...","Point(row=41, column=0)","Point(row=44, column=11)",,classic/forge/forge/agent/protocols.py
ExecutionFailure.execution_failure,function,,"def execution_failure(self, error: Exception) -> None | Awaitable[None]:
        ...","Point(row=43, column=4)","Point(row=44, column=11)",ExecutionFailure,classic/forge/forge/agent/protocols.py
AfterExecute,class,,"class AfterExecute(AgentComponent):
    @abstractmethod
    def after_execute(self, result: ""ActionResult"") -> None | Awaitable[None]:
        ...","Point(row=47, column=0)","Point(row=50, column=11)",,classic/forge/forge/agent/protocols.py
AfterExecute.after_execute,function,,"def after_execute(self, result: ""ActionResult"") -> None | Awaitable[None]:
        ...","Point(row=49, column=4)","Point(row=50, column=11)",AfterExecute,classic/forge/forge/agent/protocols.py
ForgeAgent,class,"
    The goal of the Forge is to take care of the boilerplate code,
    so you can focus on agent design.

    There is a great paper surveying the agent landscape: https://arxiv.org/abs/2308.11432
    Which I would highly recommend reading as it will help you understand the possibilities.

    ForgeAgent provides component support; https://docs.agpt.co/classic/forge/components/introduction/
    Using Components is a new way of building agents that is more flexible and easier to extend.
    Components replace some agent's logic and plugins with a more modular and composable system.
","class ForgeAgent(ProtocolAgent, BaseAgent):
    """"""
    The goal of the Forge is to take care of the boilerplate code,
    so you can focus on agent design.

    There is a great paper surveying the agent landscape: https://arxiv.org/abs/2308.11432
    Which I would highly recommend reading as it will help you understand the possibilities.

    ForgeAgent provides component support; https://docs.agpt.co/classic/forge/components/introduction/
    Using Components is a new way of building agents that is more flexible and easier to extend.
    Components replace some agent's logic and plugins with a more modular and composable system.
    """"""  # noqa: E501

    def __init__(self, database: AgentDB, workspace: FileStorage):
        """"""
        The database is used to store tasks, steps and artifact metadata.
        The workspace is used to store artifacts (files).
        """"""

        # An example agent information; you can modify this to suit your needs
        state = BaseAgentSettings(
            name=""Forge Agent"",
            description=""The Forge Agent is a generic agent that can solve tasks."",
            agent_id=str(uuid4()),
            ai_profile=AIProfile(
                ai_name=""ForgeAgent"", ai_role=""Generic Agent"", ai_goals=[""Solve tasks""]
            ),
            task=""Solve tasks"",
        )

        # ProtocolAgent adds the Agent Protocol (API) functionality
        ProtocolAgent.__init__(self, database, workspace)
        # BaseAgent provides the component handling functionality
        BaseAgent.__init__(self, state)

        # AGENT COMPONENTS
        # Components provide additional functionality to the agent
        # There are NO components added by default in the BaseAgent
        # You can create your own components or add existing ones
        # Built-in components:
        #   https://docs.agpt.co/classic/forge/components/built-in-components/

        # System component provides ""finish"" command and adds some prompt information
        self.system = SystemComponent()

    async def create_task(self, task_request: TaskRequestBody) -> Task:
        """"""
        The agent protocol, which is the core of the Forge,
        works by creating a task and then executing steps for that task.
        This method is called when the agent is asked to create a task.

        We are hooking into function to add a custom log message.
        Though you can do anything you want here.
        """"""
        task = await super().create_task(task_request)
        logger.info(
            f""üì¶ Task created with ID: {task.task_id} and ""
            f""input: {task.input[:40]}{'...' if len(task.input) > 40 else ''}""
        )
        return task

    async def execute_step(self, task_id: str, step_request: StepRequestBody) -> Step:
        """"""
        Preffered method to add agent logic is to add custom components:
        https://docs.agpt.co/classic/forge/components/creating-components/

        Outdated tutorial on how to add custom logic:
        https://aiedge.medium.com/autogpt-forge-e3de53cc58ec

        The agent protocol, which is the core of the Forge, works by creating a task and then
        executing steps for that task. This method is called when the agent is asked to execute
        a step.

        The task that is created contains an input string, for the benchmarks this is the task
        the agent has been asked to solve and additional input, which is a dictionary and
        could contain anything.

        If you want to get the task use:

        ```
        task = await self.db.get_task(task_id)
        ```

        The step request body is essentially the same as the task request and contains an input
        string, for the benchmarks this is the task the agent has been asked to solve and
        additional input, which is a dictionary and could contain anything.

        You need to implement logic that will take in this step input and output the completed step
        as a step object. You can do everything in a single step or you can break it down into
        multiple steps. Returning a request to continue in the step output, the user can then decide
        if they want the agent to continue or not.
        """"""  # noqa: E501

        step = await self.db.create_step(
            task_id=task_id, input=step_request, is_last=False
        )

        proposal = await self.propose_action()

        output = await self.execute(proposal)

        if isinstance(output, ActionSuccessResult):
            step.output = str(output.outputs)
        elif isinstance(output, ActionErrorResult):
            step.output = output.reason

        return step

    async def propose_action(self) -> ActionProposal:
        self.reset_trace()

        # Get directives
        directives = self.state.directives.model_copy(deep=True)
        directives.resources += await self.run_pipeline(DirectiveProvider.get_resources)
        directives.constraints += await self.run_pipeline(
            DirectiveProvider.get_constraints
        )
        directives.best_practices += await self.run_pipeline(
            DirectiveProvider.get_best_practices
        )

        # Get commands
        self.commands = await self.run_pipeline(CommandProvider.get_commands)

        # Get messages
        messages = await self.run_pipeline(MessageProvider.get_messages)

        prompt: ChatPrompt = ChatPrompt(
            messages=messages, functions=function_specs_from_commands(self.commands)
        )

        logger.debug(f""Executing prompt:\n{dump_prompt(prompt)}"")

        # Call the LLM and parse result
        # THIS NEEDS TO BE REPLACED WITH YOUR LLM CALL/LOGIC
        # Have a look at classic/original_autogpt/agents/agent.py
        # for an example (complete_and_parse)
        proposal = ActionProposal(
            thoughts=""I cannot solve the task!"",
            use_tool=AssistantFunctionCall(
                name=""finish"", arguments={""reason"": ""Unimplemented logic""}
            ),
            raw_message=AssistantChatMessage(
                content=""finish(reason='Unimplemented logic')""
            ),
        )

        self.config.cycle_count += 1

        return proposal

    async def execute(self, proposal: Any, user_feedback: str = """") -> ActionResult:
        tool = proposal.use_tool

        # Get commands
        self.commands = await self.run_pipeline(CommandProvider.get_commands)

        # Execute the command
        try:
            command: Optional[Command] = None
            for c in reversed(self.commands):
                if tool.name in c.names:
                    command = c

            if command is None:
                raise AgentException(f""Command {tool.name} not found"")

            command_result = command(**tool.arguments)
            if inspect.isawaitable(command_result):
                command_result = await command_result

            result = ActionSuccessResult(outputs=command_result)
        except AgentTerminated:
            result = ActionSuccessResult(outputs=""Agent terminated or finished"")
        except AgentException as e:
            result = ActionErrorResult.from_exception(e)
            logger.warning(f""{tool} raised an error: {e}"")

        await self.run_pipeline(AfterExecute.after_execute, result)

        logger.debug(""\n"".join(self.trace))

        return result

    async def do_not_execute(
        self, denied_proposal: Any, user_feedback: str
    ) -> ActionResult:
        result = ActionErrorResult(reason=""Action denied"")

        await self.run_pipeline(AfterExecute.after_execute, result)

        logger.debug(""\n"".join(self.trace))

        return result","Point(row=39, column=0)","Point(row=232, column=21)",,classic/forge/forge/agent/forge_agent.py
ForgeAgent.__init__,function,"
        The database is used to store tasks, steps and artifact metadata.
        The workspace is used to store artifacts (files).
","def __init__(self, database: AgentDB, workspace: FileStorage):
        """"""
        The database is used to store tasks, steps and artifact metadata.
        The workspace is used to store artifacts (files).
        """"""

        # An example agent information; you can modify this to suit your needs
        state = BaseAgentSettings(
            name=""Forge Agent"",
            description=""The Forge Agent is a generic agent that can solve tasks."",
            agent_id=str(uuid4()),
            ai_profile=AIProfile(
                ai_name=""ForgeAgent"", ai_role=""Generic Agent"", ai_goals=[""Solve tasks""]
            ),
            task=""Solve tasks"",
        )

        # ProtocolAgent adds the Agent Protocol (API) functionality
        ProtocolAgent.__init__(self, database, workspace)
        # BaseAgent provides the component handling functionality
        BaseAgent.__init__(self, state)

        # AGENT COMPONENTS
        # Components provide additional functionality to the agent
        # There are NO components added by default in the BaseAgent
        # You can create your own components or add existing ones
        # Built-in components:
        #   https://docs.agpt.co/classic/forge/components/built-in-components/

        # System component provides ""finish"" command and adds some prompt information
        self.system = SystemComponent()","Point(row=52, column=4)","Point(row=82, column=39)",ForgeAgent,classic/forge/forge/agent/forge_agent.py
ForgeAgent.create_task,function,"
        The agent protocol, which is the core of the Forge,
        works by creating a task and then executing steps for that task.
        This method is called when the agent is asked to create a task.

        We are hooking into function to add a custom log message.
        Though you can do anything you want here.
","async def create_task(self, task_request: TaskRequestBody) -> Task:
        """"""
        The agent protocol, which is the core of the Forge,
        works by creating a task and then executing steps for that task.
        This method is called when the agent is asked to create a task.

        We are hooking into function to add a custom log message.
        Though you can do anything you want here.
        """"""
        task = await super().create_task(task_request)
        logger.info(
            f""üì¶ Task created with ID: {task.task_id} and ""
            f""input: {task.input[:40]}{'...' if len(task.input) > 40 else ''}""
        )
        return task","Point(row=84, column=4)","Point(row=98, column=19)",ForgeAgent,classic/forge/forge/agent/forge_agent.py
ForgeAgent.execute_step,function,"
        Preffered method to add agent logic is to add custom components:
        https://docs.agpt.co/classic/forge/components/creating-components/

        Outdated tutorial on how to add custom logic:
        https://aiedge.medium.com/autogpt-forge-e3de53cc58ec

        The agent protocol, which is the core of the Forge, works by creating a task and then
        executing steps for that task. This method is called when the agent is asked to execute
        a step.

        The task that is created contains an input string, for the benchmarks this is the task
        the agent has been asked to solve and additional input, which is a dictionary and
        could contain anything.

        If you want to get the task use:

        ```
        task = await self.db.get_task(task_id)
        ```

        The step request body is essentially the same as the task request and contains an input
        string, for the benchmarks this is the task the agent has been asked to solve and
        additional input, which is a dictionary and could contain anything.

        You need to implement logic that will take in this step input and output the completed step
        as a step object. You can do everything in a single step or you can break it down into
        multiple steps. Returning a request to continue in the step output, the user can then decide
        if they want the agent to continue or not.
","async def execute_step(self, task_id: str, step_request: StepRequestBody) -> Step:
        """"""
        Preffered method to add agent logic is to add custom components:
        https://docs.agpt.co/classic/forge/components/creating-components/

        Outdated tutorial on how to add custom logic:
        https://aiedge.medium.com/autogpt-forge-e3de53cc58ec

        The agent protocol, which is the core of the Forge, works by creating a task and then
        executing steps for that task. This method is called when the agent is asked to execute
        a step.

        The task that is created contains an input string, for the benchmarks this is the task
        the agent has been asked to solve and additional input, which is a dictionary and
        could contain anything.

        If you want to get the task use:

        ```
        task = await self.db.get_task(task_id)
        ```

        The step request body is essentially the same as the task request and contains an input
        string, for the benchmarks this is the task the agent has been asked to solve and
        additional input, which is a dictionary and could contain anything.

        You need to implement logic that will take in this step input and output the completed step
        as a step object. You can do everything in a single step or you can break it down into
        multiple steps. Returning a request to continue in the step output, the user can then decide
        if they want the agent to continue or not.
        """"""  # noqa: E501

        step = await self.db.create_step(
            task_id=task_id, input=step_request, is_last=False
        )

        proposal = await self.propose_action()

        output = await self.execute(proposal)

        if isinstance(output, ActionSuccessResult):
            step.output = str(output.outputs)
        elif isinstance(output, ActionErrorResult):
            step.output = output.reason

        return step","Point(row=100, column=4)","Point(row=145, column=19)",ForgeAgent,classic/forge/forge/agent/forge_agent.py
ForgeAgent.propose_action,function,,"async def propose_action(self) -> ActionProposal:
        self.reset_trace()

        # Get directives
        directives = self.state.directives.model_copy(deep=True)
        directives.resources += await self.run_pipeline(DirectiveProvider.get_resources)
        directives.constraints += await self.run_pipeline(
            DirectiveProvider.get_constraints
        )
        directives.best_practices += await self.run_pipeline(
            DirectiveProvider.get_best_practices
        )

        # Get commands
        self.commands = await self.run_pipeline(CommandProvider.get_commands)

        # Get messages
        messages = await self.run_pipeline(MessageProvider.get_messages)

        prompt: ChatPrompt = ChatPrompt(
            messages=messages, functions=function_specs_from_commands(self.commands)
        )

        logger.debug(f""Executing prompt:\n{dump_prompt(prompt)}"")

        # Call the LLM and parse result
        # THIS NEEDS TO BE REPLACED WITH YOUR LLM CALL/LOGIC
        # Have a look at classic/original_autogpt/agents/agent.py
        # for an example (complete_and_parse)
        proposal = ActionProposal(
            thoughts=""I cannot solve the task!"",
            use_tool=AssistantFunctionCall(
                name=""finish"", arguments={""reason"": ""Unimplemented logic""}
            ),
            raw_message=AssistantChatMessage(
                content=""finish(reason='Unimplemented logic')""
            ),
        )

        self.config.cycle_count += 1

        return proposal","Point(row=147, column=4)","Point(row=188, column=23)",ForgeAgent,classic/forge/forge/agent/forge_agent.py
ForgeAgent.execute,function,,"async def execute(self, proposal: Any, user_feedback: str = """") -> ActionResult:
        tool = proposal.use_tool

        # Get commands
        self.commands = await self.run_pipeline(CommandProvider.get_commands)

        # Execute the command
        try:
            command: Optional[Command] = None
            for c in reversed(self.commands):
                if tool.name in c.names:
                    command = c

            if command is None:
                raise AgentException(f""Command {tool.name} not found"")

            command_result = command(**tool.arguments)
            if inspect.isawaitable(command_result):
                command_result = await command_result

            result = ActionSuccessResult(outputs=command_result)
        except AgentTerminated:
            result = ActionSuccessResult(outputs=""Agent terminated or finished"")
        except AgentException as e:
            result = ActionErrorResult.from_exception(e)
            logger.warning(f""{tool} raised an error: {e}"")

        await self.run_pipeline(AfterExecute.after_execute, result)

        logger.debug(""\n"".join(self.trace))

        return result","Point(row=190, column=4)","Point(row=221, column=21)",ForgeAgent,classic/forge/forge/agent/forge_agent.py
ForgeAgent.do_not_execute,function,,"async def do_not_execute(
        self, denied_proposal: Any, user_feedback: str
    ) -> ActionResult:
        result = ActionErrorResult(reason=""Action denied"")

        await self.run_pipeline(AfterExecute.after_execute, result)

        logger.debug(""\n"".join(self.trace))

        return result","Point(row=223, column=4)","Point(row=232, column=21)",ForgeAgent,classic/forge/forge/agent/forge_agent.py
AgentComponent,class,Base class for all agent components.,"class AgentComponent(ABC):
    """"""Base class for all agent components.""""""

    _run_after: list[type[AgentComponent]] = []
    _enabled: Callable[[], bool] | bool = True
    _disabled_reason: str = """"

    @property
    def enabled(self) -> bool:
        if callable(self._enabled):
            return self._enabled()
        return self._enabled

    @property
    def disabled_reason(self) -> str:
        """"""Return the reason this component is disabled.""""""
        return self._disabled_reason

    def run_after(self: AC, *components: type[AgentComponent] | AgentComponent) -> AC:
        """"""Set the components that this component should run after.""""""
        for component in components:
            t = component if isinstance(component, type) else type(component)
            if t not in self._run_after and t is not self.__class__:
                self._run_after.append(t)
        return self","Point(row=13, column=0)","Point(row=37, column=19)",,classic/forge/forge/agent/components.py
AgentComponent.enabled,function,,"def enabled(self) -> bool:
        if callable(self._enabled):
            return self._enabled()
        return self._enabled","Point(row=21, column=4)","Point(row=24, column=28)",AgentComponent,classic/forge/forge/agent/components.py
AgentComponent.disabled_reason,function,Return the reason this component is disabled.,"def disabled_reason(self) -> str:
        """"""Return the reason this component is disabled.""""""
        return self._disabled_reason","Point(row=27, column=4)","Point(row=29, column=36)",AgentComponent,classic/forge/forge/agent/components.py
AgentComponent.run_after,function,Set the components that this component should run after.,"def run_after(self: AC, *components: type[AgentComponent] | AgentComponent) -> AC:
        """"""Set the components that this component should run after.""""""
        for component in components:
            t = component if isinstance(component, type) else type(component)
            if t not in self._run_after and t is not self.__class__:
                self._run_after.append(t)
        return self","Point(row=31, column=4)","Point(row=37, column=19)",AgentComponent,classic/forge/forge/agent/components.py
ConfigurableComponent,class,A component that can be configured with a Pydantic model.,"class ConfigurableComponent(ABC, Generic[BM]):
    """"""A component that can be configured with a Pydantic model.""""""

    config_class: ClassVar[type[BM]]  # type: ignore

    def __init__(self, configuration: Optional[BM]):
        self._config: Optional[BM] = None
        if configuration is not None:
            self.config = configuration

    def __init_subclass__(cls, **kwargs):
        super().__init_subclass__(**kwargs)
        if getattr(cls, ""config_class"", None) is None:
            raise NotImplementedError(
                f""ConfigurableComponent subclass {cls.__name__} ""
                ""must define config_class class attribute.""
            )

    @property
    def config(self) -> BM:
        if not hasattr(self, ""_config"") or self._config is None:
            self.config = self.config_class()
        return self._config  # type: ignore

    @config.setter
    def config(self, config: BM):
        if not hasattr(self, ""_config"") or self._config is None:
            # Load configuration from environment variables
            updated = _update_user_config_from_env(config)
            config = self.config_class(**deep_update(config.model_dump(), updated))
        self._config = config","Point(row=40, column=0)","Point(row=70, column=29)",,classic/forge/forge/agent/components.py
ConfigurableComponent.__init__,function,,"def __init__(self, configuration: Optional[BM]):
        self._config: Optional[BM] = None
        if configuration is not None:
            self.config = configuration","Point(row=45, column=4)","Point(row=48, column=39)",ConfigurableComponent,classic/forge/forge/agent/components.py
ConfigurableComponent.__init_subclass__,function,,"def __init_subclass__(cls, **kwargs):
        super().__init_subclass__(**kwargs)
        if getattr(cls, ""config_class"", None) is None:
            raise NotImplementedError(
                f""ConfigurableComponent subclass {cls.__name__} ""
                ""must define config_class class attribute.""
            )","Point(row=50, column=4)","Point(row=56, column=13)",ConfigurableComponent,classic/forge/forge/agent/components.py
ConfigurableComponent.config,function,,"def config(self) -> BM:
        if not hasattr(self, ""_config"") or self._config is None:
            self.config = self.config_class()
        return self._config  # type: ignore","Point(row=59, column=4)","Point(row=62, column=43)",ConfigurableComponent,classic/forge/forge/agent/components.py
ConfigurableComponent.config,function,,"def config(self, config: BM):
        if not hasattr(self, ""_config"") or self._config is None:
            # Load configuration from environment variables
            updated = _update_user_config_from_env(config)
            config = self.config_class(**deep_update(config.model_dump(), updated))
        self._config = config","Point(row=65, column=4)","Point(row=70, column=29)",ConfigurableComponent,classic/forge/forge/agent/components.py
ComponentEndpointError,class,Error of a single protocol method on a component.,"class ComponentEndpointError(Exception):
    """"""Error of a single protocol method on a component.""""""

    def __init__(self, message: str, component: AgentComponent):
        self.message = message
        self.triggerer = component
        super().__init__(message)","Point(row=73, column=0)","Point(row=79, column=33)",,classic/forge/forge/agent/components.py
ComponentEndpointError.__init__,function,,"def __init__(self, message: str, component: AgentComponent):
        self.message = message
        self.triggerer = component
        super().__init__(message)","Point(row=76, column=4)","Point(row=79, column=33)",ComponentEndpointError,classic/forge/forge/agent/components.py
EndpointPipelineError,class,Error of an entire pipeline of one endpoint.,"class EndpointPipelineError(ComponentEndpointError):
    """"""Error of an entire pipeline of one endpoint.""""""","Point(row=82, column=0)","Point(row=83, column=54)",,classic/forge/forge/agent/components.py
ComponentSystemError,class,"Error of a group of pipelines;
    multiple different endpoints.","class ComponentSystemError(EndpointPipelineError):
    """"""Error of a group of pipelines;
    multiple different endpoints.""""""","Point(row=86, column=0)","Point(row=88, column=36)",,classic/forge/forge/agent/components.py
BaseAgentConfiguration,class,,"class BaseAgentConfiguration(SystemConfiguration):
    allow_fs_access: bool = UserConfigurable(default=False)

    fast_llm: ModelName = UserConfigurable(default=OpenAIModelName.GPT3_16k)
    smart_llm: ModelName = UserConfigurable(default=OpenAIModelName.GPT4)
    use_functions_api: bool = UserConfigurable(default=False)

    default_cycle_instruction: str = DEFAULT_TRIGGERING_PROMPT
    """"""The default instruction passed to the AI for a thinking cycle.""""""

    big_brain: bool = UserConfigurable(default=True)
    """"""
    Whether this agent uses the configured smart LLM (default) to think,
    as opposed to the configured fast LLM. Enabling this disables hybrid mode.
    """"""

    cycle_budget: Optional[int] = 1
    """"""
    The number of cycles that the agent is allowed to run unsupervised.

    `None` for unlimited continuous execution,
    `1` to require user approval for every step,
    `0` to stop the agent.
    """"""

    cycles_remaining: int = cycle_budget
    """"""The number of cycles remaining within the `cycle_budget`.""""""

    cycle_count: int = 0
    """"""The number of cycles that the agent has run since its initialization.""""""

    send_token_limit: Optional[int] = None
    """"""
    The token limit for prompt construction. Should leave room for the completion;
    defaults to 75% of `llm.max_tokens`.
    """"""

    @field_validator(""use_functions_api"")
    def validate_openai_functions(cls, value: bool, info: ValidationInfo):
        if value:
            smart_llm = info.data[""smart_llm""]
            fast_llm = info.data[""fast_llm""]
            assert all(
                [
                    not any(s in name for s in {""-0301"", ""-0314""})
                    for name in {smart_llm, fast_llm}
                ]
            ), (
                f""Model {smart_llm} does not support OpenAI Functions. ""
                ""Please disable OPENAI_FUNCTIONS or choose a suitable model.""
            )
        return value","Point(row=49, column=0)","Point(row=100, column=20)",,classic/forge/forge/agent/base.py
BaseAgentConfiguration.validate_openai_functions,function,,"def validate_openai_functions(cls, value: bool, info: ValidationInfo):
        if value:
            smart_llm = info.data[""smart_llm""]
            fast_llm = info.data[""fast_llm""]
            assert all(
                [
                    not any(s in name for s in {""-0301"", ""-0314""})
                    for name in {smart_llm, fast_llm}
                ]
            ), (
                f""Model {smart_llm} does not support OpenAI Functions. ""
                ""Please disable OPENAI_FUNCTIONS or choose a suitable model.""
            )
        return value","Point(row=87, column=4)","Point(row=100, column=20)",BaseAgentConfiguration,classic/forge/forge/agent/base.py
BaseAgentSettings,class,,"class BaseAgentSettings(SystemSettings):
    agent_id: str = """"

    ai_profile: AIProfile = Field(default_factory=lambda: AIProfile(ai_name=""AutoGPT""))
    """"""The AI profile or ""personality"" of the agent.""""""

    directives: AIDirectives = Field(default_factory=AIDirectives)
    """"""Directives (general instructional guidelines) for the agent.""""""

    task: str = ""Terminate immediately""  # FIXME: placeholder for forge.sdk.schema.Task
    """"""The user-given task that the agent is working on.""""""

    config: BaseAgentConfiguration = Field(default_factory=BaseAgentConfiguration)
    """"""The configuration for this BaseAgent subsystem instance.""""""","Point(row=103, column=0)","Point(row=116, column=66)",,classic/forge/forge/agent/base.py
AgentMeta,class,,"class AgentMeta(ABCMeta):
    def __call__(cls, *args, **kwargs):
        # Create instance of the class (Agent or BaseAgent)
        instance = super().__call__(*args, **kwargs)
        # Automatically collect modules after the instance is created
        instance._collect_components()
        return instance","Point(row=119, column=0)","Point(row=125, column=23)",,classic/forge/forge/agent/base.py
AgentMeta.__call__,function,,"def __call__(cls, *args, **kwargs):
        # Create instance of the class (Agent or BaseAgent)
        instance = super().__call__(*args, **kwargs)
        # Automatically collect modules after the instance is created
        instance._collect_components()
        return instance","Point(row=120, column=4)","Point(row=125, column=23)",AgentMeta,classic/forge/forge/agent/base.py
BaseAgent,class,,"class BaseAgent(Generic[AnyProposal], metaclass=AgentMeta):
    def __init__(
        self,
        settings: BaseAgentSettings,
    ):
        self.state = settings
        self.components: list[AgentComponent] = []
        self.config = settings.config
        # Execution data for debugging
        self._trace: list[str] = []

        logger.debug(f""Created {__class__} '{self.state.ai_profile.ai_name}'"")

    @property
    def trace(self) -> list[str]:
        return self._trace

    @property
    def llm(self) -> ChatModelInfo:
        """"""The LLM that the agent uses to think.""""""
        llm_name = (
            self.config.smart_llm if self.config.big_brain else self.config.fast_llm
        )
        return CHAT_MODELS[llm_name]

    @property
    def send_token_limit(self) -> int:
        return self.config.send_token_limit or self.llm.max_tokens * 3 // 4

    @abstractmethod
    async def propose_action(self) -> AnyProposal:
        ...

    @abstractmethod
    async def execute(
        self,
        proposal: AnyProposal,
        user_feedback: str = """",
    ) -> ActionResult:
        ...

    @abstractmethod
    async def do_not_execute(
        self,
        denied_proposal: AnyProposal,
        user_feedback: str,
    ) -> ActionResult:
        ...

    def reset_trace(self):
        self._trace = []

    @overload
    async def run_pipeline(
        self, protocol_method: Callable[P, Iterator[T]], *args, retry_limit: int = 3
    ) -> list[T]:
        ...

    @overload
    async def run_pipeline(
        self,
        protocol_method: Callable[P, None | Awaitable[None]],
        *args,
        retry_limit: int = 3,
    ) -> list[None]:
        ...

    async def run_pipeline(
        self,
        protocol_method: Callable[P, Iterator[T] | None | Awaitable[None]],
        *args,
        retry_limit: int = 3,
    ) -> list[T] | list[None]:
        method_name = protocol_method.__name__
        protocol_name = protocol_method.__qualname__.split(""."")[0]
        protocol_class = getattr(protocols, protocol_name)
        if not issubclass(protocol_class, AgentComponent):
            raise TypeError(f""{repr(protocol_method)} is not a protocol method"")

        # Clone parameters to revert on failure
        original_args = self._selective_copy(args)
        pipeline_attempts = 0
        method_result: list[T] = []
        self._trace.append(f""‚¨áÔ∏è  {Fore.BLUE}{method_name}{Fore.RESET}"")

        while pipeline_attempts < retry_limit:
            try:
                for component in self.components:
                    # Skip other protocols
                    if not isinstance(component, protocol_class):
                        continue

                    # Skip disabled components
                    if not component.enabled:
                        self._trace.append(
                            f""   {Fore.LIGHTBLACK_EX}""
                            f""{component.__class__.__name__}{Fore.RESET}""
                        )
                        continue

                    method = cast(
                        Callable[..., Iterator[T] | None | Awaitable[None]] | None,
                        getattr(component, method_name, None),
                    )
                    if not callable(method):
                        continue

                    component_attempts = 0
                    while component_attempts < retry_limit:
                        try:
                            component_args = self._selective_copy(args)
                            result = method(*component_args)
                            if inspect.isawaitable(result):
                                result = await result
                            if result is not None:
                                method_result.extend(result)
                            args = component_args
                            self._trace.append(f""‚úÖ {component.__class__.__name__}"")

                        except ComponentEndpointError:
                            self._trace.append(
                                f""‚ùå {Fore.YELLOW}{component.__class__.__name__}: ""
                                f""ComponentEndpointError{Fore.RESET}""
                            )
                            # Retry the same component on ComponentEndpointError
                            component_attempts += 1
                            continue
                        # Successful component execution
                        break
                # Successful pipeline execution
                break
            except EndpointPipelineError as e:
                self._trace.append(
                    f""‚ùå {Fore.LIGHTRED_EX}{e.triggerer.__class__.__name__}: ""
                    f""EndpointPipelineError{Fore.RESET}""
                )
                # Restart from the beginning on EndpointPipelineError
                # Revert to original parameters
                args = self._selective_copy(original_args)
                pipeline_attempts += 1
                continue  # Start the loop over
            except Exception as e:
                raise e
        return method_result

    def dump_component_configs(self) -> str:
        configs: dict[str, Any] = {}
        for component in self.components:
            if isinstance(component, ConfigurableComponent):
                config_type_name = component.config.__class__.__name__
                configs[config_type_name] = component.config
        return to_json(configs).decode()

    def load_component_configs(self, serialized_configs: str):
        configs_dict: dict[str, dict[str, Any]] = from_json(serialized_configs)

        for component in self.components:
            if not isinstance(component, ConfigurableComponent):
                continue
            config_type = type(component.config)
            config_type_name = config_type.__name__
            if config_type_name in configs_dict:
                # Parse the serialized data and update the existing config
                updated_data = configs_dict[config_type_name]
                data = {**component.config.model_dump(), **updated_data}
                component.config = component.config.__class__(**data)

    def _collect_components(self):
        components = [
            getattr(self, attr)
            for attr in dir(self)
            if isinstance(getattr(self, attr), AgentComponent)
        ]

        if self.components:
            # Check if any component is missing (added to Agent but not to components)
            for component in components:
                if component not in self.components:
                    logger.warning(
                        f""Component {component.__class__.__name__} ""
                        ""is attached to an agent but not added to components list""
                    )
            # Skip collecting and sorting and sort if ordering is explicit
            return
        self.components = self._topological_sort(components)

    def _topological_sort(
        self, components: list[AgentComponent]
    ) -> list[AgentComponent]:
        visited = set()
        stack = []

        def visit(node: AgentComponent):
            if node in visited:
                return
            visited.add(node)
            for neighbor_class in node._run_after:
                neighbor = next(
                    (m for m in components if isinstance(m, neighbor_class)), None
                )
                if neighbor and neighbor not in visited:
                    visit(neighbor)
            stack.append(node)

        for component in components:
            visit(component)

        return stack

    def _selective_copy(self, args: tuple[Any, ...]) -> tuple[Any, ...]:
        copied_args = []
        for item in args:
            if isinstance(item, list):
                # Shallow copy for lists
                copied_item = item[:]
            elif isinstance(item, dict):
                # Shallow copy for dicts
                copied_item = item.copy()
            elif isinstance(item, BaseModel):
                # Deep copy for Pydantic models (deep=True to also copy nested models)
                copied_item = item.model_copy(deep=True)
            else:
                # Deep copy for other objects
                copied_item = copy.deepcopy(item)
            copied_args.append(copied_item)
        return tuple(copied_args)","Point(row=128, column=0)","Point(row=353, column=33)",,classic/forge/forge/agent/base.py
BaseAgent.__init__,function,,"def __init__(
        self,
        settings: BaseAgentSettings,
    ):
        self.state = settings
        self.components: list[AgentComponent] = []
        self.config = settings.config
        # Execution data for debugging
        self._trace: list[str] = []

        logger.debug(f""Created {__class__} '{self.state.ai_profile.ai_name}'"")","Point(row=129, column=4)","Point(row=139, column=78)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent.trace,function,,"def trace(self) -> list[str]:
        return self._trace","Point(row=142, column=4)","Point(row=143, column=26)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent.llm,function,The LLM that the agent uses to think.,"def llm(self) -> ChatModelInfo:
        """"""The LLM that the agent uses to think.""""""
        llm_name = (
            self.config.smart_llm if self.config.big_brain else self.config.fast_llm
        )
        return CHAT_MODELS[llm_name]","Point(row=146, column=4)","Point(row=151, column=36)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent.send_token_limit,function,,"def send_token_limit(self) -> int:
        return self.config.send_token_limit or self.llm.max_tokens * 3 // 4","Point(row=154, column=4)","Point(row=155, column=75)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent.propose_action,function,,"async def propose_action(self) -> AnyProposal:
        ...","Point(row=158, column=4)","Point(row=159, column=11)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent.execute,function,,"async def execute(
        self,
        proposal: AnyProposal,
        user_feedback: str = """",
    ) -> ActionResult:
        ...","Point(row=162, column=4)","Point(row=167, column=11)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent.do_not_execute,function,,"async def do_not_execute(
        self,
        denied_proposal: AnyProposal,
        user_feedback: str,
    ) -> ActionResult:
        ...","Point(row=170, column=4)","Point(row=175, column=11)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent.reset_trace,function,,"def reset_trace(self):
        self._trace = []","Point(row=177, column=4)","Point(row=178, column=24)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent.run_pipeline,function,,"async def run_pipeline(
        self, protocol_method: Callable[P, Iterator[T]], *args, retry_limit: int = 3
    ) -> list[T]:
        ...","Point(row=181, column=4)","Point(row=184, column=11)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent.run_pipeline,function,,"async def run_pipeline(
        self,
        protocol_method: Callable[P, None | Awaitable[None]],
        *args,
        retry_limit: int = 3,
    ) -> list[None]:
        ...","Point(row=187, column=4)","Point(row=193, column=11)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent.run_pipeline,function,,"async def run_pipeline(
        self,
        protocol_method: Callable[P, Iterator[T] | None | Awaitable[None]],
        *args,
        retry_limit: int = 3,
    ) -> list[T] | list[None]:
        method_name = protocol_method.__name__
        protocol_name = protocol_method.__qualname__.split(""."")[0]
        protocol_class = getattr(protocols, protocol_name)
        if not issubclass(protocol_class, AgentComponent):
            raise TypeError(f""{repr(protocol_method)} is not a protocol method"")

        # Clone parameters to revert on failure
        original_args = self._selective_copy(args)
        pipeline_attempts = 0
        method_result: list[T] = []
        self._trace.append(f""‚¨áÔ∏è  {Fore.BLUE}{method_name}{Fore.RESET}"")

        while pipeline_attempts < retry_limit:
            try:
                for component in self.components:
                    # Skip other protocols
                    if not isinstance(component, protocol_class):
                        continue

                    # Skip disabled components
                    if not component.enabled:
                        self._trace.append(
                            f""   {Fore.LIGHTBLACK_EX}""
                            f""{component.__class__.__name__}{Fore.RESET}""
                        )
                        continue

                    method = cast(
                        Callable[..., Iterator[T] | None | Awaitable[None]] | None,
                        getattr(component, method_name, None),
                    )
                    if not callable(method):
                        continue

                    component_attempts = 0
                    while component_attempts < retry_limit:
                        try:
                            component_args = self._selective_copy(args)
                            result = method(*component_args)
                            if inspect.isawaitable(result):
                                result = await result
                            if result is not None:
                                method_result.extend(result)
                            args = component_args
                            self._trace.append(f""‚úÖ {component.__class__.__name__}"")

                        except ComponentEndpointError:
                            self._trace.append(
                                f""‚ùå {Fore.YELLOW}{component.__class__.__name__}: ""
                                f""ComponentEndpointError{Fore.RESET}""
                            )
                            # Retry the same component on ComponentEndpointError
                            component_attempts += 1
                            continue
                        # Successful component execution
                        break
                # Successful pipeline execution
                break
            except EndpointPipelineError as e:
                self._trace.append(
                    f""‚ùå {Fore.LIGHTRED_EX}{e.triggerer.__class__.__name__}: ""
                    f""EndpointPipelineError{Fore.RESET}""
                )
                # Restart from the beginning on EndpointPipelineError
                # Revert to original parameters
                args = self._selective_copy(original_args)
                pipeline_attempts += 1
                continue  # Start the loop over
            except Exception as e:
                raise e
        return method_result","Point(row=195, column=4)","Point(row=271, column=28)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent.dump_component_configs,function,,"def dump_component_configs(self) -> str:
        configs: dict[str, Any] = {}
        for component in self.components:
            if isinstance(component, ConfigurableComponent):
                config_type_name = component.config.__class__.__name__
                configs[config_type_name] = component.config
        return to_json(configs).decode()","Point(row=273, column=4)","Point(row=279, column=40)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent.load_component_configs,function,,"def load_component_configs(self, serialized_configs: str):
        configs_dict: dict[str, dict[str, Any]] = from_json(serialized_configs)

        for component in self.components:
            if not isinstance(component, ConfigurableComponent):
                continue
            config_type = type(component.config)
            config_type_name = config_type.__name__
            if config_type_name in configs_dict:
                # Parse the serialized data and update the existing config
                updated_data = configs_dict[config_type_name]
                data = {**component.config.model_dump(), **updated_data}
                component.config = component.config.__class__(**data)","Point(row=281, column=4)","Point(row=293, column=69)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent._collect_components,function,,"def _collect_components(self):
        components = [
            getattr(self, attr)
            for attr in dir(self)
            if isinstance(getattr(self, attr), AgentComponent)
        ]

        if self.components:
            # Check if any component is missing (added to Agent but not to components)
            for component in components:
                if component not in self.components:
                    logger.warning(
                        f""Component {component.__class__.__name__} ""
                        ""is attached to an agent but not added to components list""
                    )
            # Skip collecting and sorting and sort if ordering is explicit
            return
        self.components = self._topological_sort(components)","Point(row=295, column=4)","Point(row=312, column=60)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent._topological_sort,function,,"def _topological_sort(
        self, components: list[AgentComponent]
    ) -> list[AgentComponent]:
        visited = set()
        stack = []

        def visit(node: AgentComponent):
            if node in visited:
                return
            visited.add(node)
            for neighbor_class in node._run_after:
                neighbor = next(
                    (m for m in components if isinstance(m, neighbor_class)), None
                )
                if neighbor and neighbor not in visited:
                    visit(neighbor)
            stack.append(node)

        for component in components:
            visit(component)

        return stack","Point(row=314, column=4)","Point(row=335, column=20)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent._topological_sort.visit,function,,"def visit(node: AgentComponent):
            if node in visited:
                return
            visited.add(node)
            for neighbor_class in node._run_after:
                neighbor = next(
                    (m for m in components if isinstance(m, neighbor_class)), None
                )
                if neighbor and neighbor not in visited:
                    visit(neighbor)
            stack.append(node)","Point(row=320, column=8)","Point(row=330, column=30)",BaseAgent,classic/forge/forge/agent/base.py
BaseAgent._selective_copy,function,,"def _selective_copy(self, args: tuple[Any, ...]) -> tuple[Any, ...]:
        copied_args = []
        for item in args:
            if isinstance(item, list):
                # Shallow copy for lists
                copied_item = item[:]
            elif isinstance(item, dict):
                # Shallow copy for dicts
                copied_item = item.copy()
            elif isinstance(item, BaseModel):
                # Deep copy for Pydantic models (deep=True to also copy nested models)
                copied_item = item.model_copy(deep=True)
            else:
                # Deep copy for other objects
                copied_item = copy.deepcopy(item)
            copied_args.append(copied_item)
        return tuple(copied_args)","Point(row=337, column=4)","Point(row=353, column=33)",BaseAgent,classic/forge/forge/agent/base.py
UserConfigurable,function,,"def UserConfigurable(
    default: T | PydanticUndefinedType = PydanticUndefined,
    *args,
    default_factory: Optional[Callable[[], T]] = None,
    from_env: Optional[str | Callable[[], T | None]] = None,
    description: str = """",
    exclude: bool = False,
    **kwargs,
) -> T:
    # TODO: use this to auto-generate docs for the application configuration
    field_info: FieldInfo = Field(
        default,
        *args,
        default_factory=default_factory,
        description=description,
        exclude=exclude,
        **kwargs,
    )
    field_info.metadata.append((""user_configurable"", True))
    field_info.metadata.append((""from_env"", from_env))

    return field_info  # type: ignore","Point(row=15, column=0)","Point(row=36, column=37)",,classic/forge/forge/models/config.py
_get_field_metadata,function,,"def _get_field_metadata(field: FieldInfo, key: str, default: Any = None) -> Any:
    for item in field.metadata:
        if isinstance(item, tuple) and item[0] == key:
            return item[1]
        if isinstance(item, str) and item == key:
            return True
    return default","Point(row=39, column=0)","Point(row=45, column=18)",,classic/forge/forge/models/config.py
SystemConfiguration,class,,"class SystemConfiguration(BaseModel):
    def get_user_config(self) -> dict[str, Any]:
        return _recurse_user_config_values(self)

    @classmethod
    def from_env(cls):
        """"""
        Initializes the config object from environment variables.

        Environment variables are mapped to UserConfigurable fields using the from_env
        attribute that can be passed to UserConfigurable.
        """"""

        def infer_field_value(field: FieldInfo):
            default_value = (
                field.default
                if field.default not in (None, PydanticUndefined)
                else (
                    field.default_factory()
                    if field.default_factory
                    else PydanticUndefined
                )
            )
            if from_env := _get_field_metadata(field, ""from_env""):
                val_from_env = (
                    os.getenv(from_env) if type(from_env) is str else from_env()
                )
                if val_from_env is not None:
                    return val_from_env
            return default_value

        return _recursive_init_model(cls, infer_field_value)

    model_config = ConfigDict(
        extra=""forbid"", use_enum_values=True, validate_assignment=True
    )","Point(row=48, column=0)","Point(row=83, column=5)",,classic/forge/forge/models/config.py
SystemConfiguration.get_user_config,function,,"def get_user_config(self) -> dict[str, Any]:
        return _recurse_user_config_values(self)","Point(row=49, column=4)","Point(row=50, column=48)",SystemConfiguration,classic/forge/forge/models/config.py
SystemConfiguration.from_env,function,"
        Initializes the config object from environment variables.

        Environment variables are mapped to UserConfigurable fields using the from_env
        attribute that can be passed to UserConfigurable.
","def from_env(cls):
        """"""
        Initializes the config object from environment variables.

        Environment variables are mapped to UserConfigurable fields using the from_env
        attribute that can be passed to UserConfigurable.
        """"""

        def infer_field_value(field: FieldInfo):
            default_value = (
                field.default
                if field.default not in (None, PydanticUndefined)
                else (
                    field.default_factory()
                    if field.default_factory
                    else PydanticUndefined
                )
            )
            if from_env := _get_field_metadata(field, ""from_env""):
                val_from_env = (
                    os.getenv(from_env) if type(from_env) is str else from_env()
                )
                if val_from_env is not None:
                    return val_from_env
            return default_value

        return _recursive_init_model(cls, infer_field_value)","Point(row=53, column=4)","Point(row=79, column=60)",SystemConfiguration,classic/forge/forge/models/config.py
SystemConfiguration.from_env.infer_field_value,function,,"def infer_field_value(field: FieldInfo):
            default_value = (
                field.default
                if field.default not in (None, PydanticUndefined)
                else (
                    field.default_factory()
                    if field.default_factory
                    else PydanticUndefined
                )
            )
            if from_env := _get_field_metadata(field, ""from_env""):
                val_from_env = (
                    os.getenv(from_env) if type(from_env) is str else from_env()
                )
                if val_from_env is not None:
                    return val_from_env
            return default_value","Point(row=61, column=8)","Point(row=77, column=32)",SystemConfiguration,classic/forge/forge/models/config.py
SystemSettings,class,A base class for all system settings.,"class SystemSettings(BaseModel):
    """"""A base class for all system settings.""""""

    name: str
    description: str

    model_config = ConfigDict(
        extra=""forbid"", use_enum_values=True, validate_assignment=True
    )","Point(row=89, column=0)","Point(row=97, column=5)",,classic/forge/forge/models/config.py
Configurable,class,A base class for all configurable objects.,"class Configurable(Generic[S]):
    """"""A base class for all configurable objects.""""""

    prefix: str = """"
    default_settings: typing.ClassVar[S]  # type: ignore

    @classmethod
    def get_user_config(cls) -> dict[str, Any]:
        return _recurse_user_config_values(cls.default_settings)

    @classmethod
    def build_agent_configuration(cls, overrides: dict = {}) -> S:
        """"""Process the configuration for this object.""""""

        base_config = _update_user_config_from_env(cls.default_settings)
        final_configuration = deep_update(base_config, overrides)

        return cls.default_settings.__class__.model_validate(final_configuration)","Point(row=103, column=0)","Point(row=120, column=81)",,classic/forge/forge/models/config.py
Configurable.get_user_config,function,,"def get_user_config(cls) -> dict[str, Any]:
        return _recurse_user_config_values(cls.default_settings)","Point(row=110, column=4)","Point(row=111, column=64)",Configurable,classic/forge/forge/models/config.py
Configurable.build_agent_configuration,function,Process the configuration for this object.,"def build_agent_configuration(cls, overrides: dict = {}) -> S:
        """"""Process the configuration for this object.""""""

        base_config = _update_user_config_from_env(cls.default_settings)
        final_configuration = deep_update(base_config, overrides)

        return cls.default_settings.__class__.model_validate(final_configuration)","Point(row=114, column=4)","Point(row=120, column=81)",Configurable,classic/forge/forge/models/config.py
_update_user_config_from_env,function,"
    Update config fields of a Pydantic model instance from environment variables.

    Precedence:
    1. Non-default value already on the instance
    2. Value returned by `from_env()`
    3. Default value for the field

    Params:
        instance: The Pydantic model instance.

    Returns:
        The user config fields of the instance.
","def _update_user_config_from_env(instance: BaseModel) -> dict[str, Any]:
    """"""
    Update config fields of a Pydantic model instance from environment variables.

    Precedence:
    1. Non-default value already on the instance
    2. Value returned by `from_env()`
    3. Default value for the field

    Params:
        instance: The Pydantic model instance.

    Returns:
        The user config fields of the instance.
    """"""

    def infer_field_value(field: FieldInfo, value):
        default_value = (
            field.default
            if field.default not in (None, PydanticUndefined)
            else (field.default_factory() if field.default_factory else None)
        )
        if value == default_value and (
            from_env := _get_field_metadata(field, ""from_env"")
        ):
            val_from_env = os.getenv(from_env) if type(from_env) is str else from_env()
            if val_from_env is not None:
                return val_from_env
        return value

    def init_sub_config(model: Type[SC]) -> SC | None:
        try:
            return model.model_validate(model.from_env(), strict=True)
        except ValidationError as e:
            # Gracefully handle missing fields
            if all(e[""type""] == ""missing"" for e in e.errors()):
                return None
            raise

    return _recurse_user_config_fields(instance, infer_field_value, init_sub_config)","Point(row=123, column=0)","Point(row=162, column=84)",,classic/forge/forge/models/config.py
_update_user_config_from_env.infer_field_value,function,,"def infer_field_value(field: FieldInfo, value):
        default_value = (
            field.default
            if field.default not in (None, PydanticUndefined)
            else (field.default_factory() if field.default_factory else None)
        )
        if value == default_value and (
            from_env := _get_field_metadata(field, ""from_env"")
        ):
            val_from_env = os.getenv(from_env) if type(from_env) is str else from_env()
            if val_from_env is not None:
                return val_from_env
        return value","Point(row=139, column=4)","Point(row=151, column=20)",,classic/forge/forge/models/config.py
_update_user_config_from_env.init_sub_config,function,,"def init_sub_config(model: Type[SC]) -> SC | None:
        try:
            return model.model_validate(model.from_env(), strict=True)
        except ValidationError as e:
            # Gracefully handle missing fields
            if all(e[""type""] == ""missing"" for e in e.errors()):
                return None
            raise","Point(row=153, column=4)","Point(row=160, column=17)",,classic/forge/forge/models/config.py
_recursive_init_model,function,"
    Recursively initialize the user configuration fields of a Pydantic model.

    Parameters:
        model: The Pydantic model type.
        infer_field_value: A callback function to infer the value of each field.
            Parameters:
                ModelField: The Pydantic ModelField object describing the field.

    Returns:
        BaseModel: An instance of the model with the initialized configuration.
","def _recursive_init_model(
    model: Type[M],
    infer_field_value: Callable[[FieldInfo], Any],
) -> M:
    """"""
    Recursively initialize the user configuration fields of a Pydantic model.

    Parameters:
        model: The Pydantic model type.
        infer_field_value: A callback function to infer the value of each field.
            Parameters:
                ModelField: The Pydantic ModelField object describing the field.

    Returns:
        BaseModel: An instance of the model with the initialized configuration.
    """"""
    user_config_fields = {}
    for name, field in model.model_fields.items():
        if _get_field_metadata(field, ""user_configurable""):
            user_config_fields[name] = infer_field_value(field)
        elif isinstance(field.annotation, ModelMetaclass) and issubclass(
            field.annotation, SystemConfiguration
        ):
            try:
                user_config_fields[name] = _recursive_init_model(
                    model=field.annotation,
                    infer_field_value=infer_field_value,
                )
            except ValidationError as e:
                # Gracefully handle missing fields
                if all(e[""type""] == ""missing"" for e in e.errors()):
                    user_config_fields[name] = None
                raise

    user_config_fields = remove_none_items(user_config_fields)

    return model.model_validate(user_config_fields)","Point(row=165, column=0)","Point(row=201, column=51)",,classic/forge/forge/models/config.py
_recurse_user_config_fields,function,"
    Recursively process the user configuration fields of a Pydantic model instance.

    Params:
        model: The Pydantic model to iterate over.
        infer_field_value: A callback function to process each field.
            Params:
                ModelField: The Pydantic ModelField object describing the field.
                Any: The current value of the field.
        init_sub_config: An optional callback function to initialize a sub-config.
            Params:
                Type[SystemConfiguration]: The type of the sub-config to initialize.

    Returns:
        dict[str, Any]: The processed user configuration fields of the instance.
","def _recurse_user_config_fields(
    model: BaseModel,
    infer_field_value: Callable[[FieldInfo, Any], Any],
    init_sub_config: Optional[
        Callable[[Type[SystemConfiguration]], SystemConfiguration | None]
    ] = None,
) -> dict[str, Any]:
    """"""
    Recursively process the user configuration fields of a Pydantic model instance.

    Params:
        model: The Pydantic model to iterate over.
        infer_field_value: A callback function to process each field.
            Params:
                ModelField: The Pydantic ModelField object describing the field.
                Any: The current value of the field.
        init_sub_config: An optional callback function to initialize a sub-config.
            Params:
                Type[SystemConfiguration]: The type of the sub-config to initialize.

    Returns:
        dict[str, Any]: The processed user configuration fields of the instance.
    """"""
    user_config_fields = {}

    for name, field in model.model_fields.items():
        value = getattr(model, name)

        # Handle individual field
        if _get_field_metadata(field, ""user_configurable""):
            user_config_fields[name] = infer_field_value(field, value)

        # Recurse into nested config object
        elif isinstance(value, SystemConfiguration):
            user_config_fields[name] = _recurse_user_config_fields(
                model=value,
                infer_field_value=infer_field_value,
                init_sub_config=init_sub_config,
            )

        # Recurse into optional nested config object
        elif value is None and init_sub_config:
            field_type = get_args(field.annotation)[0]  # Optional[T] -> T
            if type(field_type) is ModelMetaclass and issubclass(
                field_type, SystemConfiguration
            ):
                sub_config = init_sub_config(field_type)
                if sub_config:
                    user_config_fields[name] = _recurse_user_config_fields(
                        model=sub_config,
                        infer_field_value=infer_field_value,
                        init_sub_config=init_sub_config,
                    )

        elif isinstance(value, list) and all(
            isinstance(i, SystemConfiguration) for i in value
        ):
            user_config_fields[name] = [
                _recurse_user_config_fields(i, infer_field_value, init_sub_config)
                for i in value
            ]
        elif isinstance(value, dict) and all(
            isinstance(i, SystemConfiguration) for i in value.values()
        ):
            user_config_fields[name] = {
                k: _recurse_user_config_fields(v, infer_field_value, init_sub_config)
                for k, v in value.items()
            }

    return user_config_fields","Point(row=204, column=0)","Point(row=273, column=29)",,classic/forge/forge/models/config.py
_recurse_user_config_values,function,"
    This function recursively traverses the user configuration values in a Pydantic
    model instance.

    Params:
        instance: A Pydantic model instance.
        get_field_value: A callback function to process each field. Parameters:
            ModelField: The Pydantic ModelField object that describes the field.
            Any: The current value of the field.

    Returns:
        A dictionary containing the processed user configuration fields of the instance.
","def _recurse_user_config_values(
    instance: BaseModel,
    get_field_value: Callable[[FieldInfo, T], T] = lambda _, v: v,
) -> dict[str, Any]:
    """"""
    This function recursively traverses the user configuration values in a Pydantic
    model instance.

    Params:
        instance: A Pydantic model instance.
        get_field_value: A callback function to process each field. Parameters:
            ModelField: The Pydantic ModelField object that describes the field.
            Any: The current value of the field.

    Returns:
        A dictionary containing the processed user configuration fields of the instance.
    """"""
    user_config_values = {}

    for name, value in instance.__dict__.items():
        field = instance.model_fields[name]
        if _get_field_metadata(field, ""user_configurable""):
            user_config_values[name] = get_field_value(field, value)
        elif isinstance(value, SystemConfiguration):
            user_config_values[name] = _recurse_user_config_values(
                instance=value, get_field_value=get_field_value
            )
        elif isinstance(value, list) and all(
            isinstance(i, SystemConfiguration) for i in value
        ):
            user_config_values[name] = [
                _recurse_user_config_values(i, get_field_value) for i in value
            ]
        elif isinstance(value, dict) and all(
            isinstance(i, SystemConfiguration) for i in value.values()
        ):
            user_config_values[name] = {
                k: _recurse_user_config_values(v, get_field_value)
                for k, v in value.items()
            }

    return user_config_values","Point(row=276, column=0)","Point(row=317, column=29)",,classic/forge/forge/models/config.py
_get_non_default_user_config_values,function,"
    Get the non-default user config fields of a Pydantic model instance.

    Params:
        instance: The Pydantic model instance.

    Returns:
        dict[str, Any]: The non-default user config values on the instance.
","def _get_non_default_user_config_values(instance: BaseModel) -> dict[str, Any]:
    """"""
    Get the non-default user config fields of a Pydantic model instance.

    Params:
        instance: The Pydantic model instance.

    Returns:
        dict[str, Any]: The non-default user config values on the instance.
    """"""

    def get_field_value(field: FieldInfo, value):
        default = field.default_factory() if field.default_factory else field.default
        if value != default:
            return value

    return remove_none_items(_recurse_user_config_values(instance, get_field_value))","Point(row=320, column=0)","Point(row=336, column=84)",,classic/forge/forge/models/config.py
_get_non_default_user_config_values.get_field_value,function,,"def get_field_value(field: FieldInfo, value):
        default = field.default_factory() if field.default_factory else field.default
        if value != default:
            return value","Point(row=331, column=4)","Point(row=334, column=24)",,classic/forge/forge/models/config.py
deep_update,function,"
    Recursively update a dictionary.

    Params:
        original_dict (dict): The dictionary to be updated.
        update_dict (dict): The dictionary to update with.

    Returns:
        dict: The updated dictionary.
","def deep_update(original_dict: dict, update_dict: dict) -> dict:
    """"""
    Recursively update a dictionary.

    Params:
        original_dict (dict): The dictionary to be updated.
        update_dict (dict): The dictionary to update with.

    Returns:
        dict: The updated dictionary.
    """"""
    for key, value in update_dict.items():
        if (
            key in original_dict
            and isinstance(original_dict[key], dict)
            and isinstance(value, dict)
        ):
            original_dict[key] = deep_update(original_dict[key], value)
        else:
            original_dict[key] = value
    return original_dict","Point(row=339, column=0)","Point(row=359, column=24)",,classic/forge/forge/models/config.py
remove_none_items,function,,"def remove_none_items(d):
    if isinstance(d, dict):
        return {
            k: remove_none_items(v)
            for k, v in d.items()
            if v not in (None, PydanticUndefined)
        }
    return d","Point(row=362, column=0)","Point(row=369, column=12)",,classic/forge/forge/models/config.py
ActionProposal,class,,"class ActionProposal(BaseModel):
    thoughts: str | ModelWithSummary
    use_tool: AssistantFunctionCall

    raw_message: AssistantChatMessage = None  # type: ignore
    """"""
    The message from which the action proposal was parsed. To be set by the parser.
    """"""

    @classmethod
    def model_json_schema(
        cls,
        by_alias: bool = True,
        ref_template: str = DEFAULT_REF_TEMPLATE,
        schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,
        mode: JsonSchemaMode = ""validation"",
        **kwargs,
    ):
        """"""
        The schema for this ActionProposal model, excluding the 'raw_message' property.
        """"""
        schema = super().model_json_schema(
            by_alias=by_alias,
            ref_template=ref_template,
            schema_generator=schema_generator,
            mode=mode,
            **kwargs,
        )
        if ""raw_message"" in schema[""properties""]:  # must check because schema is cached
            del schema[""properties""][""raw_message""]
        return schema","Point(row=16, column=0)","Point(row=46, column=21)",,classic/forge/forge/models/action.py
ActionProposal.model_json_schema,function,"
        The schema for this ActionProposal model, excluding the 'raw_message' property.
","def model_json_schema(
        cls,
        by_alias: bool = True,
        ref_template: str = DEFAULT_REF_TEMPLATE,
        schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,
        mode: JsonSchemaMode = ""validation"",
        **kwargs,
    ):
        """"""
        The schema for this ActionProposal model, excluding the 'raw_message' property.
        """"""
        schema = super().model_json_schema(
            by_alias=by_alias,
            ref_template=ref_template,
            schema_generator=schema_generator,
            mode=mode,
            **kwargs,
        )
        if ""raw_message"" in schema[""properties""]:  # must check because schema is cached
            del schema[""properties""][""raw_message""]
        return schema","Point(row=26, column=4)","Point(row=46, column=21)",ActionProposal,classic/forge/forge/models/action.py
ActionSuccessResult,class,,"class ActionSuccessResult(BaseModel):
    outputs: Any
    status: Literal[""success""] = ""success""

    def __str__(self) -> str:
        outputs = str(self.outputs).replace(""```"", r""\```"")
        multiline = ""\n"" in outputs
        return f""```\n{self.outputs}\n```"" if multiline else str(self.outputs)","Point(row=52, column=0)","Point(row=59, column=78)",,classic/forge/forge/models/action.py
ActionSuccessResult.__str__,function,,"def __str__(self) -> str:
        outputs = str(self.outputs).replace(""```"", r""\```"")
        multiline = ""\n"" in outputs
        return f""```\n{self.outputs}\n```"" if multiline else str(self.outputs)","Point(row=56, column=4)","Point(row=59, column=78)",ActionSuccessResult,classic/forge/forge/models/action.py
ErrorInfo,class,,"class ErrorInfo(BaseModel):
    args: tuple
    message: str
    exception_type: str
    repr: str

    @staticmethod
    def from_exception(exception: Exception) -> ErrorInfo:
        return ErrorInfo(
            args=exception.args,
            message=getattr(exception, ""message"", exception.args[0]),
            exception_type=exception.__class__.__name__,
            repr=repr(exception),
        )

    def __str__(self):
        return repr(self)

    def __repr__(self):
        return self.repr","Point(row=62, column=0)","Point(row=81, column=24)",,classic/forge/forge/models/action.py
ErrorInfo.from_exception,function,,"def from_exception(exception: Exception) -> ErrorInfo:
        return ErrorInfo(
            args=exception.args,
            message=getattr(exception, ""message"", exception.args[0]),
            exception_type=exception.__class__.__name__,
            repr=repr(exception),
        )","Point(row=69, column=4)","Point(row=75, column=9)",ErrorInfo,classic/forge/forge/models/action.py
ErrorInfo.__str__,function,,"def __str__(self):
        return repr(self)","Point(row=77, column=4)","Point(row=78, column=25)",ErrorInfo,classic/forge/forge/models/action.py
ErrorInfo.__repr__,function,,"def __repr__(self):
        return self.repr","Point(row=80, column=4)","Point(row=81, column=24)",ErrorInfo,classic/forge/forge/models/action.py
ActionErrorResult,class,,"class ActionErrorResult(BaseModel):
    reason: str
    error: Optional[ErrorInfo] = None
    status: Literal[""error""] = ""error""

    @staticmethod
    def from_exception(exception: Exception) -> ActionErrorResult:
        return ActionErrorResult(
            reason=getattr(exception, ""message"", exception.args[0]),
            error=ErrorInfo.from_exception(exception),
        )

    def __str__(self) -> str:
        return f""Action failed: '{self.reason}'""","Point(row=84, column=0)","Point(row=97, column=48)",,classic/forge/forge/models/action.py
ActionErrorResult.from_exception,function,,"def from_exception(exception: Exception) -> ActionErrorResult:
        return ActionErrorResult(
            reason=getattr(exception, ""message"", exception.args[0]),
            error=ErrorInfo.from_exception(exception),
        )","Point(row=90, column=4)","Point(row=94, column=9)",ActionErrorResult,classic/forge/forge/models/action.py
ActionErrorResult.__str__,function,,"def __str__(self) -> str:
        return f""Action failed: '{self.reason}'""","Point(row=96, column=4)","Point(row=97, column=48)",ActionErrorResult,classic/forge/forge/models/action.py
ActionInterruptedByHuman,class,,"class ActionInterruptedByHuman(BaseModel):
    feedback: str
    status: Literal[""interrupted_by_human""] = ""interrupted_by_human""

    def __str__(self) -> str:
        return (
            'The user interrupted the action with the following feedback: ""%s""'
            % self.feedback
        )","Point(row=100, column=0)","Point(row=108, column=9)",,classic/forge/forge/models/action.py
ActionInterruptedByHuman.__str__,function,,"def __str__(self) -> str:
        return (
            'The user interrupted the action with the following feedback: ""%s""'
            % self.feedback
        )","Point(row=104, column=4)","Point(row=108, column=9)",ActionInterruptedByHuman,classic/forge/forge/models/action.py
ResourceType,class,An enumeration of resource types.,"class ResourceType(str, enum.Enum):
    """"""An enumeration of resource types.""""""

    MODEL = ""model""","Point(row=12, column=0)","Point(row=15, column=19)",,classic/forge/forge/models/providers.py
ProviderBudget,class,,"class ProviderBudget(SystemConfiguration, Generic[_T]):
    total_budget: float = UserConfigurable(math.inf)
    total_cost: float = 0
    remaining_budget: float = math.inf
    usage: _T

    @abc.abstractmethod
    def update_usage_and_cost(self, *args, **kwargs) -> float:
        """"""Update the usage and cost of the provider.

        Returns:
            float: The (calculated) cost of the given model response.
        """"""
        ...","Point(row=18, column=0)","Point(row=31, column=11)",,classic/forge/forge/models/providers.py
ProviderBudget.update_usage_and_cost,function,"Update the usage and cost of the provider.

        Returns:
            float: The (calculated) cost of the given model response.
","def update_usage_and_cost(self, *args, **kwargs) -> float:
        """"""Update the usage and cost of the provider.

        Returns:
            float: The (calculated) cost of the given model response.
        """"""
        ...","Point(row=25, column=4)","Point(row=31, column=11)",ProviderBudget,classic/forge/forge/models/providers.py
ProviderCredentials,class,Struct for credentials.,"class ProviderCredentials(SystemConfiguration):
    """"""Struct for credentials.""""""

    def unmasked(self) -> dict:
        return unmask(self)

    model_config = ConfigDict(
        json_encoders={
            SecretStr: lambda v: v.get_secret_value() if v else None,
            SecretBytes: lambda v: v.get_secret_value() if v else None,
            Secret: lambda v: v.get_secret_value() if v else None,
        }
    )","Point(row=34, column=0)","Point(row=46, column=5)",,classic/forge/forge/models/providers.py
ProviderCredentials.unmasked,function,,"def unmasked(self) -> dict:
        return unmask(self)","Point(row=37, column=4)","Point(row=38, column=27)",ProviderCredentials,classic/forge/forge/models/providers.py
unmask,function,,"def unmask(model: BaseModel):
    unmasked_fields = {}
    for field_name, _ in model.model_fields.items():
        value = getattr(model, field_name)
        if isinstance(value, SecretStr):
            unmasked_fields[field_name] = value.get_secret_value()
        else:
            unmasked_fields[field_name] = value
    return unmasked_fields","Point(row=49, column=0)","Point(row=57, column=26)",,classic/forge/forge/models/providers.py
JSONSchema,class,,"class JSONSchema(BaseModel):
    class Type(str, enum.Enum):
        STRING = ""string""
        ARRAY = ""array""
        OBJECT = ""object""
        NUMBER = ""number""
        INTEGER = ""integer""
        BOOLEAN = ""boolean""

    # TODO: add docstrings
    description: Optional[str] = None
    type: Optional[Type] = None
    enum: Optional[list] = None
    required: bool = False
    items: Optional[""JSONSchema""] = None
    properties: Optional[dict[str, ""JSONSchema""]] = None
    minimum: Optional[int | float] = None
    maximum: Optional[int | float] = None
    minItems: Optional[int] = None
    maxItems: Optional[int] = None

    def to_dict(self) -> dict:
        schema: dict = {
            ""type"": self.type.value if self.type else None,
            ""description"": self.description,
        }
        if self.type == ""array"":
            if self.items:
                schema[""items""] = self.items.to_dict()
            schema[""minItems""] = self.minItems
            schema[""maxItems""] = self.maxItems
        elif self.type == ""object"":
            if self.properties:
                schema[""properties""] = {
                    name: prop.to_dict() for name, prop in self.properties.items()
                }
                schema[""required""] = [
                    name for name, prop in self.properties.items() if prop.required
                ]
        elif self.enum:
            schema[""enum""] = self.enum
        else:
            schema[""minumum""] = self.minimum
            schema[""maximum""] = self.maximum

        schema = {k: v for k, v in schema.items() if v is not None}

        return schema

    @staticmethod
    def from_dict(schema: dict) -> ""JSONSchema"":
        definitions = schema.get(""$defs"", {})
        schema = _resolve_type_refs_in_schema(schema, definitions)

        return JSONSchema(
            description=schema.get(""description""),
            type=schema[""type""],
            enum=schema.get(""enum""),
            items=JSONSchema.from_dict(schema[""items""]) if ""items"" in schema else None,
            properties=JSONSchema.parse_properties(schema)
            if schema[""type""] == ""object""
            else None,
            minimum=schema.get(""minimum""),
            maximum=schema.get(""maximum""),
            minItems=schema.get(""minItems""),
            maxItems=schema.get(""maxItems""),
        )

    @staticmethod
    def parse_properties(schema_node: dict) -> dict[str, ""JSONSchema""]:
        properties = (
            {k: JSONSchema.from_dict(v) for k, v in schema_node[""properties""].items()}
            if ""properties"" in schema_node
            else {}
        )
        if ""required"" in schema_node:
            for k, v in properties.items():
                v.required = k in schema_node[""required""]
        return properties

    def validate_object(self, object: object) -> tuple[bool, list[ValidationError]]:
        """"""
        Validates an object or a value against the JSONSchema.

        Params:
            object: The value/object to validate.
            schema (JSONSchema): The JSONSchema to validate against.

        Returns:
            bool: Indicates whether the given value or object is valid for the schema.
            list[ValidationError]: The issues with the value or object (if any).
        """"""
        validator = Draft7Validator(self.to_dict())

        if errors := sorted(validator.iter_errors(object), key=lambda e: e.path):
            return False, errors

        return True, []

    def to_typescript_object_interface(self, interface_name: str = """") -> str:
        if self.type != JSONSchema.Type.OBJECT:
            raise NotImplementedError(""Only `object` schemas are supported"")

        if self.properties:
            attributes: list[str] = []
            for name, property in self.properties.items():
                if property.description:
                    attributes.append(f""// {property.description}"")
                attributes.append(f""{name}: {property.typescript_type};"")
            attributes_string = ""\n"".join(attributes)
        else:
            attributes_string = ""[key: string]: any""

        return (
            f""interface {interface_name} "" if interface_name else """"
        ) + f""{{\n{indent(attributes_string, '  ')}\n}}""

    @property
    def typescript_type(self) -> str:
        if not self.type:
            return ""any""
        if self.type == JSONSchema.Type.BOOLEAN:
            return ""boolean""
        if self.type in {JSONSchema.Type.INTEGER, JSONSchema.Type.NUMBER}:
            return ""number""
        if self.type == JSONSchema.Type.STRING:
            return ""string""
        if self.type == JSONSchema.Type.ARRAY:
            return f""Array<{self.items.typescript_type}>"" if self.items else ""Array""
        if self.type == JSONSchema.Type.OBJECT:
            if not self.properties:
                return ""Record<string, any>""
            return self.to_typescript_object_interface()
        if self.enum:
            return "" | "".join(repr(v) for v in self.enum)

        raise NotImplementedError(
            f""JSONSchema.typescript_type does not support Type.{self.type.name} yet""
        )","Point(row=8, column=0)","Point(row=146, column=9)",,classic/forge/forge/models/json_schema.py
Type,class,,"class Type(str, enum.Enum):
        STRING = ""string""
        ARRAY = ""array""
        OBJECT = ""object""
        NUMBER = ""number""
        INTEGER = ""integer""
        BOOLEAN = ""boolean""","Point(row=9, column=4)","Point(row=15, column=27)",,classic/forge/forge/models/json_schema.py
JSONSchema.to_dict,function,,"def to_dict(self) -> dict:
        schema: dict = {
            ""type"": self.type.value if self.type else None,
            ""description"": self.description,
        }
        if self.type == ""array"":
            if self.items:
                schema[""items""] = self.items.to_dict()
            schema[""minItems""] = self.minItems
            schema[""maxItems""] = self.maxItems
        elif self.type == ""object"":
            if self.properties:
                schema[""properties""] = {
                    name: prop.to_dict() for name, prop in self.properties.items()
                }
                schema[""required""] = [
                    name for name, prop in self.properties.items() if prop.required
                ]
        elif self.enum:
            schema[""enum""] = self.enum
        else:
            schema[""minumum""] = self.minimum
            schema[""maximum""] = self.maximum

        schema = {k: v for k, v in schema.items() if v is not None}

        return schema","Point(row=29, column=4)","Point(row=55, column=21)",JSONSchema,classic/forge/forge/models/json_schema.py
JSONSchema.from_dict,function,,"def from_dict(schema: dict) -> ""JSONSchema"":
        definitions = schema.get(""$defs"", {})
        schema = _resolve_type_refs_in_schema(schema, definitions)

        return JSONSchema(
            description=schema.get(""description""),
            type=schema[""type""],
            enum=schema.get(""enum""),
            items=JSONSchema.from_dict(schema[""items""]) if ""items"" in schema else None,
            properties=JSONSchema.parse_properties(schema)
            if schema[""type""] == ""object""
            else None,
            minimum=schema.get(""minimum""),
            maximum=schema.get(""maximum""),
            minItems=schema.get(""minItems""),
            maxItems=schema.get(""maxItems""),
        )","Point(row=58, column=4)","Point(row=74, column=9)",JSONSchema,classic/forge/forge/models/json_schema.py
JSONSchema.parse_properties,function,,"def parse_properties(schema_node: dict) -> dict[str, ""JSONSchema""]:
        properties = (
            {k: JSONSchema.from_dict(v) for k, v in schema_node[""properties""].items()}
            if ""properties"" in schema_node
            else {}
        )
        if ""required"" in schema_node:
            for k, v in properties.items():
                v.required = k in schema_node[""required""]
        return properties","Point(row=77, column=4)","Point(row=86, column=25)",JSONSchema,classic/forge/forge/models/json_schema.py
JSONSchema.validate_object,function,"
        Validates an object or a value against the JSONSchema.

        Params:
            object: The value/object to validate.
            schema (JSONSchema): The JSONSchema to validate against.

        Returns:
            bool: Indicates whether the given value or object is valid for the schema.
            list[ValidationError]: The issues with the value or object (if any).
","def validate_object(self, object: object) -> tuple[bool, list[ValidationError]]:
        """"""
        Validates an object or a value against the JSONSchema.

        Params:
            object: The value/object to validate.
            schema (JSONSchema): The JSONSchema to validate against.

        Returns:
            bool: Indicates whether the given value or object is valid for the schema.
            list[ValidationError]: The issues with the value or object (if any).
        """"""
        validator = Draft7Validator(self.to_dict())

        if errors := sorted(validator.iter_errors(object), key=lambda e: e.path):
            return False, errors

        return True, []","Point(row=88, column=4)","Point(row=105, column=23)",JSONSchema,classic/forge/forge/models/json_schema.py
JSONSchema.to_typescript_object_interface,function,,"def to_typescript_object_interface(self, interface_name: str = """") -> str:
        if self.type != JSONSchema.Type.OBJECT:
            raise NotImplementedError(""Only `object` schemas are supported"")

        if self.properties:
            attributes: list[str] = []
            for name, property in self.properties.items():
                if property.description:
                    attributes.append(f""// {property.description}"")
                attributes.append(f""{name}: {property.typescript_type};"")
            attributes_string = ""\n"".join(attributes)
        else:
            attributes_string = ""[key: string]: any""

        return (
            f""interface {interface_name} "" if interface_name else """"
        ) + f""{{\n{indent(attributes_string, '  ')}\n}}""","Point(row=107, column=4)","Point(row=123, column=56)",JSONSchema,classic/forge/forge/models/json_schema.py
JSONSchema.typescript_type,function,,"def typescript_type(self) -> str:
        if not self.type:
            return ""any""
        if self.type == JSONSchema.Type.BOOLEAN:
            return ""boolean""
        if self.type in {JSONSchema.Type.INTEGER, JSONSchema.Type.NUMBER}:
            return ""number""
        if self.type == JSONSchema.Type.STRING:
            return ""string""
        if self.type == JSONSchema.Type.ARRAY:
            return f""Array<{self.items.typescript_type}>"" if self.items else ""Array""
        if self.type == JSONSchema.Type.OBJECT:
            if not self.properties:
                return ""Record<string, any>""
            return self.to_typescript_object_interface()
        if self.enum:
            return "" | "".join(repr(v) for v in self.enum)

        raise NotImplementedError(
            f""JSONSchema.typescript_type does not support Type.{self.type.name} yet""
        )","Point(row=126, column=4)","Point(row=146, column=9)",JSONSchema,classic/forge/forge/models/json_schema.py
_resolve_type_refs_in_schema,function,,"def _resolve_type_refs_in_schema(schema: dict, definitions: dict) -> dict:
    ...","Point(row=150, column=0)","Point(row=151, column=7)",,classic/forge/forge/models/json_schema.py
_resolve_type_refs_in_schema,function,,"def _resolve_type_refs_in_schema(schema: list, definitions: dict) -> list:
    ...","Point(row=155, column=0)","Point(row=156, column=7)",,classic/forge/forge/models/json_schema.py
_resolve_type_refs_in_schema,function,"
    Recursively resolve type $refs in the JSON schema with their definitions.
","def _resolve_type_refs_in_schema(schema: dict | list, definitions: dict) -> dict | list:
    """"""
    Recursively resolve type $refs in the JSON schema with their definitions.
    """"""
    if isinstance(schema, dict):
        if ""$ref"" in schema:
            ref_path = schema[""$ref""].split(""/"")[2:]  # Split and remove '#/definitions'
            ref_value = definitions
            for key in ref_path:
                ref_value = ref_value[key]
            return _resolve_type_refs_in_schema(ref_value, definitions)
        else:
            return {
                k: _resolve_type_refs_in_schema(v, definitions)
                for k, v in schema.items()
            }
    elif isinstance(schema, list):
        return [_resolve_type_refs_in_schema(item, definitions) for item in schema]
    else:
        return schema","Point(row=159, column=0)","Point(row=178, column=21)",,classic/forge/forge/models/json_schema.py
ModelWithSummary,class,,"class ModelWithSummary(BaseModel, ABC):
    @abstractmethod
    def summary(self) -> str:
        """"""Should produce a human readable summary of the model content.""""""
        pass","Point(row=5, column=0)","Point(row=9, column=12)",,classic/forge/forge/models/utils.py
ModelWithSummary.summary,function,Should produce a human readable summary of the model content.,"def summary(self) -> str:
        """"""Should produce a human readable summary of the model content.""""""
        pass","Point(row=7, column=4)","Point(row=9, column=12)",ModelWithSummary,classic/forge/forge/models/utils.py
MacOSTTS,class,MacOS TTS Voice.,"class MacOSTTS(VoiceBase):
    """"""MacOS TTS Voice.""""""

    def _setup(self) -> None:
        pass

    def _speech(self, text: str, voice_id: int = 0) -> bool:
        """"""Play the given text.""""""
        if voice_id == 0:
            subprocess.run([""say"", text], shell=False)
        elif voice_id == 1:
            subprocess.run([""say"", ""-v"", ""Ava (Premium)"", text], shell=False)
        else:
            subprocess.run([""say"", ""-v"", ""Samantha"", text], shell=False)
        return True","Point(row=8, column=0)","Point(row=22, column=19)",,classic/forge/forge/speech/macos_tts.py
MacOSTTS._setup,function,,"def _setup(self) -> None:
        pass","Point(row=11, column=4)","Point(row=12, column=12)",MacOSTTS,classic/forge/forge/speech/macos_tts.py
MacOSTTS._speech,function,Play the given text.,"def _speech(self, text: str, voice_id: int = 0) -> bool:
        """"""Play the given text.""""""
        if voice_id == 0:
            subprocess.run([""say"", text], shell=False)
        elif voice_id == 1:
            subprocess.run([""say"", ""-v"", ""Ava (Premium)"", text], shell=False)
        else:
            subprocess.run([""say"", ""-v"", ""Samantha"", text], shell=False)
        return True","Point(row=14, column=4)","Point(row=22, column=19)",MacOSTTS,classic/forge/forge/speech/macos_tts.py
TTSConfig,class,,"class TTSConfig(SystemConfiguration):
    speak_mode: bool = False
    elevenlabs: Optional[ElevenLabsConfig] = None
    streamelements: Optional[StreamElementsConfig] = None
    provider: Literal[
        ""elevenlabs"", ""gtts"", ""macos"", ""streamelements""
    ] = UserConfigurable(
        default=""gtts"",
        from_env=lambda: os.getenv(""TEXT_TO_SPEECH_PROVIDER"")
        or (
            ""macos""
            if os.getenv(""USE_MAC_OS_TTS"")
            else ""elevenlabs""
            if os.getenv(""ELEVENLABS_API_KEY"")
            else ""streamelements""
            if os.getenv(""USE_BRIAN_TTS"")
            else ""gtts""
        ),
    )  # type: ignore","Point(row=21, column=0)","Point(row=39, column=21)",,classic/forge/forge/speech/say.py
TextToSpeechProvider,class,,"class TextToSpeechProvider:
    def __init__(self, config: TTSConfig):
        self._config = config
        self._default_voice_engine, self._voice_engine = self._get_voice_engine(config)

    def say(self, text, voice_index: int = 0) -> None:
        def _speak() -> None:
            success = self._voice_engine.say(text, voice_index)
            if not success:
                self._default_voice_engine.say(text, voice_index)
            _QUEUE_SEMAPHORE.release()

        if self._config.speak_mode:
            _QUEUE_SEMAPHORE.acquire(True)
            thread = threading.Thread(target=_speak)
            thread.start()

    def __repr__(self):
        return ""{class_name}(provider={voice_engine_name})"".format(
            class_name=self.__class__.__name__,
            voice_engine_name=self._voice_engine.__class__.__name__,
        )

    @staticmethod
    def _get_voice_engine(config: TTSConfig) -> tuple[VoiceBase, VoiceBase]:
        """"""Get the voice engine to use for the given configuration""""""
        tts_provider = config.provider
        if tts_provider == ""elevenlabs"":
            voice_engine = ElevenLabsSpeech(config.elevenlabs)
        elif tts_provider == ""macos"":
            voice_engine = MacOSTTS()
        elif tts_provider == ""streamelements"":
            voice_engine = StreamElementsSpeech(config.streamelements)
        else:
            voice_engine = GTTSVoice()

        return GTTSVoice(), voice_engine","Point(row=42, column=0)","Point(row=78, column=40)",,classic/forge/forge/speech/say.py
TextToSpeechProvider.__init__,function,,"def __init__(self, config: TTSConfig):
        self._config = config
        self._default_voice_engine, self._voice_engine = self._get_voice_engine(config)","Point(row=43, column=4)","Point(row=45, column=87)",TextToSpeechProvider,classic/forge/forge/speech/say.py
TextToSpeechProvider.say,function,,"def say(self, text, voice_index: int = 0) -> None:
        def _speak() -> None:
            success = self._voice_engine.say(text, voice_index)
            if not success:
                self._default_voice_engine.say(text, voice_index)
            _QUEUE_SEMAPHORE.release()

        if self._config.speak_mode:
            _QUEUE_SEMAPHORE.acquire(True)
            thread = threading.Thread(target=_speak)
            thread.start()","Point(row=47, column=4)","Point(row=57, column=26)",TextToSpeechProvider,classic/forge/forge/speech/say.py
TextToSpeechProvider.say._speak,function,,"def _speak() -> None:
            success = self._voice_engine.say(text, voice_index)
            if not success:
                self._default_voice_engine.say(text, voice_index)
            _QUEUE_SEMAPHORE.release()","Point(row=48, column=8)","Point(row=52, column=38)",TextToSpeechProvider,classic/forge/forge/speech/say.py
TextToSpeechProvider.__repr__,function,,"def __repr__(self):
        return ""{class_name}(provider={voice_engine_name})"".format(
            class_name=self.__class__.__name__,
            voice_engine_name=self._voice_engine.__class__.__name__,
        )","Point(row=59, column=4)","Point(row=63, column=9)",TextToSpeechProvider,classic/forge/forge/speech/say.py
TextToSpeechProvider._get_voice_engine,function,Get the voice engine to use for the given configuration,"def _get_voice_engine(config: TTSConfig) -> tuple[VoiceBase, VoiceBase]:
        """"""Get the voice engine to use for the given configuration""""""
        tts_provider = config.provider
        if tts_provider == ""elevenlabs"":
            voice_engine = ElevenLabsSpeech(config.elevenlabs)
        elif tts_provider == ""macos"":
            voice_engine = MacOSTTS()
        elif tts_provider == ""streamelements"":
            voice_engine = StreamElementsSpeech(config.streamelements)
        else:
            voice_engine = GTTSVoice()

        return GTTSVoice(), voice_engine","Point(row=66, column=4)","Point(row=78, column=40)",TextToSpeechProvider,classic/forge/forge/speech/say.py
StreamElementsConfig,class,,"class StreamElementsConfig(SystemConfiguration):
    voice: str = UserConfigurable(default=""Brian"", from_env=""STREAMELEMENTS_VOICE"")","Point(row=15, column=0)","Point(row=16, column=83)",,classic/forge/forge/speech/stream_elements_speech.py
StreamElementsSpeech,class,Streamelements speech module for AutoGPT Forge,"class StreamElementsSpeech(VoiceBase):
    """"""Streamelements speech module for AutoGPT Forge""""""

    def _setup(self, config: StreamElementsConfig) -> None:
        """"""Setup the voices, API key, etc.""""""
        self.config = config

    def _speech(self, text: str, voice_id: int = 0) -> bool:
        """"""Speak text using the streamelements API

        Args:
            text (str): The text to speak
            voice (str): The voice to use

        Returns:
            bool: True if the request was successful, False otherwise
        """"""
        voice = self.config.voice
        tts_url = (
            f""https://api.streamelements.com/kappa/v2/speech?voice={voice}&text={text}""
        )
        response = requests.get(tts_url)

        if response.status_code == 200:
            with open(""speech.mp3"", ""wb"") as f:
                f.write(response.content)
            playsound(""speech.mp3"")
            os.remove(""speech.mp3"")
            return True
        else:
            logger.error(
                ""Request failed with status code: %s, response content: %s"",
                response.status_code,
                response.content,
            )
            return False","Point(row=19, column=0)","Point(row=54, column=24)",,classic/forge/forge/speech/stream_elements_speech.py
StreamElementsSpeech._setup,function,"Setup the voices, API key, etc.","def _setup(self, config: StreamElementsConfig) -> None:
        """"""Setup the voices, API key, etc.""""""
        self.config = config","Point(row=22, column=4)","Point(row=24, column=28)",StreamElementsSpeech,classic/forge/forge/speech/stream_elements_speech.py
StreamElementsSpeech._speech,function,"Speak text using the streamelements API

        Args:
            text (str): The text to speak
            voice (str): The voice to use

        Returns:
            bool: True if the request was successful, False otherwise
","def _speech(self, text: str, voice_id: int = 0) -> bool:
        """"""Speak text using the streamelements API

        Args:
            text (str): The text to speak
            voice (str): The voice to use

        Returns:
            bool: True if the request was successful, False otherwise
        """"""
        voice = self.config.voice
        tts_url = (
            f""https://api.streamelements.com/kappa/v2/speech?voice={voice}&text={text}""
        )
        response = requests.get(tts_url)

        if response.status_code == 200:
            with open(""speech.mp3"", ""wb"") as f:
                f.write(response.content)
            playsound(""speech.mp3"")
            os.remove(""speech.mp3"")
            return True
        else:
            logger.error(
                ""Request failed with status code: %s, response content: %s"",
                response.status_code,
                response.content,
            )
            return False","Point(row=26, column=4)","Point(row=54, column=24)",StreamElementsSpeech,classic/forge/forge/speech/stream_elements_speech.py
VoiceBase,class,"
    Base class for all voice classes.
","class VoiceBase:
    """"""
    Base class for all voice classes.
    """"""

    def __init__(self, *args, **kwargs):
        """"""
        Initialize the voice class.
        """"""
        self._url = None
        self._headers = None
        self._api_key = None
        self._voices = []
        self._mutex = Lock()
        self._setup(*args, **kwargs)

    def say(self, text: str, voice_index: int = 0) -> bool:
        """"""
        Say the given text.

        Args:
            text (str): The text to say.
            voice_index (int): The index of the voice to use.
        """"""
        text = re.sub(
            r""\b(?:https?://[-\w_.]+/?\w[-\w_.]*\.(?:[-\w_.]+/?\w[-\w_.]*\.)?[a-z]+(?:/[-\w_.%]+)*\b(?!\.))"",  # noqa: E501
            """",
            text,
        )
        with self._mutex:
            return self._speech(text, voice_index)

    @abc.abstractmethod
    def _setup(self, *args, **kwargs) -> None:
        """"""
        Setup the voices, API key, etc.
        """"""

    @abc.abstractmethod
    def _speech(self, text: str, voice_id: int = 0) -> bool:
        """"""
        Play the given text.

        Args:
            text (str): The text to play.
        """"""","Point(row=8, column=0)","Point(row=53, column=11)",,classic/forge/forge/speech/base.py
VoiceBase.__init__,function,"
        Initialize the voice class.
","def __init__(self, *args, **kwargs):
        """"""
        Initialize the voice class.
        """"""
        self._url = None
        self._headers = None
        self._api_key = None
        self._voices = []
        self._mutex = Lock()
        self._setup(*args, **kwargs)","Point(row=13, column=4)","Point(row=22, column=36)",VoiceBase,classic/forge/forge/speech/base.py
VoiceBase.say,function,"
        Say the given text.

        Args:
            text (str): The text to say.
            voice_index (int): The index of the voice to use.
","def say(self, text: str, voice_index: int = 0) -> bool:
        """"""
        Say the given text.

        Args:
            text (str): The text to say.
            voice_index (int): The index of the voice to use.
        """"""
        text = re.sub(
            r""\b(?:https?://[-\w_.]+/?\w[-\w_.]*\.(?:[-\w_.]+/?\w[-\w_.]*\.)?[a-z]+(?:/[-\w_.%]+)*\b(?!\.))"",  # noqa: E501
            """",
            text,
        )
        with self._mutex:
            return self._speech(text, voice_index)","Point(row=24, column=4)","Point(row=38, column=50)",VoiceBase,classic/forge/forge/speech/base.py
VoiceBase._setup,function,"
        Setup the voices, API key, etc.
","def _setup(self, *args, **kwargs) -> None:
        """"""
        Setup the voices, API key, etc.
        """"""","Point(row=41, column=4)","Point(row=44, column=11)",VoiceBase,classic/forge/forge/speech/base.py
VoiceBase._speech,function,"
        Play the given text.

        Args:
            text (str): The text to play.
","def _speech(self, text: str, voice_id: int = 0) -> bool:
        """"""
        Play the given text.

        Args:
            text (str): The text to play.
        """"""","Point(row=47, column=4)","Point(row=53, column=11)",VoiceBase,classic/forge/forge/speech/base.py
ElevenLabsConfig,class,,"class ElevenLabsConfig(SystemConfiguration):
    api_key: str = UserConfigurable(from_env=""ELEVENLABS_API_KEY"")
    voice_id: str = UserConfigurable(from_env=""ELEVENLABS_VOICE_ID"")","Point(row=18, column=0)","Point(row=20, column=68)",,classic/forge/forge/speech/eleven_labs.py
ElevenLabsSpeech,class,ElevenLabs speech class,"class ElevenLabsSpeech(VoiceBase):
    """"""ElevenLabs speech class""""""

    def _setup(self, config: ElevenLabsConfig) -> None:
        """"""Set up the voices, API key, etc.

        Returns:
            None: None
        """"""

        default_voices = [""ErXwobaYiN019PkySvjV"", ""EXAVITQu4vr4xnSDxMaL""]
        voice_options = {
            ""Rachel"": ""21m00Tcm4TlvDq8ikWAM"",
            ""Domi"": ""AZnzlk1XvdvUeBnXmlld"",
            ""Bella"": ""EXAVITQu4vr4xnSDxMaL"",
            ""Antoni"": ""ErXwobaYiN019PkySvjV"",
            ""Elli"": ""MF3mGyEYCl7XYWbV9V6O"",
            ""Josh"": ""TxGEqnHWrfWFTfGW9XjX"",
            ""Arnold"": ""VR6AewLTigWG4xSOukaG"",
            ""Adam"": ""pNInz6obpgDQGcFmaJgB"",
            ""Sam"": ""yoZ06aMxZJJ28mfd3POQ"",
        }
        self._headers = {
            ""Content-Type"": ""application/json"",
            ""xi-api-key"": config.api_key,
        }
        self._voices = default_voices.copy()
        if config.voice_id in voice_options:
            config.voice_id = voice_options[config.voice_id]
        self._use_custom_voice(config.voice_id, 0)

    def _use_custom_voice(self, voice, voice_index) -> None:
        """"""Use a custom voice if provided and not a placeholder

        Args:
            voice (str): The voice ID
            voice_index (int): The voice index

        Returns:
            None: None
        """"""
        # Placeholder values that should be treated as empty
        if voice and voice not in PLACEHOLDERS:
            self._voices[voice_index] = voice

    def _speech(self, text: str, voice_id: int = 0) -> bool:
        """"""Speak text using elevenlabs.io's API

        Args:
            text (str): The text to speak
            voice_index (int, optional): The voice to use. Defaults to 0.

        Returns:
            bool: True if the request was successful, False otherwise
        """"""
        tts_url = (
            f""https://api.elevenlabs.io/v1/text-to-speech/{self._voices[voice_id]}""
        )
        response = requests.post(tts_url, headers=self._headers, json={""text"": text})

        if response.status_code == 200:
            with open(""speech.mpeg"", ""wb"") as f:
                f.write(response.content)
            playsound(""speech.mpeg"", True)
            os.remove(""speech.mpeg"")
            return True
        else:
            logger.warning(""Request failed with status code:"", response.status_code)
            logger.info(""Response content:"", response.content)
            return False","Point(row=23, column=0)","Point(row=92, column=24)",,classic/forge/forge/speech/eleven_labs.py
ElevenLabsSpeech._setup,function,"Set up the voices, API key, etc.

        Returns:
            None: None
","def _setup(self, config: ElevenLabsConfig) -> None:
        """"""Set up the voices, API key, etc.

        Returns:
            None: None
        """"""

        default_voices = [""ErXwobaYiN019PkySvjV"", ""EXAVITQu4vr4xnSDxMaL""]
        voice_options = {
            ""Rachel"": ""21m00Tcm4TlvDq8ikWAM"",
            ""Domi"": ""AZnzlk1XvdvUeBnXmlld"",
            ""Bella"": ""EXAVITQu4vr4xnSDxMaL"",
            ""Antoni"": ""ErXwobaYiN019PkySvjV"",
            ""Elli"": ""MF3mGyEYCl7XYWbV9V6O"",
            ""Josh"": ""TxGEqnHWrfWFTfGW9XjX"",
            ""Arnold"": ""VR6AewLTigWG4xSOukaG"",
            ""Adam"": ""pNInz6obpgDQGcFmaJgB"",
            ""Sam"": ""yoZ06aMxZJJ28mfd3POQ"",
        }
        self._headers = {
            ""Content-Type"": ""application/json"",
            ""xi-api-key"": config.api_key,
        }
        self._voices = default_voices.copy()
        if config.voice_id in voice_options:
            config.voice_id = voice_options[config.voice_id]
        self._use_custom_voice(config.voice_id, 0)","Point(row=26, column=4)","Point(row=52, column=50)",ElevenLabsSpeech,classic/forge/forge/speech/eleven_labs.py
ElevenLabsSpeech._use_custom_voice,function,"Use a custom voice if provided and not a placeholder

        Args:
            voice (str): The voice ID
            voice_index (int): The voice index

        Returns:
            None: None
","def _use_custom_voice(self, voice, voice_index) -> None:
        """"""Use a custom voice if provided and not a placeholder

        Args:
            voice (str): The voice ID
            voice_index (int): The voice index

        Returns:
            None: None
        """"""
        # Placeholder values that should be treated as empty
        if voice and voice not in PLACEHOLDERS:
            self._voices[voice_index] = voice","Point(row=54, column=4)","Point(row=66, column=45)",ElevenLabsSpeech,classic/forge/forge/speech/eleven_labs.py
ElevenLabsSpeech._speech,function,"Speak text using elevenlabs.io's API

        Args:
            text (str): The text to speak
            voice_index (int, optional): The voice to use. Defaults to 0.

        Returns:
            bool: True if the request was successful, False otherwise
","def _speech(self, text: str, voice_id: int = 0) -> bool:
        """"""Speak text using elevenlabs.io's API

        Args:
            text (str): The text to speak
            voice_index (int, optional): The voice to use. Defaults to 0.

        Returns:
            bool: True if the request was successful, False otherwise
        """"""
        tts_url = (
            f""https://api.elevenlabs.io/v1/text-to-speech/{self._voices[voice_id]}""
        )
        response = requests.post(tts_url, headers=self._headers, json={""text"": text})

        if response.status_code == 200:
            with open(""speech.mpeg"", ""wb"") as f:
                f.write(response.content)
            playsound(""speech.mpeg"", True)
            os.remove(""speech.mpeg"")
            return True
        else:
            logger.warning(""Request failed with status code:"", response.status_code)
            logger.info(""Response content:"", response.content)
            return False","Point(row=68, column=4)","Point(row=92, column=24)",ElevenLabsSpeech,classic/forge/forge/speech/eleven_labs.py
GTTSVoice,class,GTTS Voice.,"class GTTSVoice(VoiceBase):
    """"""GTTS Voice.""""""

    def _setup(self) -> None:
        pass

    def _speech(self, text: str, voice_id: int = 0) -> bool:
        """"""Play the given text.""""""
        tts = gtts.gTTS(text)
        tts.save(""speech.mp3"")
        playsound(""speech.mp3"", True)
        os.remove(""speech.mp3"")
        return True","Point(row=11, column=0)","Point(row=23, column=19)",,classic/forge/forge/speech/gtts.py
GTTSVoice._setup,function,,"def _setup(self) -> None:
        pass","Point(row=14, column=4)","Point(row=15, column=12)",GTTSVoice,classic/forge/forge/speech/gtts.py
GTTSVoice._speech,function,Play the given text.,"def _speech(self, text: str, voice_id: int = 0) -> bool:
        """"""Play the given text.""""""
        tts = gtts.gTTS(text)
        tts.save(""speech.mp3"")
        playsound(""speech.mp3"", True)
        os.remove(""speech.mp3"")
        return True","Point(row=17, column=4)","Point(row=23, column=19)",GTTSVoice,classic/forge/forge/speech/gtts.py
ImageGeneratorConfiguration,class,,"class ImageGeneratorConfiguration(BaseModel):
    image_provider: Literal[""dalle"", ""huggingface"", ""sdwebui""] = ""dalle""
    huggingface_image_model: str = ""CompVis/stable-diffusion-v1-4""
    huggingface_api_token: Optional[SecretStr] = UserConfigurable(
        None, from_env=""HUGGINGFACE_API_TOKEN"", exclude=True
    )
    sd_webui_url: str = ""http://localhost:7860""
    sd_webui_auth: Optional[SecretStr] = UserConfigurable(
        None, from_env=""SD_WEBUI_AUTH"", exclude=True
    )","Point(row=25, column=0)","Point(row=34, column=5)",,classic/forge/forge/components/image_gen/image_gen.py
ImageGeneratorComponent,class,A component that provides commands to generate images from text prompts.,"class ImageGeneratorComponent(
    CommandProvider, ConfigurableComponent[ImageGeneratorConfiguration]
):
    """"""A component that provides commands to generate images from text prompts.""""""

    config_class = ImageGeneratorConfiguration

    def __init__(
        self,
        workspace: FileStorage,
        config: Optional[ImageGeneratorConfiguration] = None,
        openai_credentials: Optional[OpenAICredentials] = None,
    ):
        """"""openai_credentials only needed for `dalle` provider.""""""
        ConfigurableComponent.__init__(self, config)
        self.openai_credentials = openai_credentials
        self._enabled = bool(self.config.image_provider)
        self._disabled_reason = ""No image provider set.""
        self.workspace = workspace

    def get_commands(self) -> Iterator[Command]:
        if (
            self.openai_credentials
            or self.config.huggingface_api_token
            or self.config.sd_webui_auth
        ):
            yield self.generate_image

    @command(
        parameters={
            ""prompt"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The prompt used to generate the image"",
                required=True,
            ),
            ""size"": JSONSchema(
                type=JSONSchema.Type.INTEGER,
                description=""The size of the image [256, 512, 1024]"",
                required=False,
            ),
        },
    )
    def generate_image(self, prompt: str, size: int) -> str:
        """"""Generate an image from a prompt.

        Args:
            prompt (str): The prompt to use
            size (int, optional): The size of the image. Defaults to 256.
                Not supported by HuggingFace.

        Returns:
            str: The filename of the image
        """"""
        filename = self.workspace.root / f""{str(uuid.uuid4())}.jpg""

        if self.openai_credentials and (
            self.config.image_provider == ""dalle""
            or not (self.config.huggingface_api_token or self.config.sd_webui_url)
        ):
            return self.generate_image_with_dalle(prompt, filename, size)

        elif self.config.huggingface_api_token and (
            self.config.image_provider == ""huggingface""
            or not (self.openai_credentials or self.config.sd_webui_url)
        ):
            return self.generate_image_with_hf(prompt, filename)

        elif self.config.sd_webui_url and (
            self.config.image_provider == ""sdwebui"" or self.config.sd_webui_auth
        ):
            return self.generate_image_with_sd_webui(prompt, filename, size)

        return ""Error: No image generation provider available""

    def generate_image_with_hf(self, prompt: str, output_file: Path) -> str:
        """"""Generate an image with HuggingFace's API.

        Args:
            prompt (str): The prompt to use
            filename (Path): The filename to save the image to

        Returns:
            str: The filename of the image
        """"""
        API_URL = f""https://api-inference.huggingface.co/models/{self.config.huggingface_image_model}""  # noqa: E501
        if self.config.huggingface_api_token is None:
            raise ValueError(
                ""You need to set your Hugging Face API token in the config file.""
            )
        headers = {
            ""Authorization"": (
                f""Bearer {self.config.huggingface_api_token.get_secret_value()}""
            ),
            ""X-Use-Cache"": ""false"",
        }

        retry_count = 0
        while retry_count < 10:
            response = requests.post(
                API_URL,
                headers=headers,
                json={
                    ""inputs"": prompt,
                },
            )

            if response.ok:
                try:
                    image = Image.open(io.BytesIO(response.content))
                    logger.info(f""Image Generated for prompt:{prompt}"")
                    image.save(output_file)
                    return f""Saved to disk: {output_file}""
                except Exception as e:
                    logger.error(e)
                    break
            else:
                try:
                    error = json.loads(response.text)
                    if ""estimated_time"" in error:
                        delay = error[""estimated_time""]
                        logger.debug(response.text)
                        logger.info(""Retrying in"", delay)
                        time.sleep(delay)
                    else:
                        break
                except Exception as e:
                    logger.error(e)
                    break

            retry_count += 1

        return ""Error creating image.""

    def generate_image_with_dalle(
        self, prompt: str, output_file: Path, size: int
    ) -> str:
        """"""Generate an image with DALL-E.

        Args:
            prompt (str): The prompt to use
            filename (Path): The filename to save the image to
            size (int): The size of the image

        Returns:
            str: The filename of the image
        """"""
        assert self.openai_credentials  # otherwise this tool is disabled

        # Check for supported image sizes
        if size not in [256, 512, 1024]:
            closest = min([256, 512, 1024], key=lambda x: abs(x - size))
            logger.info(
                ""DALL-E only supports image sizes of 256x256, 512x512, or 1024x1024. ""
                f""Setting to {closest}, was {size}.""
            )
            size = closest

        # TODO: integrate in `forge.llm.providers`(?)
        response = OpenAI(
            api_key=self.openai_credentials.api_key.get_secret_value(),
            organization=self.openai_credentials.organization.get_secret_value()
            if self.openai_credentials.organization
            else None,
        ).images.generate(
            prompt=prompt,
            n=1,
            # TODO: improve typing of size config item(s)
            size=f""{size}x{size}"",  # type: ignore
            response_format=""b64_json"",
        )
        assert response.data[0].b64_json is not None  # response_format = ""b64_json""

        logger.info(f""Image Generated for prompt: {prompt}"")

        image_data = b64decode(response.data[0].b64_json)

        with open(output_file, mode=""wb"") as png:
            png.write(image_data)

        return f""Saved to disk: {output_file}""

    def generate_image_with_sd_webui(
        self,
        prompt: str,
        output_file: Path,
        size: int = 512,
        negative_prompt: str = """",
        extra: dict = {},
    ) -> str:
        """"""Generate an image with Stable Diffusion webui.
        Args:
            prompt (str): The prompt to use
            filename (str): The filename to save the image to
            size (int, optional): The size of the image. Defaults to 256.
            negative_prompt (str, optional): The negative prompt to use. Defaults to """".
            extra (dict, optional): Extra parameters to pass to the API. Defaults to {}.
        Returns:
            str: The filename of the image
        """"""
        # Create a session and set the basic auth if needed
        s = requests.Session()
        if self.config.sd_webui_auth:
            username, password = self.config.sd_webui_auth.get_secret_value().split("":"")
            s.auth = (username, password or """")

        # Generate the images
        response = requests.post(
            f""{self.config.sd_webui_url}/sdapi/v1/txt2img"",
            json={
                ""prompt"": prompt,
                ""negative_prompt"": negative_prompt,
                ""sampler_index"": ""DDIM"",
                ""steps"": 20,
                ""config_scale"": 7.0,
                ""width"": size,
                ""height"": size,
                ""n_iter"": 1,
                **extra,
            },
        )

        logger.info(f""Image Generated for prompt: '{prompt}'"")

        # Save the image to disk
        response = response.json()
        b64 = b64decode(response[""images""][0].split("","", 1)[0])
        image = Image.open(io.BytesIO(b64))
        image.save(output_file)

        return f""Saved to disk: {output_file}""","Point(row=37, column=0)","Point(row=266, column=46)",,classic/forge/forge/components/image_gen/image_gen.py
ImageGeneratorComponent.__init__,function,openai_credentials only needed for `dalle` provider.,"def __init__(
        self,
        workspace: FileStorage,
        config: Optional[ImageGeneratorConfiguration] = None,
        openai_credentials: Optional[OpenAICredentials] = None,
    ):
        """"""openai_credentials only needed for `dalle` provider.""""""
        ConfigurableComponent.__init__(self, config)
        self.openai_credentials = openai_credentials
        self._enabled = bool(self.config.image_provider)
        self._disabled_reason = ""No image provider set.""
        self.workspace = workspace","Point(row=44, column=4)","Point(row=55, column=34)",ImageGeneratorComponent,classic/forge/forge/components/image_gen/image_gen.py
ImageGeneratorComponent.get_commands,function,,"def get_commands(self) -> Iterator[Command]:
        if (
            self.openai_credentials
            or self.config.huggingface_api_token
            or self.config.sd_webui_auth
        ):
            yield self.generate_image","Point(row=57, column=4)","Point(row=63, column=37)",ImageGeneratorComponent,classic/forge/forge/components/image_gen/image_gen.py
ImageGeneratorComponent.generate_image,function,"Generate an image from a prompt.

        Args:
            prompt (str): The prompt to use
            size (int, optional): The size of the image. Defaults to 256.
                Not supported by HuggingFace.

        Returns:
            str: The filename of the image
","def generate_image(self, prompt: str, size: int) -> str:
        """"""Generate an image from a prompt.

        Args:
            prompt (str): The prompt to use
            size (int, optional): The size of the image. Defaults to 256.
                Not supported by HuggingFace.

        Returns:
            str: The filename of the image
        """"""
        filename = self.workspace.root / f""{str(uuid.uuid4())}.jpg""

        if self.openai_credentials and (
            self.config.image_provider == ""dalle""
            or not (self.config.huggingface_api_token or self.config.sd_webui_url)
        ):
            return self.generate_image_with_dalle(prompt, filename, size)

        elif self.config.huggingface_api_token and (
            self.config.image_provider == ""huggingface""
            or not (self.openai_credentials or self.config.sd_webui_url)
        ):
            return self.generate_image_with_hf(prompt, filename)

        elif self.config.sd_webui_url and (
            self.config.image_provider == ""sdwebui"" or self.config.sd_webui_auth
        ):
            return self.generate_image_with_sd_webui(prompt, filename, size)

        return ""Error: No image generation provider available""","Point(row=79, column=4)","Point(row=109, column=62)",ImageGeneratorComponent,classic/forge/forge/components/image_gen/image_gen.py
ImageGeneratorComponent.generate_image_with_hf,function,"Generate an image with HuggingFace's API.

        Args:
            prompt (str): The prompt to use
            filename (Path): The filename to save the image to

        Returns:
            str: The filename of the image
","def generate_image_with_hf(self, prompt: str, output_file: Path) -> str:
        """"""Generate an image with HuggingFace's API.

        Args:
            prompt (str): The prompt to use
            filename (Path): The filename to save the image to

        Returns:
            str: The filename of the image
        """"""
        API_URL = f""https://api-inference.huggingface.co/models/{self.config.huggingface_image_model}""  # noqa: E501
        if self.config.huggingface_api_token is None:
            raise ValueError(
                ""You need to set your Hugging Face API token in the config file.""
            )
        headers = {
            ""Authorization"": (
                f""Bearer {self.config.huggingface_api_token.get_secret_value()}""
            ),
            ""X-Use-Cache"": ""false"",
        }

        retry_count = 0
        while retry_count < 10:
            response = requests.post(
                API_URL,
                headers=headers,
                json={
                    ""inputs"": prompt,
                },
            )

            if response.ok:
                try:
                    image = Image.open(io.BytesIO(response.content))
                    logger.info(f""Image Generated for prompt:{prompt}"")
                    image.save(output_file)
                    return f""Saved to disk: {output_file}""
                except Exception as e:
                    logger.error(e)
                    break
            else:
                try:
                    error = json.loads(response.text)
                    if ""estimated_time"" in error:
                        delay = error[""estimated_time""]
                        logger.debug(response.text)
                        logger.info(""Retrying in"", delay)
                        time.sleep(delay)
                    else:
                        break
                except Exception as e:
                    logger.error(e)
                    break

            retry_count += 1

        return ""Error creating image.""","Point(row=111, column=4)","Point(row=168, column=38)",ImageGeneratorComponent,classic/forge/forge/components/image_gen/image_gen.py
ImageGeneratorComponent.generate_image_with_dalle,function,"Generate an image with DALL-E.

        Args:
            prompt (str): The prompt to use
            filename (Path): The filename to save the image to
            size (int): The size of the image

        Returns:
            str: The filename of the image
","def generate_image_with_dalle(
        self, prompt: str, output_file: Path, size: int
    ) -> str:
        """"""Generate an image with DALL-E.

        Args:
            prompt (str): The prompt to use
            filename (Path): The filename to save the image to
            size (int): The size of the image

        Returns:
            str: The filename of the image
        """"""
        assert self.openai_credentials  # otherwise this tool is disabled

        # Check for supported image sizes
        if size not in [256, 512, 1024]:
            closest = min([256, 512, 1024], key=lambda x: abs(x - size))
            logger.info(
                ""DALL-E only supports image sizes of 256x256, 512x512, or 1024x1024. ""
                f""Setting to {closest}, was {size}.""
            )
            size = closest

        # TODO: integrate in `forge.llm.providers`(?)
        response = OpenAI(
            api_key=self.openai_credentials.api_key.get_secret_value(),
            organization=self.openai_credentials.organization.get_secret_value()
            if self.openai_credentials.organization
            else None,
        ).images.generate(
            prompt=prompt,
            n=1,
            # TODO: improve typing of size config item(s)
            size=f""{size}x{size}"",  # type: ignore
            response_format=""b64_json"",
        )
        assert response.data[0].b64_json is not None  # response_format = ""b64_json""

        logger.info(f""Image Generated for prompt: {prompt}"")

        image_data = b64decode(response.data[0].b64_json)

        with open(output_file, mode=""wb"") as png:
            png.write(image_data)

        return f""Saved to disk: {output_file}""","Point(row=170, column=4)","Point(row=216, column=46)",ImageGeneratorComponent,classic/forge/forge/components/image_gen/image_gen.py
ImageGeneratorComponent.generate_image_with_sd_webui,function,"Generate an image with Stable Diffusion webui.
        Args:
            prompt (str): The prompt to use
            filename (str): The filename to save the image to
            size (int, optional): The size of the image. Defaults to 256.
            negative_prompt (str, optional): The negative prompt to use. Defaults to """".
            extra (dict, optional): Extra parameters to pass to the API. Defaults to {}.
        Returns:
            str: The filename of the image
","def generate_image_with_sd_webui(
        self,
        prompt: str,
        output_file: Path,
        size: int = 512,
        negative_prompt: str = """",
        extra: dict = {},
    ) -> str:
        """"""Generate an image with Stable Diffusion webui.
        Args:
            prompt (str): The prompt to use
            filename (str): The filename to save the image to
            size (int, optional): The size of the image. Defaults to 256.
            negative_prompt (str, optional): The negative prompt to use. Defaults to """".
            extra (dict, optional): Extra parameters to pass to the API. Defaults to {}.
        Returns:
            str: The filename of the image
        """"""
        # Create a session and set the basic auth if needed
        s = requests.Session()
        if self.config.sd_webui_auth:
            username, password = self.config.sd_webui_auth.get_secret_value().split("":"")
            s.auth = (username, password or """")

        # Generate the images
        response = requests.post(
            f""{self.config.sd_webui_url}/sdapi/v1/txt2img"",
            json={
                ""prompt"": prompt,
                ""negative_prompt"": negative_prompt,
                ""sampler_index"": ""DDIM"",
                ""steps"": 20,
                ""config_scale"": 7.0,
                ""width"": size,
                ""height"": size,
                ""n_iter"": 1,
                **extra,
            },
        )

        logger.info(f""Image Generated for prompt: '{prompt}'"")

        # Save the image to disk
        response = response.json()
        b64 = b64decode(response[""images""][0].split("","", 1)[0])
        image = Image.open(io.BytesIO(b64))
        image.save(output_file)

        return f""Saved to disk: {output_file}""","Point(row=218, column=4)","Point(row=266, column=46)",ImageGeneratorComponent,classic/forge/forge/components/image_gen/image_gen.py
image_gen_component,function,,"def image_gen_component(storage: FileStorage):
    try:
        cred = OpenAICredentials.from_env()
    except ValidationError:
        cred = OpenAICredentials(api_key=SecretStr(""test""))

    return ImageGeneratorComponent(storage, openai_credentials=cred)","Point(row=16, column=0)","Point(row=22, column=68)",,classic/forge/forge/components/image_gen/test_image_gen.py
huggingface_image_gen_component,function,,"def huggingface_image_gen_component(storage: FileStorage):
    config = ImageGeneratorConfiguration(
        image_provider=""huggingface"",
        huggingface_api_token=SecretStr(""1""),
        huggingface_image_model=""CompVis/stable-diffusion-v1-4"",
    )
    return ImageGeneratorComponent(storage, config=config)","Point(row=26, column=0)","Point(row=32, column=58)",,classic/forge/forge/components/image_gen/test_image_gen.py
image_size,function,Parametrize image size.,"def image_size(request):
    """"""Parametrize image size.""""""
    return request.param","Point(row=36, column=0)","Point(row=38, column=24)",,classic/forge/forge/components/image_gen/test_image_gen.py
test_dalle,function,Test DALL-E image generation.,"def test_dalle(
    image_gen_component: ImageGeneratorComponent,
    image_size,
):
    """"""Test DALL-E image generation.""""""
    generate_and_validate(
        image_gen_component,
        image_provider=""dalle"",
        image_size=image_size,
    )","Point(row=42, column=0)","Point(row=51, column=5)",,classic/forge/forge/components/image_gen/test_image_gen.py
test_huggingface,function,Test HuggingFace image generation.,"def test_huggingface(
    image_gen_component: ImageGeneratorComponent,
    image_size,
    image_model,
):
    """"""Test HuggingFace image generation.""""""
    generate_and_validate(
        image_gen_component,
        image_provider=""huggingface"",
        image_size=image_size,
        hugging_face_image_model=image_model,
    )","Point(row=62, column=0)","Point(row=73, column=5)",,classic/forge/forge/components/image_gen/test_image_gen.py
test_sd_webui,function,Test SD WebUI image generation.,"def test_sd_webui(image_gen_component: ImageGeneratorComponent, image_size):
    """"""Test SD WebUI image generation.""""""
    generate_and_validate(
        image_gen_component,
        image_provider=""sd_webui"",
        image_size=image_size,
    )","Point(row=77, column=0)","Point(row=83, column=5)",,classic/forge/forge/components/image_gen/test_image_gen.py
test_sd_webui_negative_prompt,function,,"def test_sd_webui_negative_prompt(
    image_gen_component: ImageGeneratorComponent, image_size
):
    gen_image = functools.partial(
        image_gen_component.generate_image_with_sd_webui,
        prompt=""astronaut riding a horse"",
        size=image_size,
        extra={""seed"": 123},
    )

    # Generate an image with a negative prompt
    image_path = lst(
        gen_image(negative_prompt=""horse"", output_file=Path(""negative.jpg""))
    )
    with Image.open(image_path) as img:
        neg_image_hash = hashlib.md5(img.tobytes()).hexdigest()

    # Generate an image without a negative prompt
    image_path = lst(gen_image(output_file=Path(""positive.jpg"")))
    with Image.open(image_path) as img:
        image_hash = hashlib.md5(img.tobytes()).hexdigest()

    assert image_hash != neg_image_hash","Point(row=87, column=0)","Point(row=109, column=39)",,classic/forge/forge/components/image_gen/test_image_gen.py
lst,function,Extract the file path from the output of `generate_image()`,"def lst(txt):
    """"""Extract the file path from the output of `generate_image()`""""""
    return Path(txt.split("": "", maxsplit=1)[1].strip())","Point(row=112, column=0)","Point(row=114, column=55)",,classic/forge/forge/components/image_gen/test_image_gen.py
generate_and_validate,function,Generate an image and validate the output.,"def generate_and_validate(
    image_gen_component: ImageGeneratorComponent,
    image_size,
    image_provider,
    hugging_face_image_model=None,
    **kwargs,
):
    """"""Generate an image and validate the output.""""""
    image_gen_component.config.image_provider = image_provider
    if hugging_face_image_model:
        image_gen_component.config.huggingface_image_model = hugging_face_image_model
    prompt = ""astronaut riding a horse""

    image_path = lst(image_gen_component.generate_image(prompt, image_size, **kwargs))
    assert image_path.exists()
    with Image.open(image_path) as img:
        assert img.size == (image_size, image_size)","Point(row=117, column=0)","Point(row=133, column=51)",,classic/forge/forge/components/image_gen/test_image_gen.py
test_huggingface_fail_request_with_delay,function,,"def test_huggingface_fail_request_with_delay(
    huggingface_image_gen_component: ImageGeneratorComponent,
    image_size,
    image_model,
    return_text,
    delay,
):
    return_text = return_text.replace(""[model]"", image_model).replace(
        ""[delay]"", str(delay)
    )

    with patch(""requests.post"") as mock_post:
        if return_text == """":
            # Test bad image
            mock_post.return_value.status_code = 200
            mock_post.return_value.ok = True
            mock_post.return_value.content = b""bad image""
        else:
            # Test delay and bad json
            mock_post.return_value.status_code = 500
            mock_post.return_value.ok = False
            mock_post.return_value.text = return_text

        huggingface_image_gen_component.config.huggingface_image_model = image_model
        prompt = ""astronaut riding a horse""

        with patch(""time.sleep"") as mock_sleep:
            # Verify request fails.
            result = huggingface_image_gen_component.generate_image(prompt, image_size)
            assert result == ""Error creating image.""

            # Verify retry was called with delay if delay is in return_text
            if ""estimated_time"" in return_text:
                mock_sleep.assert_called_with(delay)
            else:
                mock_sleep.assert_not_called()","Point(row=151, column=0)","Point(row=186, column=46)",,classic/forge/forge/components/image_gen/test_image_gen.py
test_huggingface_fail_request_no_delay,function,,"def test_huggingface_fail_request_no_delay(
    mocker, huggingface_image_gen_component: ImageGeneratorComponent
):
    # Mock requests.post
    mock_post = mocker.patch(""requests.post"")
    mock_post.return_value.status_code = 500
    mock_post.return_value.ok = False
    mock_post.return_value.text = (
        '{""error"":""Model CompVis/stable-diffusion-v1-4 is currently loading""}'
    )

    # Mock time.sleep
    mock_sleep = mocker.patch(""time.sleep"")

    result = huggingface_image_gen_component.generate_image(
        ""astronaut riding a horse"", 512
    )

    assert result == ""Error creating image.""

    # Verify retry was not called.
    mock_sleep.assert_not_called()","Point(row=189, column=0)","Point(row=210, column=34)",,classic/forge/forge/components/image_gen/test_image_gen.py
test_huggingface_fail_request_bad_json,function,,"def test_huggingface_fail_request_bad_json(
    mocker, huggingface_image_gen_component: ImageGeneratorComponent
):
    # Mock requests.post
    mock_post = mocker.patch(""requests.post"")
    mock_post.return_value.status_code = 500
    mock_post.return_value.ok = False
    mock_post.return_value.text = '{""error:}'

    # Mock time.sleep
    mock_sleep = mocker.patch(""time.sleep"")

    result = huggingface_image_gen_component.generate_image(
        ""astronaut riding a horse"", 512
    )

    assert result == ""Error creating image.""

    # Verify retry was not called.
    mock_sleep.assert_not_called()","Point(row=213, column=0)","Point(row=232, column=34)",,classic/forge/forge/components/image_gen/test_image_gen.py
test_huggingface_fail_request_bad_image,function,,"def test_huggingface_fail_request_bad_image(
    mocker, huggingface_image_gen_component: ImageGeneratorComponent
):
    # Mock requests.post
    mock_post = mocker.patch(""requests.post"")
    mock_post.return_value.status_code = 200

    result = huggingface_image_gen_component.generate_image(
        ""astronaut riding a horse"", 512
    )

    assert result == ""Error creating image.""","Point(row=235, column=0)","Point(row=246, column=44)",,classic/forge/forge/components/image_gen/test_image_gen.py
BaseContextItem,class,,"class BaseContextItem(ABC):
    @property
    @abstractmethod
    def description(self) -> str:
        """"""Description of the context item""""""
        ...

    @property
    @abstractmethod
    def source(self) -> Optional[str]:
        """"""A string indicating the source location of the context item""""""
        ...

    @abstractmethod
    def get_content(self, workspace: FileStorage) -> str:
        """"""The content represented by the context item""""""
        ...

    def fmt(self, workspace: FileStorage) -> str:
        return (
            f""{self.description} (source: {self.source})\n""
            ""```\n""
            f""{self.get_content(workspace)}\n""
            ""```""
        )","Point(row=13, column=0)","Point(row=37, column=9)",,classic/forge/forge/components/context/context_item.py
BaseContextItem.description,function,Description of the context item,"def description(self) -> str:
        """"""Description of the context item""""""
        ...","Point(row=16, column=4)","Point(row=18, column=11)",BaseContextItem,classic/forge/forge/components/context/context_item.py
BaseContextItem.source,function,A string indicating the source location of the context item,"def source(self) -> Optional[str]:
        """"""A string indicating the source location of the context item""""""
        ...","Point(row=22, column=4)","Point(row=24, column=11)",BaseContextItem,classic/forge/forge/components/context/context_item.py
BaseContextItem.get_content,function,The content represented by the context item,"def get_content(self, workspace: FileStorage) -> str:
        """"""The content represented by the context item""""""
        ...","Point(row=27, column=4)","Point(row=29, column=11)",BaseContextItem,classic/forge/forge/components/context/context_item.py
BaseContextItem.fmt,function,,"def fmt(self, workspace: FileStorage) -> str:
        return (
            f""{self.description} (source: {self.source})\n""
            ""```\n""
            f""{self.get_content(workspace)}\n""
            ""```""
        )","Point(row=31, column=4)","Point(row=37, column=9)",BaseContextItem,classic/forge/forge/components/context/context_item.py
FileContextItem,class,,"class FileContextItem(BaseModel, BaseContextItem):
    path: Path
    type: Literal[""file""] = ""file""

    @property
    def description(self) -> str:
        return f""The current content of the file '{self.path}'""

    @property
    def source(self) -> str:
        return str(self.path)

    def get_content(self, workspace: FileStorage) -> str:
        with workspace.open_file(self.path, ""r"", True) as file:
            return decode_textual_file(file, self.path.suffix, logger)","Point(row=40, column=0)","Point(row=54, column=70)",,classic/forge/forge/components/context/context_item.py
FileContextItem.description,function,,"def description(self) -> str:
        return f""The current content of the file '{self.path}'""","Point(row=45, column=4)","Point(row=46, column=63)",FileContextItem,classic/forge/forge/components/context/context_item.py
FileContextItem.source,function,,"def source(self) -> str:
        return str(self.path)","Point(row=49, column=4)","Point(row=50, column=29)",FileContextItem,classic/forge/forge/components/context/context_item.py
FileContextItem.get_content,function,,"def get_content(self, workspace: FileStorage) -> str:
        with workspace.open_file(self.path, ""r"", True) as file:
            return decode_textual_file(file, self.path.suffix, logger)","Point(row=52, column=4)","Point(row=54, column=70)",FileContextItem,classic/forge/forge/components/context/context_item.py
FolderContextItem,class,,"class FolderContextItem(BaseModel, BaseContextItem):
    path: Path
    type: Literal[""folder""] = ""folder""

    @property
    def description(self) -> str:
        return f""The contents of the folder '{self.path}' in the workspace""

    @property
    def source(self) -> str:
        return str(self.path)

    def get_content(self, workspace: FileStorage) -> str:
        files = [str(p) for p in workspace.list_files(self.path)]
        folders = [f""{str(p)}/"" for p in workspace.list_folders(self.path)]
        items = folders + files
        items.sort()
        return ""\n"".join(items)","Point(row=57, column=0)","Point(row=74, column=31)",,classic/forge/forge/components/context/context_item.py
FolderContextItem.description,function,,"def description(self) -> str:
        return f""The contents of the folder '{self.path}' in the workspace""","Point(row=62, column=4)","Point(row=63, column=75)",FolderContextItem,classic/forge/forge/components/context/context_item.py
FolderContextItem.source,function,,"def source(self) -> str:
        return str(self.path)","Point(row=66, column=4)","Point(row=67, column=29)",FolderContextItem,classic/forge/forge/components/context/context_item.py
FolderContextItem.get_content,function,,"def get_content(self, workspace: FileStorage) -> str:
        files = [str(p) for p in workspace.list_files(self.path)]
        folders = [f""{str(p)}/"" for p in workspace.list_folders(self.path)]
        items = folders + files
        items.sort()
        return ""\n"".join(items)","Point(row=69, column=4)","Point(row=74, column=31)",FolderContextItem,classic/forge/forge/components/context/context_item.py
StaticContextItem,class,,"class StaticContextItem(BaseModel, BaseContextItem):
    item_description: str = Field(alias=""description"")
    item_source: Optional[str] = Field(alias=""source"")
    item_content: str = Field(alias=""content"")
    type: Literal[""static""] = ""static""","Point(row=77, column=0)","Point(row=81, column=38)",,classic/forge/forge/components/context/context_item.py
AgentContext,class,,"class AgentContext(BaseModel):
    items: list[Annotated[ContextItem, Field(discriminator=""type"")]] = Field(
        default_factory=list
    )

    def __bool__(self) -> bool:
        return len(self.items) > 0

    def __contains__(self, item: ContextItem) -> bool:
        return any([i.source == item.source for i in self.items])

    def add(self, item: ContextItem) -> None:
        self.items.append(item)

    def close(self, index: int) -> None:
        self.items.pop(index - 1)

    def clear(self) -> None:
        self.items.clear()

    def format_numbered(self, workspace: FileStorage) -> str:
        return ""\n\n"".join(
            [f""{i}. {c.fmt(workspace)}"" for i, c in enumerate(self.items, 1)]
        )","Point(row=17, column=0)","Point(row=40, column=9)",,classic/forge/forge/components/context/context.py
AgentContext.__bool__,function,,"def __bool__(self) -> bool:
        return len(self.items) > 0","Point(row=22, column=4)","Point(row=23, column=34)",AgentContext,classic/forge/forge/components/context/context.py
AgentContext.__contains__,function,,"def __contains__(self, item: ContextItem) -> bool:
        return any([i.source == item.source for i in self.items])","Point(row=25, column=4)","Point(row=26, column=65)",AgentContext,classic/forge/forge/components/context/context.py
AgentContext.add,function,,"def add(self, item: ContextItem) -> None:
        self.items.append(item)","Point(row=28, column=4)","Point(row=29, column=31)",AgentContext,classic/forge/forge/components/context/context.py
AgentContext.close,function,,"def close(self, index: int) -> None:
        self.items.pop(index - 1)","Point(row=31, column=4)","Point(row=32, column=33)",AgentContext,classic/forge/forge/components/context/context.py
AgentContext.clear,function,,"def clear(self) -> None:
        self.items.clear()","Point(row=34, column=4)","Point(row=35, column=26)",AgentContext,classic/forge/forge/components/context/context.py
AgentContext.format_numbered,function,,"def format_numbered(self, workspace: FileStorage) -> str:
        return ""\n\n"".join(
            [f""{i}. {c.fmt(workspace)}"" for i, c in enumerate(self.items, 1)]
        )","Point(row=37, column=4)","Point(row=40, column=9)",AgentContext,classic/forge/forge/components/context/context.py
ContextComponent,class,Adds ability to keep files and folders open in the context (prompt).,"class ContextComponent(MessageProvider, CommandProvider):
    """"""Adds ability to keep files and folders open in the context (prompt).""""""

    def __init__(self, workspace: FileStorage, context: AgentContext):
        self.context = context
        self.workspace = workspace

    def get_messages(self) -> Iterator[ChatMessage]:
        if self.context:
            yield ChatMessage.system(
                ""## Context\n""
                f""{self.context.format_numbered(self.workspace)}\n\n""
                ""When a context item is no longer needed and you are not done yet, ""
                ""you can hide the item by specifying its number in the list above ""
                ""to `hide_context_item`."",
            )

    def get_commands(self) -> Iterator[Command]:
        yield self.open_file
        yield self.open_folder
        if self.context:
            yield self.close_context_item

    @command(
        parameters={
            ""file_path"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The path of the file to open"",
                required=True,
            )
        }
    )
    async def open_file(self, file_path: str | Path) -> str:
        """"""Opens a file for editing or continued viewing;
        creates it if it does not exist yet.
        Note: If you only need to read or write a file once,
        use `write_to_file` instead.

        Args:
            file_path (str | Path): The path of the file to open

        Returns:
            str: A status message indicating what happened
        """"""
        if not isinstance(file_path, Path):
            file_path = Path(file_path)

        created = False
        if not self.workspace.exists(file_path):
            await self.workspace.write_file(file_path, """")
            created = True

        # Try to make the file path relative
        with contextlib.suppress(ValueError):
            file_path = file_path.relative_to(self.workspace.root)

        file = FileContextItem(path=file_path)
        self.context.add(file)
        return (
            f""File {file_path}{' created,' if created else ''} has been opened""
            "" and added to the context ‚úÖ""
        )

    @command(
        parameters={
            ""path"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The path of the folder to open"",
                required=True,
            )
        }
    )
    def open_folder(self, path: str | Path) -> str:
        """"""Open a folder to keep track of its content

        Args:
            path (str | Path): The path of the folder to open

        Returns:
            str: A status message indicating what happened
        """"""
        if not isinstance(path, Path):
            path = Path(path)

        if not self.workspace.exists(path):
            raise FileNotFoundError(
                f""open_folder {path} failed: no such file or directory""
            )

        # Try to make the path relative
        with contextlib.suppress(ValueError):
            path = path.relative_to(self.workspace.root)

        folder = FolderContextItem(path=path)
        self.context.add(folder)
        return f""Folder {path} has been opened and added to the context ‚úÖ""

    @command(
        parameters={
            ""number"": JSONSchema(
                type=JSONSchema.Type.INTEGER,
                description=""The 1-based index of the context item to hide"",
                required=True,
            )
        }
    )
    def close_context_item(self, number: int) -> str:
        """"""Hide an open file, folder or other context item, to save tokens.

        Args:
            number (int): The 1-based index of the context item to hide

        Returns:
            str: A status message indicating what happened
        """"""
        if number > len(self.context.items) or number == 0:
            raise InvalidArgumentError(f""Index {number} out of range"")

        self.context.close(number)
        return f""Context item {number} hidden ‚úÖ""","Point(row=43, column=0)","Point(row=162, column=50)",,classic/forge/forge/components/context/context.py
ContextComponent.__init__,function,,"def __init__(self, workspace: FileStorage, context: AgentContext):
        self.context = context
        self.workspace = workspace","Point(row=46, column=4)","Point(row=48, column=34)",ContextComponent,classic/forge/forge/components/context/context.py
ContextComponent.get_messages,function,,"def get_messages(self) -> Iterator[ChatMessage]:
        if self.context:
            yield ChatMessage.system(
                ""## Context\n""
                f""{self.context.format_numbered(self.workspace)}\n\n""
                ""When a context item is no longer needed and you are not done yet, ""
                ""you can hide the item by specifying its number in the list above ""
                ""to `hide_context_item`."",
            )","Point(row=50, column=4)","Point(row=58, column=13)",ContextComponent,classic/forge/forge/components/context/context.py
ContextComponent.get_commands,function,,"def get_commands(self) -> Iterator[Command]:
        yield self.open_file
        yield self.open_folder
        if self.context:
            yield self.close_context_item","Point(row=60, column=4)","Point(row=64, column=41)",ContextComponent,classic/forge/forge/components/context/context.py
ContextComponent.open_file,function,"Opens a file for editing or continued viewing;
        creates it if it does not exist yet.
        Note: If you only need to read or write a file once,
        use `write_to_file` instead.

        Args:
            file_path (str | Path): The path of the file to open

        Returns:
            str: A status message indicating what happened
","async def open_file(self, file_path: str | Path) -> str:
        """"""Opens a file for editing or continued viewing;
        creates it if it does not exist yet.
        Note: If you only need to read or write a file once,
        use `write_to_file` instead.

        Args:
            file_path (str | Path): The path of the file to open

        Returns:
            str: A status message indicating what happened
        """"""
        if not isinstance(file_path, Path):
            file_path = Path(file_path)

        created = False
        if not self.workspace.exists(file_path):
            await self.workspace.write_file(file_path, """")
            created = True

        # Try to make the file path relative
        with contextlib.suppress(ValueError):
            file_path = file_path.relative_to(self.workspace.root)

        file = FileContextItem(path=file_path)
        self.context.add(file)
        return (
            f""File {file_path}{' created,' if created else ''} has been opened""
            "" and added to the context ‚úÖ""
        )","Point(row=75, column=4)","Point(row=104, column=9)",ContextComponent,classic/forge/forge/components/context/context.py
ContextComponent.open_folder,function,"Open a folder to keep track of its content

        Args:
            path (str | Path): The path of the folder to open

        Returns:
            str: A status message indicating what happened
","def open_folder(self, path: str | Path) -> str:
        """"""Open a folder to keep track of its content

        Args:
            path (str | Path): The path of the folder to open

        Returns:
            str: A status message indicating what happened
        """"""
        if not isinstance(path, Path):
            path = Path(path)

        if not self.workspace.exists(path):
            raise FileNotFoundError(
                f""open_folder {path} failed: no such file or directory""
            )

        # Try to make the path relative
        with contextlib.suppress(ValueError):
            path = path.relative_to(self.workspace.root)

        folder = FolderContextItem(path=path)
        self.context.add(folder)
        return f""Folder {path} has been opened and added to the context ‚úÖ""","Point(row=115, column=4)","Point(row=138, column=76)",ContextComponent,classic/forge/forge/components/context/context.py
ContextComponent.close_context_item,function,"Hide an open file, folder or other context item, to save tokens.

        Args:
            number (int): The 1-based index of the context item to hide

        Returns:
            str: A status message indicating what happened
","def close_context_item(self, number: int) -> str:
        """"""Hide an open file, folder or other context item, to save tokens.

        Args:
            number (int): The 1-based index of the context item to hide

        Returns:
            str: A status message indicating what happened
        """"""
        if number > len(self.context.items) or number == 0:
            raise InvalidArgumentError(f""Index {number} out of range"")

        self.context.close(number)
        return f""Context item {number} hidden ‚úÖ""","Point(row=149, column=4)","Point(row=162, column=50)",ContextComponent,classic/forge/forge/components/context/context.py
web_selenium_component,function,,"def web_selenium_component(app_data_dir: Path):
    return WebSeleniumComponent(MultiProvider(), app_data_dir)","Point(row=10, column=0)","Point(row=11, column=62)",,classic/forge/forge/components/web/test_selenium.py
test_browse_website_nonexistent_url,function,,"async def test_browse_website_nonexistent_url(
    web_selenium_component: WebSeleniumComponent,
):
    url = ""https://auto-gpt-thinks-this-website-does-not-exist.com""
    question = ""How to execute a barrel roll""

    with pytest.raises(BrowsingError, match=""NAME_NOT_RESOLVED"") as raised:
        await web_selenium_component.read_webpage(url=url, question=question)

        # Sanity check that the response is not too long
        assert len(raised.exconly()) < 200","Point(row=15, column=0)","Point(row=25, column=42)",,classic/forge/forge/components/web/test_selenium.py
web_search_component,function,,"def web_search_component():
    component = WebSearchComponent()
    if component.config.google_api_key is None:
        component.config.google_api_key = SecretStr(""test"")
    if component.config.google_custom_search_engine_id is None:
        component.config.google_custom_search_engine_id = SecretStr(""test"")
    return component","Point(row=13, column=0)","Point(row=19, column=20)",,classic/forge/forge/components/web/test_search.py
test_safe_google_results,function,,"def test_safe_google_results(
    query, expected_output, web_search_component: WebSearchComponent
):
    result = web_search_component.safe_google_results(query)
    assert isinstance(result, str)
    assert result == expected_output","Point(row=27, column=0)","Point(row=32, column=36)",,classic/forge/forge/components/web/test_search.py
test_safe_google_results_invalid_input,function,,"def test_safe_google_results_invalid_input(web_search_component: WebSearchComponent):
    with pytest.raises(AttributeError):
        web_search_component.safe_google_results(123)  # type: ignore","Point(row=36, column=0)","Point(row=38, column=69)",,classic/forge/forge/components/web/test_search.py
test_google_search,function,,"def test_google_search(
    query,
    num_results,
    expected_output_parts,
    return_value,
    mocker,
    web_search_component: WebSearchComponent,
):
    mock_ddg = mocker.Mock()
    mock_ddg.return_value = return_value

    mocker.patch(""forge.components.web.search.DDGS.text"", mock_ddg)
    actual_output = web_search_component.web_search(query, num_results=num_results)
    for o in expected_output_parts:
        assert o in actual_output","Point(row=54, column=0)","Point(row=68, column=33)",,classic/forge/forge/components/web/test_search.py
mock_googleapiclient,function,,"def mock_googleapiclient(mocker):
    mock_build = mocker.patch(""googleapiclient.discovery.build"")
    mock_service = mocker.Mock()
    mock_build.return_value = mock_service
    return mock_service.cse().list().execute().get","Point(row=72, column=0)","Point(row=76, column=50)",,classic/forge/forge/components/web/test_search.py
test_google_official_search,function,,"def test_google_official_search(
    query,
    num_results,
    expected_output,
    search_results,
    mock_googleapiclient,
    web_search_component: WebSearchComponent,
):
    mock_googleapiclient.return_value = search_results
    actual_output = web_search_component.google(query, num_results=num_results)
    assert actual_output == web_search_component.safe_google_results(expected_output)","Point(row=99, column=0)","Point(row=109, column=85)",,classic/forge/forge/components/web/test_search.py
test_google_official_search_errors,function,,"def test_google_official_search_errors(
    query,
    num_results,
    expected_error_type,
    mock_googleapiclient,
    http_code,
    error_msg,
    web_search_component: WebSearchComponent,
):
    response_content = {
        ""error"": {""code"": http_code, ""message"": error_msg, ""reason"": ""backendError""}
    }
    error = HttpError(
        resp=Response({""status"": http_code, ""reason"": error_msg}),
        content=str.encode(json.dumps(response_content)),
        uri=""https://www.googleapis.com/customsearch/v1?q=invalid+query&cx"",
    )

    mock_googleapiclient.side_effect = error
    with pytest.raises(expected_error_type):
        web_search_component.google(query, num_results=num_results)","Point(row=131, column=0)","Point(row=151, column=67)",,classic/forge/forge/components/web/test_search.py
BrowsingError,class,An error occurred while trying to browse the page,"class BrowsingError(CommandExecutionError):
    """"""An error occurred while trying to browse the page""""""","Point(row=52, column=0)","Point(row=53, column=59)",,classic/forge/forge/components/web/selenium.py
WebSeleniumConfiguration,class,,"class WebSeleniumConfiguration(BaseModel):
    llm_name: ModelName = OpenAIModelName.GPT3
    """"""Name of the llm model used to read websites""""""
    web_browser: Literal[""chrome"", ""firefox"", ""safari"", ""edge""] = ""chrome""
    """"""Web browser used by Selenium""""""
    headless: bool = True
    """"""Run browser in headless mode""""""
    user_agent: str = (
        ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) ""
        ""AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36""
    )
    """"""User agent used by the browser""""""
    browse_spacy_language_model: str = ""en_core_web_sm""
    """"""Spacy language model used for chunking text""""""
    selenium_proxy: Optional[str] = None
    """"""Http proxy to use with Selenium""""""","Point(row=56, column=0)","Point(row=71, column=41)",,classic/forge/forge/components/web/selenium.py
WebSeleniumComponent,class,Provides commands to browse the web using Selenium.,"class WebSeleniumComponent(
    DirectiveProvider, CommandProvider, ConfigurableComponent[WebSeleniumConfiguration]
):
    """"""Provides commands to browse the web using Selenium.""""""

    config_class = WebSeleniumConfiguration

    def __init__(
        self,
        llm_provider: MultiProvider,
        data_dir: Path,
        config: Optional[WebSeleniumConfiguration] = None,
    ):
        ConfigurableComponent.__init__(self, config)
        self.llm_provider = llm_provider
        self.data_dir = data_dir

    def get_resources(self) -> Iterator[str]:
        yield ""Ability to read websites.""

    def get_commands(self) -> Iterator[Command]:
        yield self.read_webpage

    @command(
        [""read_webpage""],
        (
            ""Read a webpage, and extract specific information from it.""
            "" You must specify either topics_of_interest,""
            "" a question, or get_raw_content.""
        ),
        {
            ""url"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The URL to visit"",
                required=True,
            ),
            ""topics_of_interest"": JSONSchema(
                type=JSONSchema.Type.ARRAY,
                items=JSONSchema(type=JSONSchema.Type.STRING),
                description=(
                    ""A list of topics about which you want to extract information ""
                    ""from the page.""
                ),
                required=False,
            ),
            ""question"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=(
                    ""A question you want to answer using the content of the webpage.""
                ),
                required=False,
            ),
            ""get_raw_content"": JSONSchema(
                type=JSONSchema.Type.BOOLEAN,
                description=(
                    ""If true, the unprocessed content of the webpage will be returned. ""
                    ""This consumes a lot of tokens, so use it with caution.""
                ),
                required=False,
            ),
        },
    )
    @validate_url
    async def read_webpage(
        self,
        url: str,
        *,
        topics_of_interest: list[str] = [],
        get_raw_content: bool = False,
        question: str = """",
    ) -> str:
        """"""Browse a website and return the answer and links to the user

        Args:
            url (str): The url of the website to browse
            question (str): The question to answer using the content of the webpage

        Returns:
            str: The answer and links to the user and the webdriver
        """"""
        driver = None
        try:
            driver = await self.open_page_in_browser(url)

            text = self.scrape_text_with_selenium(driver)
            links = self.scrape_links_with_selenium(driver, url)

            return_literal_content = True
            summarized = False
            if not text:
                return f""Website did not contain any text.\n\nLinks: {links}""
            elif get_raw_content:
                if (
                    output_tokens := self.llm_provider.count_tokens(
                        text, self.config.llm_name
                    )
                ) > MAX_RAW_CONTENT_LENGTH:
                    oversize_factor = round(output_tokens / MAX_RAW_CONTENT_LENGTH, 1)
                    raise TooMuchOutputError(
                        f""Page content is {oversize_factor}x the allowed length ""
                        ""for `get_raw_content=true`""
                    )
                return text + (f""\n\nLinks: {links}"" if links else """")
            else:
                text = await self.summarize_webpage(
                    text, question or None, topics_of_interest
                )
                return_literal_content = bool(question)
                summarized = True

            # Limit links to LINKS_TO_RETURN
            if len(links) > LINKS_TO_RETURN:
                links = links[:LINKS_TO_RETURN]

            text_fmt = f""'''{text}'''"" if ""\n"" in text else f""'{text}'""
            links_fmt = ""\n"".join(f""- {link}"" for link in links)
            return (
                f""Page content{' (summary)' if summarized else ''}:""
                if return_literal_content
                else ""Answer gathered from webpage:""
            ) + f"" {text_fmt}\n\nLinks:\n{links_fmt}""

        except WebDriverException as e:
            # These errors are often quite long and include lots of context.
            # Just grab the first line.
            msg = e.msg.split(""\n"")[0] if e.msg else str(e)
            if ""net::"" in msg:
                raise BrowsingError(
                    ""A networking error occurred while trying to load the page: %s""
                    % re.sub(r""^unknown error: "", """", msg)
                )
            raise CommandExecutionError(msg)
        finally:
            if driver:
                driver.close()

    def scrape_text_with_selenium(self, driver: WebDriver) -> str:
        """"""Scrape text from a browser window using selenium

        Args:
            driver (WebDriver): A driver object representing
            the browser window to scrape

        Returns:
            str: the text scraped from the website
        """"""

        # Get the HTML content directly from the browser's DOM
        page_source = driver.execute_script(""return document.body.outerHTML;"")
        soup = BeautifulSoup(page_source, ""html.parser"")

        for script in soup([""script"", ""style""]):
            script.extract()

        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split(""  ""))
        text = ""\n"".join(chunk for chunk in chunks if chunk)
        return text

    def scrape_links_with_selenium(self, driver: WebDriver, base_url: str) -> list[str]:
        """"""Scrape links from a website using selenium

        Args:
            driver (WebDriver): A driver object representing
            the browser window to scrape
            base_url (str): The base URL to use for resolving relative links

        Returns:
            List[str]: The links scraped from the website
        """"""
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, ""html.parser"")

        for script in soup([""script"", ""style""]):
            script.extract()

        hyperlinks = extract_hyperlinks(soup, base_url)

        return format_hyperlinks(hyperlinks)

    async def open_page_in_browser(self, url: str) -> WebDriver:
        """"""Open a browser window and load a web page using Selenium

        Params:
            url (str): The URL of the page to load
            config (Config): The applicable application configuration

        Returns:
            driver (WebDriver): A driver object representing
            the browser window to scrape
        """"""
        logging.getLogger(""selenium"").setLevel(logging.CRITICAL)

        options_available: dict[str, Type[BrowserOptions]] = {
            ""chrome"": ChromeOptions,
            ""edge"": EdgeOptions,
            ""firefox"": FirefoxOptions,
            ""safari"": SafariOptions,
        }

        options: BrowserOptions = options_available[self.config.web_browser]()
        options.add_argument(f""user-agent={self.config.user_agent}"")

        if isinstance(options, FirefoxOptions):
            if self.config.headless:
                options.headless = True  # type: ignore
                options.add_argument(""--disable-gpu"")
            driver = FirefoxDriver(
                service=GeckoDriverService(GeckoDriverManager().install()),
                options=options,
            )
        elif isinstance(options, EdgeOptions):
            driver = EdgeDriver(
                service=EdgeDriverService(EdgeDriverManager().install()),
                options=options,
            )
        elif isinstance(options, SafariOptions):
            # Requires a bit more setup on the users end.
            # See https://developer.apple.com/documentation/webkit/testing_with_webdriver_in_safari  # noqa: E501
            driver = SafariDriver(options=options)
        elif isinstance(options, ChromeOptions):
            if platform == ""linux"" or platform == ""linux2"":
                options.add_argument(""--disable-dev-shm-usage"")
                options.add_argument(""--remote-debugging-port=9222"")

            options.add_argument(""--no-sandbox"")
            if self.config.headless:
                options.add_argument(""--headless=new"")
                options.add_argument(""--disable-gpu"")

            if self.config.selenium_proxy:
                options.add_argument(f""--proxy-server={self.config.selenium_proxy}"")

            self._sideload_chrome_extensions(options, self.data_dir / ""assets"" / ""crx"")

            if (chromium_driver_path := Path(""/usr/bin/chromedriver"")).exists():
                chrome_service = ChromeDriverService(str(chromium_driver_path))
            else:
                try:
                    chrome_driver = ChromeDriverManager().install()
                except AttributeError as e:
                    if ""'NoneType' object has no attribute 'split'"" in str(e):
                        # https://github.com/SergeyPirogov/webdriver_manager/issues/649
                        logger.critical(
                            ""Connecting to browser failed:""
                            "" is Chrome or Chromium installed?""
                        )
                    raise
                chrome_service = ChromeDriverService(chrome_driver)
            driver = ChromeDriver(service=chrome_service, options=options)

        driver.get(url)

        # Wait for page to be ready, sleep 2 seconds, wait again until page ready.
        # This allows the cookiewall squasher time to get rid of cookie walls.
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, ""body""))
        )
        await asyncio.sleep(2)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, ""body""))
        )

        return driver

    def _sideload_chrome_extensions(
        self, options: ChromeOptions, dl_folder: Path
    ) -> None:
        crx_download_url_template = ""https://clients2.google.com/service/update2/crx?response=redirect&prodversion=49.0&acceptformat=crx3&x=id%3D{crx_id}%26installsource%3Dondemand%26uc""  # noqa
        cookiewall_squasher_crx_id = ""edibdbjcniadpccecjdfdjjppcpchdlm""
        adblocker_crx_id = ""cjpalhdlnbpafiamejdnhcphjbkeiagm""

        # Make sure the target folder exists
        dl_folder.mkdir(parents=True, exist_ok=True)

        for crx_id in (cookiewall_squasher_crx_id, adblocker_crx_id):
            crx_path = dl_folder / f""{crx_id}.crx""
            if not crx_path.exists():
                logger.debug(f""Downloading CRX {crx_id}..."")
                crx_download_url = crx_download_url_template.format(crx_id=crx_id)
                urlretrieve(crx_download_url, crx_path)
                logger.debug(f""Downloaded {crx_path.name}"")
            options.add_extension(str(crx_path))

    async def summarize_webpage(
        self,
        text: str,
        question: str | None,
        topics_of_interest: list[str],
    ) -> str:
        """"""Summarize text using the OpenAI API

        Args:
            url (str): The url of the text
            text (str): The text to summarize
            question (str): The question to ask the model
            driver (WebDriver): The webdriver to use to scroll the page

        Returns:
            str: The summary of the text
        """"""
        if not text:
            raise ValueError(""No text to summarize"")

        text_length = len(text)
        logger.debug(f""Web page content length: {text_length} characters"")

        result = None
        information = None
        if topics_of_interest:
            information = await extract_information(
                text,
                topics_of_interest=topics_of_interest,
                llm_provider=self.llm_provider,
                model_name=self.config.llm_name,
                spacy_model=self.config.browse_spacy_language_model,
            )
            return ""\n"".join(f""* {i}"" for i in information)
        else:
            result, _ = await summarize_text(
                text,
                question=question,
                llm_provider=self.llm_provider,
                model_name=self.config.llm_name,
                spacy_model=self.config.browse_spacy_language_model,
            )
            return result","Point(row=74, column=0)","Point(row=401, column=25)",,classic/forge/forge/components/web/selenium.py
WebSeleniumComponent.__init__,function,,"def __init__(
        self,
        llm_provider: MultiProvider,
        data_dir: Path,
        config: Optional[WebSeleniumConfiguration] = None,
    ):
        ConfigurableComponent.__init__(self, config)
        self.llm_provider = llm_provider
        self.data_dir = data_dir","Point(row=81, column=4)","Point(row=89, column=32)",WebSeleniumComponent,classic/forge/forge/components/web/selenium.py
WebSeleniumComponent.get_resources,function,,"def get_resources(self) -> Iterator[str]:
        yield ""Ability to read websites.""","Point(row=91, column=4)","Point(row=92, column=41)",WebSeleniumComponent,classic/forge/forge/components/web/selenium.py
WebSeleniumComponent.get_commands,function,,"def get_commands(self) -> Iterator[Command]:
        yield self.read_webpage","Point(row=94, column=4)","Point(row=95, column=31)",WebSeleniumComponent,classic/forge/forge/components/web/selenium.py
WebSeleniumComponent.read_webpage,function,"Browse a website and return the answer and links to the user

        Args:
            url (str): The url of the website to browse
            question (str): The question to answer using the content of the webpage

        Returns:
            str: The answer and links to the user and the webdriver
","async def read_webpage(
        self,
        url: str,
        *,
        topics_of_interest: list[str] = [],
        get_raw_content: bool = False,
        question: str = """",
    ) -> str:
        """"""Browse a website and return the answer and links to the user

        Args:
            url (str): The url of the website to browse
            question (str): The question to answer using the content of the webpage

        Returns:
            str: The answer and links to the user and the webdriver
        """"""
        driver = None
        try:
            driver = await self.open_page_in_browser(url)

            text = self.scrape_text_with_selenium(driver)
            links = self.scrape_links_with_selenium(driver, url)

            return_literal_content = True
            summarized = False
            if not text:
                return f""Website did not contain any text.\n\nLinks: {links}""
            elif get_raw_content:
                if (
                    output_tokens := self.llm_provider.count_tokens(
                        text, self.config.llm_name
                    )
                ) > MAX_RAW_CONTENT_LENGTH:
                    oversize_factor = round(output_tokens / MAX_RAW_CONTENT_LENGTH, 1)
                    raise TooMuchOutputError(
                        f""Page content is {oversize_factor}x the allowed length ""
                        ""for `get_raw_content=true`""
                    )
                return text + (f""\n\nLinks: {links}"" if links else """")
            else:
                text = await self.summarize_webpage(
                    text, question or None, topics_of_interest
                )
                return_literal_content = bool(question)
                summarized = True

            # Limit links to LINKS_TO_RETURN
            if len(links) > LINKS_TO_RETURN:
                links = links[:LINKS_TO_RETURN]

            text_fmt = f""'''{text}'''"" if ""\n"" in text else f""'{text}'""
            links_fmt = ""\n"".join(f""- {link}"" for link in links)
            return (
                f""Page content{' (summary)' if summarized else ''}:""
                if return_literal_content
                else ""Answer gathered from webpage:""
            ) + f"" {text_fmt}\n\nLinks:\n{links_fmt}""

        except WebDriverException as e:
            # These errors are often quite long and include lots of context.
            # Just grab the first line.
            msg = e.msg.split(""\n"")[0] if e.msg else str(e)
            if ""net::"" in msg:
                raise BrowsingError(
                    ""A networking error occurred while trying to load the page: %s""
                    % re.sub(r""^unknown error: "", """", msg)
                )
            raise CommandExecutionError(msg)
        finally:
            if driver:
                driver.close()","Point(row=137, column=4)","Point(row=208, column=30)",WebSeleniumComponent,classic/forge/forge/components/web/selenium.py
WebSeleniumComponent.scrape_text_with_selenium,function,"Scrape text from a browser window using selenium

        Args:
            driver (WebDriver): A driver object representing
            the browser window to scrape

        Returns:
            str: the text scraped from the website
","def scrape_text_with_selenium(self, driver: WebDriver) -> str:
        """"""Scrape text from a browser window using selenium

        Args:
            driver (WebDriver): A driver object representing
            the browser window to scrape

        Returns:
            str: the text scraped from the website
        """"""

        # Get the HTML content directly from the browser's DOM
        page_source = driver.execute_script(""return document.body.outerHTML;"")
        soup = BeautifulSoup(page_source, ""html.parser"")

        for script in soup([""script"", ""style""]):
            script.extract()

        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split(""  ""))
        text = ""\n"".join(chunk for chunk in chunks if chunk)
        return text","Point(row=210, column=4)","Point(row=232, column=19)",WebSeleniumComponent,classic/forge/forge/components/web/selenium.py
WebSeleniumComponent.scrape_links_with_selenium,function,"Scrape links from a website using selenium

        Args:
            driver (WebDriver): A driver object representing
            the browser window to scrape
            base_url (str): The base URL to use for resolving relative links

        Returns:
            List[str]: The links scraped from the website
","def scrape_links_with_selenium(self, driver: WebDriver, base_url: str) -> list[str]:
        """"""Scrape links from a website using selenium

        Args:
            driver (WebDriver): A driver object representing
            the browser window to scrape
            base_url (str): The base URL to use for resolving relative links

        Returns:
            List[str]: The links scraped from the website
        """"""
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, ""html.parser"")

        for script in soup([""script"", ""style""]):
            script.extract()

        hyperlinks = extract_hyperlinks(soup, base_url)

        return format_hyperlinks(hyperlinks)","Point(row=234, column=4)","Point(row=253, column=44)",WebSeleniumComponent,classic/forge/forge/components/web/selenium.py
WebSeleniumComponent.open_page_in_browser,function,"Open a browser window and load a web page using Selenium

        Params:
            url (str): The URL of the page to load
            config (Config): The applicable application configuration

        Returns:
            driver (WebDriver): A driver object representing
            the browser window to scrape
","async def open_page_in_browser(self, url: str) -> WebDriver:
        """"""Open a browser window and load a web page using Selenium

        Params:
            url (str): The URL of the page to load
            config (Config): The applicable application configuration

        Returns:
            driver (WebDriver): A driver object representing
            the browser window to scrape
        """"""
        logging.getLogger(""selenium"").setLevel(logging.CRITICAL)

        options_available: dict[str, Type[BrowserOptions]] = {
            ""chrome"": ChromeOptions,
            ""edge"": EdgeOptions,
            ""firefox"": FirefoxOptions,
            ""safari"": SafariOptions,
        }

        options: BrowserOptions = options_available[self.config.web_browser]()
        options.add_argument(f""user-agent={self.config.user_agent}"")

        if isinstance(options, FirefoxOptions):
            if self.config.headless:
                options.headless = True  # type: ignore
                options.add_argument(""--disable-gpu"")
            driver = FirefoxDriver(
                service=GeckoDriverService(GeckoDriverManager().install()),
                options=options,
            )
        elif isinstance(options, EdgeOptions):
            driver = EdgeDriver(
                service=EdgeDriverService(EdgeDriverManager().install()),
                options=options,
            )
        elif isinstance(options, SafariOptions):
            # Requires a bit more setup on the users end.
            # See https://developer.apple.com/documentation/webkit/testing_with_webdriver_in_safari  # noqa: E501
            driver = SafariDriver(options=options)
        elif isinstance(options, ChromeOptions):
            if platform == ""linux"" or platform == ""linux2"":
                options.add_argument(""--disable-dev-shm-usage"")
                options.add_argument(""--remote-debugging-port=9222"")

            options.add_argument(""--no-sandbox"")
            if self.config.headless:
                options.add_argument(""--headless=new"")
                options.add_argument(""--disable-gpu"")

            if self.config.selenium_proxy:
                options.add_argument(f""--proxy-server={self.config.selenium_proxy}"")

            self._sideload_chrome_extensions(options, self.data_dir / ""assets"" / ""crx"")

            if (chromium_driver_path := Path(""/usr/bin/chromedriver"")).exists():
                chrome_service = ChromeDriverService(str(chromium_driver_path))
            else:
                try:
                    chrome_driver = ChromeDriverManager().install()
                except AttributeError as e:
                    if ""'NoneType' object has no attribute 'split'"" in str(e):
                        # https://github.com/SergeyPirogov/webdriver_manager/issues/649
                        logger.critical(
                            ""Connecting to browser failed:""
                            "" is Chrome or Chromium installed?""
                        )
                    raise
                chrome_service = ChromeDriverService(chrome_driver)
            driver = ChromeDriver(service=chrome_service, options=options)

        driver.get(url)

        # Wait for page to be ready, sleep 2 seconds, wait again until page ready.
        # This allows the cookiewall squasher time to get rid of cookie walls.
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, ""body""))
        )
        await asyncio.sleep(2)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, ""body""))
        )

        return driver","Point(row=255, column=4)","Point(row=338, column=21)",WebSeleniumComponent,classic/forge/forge/components/web/selenium.py
WebSeleniumComponent._sideload_chrome_extensions,function,,"def _sideload_chrome_extensions(
        self, options: ChromeOptions, dl_folder: Path
    ) -> None:
        crx_download_url_template = ""https://clients2.google.com/service/update2/crx?response=redirect&prodversion=49.0&acceptformat=crx3&x=id%3D{crx_id}%26installsource%3Dondemand%26uc""  # noqa
        cookiewall_squasher_crx_id = ""edibdbjcniadpccecjdfdjjppcpchdlm""
        adblocker_crx_id = ""cjpalhdlnbpafiamejdnhcphjbkeiagm""

        # Make sure the target folder exists
        dl_folder.mkdir(parents=True, exist_ok=True)

        for crx_id in (cookiewall_squasher_crx_id, adblocker_crx_id):
            crx_path = dl_folder / f""{crx_id}.crx""
            if not crx_path.exists():
                logger.debug(f""Downloading CRX {crx_id}..."")
                crx_download_url = crx_download_url_template.format(crx_id=crx_id)
                urlretrieve(crx_download_url, crx_path)
                logger.debug(f""Downloaded {crx_path.name}"")
            options.add_extension(str(crx_path))","Point(row=340, column=4)","Point(row=357, column=48)",WebSeleniumComponent,classic/forge/forge/components/web/selenium.py
WebSeleniumComponent.summarize_webpage,function,"Summarize text using the OpenAI API

        Args:
            url (str): The url of the text
            text (str): The text to summarize
            question (str): The question to ask the model
            driver (WebDriver): The webdriver to use to scroll the page

        Returns:
            str: The summary of the text
","async def summarize_webpage(
        self,
        text: str,
        question: str | None,
        topics_of_interest: list[str],
    ) -> str:
        """"""Summarize text using the OpenAI API

        Args:
            url (str): The url of the text
            text (str): The text to summarize
            question (str): The question to ask the model
            driver (WebDriver): The webdriver to use to scroll the page

        Returns:
            str: The summary of the text
        """"""
        if not text:
            raise ValueError(""No text to summarize"")

        text_length = len(text)
        logger.debug(f""Web page content length: {text_length} characters"")

        result = None
        information = None
        if topics_of_interest:
            information = await extract_information(
                text,
                topics_of_interest=topics_of_interest,
                llm_provider=self.llm_provider,
                model_name=self.config.llm_name,
                spacy_model=self.config.browse_spacy_language_model,
            )
            return ""\n"".join(f""* {i}"" for i in information)
        else:
            result, _ = await summarize_text(
                text,
                question=question,
                llm_provider=self.llm_provider,
                model_name=self.config.llm_name,
                spacy_model=self.config.browse_spacy_language_model,
            )
            return result","Point(row=359, column=4)","Point(row=401, column=25)",WebSeleniumComponent,classic/forge/forge/components/web/selenium.py
WebSearchConfiguration,class,,"class WebSearchConfiguration(BaseModel):
    google_api_key: Optional[SecretStr] = UserConfigurable(
        None, from_env=""GOOGLE_API_KEY"", exclude=True
    )
    google_custom_search_engine_id: Optional[SecretStr] = UserConfigurable(
        None, from_env=""GOOGLE_CUSTOM_SEARCH_ENGINE_ID"", exclude=True
    )
    duckduckgo_max_attempts: int = 3
    duckduckgo_backend: Literal[""api"", ""html"", ""lite""] = ""api""","Point(row=18, column=0)","Point(row=26, column=62)",,classic/forge/forge/components/web/search.py
WebSearchComponent,class,Provides commands to search the web.,"class WebSearchComponent(
    DirectiveProvider, CommandProvider, ConfigurableComponent[WebSearchConfiguration]
):
    """"""Provides commands to search the web.""""""

    config_class = WebSearchConfiguration

    def __init__(self, config: Optional[WebSearchConfiguration] = None):
        ConfigurableComponent.__init__(self, config)

        if (
            not self.config.google_api_key
            or not self.config.google_custom_search_engine_id
        ):
            logger.info(
                ""Configure google_api_key and custom_search_engine_id ""
                ""to use Google API search.""
            )

    def get_resources(self) -> Iterator[str]:
        yield ""Internet access for searches and information gathering.""

    def get_commands(self) -> Iterator[Command]:
        yield self.web_search

        if self.config.google_api_key and self.config.google_custom_search_engine_id:
            yield self.google

    @command(
        [""web_search"", ""search""],
        ""Searches the web"",
        {
            ""query"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The search query"",
                required=True,
            ),
            ""num_results"": JSONSchema(
                type=JSONSchema.Type.INTEGER,
                description=""The number of results to return"",
                minimum=1,
                maximum=10,
                required=False,
            ),
        },
    )
    def web_search(self, query: str, num_results: int = 8) -> str:
        """"""Return the results of a Google search

        Args:
            query (str): The search query.
            num_results (int): The number of results to return.

        Returns:
            str: The results of the search.
        """"""
        search_results = []
        attempts = 0

        while attempts < self.config.duckduckgo_max_attempts:
            if not query:
                return json.dumps(search_results)

            search_results = DDGS().text(
                query, max_results=num_results, backend=self.config.duckduckgo_backend
            )

            if search_results:
                break

            time.sleep(1)
            attempts += 1

        search_results = [
            {
                ""title"": r[""title""],
                ""url"": r[""href""],
                **({""exerpt"": r[""body""]} if r.get(""body"") else {}),
            }
            for r in search_results
        ]

        results = (""## Search results\n"") + ""\n\n"".join(
            f""### \""{r['title']}\""\n""
            f""**URL:** {r['url']}  \n""
            ""**Excerpt:** "" + (f'""{exerpt}""' if (exerpt := r.get(""exerpt"")) else ""N/A"")
            for r in search_results
        )
        return self.safe_google_results(results)

    @command(
        [""google""],
        ""Google Search"",
        {
            ""query"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The search query"",
                required=True,
            ),
            ""num_results"": JSONSchema(
                type=JSONSchema.Type.INTEGER,
                description=""The number of results to return"",
                minimum=1,
                maximum=10,
                required=False,
            ),
        },
    )
    def google(self, query: str, num_results: int = 8) -> str | list[str]:
        """"""Return the results of a Google search using the official Google API

        Args:
            query (str): The search query.
            num_results (int): The number of results to return.

        Returns:
            str: The results of the search.
        """"""

        from googleapiclient.discovery import build
        from googleapiclient.errors import HttpError

        try:
            # Should be the case if this command is enabled:
            assert self.config.google_api_key
            assert self.config.google_custom_search_engine_id

            # Initialize the Custom Search API service
            service = build(
                ""customsearch"",
                ""v1"",
                developerKey=self.config.google_api_key.get_secret_value(),
            )

            # Send the search query and retrieve the results
            result = (
                service.cse()
                .list(
                    q=query,
                    cx=self.config.google_custom_search_engine_id.get_secret_value(),
                    num=num_results,
                )
                .execute()
            )

            # Extract the search result items from the response
            search_results = result.get(""items"", [])

            # Create a list of only the URLs from the search results
            search_results_links = [item[""link""] for item in search_results]  # type: ignore # noqa

        except HttpError as e:
            # Handle errors in the API call
            error_details = json.loads(e.content.decode())

            # Check if the error is related to an invalid or missing API key
            if error_details.get(""error"", {}).get(
                ""code""
            ) == 403 and ""invalid API key"" in error_details.get(""error"", {}).get(
                ""message"", """"
            ):
                raise ConfigurationError(
                    ""The provided Google API key is invalid or missing.""
                )
            raise
        # google_result can be a list or a string depending on the search results

        # Return the list of search result URLs
        return self.safe_google_results(search_results_links)

    def safe_google_results(self, results: str | list) -> str:
        """"""
            Return the results of a Google search in a safe format.

        Args:
            results (str | list): The search results.

        Returns:
            str: The results of the search.
        """"""
        if isinstance(results, list):
            safe_message = json.dumps(
                [result.encode(""utf-8"", ""ignore"").decode(""utf-8"") for result in results]
            )
        else:
            safe_message = results.encode(""utf-8"", ""ignore"").decode(""utf-8"")
        return safe_message","Point(row=29, column=0)","Point(row=215, column=27)",,classic/forge/forge/components/web/search.py
WebSearchComponent.__init__,function,,"def __init__(self, config: Optional[WebSearchConfiguration] = None):
        ConfigurableComponent.__init__(self, config)

        if (
            not self.config.google_api_key
            or not self.config.google_custom_search_engine_id
        ):
            logger.info(
                ""Configure google_api_key and custom_search_engine_id ""
                ""to use Google API search.""
            )","Point(row=36, column=4)","Point(row=46, column=13)",WebSearchComponent,classic/forge/forge/components/web/search.py
WebSearchComponent.get_resources,function,,"def get_resources(self) -> Iterator[str]:
        yield ""Internet access for searches and information gathering.""","Point(row=48, column=4)","Point(row=49, column=71)",WebSearchComponent,classic/forge/forge/components/web/search.py
WebSearchComponent.get_commands,function,,"def get_commands(self) -> Iterator[Command]:
        yield self.web_search

        if self.config.google_api_key and self.config.google_custom_search_engine_id:
            yield self.google","Point(row=51, column=4)","Point(row=55, column=29)",WebSearchComponent,classic/forge/forge/components/web/search.py
WebSearchComponent.web_search,function,"Return the results of a Google search

        Args:
            query (str): The search query.
            num_results (int): The number of results to return.

        Returns:
            str: The results of the search.
","def web_search(self, query: str, num_results: int = 8) -> str:
        """"""Return the results of a Google search

        Args:
            query (str): The search query.
            num_results (int): The number of results to return.

        Returns:
            str: The results of the search.
        """"""
        search_results = []
        attempts = 0

        while attempts < self.config.duckduckgo_max_attempts:
            if not query:
                return json.dumps(search_results)

            search_results = DDGS().text(
                query, max_results=num_results, backend=self.config.duckduckgo_backend
            )

            if search_results:
                break

            time.sleep(1)
            attempts += 1

        search_results = [
            {
                ""title"": r[""title""],
                ""url"": r[""href""],
                **({""exerpt"": r[""body""]} if r.get(""body"") else {}),
            }
            for r in search_results
        ]

        results = (""## Search results\n"") + ""\n\n"".join(
            f""### \""{r['title']}\""\n""
            f""**URL:** {r['url']}  \n""
            ""**Excerpt:** "" + (f'""{exerpt}""' if (exerpt := r.get(""exerpt"")) else ""N/A"")
            for r in search_results
        )
        return self.safe_google_results(results)","Point(row=75, column=4)","Point(row=117, column=48)",WebSearchComponent,classic/forge/forge/components/web/search.py
WebSearchComponent.google,function,"Return the results of a Google search using the official Google API

        Args:
            query (str): The search query.
            num_results (int): The number of results to return.

        Returns:
            str: The results of the search.
","def google(self, query: str, num_results: int = 8) -> str | list[str]:
        """"""Return the results of a Google search using the official Google API

        Args:
            query (str): The search query.
            num_results (int): The number of results to return.

        Returns:
            str: The results of the search.
        """"""

        from googleapiclient.discovery import build
        from googleapiclient.errors import HttpError

        try:
            # Should be the case if this command is enabled:
            assert self.config.google_api_key
            assert self.config.google_custom_search_engine_id

            # Initialize the Custom Search API service
            service = build(
                ""customsearch"",
                ""v1"",
                developerKey=self.config.google_api_key.get_secret_value(),
            )

            # Send the search query and retrieve the results
            result = (
                service.cse()
                .list(
                    q=query,
                    cx=self.config.google_custom_search_engine_id.get_secret_value(),
                    num=num_results,
                )
                .execute()
            )

            # Extract the search result items from the response
            search_results = result.get(""items"", [])

            # Create a list of only the URLs from the search results
            search_results_links = [item[""link""] for item in search_results]  # type: ignore # noqa

        except HttpError as e:
            # Handle errors in the API call
            error_details = json.loads(e.content.decode())

            # Check if the error is related to an invalid or missing API key
            if error_details.get(""error"", {}).get(
                ""code""
            ) == 403 and ""invalid API key"" in error_details.get(""error"", {}).get(
                ""message"", """"
            ):
                raise ConfigurationError(
                    ""The provided Google API key is invalid or missing.""
                )
            raise
        # google_result can be a list or a string depending on the search results

        # Return the list of search result URLs
        return self.safe_google_results(search_results_links)","Point(row=137, column=4)","Point(row=197, column=61)",WebSearchComponent,classic/forge/forge/components/web/search.py
WebSearchComponent.safe_google_results,function,"
            Return the results of a Google search in a safe format.

        Args:
            results (str | list): The search results.

        Returns:
            str: The results of the search.
","def safe_google_results(self, results: str | list) -> str:
        """"""
            Return the results of a Google search in a safe format.

        Args:
            results (str | list): The search results.

        Returns:
            str: The results of the search.
        """"""
        if isinstance(results, list):
            safe_message = json.dumps(
                [result.encode(""utf-8"", ""ignore"").decode(""utf-8"") for result in results]
            )
        else:
            safe_message = results.encode(""utf-8"", ""ignore"").decode(""utf-8"")
        return safe_message","Point(row=199, column=4)","Point(row=215, column=27)",WebSearchComponent,classic/forge/forge/components/web/search.py
UserInteractionComponent,class,Provides commands to interact with the user.,"class UserInteractionComponent(CommandProvider):
    """"""Provides commands to interact with the user.""""""

    def get_commands(self) -> Iterator[Command]:
        yield self.ask_user

    @command(
        names=[ASK_COMMAND],
        parameters={
            ""question"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The question or prompt to the user"",
                required=True,
            )
        },
    )
    def ask_user(self, question: str) -> str:
        """"""If you need more details or information regarding the given goals,
        you can ask the user for input.""""""
        print(f""\nQ: {question}"")
        resp = click.prompt(""A"")
        return f""The user's answer: '{resp}'""","Point(row=10, column=0)","Point(row=31, column=45)",,classic/forge/forge/components/user_interaction/user_interaction.py
UserInteractionComponent.get_commands,function,,"def get_commands(self) -> Iterator[Command]:
        yield self.ask_user","Point(row=13, column=4)","Point(row=14, column=27)",UserInteractionComponent,classic/forge/forge/components/user_interaction/user_interaction.py
UserInteractionComponent.ask_user,function,"If you need more details or information regarding the given goals,
        you can ask the user for input.","def ask_user(self, question: str) -> str:
        """"""If you need more details or information regarding the given goals,
        you can ask the user for input.""""""
        print(f""\nQ: {question}"")
        resp = click.prompt(""A"")
        return f""The user's answer: '{resp}'""","Point(row=26, column=4)","Point(row=31, column=45)",UserInteractionComponent,classic/forge/forge/components/user_interaction/user_interaction.py
ActionHistoryConfiguration,class,,"class ActionHistoryConfiguration(BaseModel):
    llm_name: ModelName = OpenAIModelName.GPT3
    """"""Name of the llm model used to compress the history""""""
    max_tokens: int = 1024
    """"""Maximum number of tokens to use up with generated history messages""""""
    spacy_language_model: str = ""en_core_web_sm""
    """"""Language model used for summary chunking using spacy""""""
    full_message_count: int = 4
    """"""Number of latest non-summarized messages to include in the history""""""","Point(row=17, column=0)","Point(row=25, column=76)",,classic/forge/forge/components/action_history/action_history.py
ActionHistoryComponent,class,Keeps track of the event history and provides a summary of the steps.,"class ActionHistoryComponent(
    MessageProvider,
    AfterParse[AnyProposal],
    AfterExecute,
    ConfigurableComponent[ActionHistoryConfiguration],
):
    """"""Keeps track of the event history and provides a summary of the steps.""""""

    config_class = ActionHistoryConfiguration

    def __init__(
        self,
        event_history: EpisodicActionHistory[AnyProposal],
        count_tokens: Callable[[str], int],
        llm_provider: MultiProvider,
        config: Optional[ActionHistoryConfiguration] = None,
    ) -> None:
        ConfigurableComponent.__init__(self, config)
        self.event_history = event_history
        self.count_tokens = count_tokens
        self.llm_provider = llm_provider

    def get_messages(self) -> Iterator[ChatMessage]:
        messages: list[ChatMessage] = []
        step_summaries: list[str] = []
        tokens: int = 0
        n_episodes = len(self.event_history.episodes)

        # Include a summary for all except a few latest steps
        for i, episode in enumerate(reversed(self.event_history.episodes)):
            # Use full format for a few steps, summary or format for older steps
            if i < self.config.full_message_count:
                messages.insert(0, episode.action.raw_message)
                tokens += self.count_tokens(str(messages[0]))  # HACK
                if episode.result:
                    result_message = self._make_result_message(episode, episode.result)
                    messages.insert(1, result_message)
                    tokens += self.count_tokens(str(result_message))  # HACK
                continue
            elif episode.summary is None:
                step_content = indent(episode.format(), 2).strip()
            else:
                step_content = episode.summary

            step = f""* Step {n_episodes - i}: {step_content}""

            if self.config.max_tokens and self.count_tokens:
                step_tokens = self.count_tokens(step)
                if tokens + step_tokens > self.config.max_tokens:
                    break
                tokens += step_tokens

            step_summaries.insert(0, step)

        if step_summaries:
            step_summaries_fmt = ""\n\n"".join(step_summaries)
            yield ChatMessage.system(
                f""## Progress on your Task so far\n""
                ""Here is a summary of the steps that you have executed so far, ""
                ""use this as your consideration for determining the next action!\n""
                f""{step_summaries_fmt}""
            )

        yield from messages

    def after_parse(self, result: AnyProposal) -> None:
        self.event_history.register_action(result)

    async def after_execute(self, result: ActionResult) -> None:
        self.event_history.register_result(result)
        await self.event_history.handle_compression(
            self.llm_provider, self.config.llm_name, self.config.spacy_language_model
        )

    @staticmethod
    def _make_result_message(episode: Episode, result: ActionResult) -> ChatMessage:
        if result.status == ""success"":
            return (
                ToolResultMessage(
                    content=str(result.outputs),
                    tool_call_id=episode.action.raw_message.tool_calls[0].id,
                )
                if episode.action.raw_message.tool_calls
                else ChatMessage.user(
                    f""{episode.action.use_tool.name} returned: ""
                    + (
                        f""```\n{result.outputs}\n```""
                        if ""\n"" in str(result.outputs)
                        else f""`{result.outputs}`""
                    )
                )
            )
        elif result.status == ""error"":
            return (
                ToolResultMessage(
                    content=f""{result.reason}\n\n{result.error or ''}"".strip(),
                    is_error=True,
                    tool_call_id=episode.action.raw_message.tool_calls[0].id,
                )
                if episode.action.raw_message.tool_calls
                else ChatMessage.user(
                    f""{episode.action.use_tool.name} raised an error: ```\n""
                    f""{result.reason}\n""
                    ""```""
                )
            )
        else:
            return ChatMessage.user(result.feedback)

    def _compile_progress(
        self,
        episode_history: list[Episode[AnyProposal]],
        max_tokens: Optional[int] = None,
        count_tokens: Optional[Callable[[str], int]] = None,
    ) -> str:
        if max_tokens and not count_tokens:
            raise ValueError(""count_tokens is required if max_tokens is set"")

        steps: list[str] = []
        tokens: int = 0
        n_episodes = len(episode_history)

        for i, episode in enumerate(reversed(episode_history)):
            # Use full format for a few latest steps, summary or format for older steps
            if i < self.config.full_message_count or episode.summary is None:
                step_content = indent(episode.format(), 2).strip()
            else:
                step_content = episode.summary

            step = f""* Step {n_episodes - i}: {step_content}""

            if max_tokens and count_tokens:
                step_tokens = count_tokens(step)
                if tokens + step_tokens > max_tokens:
                    break
                tokens += step_tokens

            steps.insert(0, step)

        return ""\n\n"".join(steps)","Point(row=28, column=0)","Point(row=167, column=33)",,classic/forge/forge/components/action_history/action_history.py
ActionHistoryComponent.__init__,function,,"def __init__(
        self,
        event_history: EpisodicActionHistory[AnyProposal],
        count_tokens: Callable[[str], int],
        llm_provider: MultiProvider,
        config: Optional[ActionHistoryConfiguration] = None,
    ) -> None:
        ConfigurableComponent.__init__(self, config)
        self.event_history = event_history
        self.count_tokens = count_tokens
        self.llm_provider = llm_provider","Point(row=38, column=4)","Point(row=48, column=40)",ActionHistoryComponent,classic/forge/forge/components/action_history/action_history.py
ActionHistoryComponent.get_messages,function,,"def get_messages(self) -> Iterator[ChatMessage]:
        messages: list[ChatMessage] = []
        step_summaries: list[str] = []
        tokens: int = 0
        n_episodes = len(self.event_history.episodes)

        # Include a summary for all except a few latest steps
        for i, episode in enumerate(reversed(self.event_history.episodes)):
            # Use full format for a few steps, summary or format for older steps
            if i < self.config.full_message_count:
                messages.insert(0, episode.action.raw_message)
                tokens += self.count_tokens(str(messages[0]))  # HACK
                if episode.result:
                    result_message = self._make_result_message(episode, episode.result)
                    messages.insert(1, result_message)
                    tokens += self.count_tokens(str(result_message))  # HACK
                continue
            elif episode.summary is None:
                step_content = indent(episode.format(), 2).strip()
            else:
                step_content = episode.summary

            step = f""* Step {n_episodes - i}: {step_content}""

            if self.config.max_tokens and self.count_tokens:
                step_tokens = self.count_tokens(step)
                if tokens + step_tokens > self.config.max_tokens:
                    break
                tokens += step_tokens

            step_summaries.insert(0, step)

        if step_summaries:
            step_summaries_fmt = ""\n\n"".join(step_summaries)
            yield ChatMessage.system(
                f""## Progress on your Task so far\n""
                ""Here is a summary of the steps that you have executed so far, ""
                ""use this as your consideration for determining the next action!\n""
                f""{step_summaries_fmt}""
            )

        yield from messages","Point(row=50, column=4)","Point(row=91, column=27)",ActionHistoryComponent,classic/forge/forge/components/action_history/action_history.py
ActionHistoryComponent.after_parse,function,,"def after_parse(self, result: AnyProposal) -> None:
        self.event_history.register_action(result)","Point(row=93, column=4)","Point(row=94, column=50)",ActionHistoryComponent,classic/forge/forge/components/action_history/action_history.py
ActionHistoryComponent.after_execute,function,,"async def after_execute(self, result: ActionResult) -> None:
        self.event_history.register_result(result)
        await self.event_history.handle_compression(
            self.llm_provider, self.config.llm_name, self.config.spacy_language_model
        )","Point(row=96, column=4)","Point(row=100, column=9)",ActionHistoryComponent,classic/forge/forge/components/action_history/action_history.py
ActionHistoryComponent._make_result_message,function,,"def _make_result_message(episode: Episode, result: ActionResult) -> ChatMessage:
        if result.status == ""success"":
            return (
                ToolResultMessage(
                    content=str(result.outputs),
                    tool_call_id=episode.action.raw_message.tool_calls[0].id,
                )
                if episode.action.raw_message.tool_calls
                else ChatMessage.user(
                    f""{episode.action.use_tool.name} returned: ""
                    + (
                        f""```\n{result.outputs}\n```""
                        if ""\n"" in str(result.outputs)
                        else f""`{result.outputs}`""
                    )
                )
            )
        elif result.status == ""error"":
            return (
                ToolResultMessage(
                    content=f""{result.reason}\n\n{result.error or ''}"".strip(),
                    is_error=True,
                    tool_call_id=episode.action.raw_message.tool_calls[0].id,
                )
                if episode.action.raw_message.tool_calls
                else ChatMessage.user(
                    f""{episode.action.use_tool.name} raised an error: ```\n""
                    f""{result.reason}\n""
                    ""```""
                )
            )
        else:
            return ChatMessage.user(result.feedback)","Point(row=103, column=4)","Point(row=135, column=52)",ActionHistoryComponent,classic/forge/forge/components/action_history/action_history.py
ActionHistoryComponent._compile_progress,function,,"def _compile_progress(
        self,
        episode_history: list[Episode[AnyProposal]],
        max_tokens: Optional[int] = None,
        count_tokens: Optional[Callable[[str], int]] = None,
    ) -> str:
        if max_tokens and not count_tokens:
            raise ValueError(""count_tokens is required if max_tokens is set"")

        steps: list[str] = []
        tokens: int = 0
        n_episodes = len(episode_history)

        for i, episode in enumerate(reversed(episode_history)):
            # Use full format for a few latest steps, summary or format for older steps
            if i < self.config.full_message_count or episode.summary is None:
                step_content = indent(episode.format(), 2).strip()
            else:
                step_content = episode.summary

            step = f""* Step {n_episodes - i}: {step_content}""

            if max_tokens and count_tokens:
                step_tokens = count_tokens(step)
                if tokens + step_tokens > max_tokens:
                    break
                tokens += step_tokens

            steps.insert(0, step)

        return ""\n\n"".join(steps)","Point(row=137, column=4)","Point(row=167, column=33)",ActionHistoryComponent,classic/forge/forge/components/action_history/action_history.py
Episode,class,,"class Episode(BaseModel, Generic[AnyProposal]):
    action: AnyProposal
    result: ActionResult | None
    summary: str | None = None

    def format(self):
        step = f""Executed `{self.action.use_tool}`\n""
        reasoning = (
            _r.summary()
            if isinstance(_r := self.action.thoughts, ModelWithSummary)
            else _r
        )
        step += f'- **Reasoning:** ""{reasoning}""\n'
        step += (
            ""- **Status:** ""
            f""`{self.result.status if self.result else 'did_not_finish'}`\n""
        )
        if self.result:
            if self.result.status == ""success"":
                result = str(self.result)
                result = ""\n"" + indent(result) if ""\n"" in result else result
                step += f""- **Output:** {result}""
            elif self.result.status == ""error"":
                step += f""- **Reason:** {self.result.reason}\n""
                if self.result.error:
                    step += f""- **Error:** {self.result.error}\n""
            elif self.result.status == ""interrupted_by_human"":
                step += f""- **Feedback:** {self.result.feedback}\n""
        return step

    def __str__(self) -> str:
        executed_action = f""Executed `{self.action.use_tool}`""
        action_result = f"": {self.result}"" if self.result else "".""
        return executed_action + action_result","Point(row=17, column=0)","Point(row=50, column=46)",,classic/forge/forge/components/action_history/model.py
Episode.format,function,,"def format(self):
        step = f""Executed `{self.action.use_tool}`\n""
        reasoning = (
            _r.summary()
            if isinstance(_r := self.action.thoughts, ModelWithSummary)
            else _r
        )
        step += f'- **Reasoning:** ""{reasoning}""\n'
        step += (
            ""- **Status:** ""
            f""`{self.result.status if self.result else 'did_not_finish'}`\n""
        )
        if self.result:
            if self.result.status == ""success"":
                result = str(self.result)
                result = ""\n"" + indent(result) if ""\n"" in result else result
                step += f""- **Output:** {result}""
            elif self.result.status == ""error"":
                step += f""- **Reason:** {self.result.reason}\n""
                if self.result.error:
                    step += f""- **Error:** {self.result.error}\n""
            elif self.result.status == ""interrupted_by_human"":
                step += f""- **Feedback:** {self.result.feedback}\n""
        return step","Point(row=22, column=4)","Point(row=45, column=19)",Episode,classic/forge/forge/components/action_history/model.py
Episode.__str__,function,,"def __str__(self) -> str:
        executed_action = f""Executed `{self.action.use_tool}`""
        action_result = f"": {self.result}"" if self.result else "".""
        return executed_action + action_result","Point(row=47, column=4)","Point(row=50, column=46)",Episode,classic/forge/forge/components/action_history/model.py
EpisodicActionHistory,class,Utility container for an action history,"class EpisodicActionHistory(BaseModel, Generic[AnyProposal]):
    """"""Utility container for an action history""""""

    episodes: list[Episode[AnyProposal]] = Field(default_factory=list)
    cursor: int = 0
    _lock = asyncio.Lock()

    @property
    def current_episode(self) -> Episode[AnyProposal] | None:
        if self.cursor == len(self):
            return None
        return self[self.cursor]

    def __getitem__(self, key: int) -> Episode[AnyProposal]:
        return self.episodes[key]

    def __len__(self) -> int:
        return len(self.episodes)

    def __bool__(self) -> bool:
        return len(self.episodes) > 0

    def register_action(self, action: AnyProposal) -> None:
        if not self.current_episode:
            self.episodes.append(Episode(action=action, result=None))
            assert self.current_episode
        elif self.current_episode.action:
            raise ValueError(""Action for current cycle already set"")

    def register_result(self, result: ActionResult) -> None:
        if not self.current_episode:
            raise RuntimeError(""Cannot register result for cycle without action"")
        elif self.current_episode.result:
            raise ValueError(""Result for current cycle already set"")

        self.current_episode.result = result
        self.cursor = len(self.episodes)

    def rewind(self, number_of_episodes: int = 0) -> None:
        """"""Resets the history to an earlier state.

        Params:
            number_of_cycles (int): The number of cycles to rewind. Default is 0.
                When set to 0, it will only reset the current cycle.
        """"""
        # Remove partial record of current cycle
        if self.current_episode:
            if self.current_episode.action and not self.current_episode.result:
                self.episodes.pop(self.cursor)

        # Rewind the specified number of cycles
        if number_of_episodes > 0:
            self.episodes = self.episodes[:-number_of_episodes]
            self.cursor = len(self.episodes)

    async def handle_compression(
        self,
        llm_provider: MultiProvider,
        model_name: ModelName,
        spacy_model: str,
    ) -> None:
        """"""Compresses each episode in the action history using an LLM.

        This method iterates over all episodes in the action history without a summary,
        and generates a summary for them using an LLM.
        """"""
        compress_instruction = (
            ""The text represents an action, the reason for its execution, ""
            ""and its result. ""
            ""Condense the action taken and its result into one line. ""
            ""Preserve any specific factual information gathered by the action.""
        )
        async with self._lock:
            # Gather all episodes without a summary
            episodes_to_summarize = [ep for ep in self.episodes if ep.summary is None]

            # Parallelize summarization calls
            summarize_coroutines = [
                summarize_text(
                    episode.format(),
                    instruction=compress_instruction,
                    llm_provider=llm_provider,
                    model_name=model_name,
                    spacy_model=spacy_model,
                )
                for episode in episodes_to_summarize
            ]
            summaries = await asyncio.gather(*summarize_coroutines)

            # Assign summaries to episodes
            for episode, (summary, _) in zip(episodes_to_summarize, summaries):
                episode.summary = summary

    def fmt_list(self) -> str:
        return format_numbered_list(self.episodes)

    def fmt_paragraph(self) -> str:
        steps: list[str] = []

        for i, episode in enumerate(self.episodes, 1):
            step = f""### Step {i}: {episode.format()}\n""

            steps.append(step)

        return ""\n\n"".join(steps)","Point(row=53, column=0)","Point(row=157, column=33)",,classic/forge/forge/components/action_history/model.py
EpisodicActionHistory.current_episode,function,,"def current_episode(self) -> Episode[AnyProposal] | None:
        if self.cursor == len(self):
            return None
        return self[self.cursor]","Point(row=61, column=4)","Point(row=64, column=32)",EpisodicActionHistory,classic/forge/forge/components/action_history/model.py
EpisodicActionHistory.__getitem__,function,,"def __getitem__(self, key: int) -> Episode[AnyProposal]:
        return self.episodes[key]","Point(row=66, column=4)","Point(row=67, column=33)",EpisodicActionHistory,classic/forge/forge/components/action_history/model.py
EpisodicActionHistory.__len__,function,,"def __len__(self) -> int:
        return len(self.episodes)","Point(row=69, column=4)","Point(row=70, column=33)",EpisodicActionHistory,classic/forge/forge/components/action_history/model.py
EpisodicActionHistory.__bool__,function,,"def __bool__(self) -> bool:
        return len(self.episodes) > 0","Point(row=72, column=4)","Point(row=73, column=37)",EpisodicActionHistory,classic/forge/forge/components/action_history/model.py
EpisodicActionHistory.register_action,function,,"def register_action(self, action: AnyProposal) -> None:
        if not self.current_episode:
            self.episodes.append(Episode(action=action, result=None))
            assert self.current_episode
        elif self.current_episode.action:
            raise ValueError(""Action for current cycle already set"")","Point(row=75, column=4)","Point(row=80, column=68)",EpisodicActionHistory,classic/forge/forge/components/action_history/model.py
EpisodicActionHistory.register_result,function,,"def register_result(self, result: ActionResult) -> None:
        if not self.current_episode:
            raise RuntimeError(""Cannot register result for cycle without action"")
        elif self.current_episode.result:
            raise ValueError(""Result for current cycle already set"")

        self.current_episode.result = result
        self.cursor = len(self.episodes)","Point(row=82, column=4)","Point(row=89, column=40)",EpisodicActionHistory,classic/forge/forge/components/action_history/model.py
EpisodicActionHistory.rewind,function,"Resets the history to an earlier state.

        Params:
            number_of_cycles (int): The number of cycles to rewind. Default is 0.
                When set to 0, it will only reset the current cycle.
","def rewind(self, number_of_episodes: int = 0) -> None:
        """"""Resets the history to an earlier state.

        Params:
            number_of_cycles (int): The number of cycles to rewind. Default is 0.
                When set to 0, it will only reset the current cycle.
        """"""
        # Remove partial record of current cycle
        if self.current_episode:
            if self.current_episode.action and not self.current_episode.result:
                self.episodes.pop(self.cursor)

        # Rewind the specified number of cycles
        if number_of_episodes > 0:
            self.episodes = self.episodes[:-number_of_episodes]
            self.cursor = len(self.episodes)","Point(row=91, column=4)","Point(row=106, column=44)",EpisodicActionHistory,classic/forge/forge/components/action_history/model.py
EpisodicActionHistory.handle_compression,function,"Compresses each episode in the action history using an LLM.

        This method iterates over all episodes in the action history without a summary,
        and generates a summary for them using an LLM.
","async def handle_compression(
        self,
        llm_provider: MultiProvider,
        model_name: ModelName,
        spacy_model: str,
    ) -> None:
        """"""Compresses each episode in the action history using an LLM.

        This method iterates over all episodes in the action history without a summary,
        and generates a summary for them using an LLM.
        """"""
        compress_instruction = (
            ""The text represents an action, the reason for its execution, ""
            ""and its result. ""
            ""Condense the action taken and its result into one line. ""
            ""Preserve any specific factual information gathered by the action.""
        )
        async with self._lock:
            # Gather all episodes without a summary
            episodes_to_summarize = [ep for ep in self.episodes if ep.summary is None]

            # Parallelize summarization calls
            summarize_coroutines = [
                summarize_text(
                    episode.format(),
                    instruction=compress_instruction,
                    llm_provider=llm_provider,
                    model_name=model_name,
                    spacy_model=spacy_model,
                )
                for episode in episodes_to_summarize
            ]
            summaries = await asyncio.gather(*summarize_coroutines)

            # Assign summaries to episodes
            for episode, (summary, _) in zip(episodes_to_summarize, summaries):
                episode.summary = summary","Point(row=108, column=4)","Point(row=144, column=41)",EpisodicActionHistory,classic/forge/forge/components/action_history/model.py
EpisodicActionHistory.fmt_list,function,,"def fmt_list(self) -> str:
        return format_numbered_list(self.episodes)","Point(row=146, column=4)","Point(row=147, column=50)",EpisodicActionHistory,classic/forge/forge/components/action_history/model.py
EpisodicActionHistory.fmt_paragraph,function,,"def fmt_paragraph(self) -> str:
        steps: list[str] = []

        for i, episode in enumerate(self.episodes, 1):
            step = f""### Step {i}: {episode.format()}\n""

            steps.append(step)

        return ""\n\n"".join(steps)","Point(row=149, column=4)","Point(row=157, column=33)",EpisodicActionHistory,classic/forge/forge/components/action_history/model.py
SystemComponent,class,Component for system messages and commands.,"class SystemComponent(DirectiveProvider, MessageProvider, CommandProvider):
    """"""Component for system messages and commands.""""""

    def get_constraints(self) -> Iterator[str]:
        yield ""Exclusively use the commands listed below.""
        yield (
            ""You can only act proactively, and are unable to start background jobs or ""
            ""set up webhooks for yourself. ""
            ""Take this into account when planning your actions.""
        )
        yield (
            ""You are unable to interact with physical objects. ""
            ""If this is absolutely necessary to fulfill a task or objective or ""
            ""to complete a step, you must ask the user to do it for you. ""
            ""If the user refuses this, and there is no other way to achieve your ""
            ""goals, you must terminate to avoid wasting time and energy.""
        )

    def get_resources(self) -> Iterator[str]:
        yield (
            ""You are a Large Language Model, trained on millions of pages of text, ""
            ""including a lot of factual knowledge. Make use of this factual knowledge ""
            ""to avoid unnecessary gathering of information.""
        )

    def get_best_practices(self) -> Iterator[str]:
        yield (
            ""Continuously review and analyze your actions to ensure ""
            ""you are performing to the best of your abilities.""
        )
        yield ""Constructively self-criticize your big-picture behavior constantly.""
        yield ""Reflect on past decisions and strategies to refine your approach.""
        yield (
            ""Every command has a cost, so be smart and efficient. ""
            ""Aim to complete tasks in the least number of steps.""
        )
        yield (
            ""Only make use of your information gathering abilities to find ""
            ""information that you don't yet have knowledge of.""
        )

    def get_messages(self) -> Iterator[ChatMessage]:
        # Clock
        yield ChatMessage.system(
            f""## Clock\nThe current time and date is {time.strftime('%c')}""
        )

    def get_commands(self) -> Iterator[Command]:
        yield self.finish

    @command(
        names=[FINISH_COMMAND],
        parameters={
            ""reason"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""A summary to the user of how the goals were accomplished"",
                required=True,
            ),
        },
    )
    def finish(self, reason: str):
        """"""Use this to shut down once you have completed your task,
        or when there are insurmountable problems that make it impossible
        for you to finish your task.""""""
        raise AgentFinished(reason)","Point(row=14, column=0)","Point(row=78, column=35)",,classic/forge/forge/components/system/system.py
SystemComponent.get_constraints,function,,"def get_constraints(self) -> Iterator[str]:
        yield ""Exclusively use the commands listed below.""
        yield (
            ""You can only act proactively, and are unable to start background jobs or ""
            ""set up webhooks for yourself. ""
            ""Take this into account when planning your actions.""
        )
        yield (
            ""You are unable to interact with physical objects. ""
            ""If this is absolutely necessary to fulfill a task or objective or ""
            ""to complete a step, you must ask the user to do it for you. ""
            ""If the user refuses this, and there is no other way to achieve your ""
            ""goals, you must terminate to avoid wasting time and energy.""
        )","Point(row=17, column=4)","Point(row=30, column=9)",SystemComponent,classic/forge/forge/components/system/system.py
SystemComponent.get_resources,function,,"def get_resources(self) -> Iterator[str]:
        yield (
            ""You are a Large Language Model, trained on millions of pages of text, ""
            ""including a lot of factual knowledge. Make use of this factual knowledge ""
            ""to avoid unnecessary gathering of information.""
        )","Point(row=32, column=4)","Point(row=37, column=9)",SystemComponent,classic/forge/forge/components/system/system.py
SystemComponent.get_best_practices,function,,"def get_best_practices(self) -> Iterator[str]:
        yield (
            ""Continuously review and analyze your actions to ensure ""
            ""you are performing to the best of your abilities.""
        )
        yield ""Constructively self-criticize your big-picture behavior constantly.""
        yield ""Reflect on past decisions and strategies to refine your approach.""
        yield (
            ""Every command has a cost, so be smart and efficient. ""
            ""Aim to complete tasks in the least number of steps.""
        )
        yield (
            ""Only make use of your information gathering abilities to find ""
            ""information that you don't yet have knowledge of.""
        )","Point(row=39, column=4)","Point(row=53, column=9)",SystemComponent,classic/forge/forge/components/system/system.py
SystemComponent.get_messages,function,,"def get_messages(self) -> Iterator[ChatMessage]:
        # Clock
        yield ChatMessage.system(
            f""## Clock\nThe current time and date is {time.strftime('%c')}""
        )","Point(row=55, column=4)","Point(row=59, column=9)",SystemComponent,classic/forge/forge/components/system/system.py
SystemComponent.get_commands,function,,"def get_commands(self) -> Iterator[Command]:
        yield self.finish","Point(row=61, column=4)","Point(row=62, column=25)",SystemComponent,classic/forge/forge/components/system/system.py
SystemComponent.finish,function,"Use this to shut down once you have completed your task,
        or when there are insurmountable problems that make it impossible
        for you to finish your task.","def finish(self, reason: str):
        """"""Use this to shut down once you have completed your task,
        or when there are insurmountable problems that make it impossible
        for you to finish your task.""""""
        raise AgentFinished(reason)","Point(row=74, column=4)","Point(row=78, column=35)",SystemComponent,classic/forge/forge/components/system/system.py
WatchdogComponent,class,"
    Adds a watchdog feature to an agent class. Whenever the agent starts
    looping, the watchdog will switch from the FAST_LLM to the SMART_LLM and re-think.
","class WatchdogComponent(AfterParse[AnyProposal]):
    """"""
    Adds a watchdog feature to an agent class. Whenever the agent starts
    looping, the watchdog will switch from the FAST_LLM to the SMART_LLM and re-think.
    """"""

    def __init__(
        self,
        config: BaseAgentConfiguration,
        event_history: EpisodicActionHistory[AnyProposal],
    ):
        self.config = config
        self.event_history = event_history
        self.revert_big_brain = False

    def after_parse(self, result: AnyProposal) -> None:
        if self.revert_big_brain:
            self.config.big_brain = False
            self.revert_big_brain = False

        if not self.config.big_brain and self.config.fast_llm != self.config.smart_llm:
            previous_command, previous_command_args = None, None
            if len(self.event_history) > 1:
                # Detect repetitive commands
                previous_cycle = self.event_history.episodes[
                    self.event_history.cursor - 1
                ]
                previous_command = previous_cycle.action.use_tool.name
                previous_command_args = previous_cycle.action.use_tool.arguments

            rethink_reason = """"

            if not result.use_tool:
                rethink_reason = ""AI did not specify a command""
            elif (
                result.use_tool.name == previous_command
                and result.use_tool.arguments == previous_command_args
            ):
                rethink_reason = f""Repititive command detected ({result.use_tool.name})""

            if rethink_reason:
                logger.info(f""{rethink_reason}, re-thinking with SMART_LLM..."")
                self.event_history.rewind()
                self.big_brain = True
                self.revert_big_brain = True
                # Trigger retry of all pipelines prior to this component
                raise ComponentSystemError(rethink_reason, self)","Point(row=16, column=0)","Point(row=62, column=64)",,classic/forge/forge/components/watchdog/watchdog.py
WatchdogComponent.__init__,function,,"def __init__(
        self,
        config: BaseAgentConfiguration,
        event_history: EpisodicActionHistory[AnyProposal],
    ):
        self.config = config
        self.event_history = event_history
        self.revert_big_brain = False","Point(row=22, column=4)","Point(row=29, column=37)",WatchdogComponent,classic/forge/forge/components/watchdog/watchdog.py
WatchdogComponent.after_parse,function,,"def after_parse(self, result: AnyProposal) -> None:
        if self.revert_big_brain:
            self.config.big_brain = False
            self.revert_big_brain = False

        if not self.config.big_brain and self.config.fast_llm != self.config.smart_llm:
            previous_command, previous_command_args = None, None
            if len(self.event_history) > 1:
                # Detect repetitive commands
                previous_cycle = self.event_history.episodes[
                    self.event_history.cursor - 1
                ]
                previous_command = previous_cycle.action.use_tool.name
                previous_command_args = previous_cycle.action.use_tool.arguments

            rethink_reason = """"

            if not result.use_tool:
                rethink_reason = ""AI did not specify a command""
            elif (
                result.use_tool.name == previous_command
                and result.use_tool.arguments == previous_command_args
            ):
                rethink_reason = f""Repititive command detected ({result.use_tool.name})""

            if rethink_reason:
                logger.info(f""{rethink_reason}, re-thinking with SMART_LLM..."")
                self.event_history.rewind()
                self.big_brain = True
                self.revert_big_brain = True
                # Trigger retry of all pipelines prior to this component
                raise ComponentSystemError(rethink_reason, self)","Point(row=31, column=4)","Point(row=62, column=64)",WatchdogComponent,classic/forge/forge/components/watchdog/watchdog.py
file_content,function,,"def file_content():
    return ""This is a test file.\n""","Point(row=12, column=0)","Point(row=13, column=35)",,classic/forge/forge/components/file_manager/test_file_manager.py
file_manager_component,function,,"def file_manager_component(storage: FileStorage):
    return FileManagerComponent(
        storage,
        BaseAgentSettings(
            agent_id=""TestAgent"", name=""TestAgent"", description=""Test Agent description""
        ),
    )","Point(row=17, column=0)","Point(row=23, column=5)",,classic/forge/forge/components/file_manager/test_file_manager.py
test_file_name,function,,"def test_file_name():
    return Path(""test_file.txt"")","Point(row=27, column=0)","Point(row=28, column=32)",,classic/forge/forge/components/file_manager/test_file_manager.py
test_file_path,function,,"def test_file_path(test_file_name: Path, storage: FileStorage):
    return storage.get_path(test_file_name)","Point(row=32, column=0)","Point(row=33, column=43)",,classic/forge/forge/components/file_manager/test_file_manager.py
test_directory,function,,"def test_directory(storage: FileStorage):
    return storage.get_path(""test_directory"")","Point(row=37, column=0)","Point(row=38, column=45)",,classic/forge/forge/components/file_manager/test_file_manager.py
test_nested_file,function,,"def test_nested_file(storage: FileStorage):
    return storage.get_path(""nested/test_file.txt"")","Point(row=42, column=0)","Point(row=43, column=51)",,classic/forge/forge/components/file_manager/test_file_manager.py
test_read_file,function,,"async def test_read_file(
    test_file_path: Path,
    file_content,
    file_manager_component: FileManagerComponent,
):
    await file_manager_component.workspace.write_file(test_file_path.name, file_content)
    content = file_manager_component.read_file(test_file_path.name)
    assert content.replace(""\r"", """") == file_content","Point(row=47, column=0)","Point(row=54, column=52)",,classic/forge/forge/components/file_manager/test_file_manager.py
test_read_file_not_found,function,,"def test_read_file_not_found(file_manager_component: FileManagerComponent):
    filename = ""does_not_exist.txt""
    with pytest.raises(FileNotFoundError):
        file_manager_component.read_file(filename)","Point(row=57, column=0)","Point(row=60, column=50)",,classic/forge/forge/components/file_manager/test_file_manager.py
test_write_to_file_relative_path,function,,"async def test_write_to_file_relative_path(
    test_file_name: Path, file_manager_component: FileManagerComponent
):
    new_content = ""This is new content.\n""
    await file_manager_component.write_to_file(test_file_name, new_content)
    with open(
        file_manager_component.workspace.get_path(test_file_name), ""r"", encoding=""utf-8""
    ) as f:
        content = f.read()
    assert content == new_content","Point(row=64, column=0)","Point(row=73, column=33)",,classic/forge/forge/components/file_manager/test_file_manager.py
test_write_to_file_absolute_path,function,,"async def test_write_to_file_absolute_path(
    test_file_path: Path, file_manager_component: FileManagerComponent
):
    new_content = ""This is new content.\n""
    await file_manager_component.write_to_file(test_file_path, new_content)
    with open(test_file_path, ""r"", encoding=""utf-8"") as f:
        content = f.read()
    assert content == new_content","Point(row=77, column=0)","Point(row=84, column=33)",,classic/forge/forge/components/file_manager/test_file_manager.py
test_list_files,function,,"async def test_list_files(file_manager_component: FileManagerComponent):
    # Create files A and B
    file_a_name = ""file_a.txt""
    file_b_name = ""file_b.txt""
    test_directory = Path(""test_directory"")

    await file_manager_component.workspace.write_file(file_a_name, ""This is file A."")
    await file_manager_component.workspace.write_file(file_b_name, ""This is file B."")

    # Create a subdirectory and place a copy of file_a in it
    file_manager_component.workspace.make_dir(test_directory)
    await file_manager_component.workspace.write_file(
        test_directory / file_a_name, ""This is file A in the subdirectory.""
    )

    files = file_manager_component.list_folder(""."")
    assert file_a_name in files
    assert file_b_name in files
    assert os.path.join(test_directory, file_a_name) in files

    # Clean up
    file_manager_component.workspace.delete_file(file_a_name)
    file_manager_component.workspace.delete_file(file_b_name)
    file_manager_component.workspace.delete_file(test_directory / file_a_name)
    file_manager_component.workspace.delete_dir(test_directory)

    # Case 2: Search for a file that does not exist and make sure we don't throw
    non_existent_file = ""non_existent_file.txt""
    files = file_manager_component.list_folder("""")
    assert non_existent_file not in files","Point(row=88, column=0)","Point(row=117, column=41)",,classic/forge/forge/components/file_manager/test_file_manager.py
FileManagerConfiguration,class,,"class FileManagerConfiguration(BaseModel):
    storage_path: str
    """"""Path to agent files, e.g. state""""""
    workspace_path: str
    """"""Path to files that agent has access to""""""

    model_config = ConfigDict(
        # Prevent mutation of the configuration
        # as this wouldn't be reflected in the file storage
        frozen=False
    )","Point(row=18, column=0)","Point(row=28, column=5)",,classic/forge/forge/components/file_manager/file_manager.py
FileManagerComponent,class,"
    Adds general file manager (e.g. Agent state),
    workspace manager (e.g. Agent output files) support and
    commands to perform operations on files and folders.
","class FileManagerComponent(
    DirectiveProvider, CommandProvider, ConfigurableComponent[FileManagerConfiguration]
):
    """"""
    Adds general file manager (e.g. Agent state),
    workspace manager (e.g. Agent output files) support and
    commands to perform operations on files and folders.
    """"""

    config_class = FileManagerConfiguration

    STATE_FILE = ""state.json""
    """"""The name of the file where the agent's state is stored.""""""

    def __init__(
        self,
        file_storage: FileStorage,
        agent_state: BaseAgentSettings,
        config: Optional[FileManagerConfiguration] = None,
    ):
        """"""Initialise the FileManagerComponent.
        Either `agent_id` or `config` must be provided.

        Args:
            file_storage (FileStorage): The file storage instance to use.
            state (BaseAgentSettings): The agent's state.
            config (FileManagerConfiguration, optional): The configuration for
            the file manager. Defaults to None.
        """"""
        if not agent_state.agent_id:
            raise ValueError(""Agent must have an ID."")

        self.agent_state = agent_state

        if not config:
            storage_path = f""agents/{self.agent_state.agent_id}/""
            workspace_path = f""agents/{self.agent_state.agent_id}/workspace""
            ConfigurableComponent.__init__(
                self,
                FileManagerConfiguration(
                    storage_path=storage_path, workspace_path=workspace_path
                ),
            )
        else:
            ConfigurableComponent.__init__(self, config)

        self.storage = file_storage.clone_with_subroot(self.config.storage_path)
        """"""Agent-related files, e.g. state, logs.
        Use `workspace` to access the agent's workspace files.""""""
        self.workspace = file_storage.clone_with_subroot(self.config.workspace_path)
        """"""Workspace that the agent has access to, e.g. for reading/writing files.
        Use `storage` to access agent-related files, e.g. state, logs.""""""
        self._file_storage = file_storage

    async def save_state(self, save_as_id: Optional[str] = None) -> None:
        """"""Save the agent's data and state.""""""
        if save_as_id:
            self._file_storage.make_dir(f""agents/{save_as_id}"")
            # Save state
            await self._file_storage.write_file(
                f""agents/{save_as_id}/{self.STATE_FILE}"",
                self.agent_state.model_dump_json(),
            )
            # Copy workspace
            self._file_storage.copy(
                self.config.workspace_path,
                f""agents/{save_as_id}/workspace"",
            )
        else:
            await self.storage.write_file(
                self.storage.root / self.STATE_FILE, self.agent_state.model_dump_json()
            )

    def get_resources(self) -> Iterator[str]:
        yield ""The ability to read and write files.""

    def get_commands(self) -> Iterator[Command]:
        yield self.read_file
        yield self.write_to_file
        yield self.list_folder

    @command(
        parameters={
            ""filename"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The path of the file to read"",
                required=True,
            )
        },
    )
    def read_file(self, filename: str | Path) -> str:
        """"""Read a file and return the contents

        Args:
            filename (str): The name of the file to read

        Returns:
            str: The contents of the file
        """"""
        file = self.workspace.open_file(filename, binary=True)
        content = decode_textual_file(file, os.path.splitext(filename)[1], logger)

        return content

    @command(
        [""write_file"", ""create_file""],
        ""Write a file, creating it if necessary. ""
        ""If the file exists, it is overwritten."",
        {
            ""filename"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The name of the file to write to"",
                required=True,
            ),
            ""contents"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The contents to write to the file"",
                required=True,
            ),
        },
    )
    async def write_to_file(self, filename: str | Path, contents: str) -> str:
        """"""Write contents to a file

        Args:
            filename (str): The name of the file to write to
            contents (str): The contents to write to the file

        Returns:
            str: A message indicating success or failure
        """"""
        if directory := os.path.dirname(filename):
            self.workspace.make_dir(directory)
        await self.workspace.write_file(filename, contents)
        return f""File {filename} has been written successfully.""

    @command(
        parameters={
            ""folder"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The folder to list files in"",
                required=True,
            )
        },
    )
    def list_folder(self, folder: str | Path) -> list[str]:
        """"""Lists files in a folder recursively

        Args:
            folder (str): The folder to search in

        Returns:
            list[str]: A list of files found in the folder
        """"""
        return [str(p) for p in self.workspace.list_files(folder)]","Point(row=31, column=0)","Point(row=185, column=66)",,classic/forge/forge/components/file_manager/file_manager.py
FileManagerComponent.__init__,function,"Initialise the FileManagerComponent.
        Either `agent_id` or `config` must be provided.

        Args:
            file_storage (FileStorage): The file storage instance to use.
            state (BaseAgentSettings): The agent's state.
            config (FileManagerConfiguration, optional): The configuration for
            the file manager. Defaults to None.
","def __init__(
        self,
        file_storage: FileStorage,
        agent_state: BaseAgentSettings,
        config: Optional[FileManagerConfiguration] = None,
    ):
        """"""Initialise the FileManagerComponent.
        Either `agent_id` or `config` must be provided.

        Args:
            file_storage (FileStorage): The file storage instance to use.
            state (BaseAgentSettings): The agent's state.
            config (FileManagerConfiguration, optional): The configuration for
            the file manager. Defaults to None.
        """"""
        if not agent_state.agent_id:
            raise ValueError(""Agent must have an ID."")

        self.agent_state = agent_state

        if not config:
            storage_path = f""agents/{self.agent_state.agent_id}/""
            workspace_path = f""agents/{self.agent_state.agent_id}/workspace""
            ConfigurableComponent.__init__(
                self,
                FileManagerConfiguration(
                    storage_path=storage_path, workspace_path=workspace_path
                ),
            )
        else:
            ConfigurableComponent.__init__(self, config)

        self.storage = file_storage.clone_with_subroot(self.config.storage_path)
        """"""Agent-related files, e.g. state, logs.
        Use `workspace` to access the agent's workspace files.""""""
        self.workspace = file_storage.clone_with_subroot(self.config.workspace_path)
        """"""Workspace that the agent has access to, e.g. for reading/writing files.
        Use `storage` to access agent-related files, e.g. state, logs.""""""
        self._file_storage = file_storage","Point(row=45, column=4)","Point(row=83, column=41)",FileManagerComponent,classic/forge/forge/components/file_manager/file_manager.py
FileManagerComponent.save_state,function,Save the agent's data and state.,"async def save_state(self, save_as_id: Optional[str] = None) -> None:
        """"""Save the agent's data and state.""""""
        if save_as_id:
            self._file_storage.make_dir(f""agents/{save_as_id}"")
            # Save state
            await self._file_storage.write_file(
                f""agents/{save_as_id}/{self.STATE_FILE}"",
                self.agent_state.model_dump_json(),
            )
            # Copy workspace
            self._file_storage.copy(
                self.config.workspace_path,
                f""agents/{save_as_id}/workspace"",
            )
        else:
            await self.storage.write_file(
                self.storage.root / self.STATE_FILE, self.agent_state.model_dump_json()
            )","Point(row=85, column=4)","Point(row=102, column=13)",FileManagerComponent,classic/forge/forge/components/file_manager/file_manager.py
FileManagerComponent.get_resources,function,,"def get_resources(self) -> Iterator[str]:
        yield ""The ability to read and write files.""","Point(row=104, column=4)","Point(row=105, column=52)",FileManagerComponent,classic/forge/forge/components/file_manager/file_manager.py
FileManagerComponent.get_commands,function,,"def get_commands(self) -> Iterator[Command]:
        yield self.read_file
        yield self.write_to_file
        yield self.list_folder","Point(row=107, column=4)","Point(row=110, column=30)",FileManagerComponent,classic/forge/forge/components/file_manager/file_manager.py
FileManagerComponent.read_file,function,"Read a file and return the contents

        Args:
            filename (str): The name of the file to read

        Returns:
            str: The contents of the file
","def read_file(self, filename: str | Path) -> str:
        """"""Read a file and return the contents

        Args:
            filename (str): The name of the file to read

        Returns:
            str: The contents of the file
        """"""
        file = self.workspace.open_file(filename, binary=True)
        content = decode_textual_file(file, os.path.splitext(filename)[1], logger)

        return content","Point(row=121, column=4)","Point(row=133, column=22)",FileManagerComponent,classic/forge/forge/components/file_manager/file_manager.py
FileManagerComponent.write_to_file,function,"Write contents to a file

        Args:
            filename (str): The name of the file to write to
            contents (str): The contents to write to the file

        Returns:
            str: A message indicating success or failure
","async def write_to_file(self, filename: str | Path, contents: str) -> str:
        """"""Write contents to a file

        Args:
            filename (str): The name of the file to write to
            contents (str): The contents to write to the file

        Returns:
            str: A message indicating success or failure
        """"""
        if directory := os.path.dirname(filename):
            self.workspace.make_dir(directory)
        await self.workspace.write_file(filename, contents)
        return f""File {filename} has been written successfully.""","Point(row=152, column=4)","Point(row=165, column=64)",FileManagerComponent,classic/forge/forge/components/file_manager/file_manager.py
FileManagerComponent.list_folder,function,"Lists files in a folder recursively

        Args:
            folder (str): The folder to search in

        Returns:
            list[str]: A list of files found in the folder
","def list_folder(self, folder: str | Path) -> list[str]:
        """"""Lists files in a folder recursively

        Args:
            folder (str): The folder to search in

        Returns:
            list[str]: A list of files found in the folder
        """"""
        return [str(p) for p in self.workspace.list_files(folder)]","Point(row=176, column=4)","Point(row=185, column=66)",FileManagerComponent,classic/forge/forge/components/file_manager/file_manager.py
we_are_running_in_a_docker_container,function,"Check if we are running in a Docker container

    Returns:
        bool: True if we are running in a Docker container, False otherwise
","def we_are_running_in_a_docker_container() -> bool:
    """"""Check if we are running in a Docker container

    Returns:
        bool: True if we are running in a Docker container, False otherwise
    """"""
    return os.path.exists(""/.dockerenv"")","Point(row=28, column=0)","Point(row=34, column=40)",,classic/forge/forge/components/code_executor/code_executor.py
is_docker_available,function,"Check if Docker is available and supports Linux containers

    Returns:
        bool: True if Docker is available and supports Linux containers, False otherwise
","def is_docker_available() -> bool:
    """"""Check if Docker is available and supports Linux containers

    Returns:
        bool: True if Docker is available and supports Linux containers, False otherwise
    """"""
    try:
        client = docker.from_env()
        docker_info = client.info()
        return docker_info[""OSType""] == ""linux""
    except Exception:
        return False","Point(row=37, column=0)","Point(row=48, column=20)",,classic/forge/forge/components/code_executor/code_executor.py
CodeExecutionError,class,The operation (an attempt to run arbitrary code) returned an error,"class CodeExecutionError(CommandExecutionError):
    """"""The operation (an attempt to run arbitrary code) returned an error""""""","Point(row=51, column=0)","Point(row=52, column=76)",,classic/forge/forge/components/code_executor/code_executor.py
CodeExecutorConfiguration,class,,"class CodeExecutorConfiguration(BaseModel):
    execute_local_commands: bool = False
    """"""Enable shell command execution""""""
    shell_command_control: Literal[""allowlist"", ""denylist""] = ""allowlist""
    """"""Controls which list is used""""""
    shell_allowlist: list[str] = Field(default_factory=list)
    """"""List of allowed shell commands""""""
    shell_denylist: list[str] = Field(default_factory=list)
    """"""List of prohibited shell commands""""""
    docker_container_name: str = ""agent_sandbox""
    """"""Name of the Docker container used for code execution""""""","Point(row=55, column=0)","Point(row=65, column=62)",,classic/forge/forge/components/code_executor/code_executor.py
CodeExecutorComponent,class,Provides commands to execute Python code and shell commands.,"class CodeExecutorComponent(
    CommandProvider, ConfigurableComponent[CodeExecutorConfiguration]
):
    """"""Provides commands to execute Python code and shell commands.""""""

    config_class = CodeExecutorConfiguration

    def __init__(
        self,
        workspace: FileStorage,
        config: Optional[CodeExecutorConfiguration] = None,
    ):
        ConfigurableComponent.__init__(self, config)
        self.workspace = workspace

        # Change container name if it's empty or default to prevent different agents
        # from using the same container
        default_container_name = self.config.model_fields[
            ""docker_container_name""
        ].default
        if (
            not self.config.docker_container_name
            or self.config.docker_container_name == default_container_name
        ):
            random_suffix = """".join(random.choices(string.ascii_lowercase, k=8))
            self.config.docker_container_name = (
                f""{default_container_name}_{random_suffix}""
            )

        if not we_are_running_in_a_docker_container() and not is_docker_available():
            logger.info(
                ""Docker is not available or does not support Linux containers. ""
                ""The code execution commands will not be available.""
            )

        if not self.config.execute_local_commands:
            logger.info(
                ""Local shell commands are disabled. To enable them,""
                "" set EXECUTE_LOCAL_COMMANDS to 'True' in your config file.""
            )

    def get_commands(self) -> Iterator[Command]:
        if we_are_running_in_a_docker_container() or is_docker_available():
            yield self.execute_python_code
            yield self.execute_python_file

        if self.config.execute_local_commands:
            yield self.execute_shell
            yield self.execute_shell_popen

    @command(
        [""execute_python_code""],
        ""Executes the given Python code inside a single-use Docker container""
        "" with access to your workspace folder"",
        {
            ""code"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The Python code to run"",
                required=True,
            ),
        },
    )
    async def execute_python_code(self, code: str) -> str:
        """"""
        Create and execute a Python file in a Docker container
        and return the STDOUT of the executed code.

        If the code generates any data that needs to be captured,
        use a print statement.

        Args:
            code (str): The Python code to run.
            agent (Agent): The Agent executing the command.

        Returns:
            str: The STDOUT captured from the code when it ran.
        """"""

        temp_path = """"
        while True:
            temp_path = f""temp{self._generate_random_string()}.py""
            if not self.workspace.exists(temp_path):
                break
        await self.workspace.write_file(temp_path, code)

        try:
            return self.execute_python_file(temp_path)
        except Exception as e:
            raise CommandExecutionError(*e.args)
        finally:
            self.workspace.delete_file(temp_path)

    @command(
        [""execute_python_file""],
        ""Execute an existing Python file inside a single-use Docker container""
        "" with access to your workspace folder"",
        {
            ""filename"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The name of the file to execute"",
                required=True,
            ),
            ""args"": JSONSchema(
                type=JSONSchema.Type.ARRAY,
                description=""The (command line) arguments to pass to the script"",
                required=False,
                items=JSONSchema(type=JSONSchema.Type.STRING),
            ),
        },
    )
    def execute_python_file(self, filename: str | Path, args: list[str] = []) -> str:
        """"""Execute a Python file in a Docker container and return the output

        Args:
            filename (Path): The name of the file to execute
            args (list, optional): The arguments with which to run the python script

        Returns:
            str: The output of the file
        """"""
        logger.info(f""Executing python file '{filename}'"")

        if not str(filename).endswith("".py""):
            raise InvalidArgumentError(""Invalid file type. Only .py files are allowed."")

        file_path = self.workspace.get_path(filename)
        if not self.workspace.exists(file_path):
            # Mimic the response that you get from the command line to make it
            # intuitively understandable for the LLM
            raise FileNotFoundError(
                f""python: can't open file '{filename}': ""
                f""[Errno 2] No such file or directory""
            )

        if we_are_running_in_a_docker_container():
            logger.debug(
                ""App is running in a Docker container; ""
                f""executing {file_path} directly...""
            )
            with self.workspace.mount() as local_path:
                result = subprocess.run(
                    [""python"", ""-B"", str(file_path.relative_to(self.workspace.root))]
                    + args,
                    capture_output=True,
                    encoding=""utf8"",
                    cwd=str(local_path),
                )
                if result.returncode == 0:
                    return result.stdout
                else:
                    raise CodeExecutionError(result.stderr)

        logger.debug(""App is not running in a Docker container"")
        return self._run_python_code_in_docker(file_path, args)

    def validate_command(self, command_line: str) -> tuple[bool, bool]:
        """"""Check whether a command is allowed and whether it may be executed in a shell.

        If shell command control is enabled, we disallow executing in a shell, because
        otherwise the model could circumvent the command filter using shell features.

        Args:
            command_line (str): The command line to validate
            config (Config): The app config including shell command control settings

        Returns:
            bool: True if the command is allowed, False otherwise
            bool: True if the command may be executed in a shell, False otherwise
        """"""
        if not command_line:
            return False, False

        command_name = shlex.split(command_line)[0]

        if self.config.shell_command_control == ""allowlist"":
            return command_name in self.config.shell_allowlist, False
        elif self.config.shell_command_control == ""denylist"":
            return command_name not in self.config.shell_denylist, False
        else:
            return True, True

    @command(
        [""execute_shell""],
        ""Execute a Shell Command, non-interactive commands only"",
        {
            ""command_line"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The command line to execute"",
                required=True,
            )
        },
    )
    def execute_shell(self, command_line: str) -> str:
        """"""Execute a shell command and return the output

        Args:
            command_line (str): The command line to execute

        Returns:
            str: The output of the command
        """"""
        allow_execute, allow_shell = self.validate_command(command_line)
        if not allow_execute:
            logger.info(f""Command '{command_line}' not allowed"")
            raise OperationNotAllowedError(""This shell command is not allowed."")

        current_dir = Path.cwd()
        # Change dir into workspace if necessary
        if not current_dir.is_relative_to(self.workspace.root):
            os.chdir(self.workspace.root)

        logger.info(
            f""Executing command '{command_line}' in working directory '{os.getcwd()}'""
        )

        result = subprocess.run(
            command_line if allow_shell else shlex.split(command_line),
            capture_output=True,
            shell=allow_shell,
        )
        output = f""STDOUT:\n{result.stdout.decode()}\nSTDERR:\n{result.stderr.decode()}""

        # Change back to whatever the prior working dir was
        os.chdir(current_dir)

        return output

    @command(
        [""execute_shell_popen""],
        ""Execute a Shell Command, non-interactive commands only"",
        {
            ""command_line"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The command line to execute"",
                required=True,
            )
        },
    )
    def execute_shell_popen(self, command_line: str) -> str:
        """"""Execute a shell command with Popen and returns an english description
        of the event and the process id

        Args:
            command_line (str): The command line to execute

        Returns:
            str: Description of the fact that the process started and its id
        """"""
        allow_execute, allow_shell = self.validate_command(command_line)
        if not allow_execute:
            logger.info(f""Command '{command_line}' not allowed"")
            raise OperationNotAllowedError(""This shell command is not allowed."")

        current_dir = Path.cwd()
        # Change dir into workspace if necessary
        if not current_dir.is_relative_to(self.workspace.root):
            os.chdir(self.workspace.root)

        logger.info(
            f""Executing command '{command_line}' in working directory '{os.getcwd()}'""
        )

        do_not_show_output = subprocess.DEVNULL
        process = subprocess.Popen(
            command_line if allow_shell else shlex.split(command_line),
            shell=allow_shell,
            stdout=do_not_show_output,
            stderr=do_not_show_output,
        )

        # Change back to whatever the prior working dir was
        os.chdir(current_dir)

        return f""Subprocess started with PID:'{str(process.pid)}'""

    def _run_python_code_in_docker(self, filename: str | Path, args: list[str]) -> str:
        """"""Run a Python script in a Docker container""""""
        file_path = self.workspace.get_path(filename)
        try:
            client = docker.from_env()
            image_name = ""python:3-alpine""
            container_is_fresh = False
            container_name = self.config.docker_container_name
            with self.workspace.mount() as local_path:
                try:
                    container: DockerContainer = client.containers.get(
                        container_name
                    )  # type: ignore
                except NotFound:
                    try:
                        client.images.get(image_name)
                        logger.debug(f""Image '{image_name}' found locally"")
                    except ImageNotFound:
                        logger.info(
                            f""Image '{image_name}' not found locally,""
                            "" pulling from Docker Hub...""
                        )
                        # Use the low-level API to stream the pull response
                        low_level_client = docker.APIClient()
                        for line in low_level_client.pull(
                            image_name, stream=True, decode=True
                        ):
                            # Print the status and progress, if available
                            status = line.get(""status"")
                            progress = line.get(""progress"")
                            if status and progress:
                                logger.info(f""{status}: {progress}"")
                            elif status:
                                logger.info(status)

                    logger.debug(f""Creating new {image_name} container..."")
                    container: DockerContainer = client.containers.run(
                        image_name,
                        [""sleep"", ""60""],  # Max 60 seconds to prevent permanent hangs
                        volumes={
                            str(local_path.resolve()): {
                                ""bind"": ""/workspace"",
                                ""mode"": ""rw"",
                            }
                        },
                        working_dir=""/workspace"",
                        stderr=True,
                        stdout=True,
                        detach=True,
                        name=container_name,
                    )  # type: ignore
                    container_is_fresh = True

                if not container.status == ""running"":
                    container.start()
                elif not container_is_fresh:
                    container.restart()

                logger.debug(f""Running {file_path} in container {container.name}..."")

                exec_result = container.exec_run(
                    [
                        ""python"",
                        ""-B"",
                        file_path.relative_to(self.workspace.root).as_posix(),
                    ]
                    + args,
                    stderr=True,
                    stdout=True,
                )

                if exec_result.exit_code != 0:
                    raise CodeExecutionError(exec_result.output.decode(""utf-8""))

                return exec_result.output.decode(""utf-8"")

        except DockerException as e:
            logger.warning(
                ""Could not run the script in a container. ""
                ""If you haven't already, please install Docker: ""
                ""https://docs.docker.com/get-docker/""
            )
            raise CommandExecutionError(f""Could not run the script in a container: {e}"")

    def _generate_random_string(self, length: int = 8):
        # Create a string of all letters and digits
        characters = string.ascii_letters + string.digits
        # Use random.choices to generate a random string
        random_string = """".join(random.choices(characters, k=length))
        return random_string","Point(row=68, column=0)","Point(row=432, column=28)",,classic/forge/forge/components/code_executor/code_executor.py
CodeExecutorComponent.__init__,function,,"def __init__(
        self,
        workspace: FileStorage,
        config: Optional[CodeExecutorConfiguration] = None,
    ):
        ConfigurableComponent.__init__(self, config)
        self.workspace = workspace

        # Change container name if it's empty or default to prevent different agents
        # from using the same container
        default_container_name = self.config.model_fields[
            ""docker_container_name""
        ].default
        if (
            not self.config.docker_container_name
            or self.config.docker_container_name == default_container_name
        ):
            random_suffix = """".join(random.choices(string.ascii_lowercase, k=8))
            self.config.docker_container_name = (
                f""{default_container_name}_{random_suffix}""
            )

        if not we_are_running_in_a_docker_container() and not is_docker_available():
            logger.info(
                ""Docker is not available or does not support Linux containers. ""
                ""The code execution commands will not be available.""
            )

        if not self.config.execute_local_commands:
            logger.info(
                ""Local shell commands are disabled. To enable them,""
                "" set EXECUTE_LOCAL_COMMANDS to 'True' in your config file.""
            )","Point(row=75, column=4)","Point(row=107, column=13)",CodeExecutorComponent,classic/forge/forge/components/code_executor/code_executor.py
CodeExecutorComponent.get_commands,function,,"def get_commands(self) -> Iterator[Command]:
        if we_are_running_in_a_docker_container() or is_docker_available():
            yield self.execute_python_code
            yield self.execute_python_file

        if self.config.execute_local_commands:
            yield self.execute_shell
            yield self.execute_shell_popen","Point(row=109, column=4)","Point(row=116, column=42)",CodeExecutorComponent,classic/forge/forge/components/code_executor/code_executor.py
CodeExecutorComponent.execute_python_code,function,"
        Create and execute a Python file in a Docker container
        and return the STDOUT of the executed code.

        If the code generates any data that needs to be captured,
        use a print statement.

        Args:
            code (str): The Python code to run.
            agent (Agent): The Agent executing the command.

        Returns:
            str: The STDOUT captured from the code when it ran.
","async def execute_python_code(self, code: str) -> str:
        """"""
        Create and execute a Python file in a Docker container
        and return the STDOUT of the executed code.

        If the code generates any data that needs to be captured,
        use a print statement.

        Args:
            code (str): The Python code to run.
            agent (Agent): The Agent executing the command.

        Returns:
            str: The STDOUT captured from the code when it ran.
        """"""

        temp_path = """"
        while True:
            temp_path = f""temp{self._generate_random_string()}.py""
            if not self.workspace.exists(temp_path):
                break
        await self.workspace.write_file(temp_path, code)

        try:
            return self.execute_python_file(temp_path)
        except Exception as e:
            raise CommandExecutionError(*e.args)
        finally:
            self.workspace.delete_file(temp_path)","Point(row=130, column=4)","Point(row=158, column=49)",CodeExecutorComponent,classic/forge/forge/components/code_executor/code_executor.py
CodeExecutorComponent.execute_python_file,function,"Execute a Python file in a Docker container and return the output

        Args:
            filename (Path): The name of the file to execute
            args (list, optional): The arguments with which to run the python script

        Returns:
            str: The output of the file
","def execute_python_file(self, filename: str | Path, args: list[str] = []) -> str:
        """"""Execute a Python file in a Docker container and return the output

        Args:
            filename (Path): The name of the file to execute
            args (list, optional): The arguments with which to run the python script

        Returns:
            str: The output of the file
        """"""
        logger.info(f""Executing python file '{filename}'"")

        if not str(filename).endswith("".py""):
            raise InvalidArgumentError(""Invalid file type. Only .py files are allowed."")

        file_path = self.workspace.get_path(filename)
        if not self.workspace.exists(file_path):
            # Mimic the response that you get from the command line to make it
            # intuitively understandable for the LLM
            raise FileNotFoundError(
                f""python: can't open file '{filename}': ""
                f""[Errno 2] No such file or directory""
            )

        if we_are_running_in_a_docker_container():
            logger.debug(
                ""App is running in a Docker container; ""
                f""executing {file_path} directly...""
            )
            with self.workspace.mount() as local_path:
                result = subprocess.run(
                    [""python"", ""-B"", str(file_path.relative_to(self.workspace.root))]
                    + args,
                    capture_output=True,
                    encoding=""utf8"",
                    cwd=str(local_path),
                )
                if result.returncode == 0:
                    return result.stdout
                else:
                    raise CodeExecutionError(result.stderr)

        logger.debug(""App is not running in a Docker container"")
        return self._run_python_code_in_docker(file_path, args)","Point(row=178, column=4)","Point(row=221, column=63)",CodeExecutorComponent,classic/forge/forge/components/code_executor/code_executor.py
CodeExecutorComponent.validate_command,function,"Check whether a command is allowed and whether it may be executed in a shell.

        If shell command control is enabled, we disallow executing in a shell, because
        otherwise the model could circumvent the command filter using shell features.

        Args:
            command_line (str): The command line to validate
            config (Config): The app config including shell command control settings

        Returns:
            bool: True if the command is allowed, False otherwise
            bool: True if the command may be executed in a shell, False otherwise
","def validate_command(self, command_line: str) -> tuple[bool, bool]:
        """"""Check whether a command is allowed and whether it may be executed in a shell.

        If shell command control is enabled, we disallow executing in a shell, because
        otherwise the model could circumvent the command filter using shell features.

        Args:
            command_line (str): The command line to validate
            config (Config): The app config including shell command control settings

        Returns:
            bool: True if the command is allowed, False otherwise
            bool: True if the command may be executed in a shell, False otherwise
        """"""
        if not command_line:
            return False, False

        command_name = shlex.split(command_line)[0]

        if self.config.shell_command_control == ""allowlist"":
            return command_name in self.config.shell_allowlist, False
        elif self.config.shell_command_control == ""denylist"":
            return command_name not in self.config.shell_denylist, False
        else:
            return True, True","Point(row=223, column=4)","Point(row=247, column=29)",CodeExecutorComponent,classic/forge/forge/components/code_executor/code_executor.py
CodeExecutorComponent.execute_shell,function,"Execute a shell command and return the output

        Args:
            command_line (str): The command line to execute

        Returns:
            str: The output of the command
","def execute_shell(self, command_line: str) -> str:
        """"""Execute a shell command and return the output

        Args:
            command_line (str): The command line to execute

        Returns:
            str: The output of the command
        """"""
        allow_execute, allow_shell = self.validate_command(command_line)
        if not allow_execute:
            logger.info(f""Command '{command_line}' not allowed"")
            raise OperationNotAllowedError(""This shell command is not allowed."")

        current_dir = Path.cwd()
        # Change dir into workspace if necessary
        if not current_dir.is_relative_to(self.workspace.root):
            os.chdir(self.workspace.root)

        logger.info(
            f""Executing command '{command_line}' in working directory '{os.getcwd()}'""
        )

        result = subprocess.run(
            command_line if allow_shell else shlex.split(command_line),
            capture_output=True,
            shell=allow_shell,
        )
        output = f""STDOUT:\n{result.stdout.decode()}\nSTDERR:\n{result.stderr.decode()}""

        # Change back to whatever the prior working dir was
        os.chdir(current_dir)

        return output","Point(row=260, column=4)","Point(row=293, column=21)",CodeExecutorComponent,classic/forge/forge/components/code_executor/code_executor.py
CodeExecutorComponent.execute_shell_popen,function,"Execute a shell command with Popen and returns an english description
        of the event and the process id

        Args:
            command_line (str): The command line to execute

        Returns:
            str: Description of the fact that the process started and its id
","def execute_shell_popen(self, command_line: str) -> str:
        """"""Execute a shell command with Popen and returns an english description
        of the event and the process id

        Args:
            command_line (str): The command line to execute

        Returns:
            str: Description of the fact that the process started and its id
        """"""
        allow_execute, allow_shell = self.validate_command(command_line)
        if not allow_execute:
            logger.info(f""Command '{command_line}' not allowed"")
            raise OperationNotAllowedError(""This shell command is not allowed."")

        current_dir = Path.cwd()
        # Change dir into workspace if necessary
        if not current_dir.is_relative_to(self.workspace.root):
            os.chdir(self.workspace.root)

        logger.info(
            f""Executing command '{command_line}' in working directory '{os.getcwd()}'""
        )

        do_not_show_output = subprocess.DEVNULL
        process = subprocess.Popen(
            command_line if allow_shell else shlex.split(command_line),
            shell=allow_shell,
            stdout=do_not_show_output,
            stderr=do_not_show_output,
        )

        # Change back to whatever the prior working dir was
        os.chdir(current_dir)

        return f""Subprocess started with PID:'{str(process.pid)}'""","Point(row=306, column=4)","Point(row=341, column=66)",CodeExecutorComponent,classic/forge/forge/components/code_executor/code_executor.py
CodeExecutorComponent._run_python_code_in_docker,function,Run a Python script in a Docker container,"def _run_python_code_in_docker(self, filename: str | Path, args: list[str]) -> str:
        """"""Run a Python script in a Docker container""""""
        file_path = self.workspace.get_path(filename)
        try:
            client = docker.from_env()
            image_name = ""python:3-alpine""
            container_is_fresh = False
            container_name = self.config.docker_container_name
            with self.workspace.mount() as local_path:
                try:
                    container: DockerContainer = client.containers.get(
                        container_name
                    )  # type: ignore
                except NotFound:
                    try:
                        client.images.get(image_name)
                        logger.debug(f""Image '{image_name}' found locally"")
                    except ImageNotFound:
                        logger.info(
                            f""Image '{image_name}' not found locally,""
                            "" pulling from Docker Hub...""
                        )
                        # Use the low-level API to stream the pull response
                        low_level_client = docker.APIClient()
                        for line in low_level_client.pull(
                            image_name, stream=True, decode=True
                        ):
                            # Print the status and progress, if available
                            status = line.get(""status"")
                            progress = line.get(""progress"")
                            if status and progress:
                                logger.info(f""{status}: {progress}"")
                            elif status:
                                logger.info(status)

                    logger.debug(f""Creating new {image_name} container..."")
                    container: DockerContainer = client.containers.run(
                        image_name,
                        [""sleep"", ""60""],  # Max 60 seconds to prevent permanent hangs
                        volumes={
                            str(local_path.resolve()): {
                                ""bind"": ""/workspace"",
                                ""mode"": ""rw"",
                            }
                        },
                        working_dir=""/workspace"",
                        stderr=True,
                        stdout=True,
                        detach=True,
                        name=container_name,
                    )  # type: ignore
                    container_is_fresh = True

                if not container.status == ""running"":
                    container.start()
                elif not container_is_fresh:
                    container.restart()

                logger.debug(f""Running {file_path} in container {container.name}..."")

                exec_result = container.exec_run(
                    [
                        ""python"",
                        ""-B"",
                        file_path.relative_to(self.workspace.root).as_posix(),
                    ]
                    + args,
                    stderr=True,
                    stdout=True,
                )

                if exec_result.exit_code != 0:
                    raise CodeExecutionError(exec_result.output.decode(""utf-8""))

                return exec_result.output.decode(""utf-8"")

        except DockerException as e:
            logger.warning(
                ""Could not run the script in a container. ""
                ""If you haven't already, please install Docker: ""
                ""https://docs.docker.com/get-docker/""
            )
            raise CommandExecutionError(f""Could not run the script in a container: {e}"")","Point(row=343, column=4)","Point(row=425, column=88)",CodeExecutorComponent,classic/forge/forge/components/code_executor/code_executor.py
CodeExecutorComponent._generate_random_string,function,,"def _generate_random_string(self, length: int = 8):
        # Create a string of all letters and digits
        characters = string.ascii_letters + string.digits
        # Use random.choices to generate a random string
        random_string = """".join(random.choices(characters, k=length))
        return random_string","Point(row=427, column=4)","Point(row=432, column=28)",CodeExecutorComponent,classic/forge/forge/components/code_executor/code_executor.py
code_executor_component,function,,"def code_executor_component(storage: FileStorage):
    return CodeExecutorComponent(storage)","Point(row=18, column=0)","Point(row=19, column=41)",,classic/forge/forge/components/code_executor/test_code_executor.py
random_code,function,,"def random_code(random_string) -> str:
    return f""print('Hello {random_string}!')""","Point(row=23, column=0)","Point(row=24, column=45)",,classic/forge/forge/components/code_executor/test_code_executor.py
python_test_file,function,,"def python_test_file(storage: FileStorage, random_code: str):
    temp_file = tempfile.NamedTemporaryFile(dir=storage.root, suffix="".py"")
    temp_file.write(str.encode(random_code))
    temp_file.flush()

    yield Path(temp_file.name)
    temp_file.close()","Point(row=28, column=0)","Point(row=34, column=21)",,classic/forge/forge/components/code_executor/test_code_executor.py
python_test_args_file,function,,"def python_test_args_file(storage: FileStorage):
    temp_file = tempfile.NamedTemporaryFile(dir=storage.root, suffix="".py"")
    temp_file.write(str.encode(""import sys\nprint(sys.argv[1], sys.argv[2])""))
    temp_file.flush()

    yield Path(temp_file.name)
    temp_file.close()","Point(row=38, column=0)","Point(row=44, column=21)",,classic/forge/forge/components/code_executor/test_code_executor.py
random_string,function,,"def random_string():
    return """".join(random.choice(string.ascii_lowercase) for _ in range(10))","Point(row=48, column=0)","Point(row=49, column=76)",,classic/forge/forge/components/code_executor/test_code_executor.py
test_execute_python_file,function,,"def test_execute_python_file(
    code_executor_component: CodeExecutorComponent,
    python_test_file: Path,
    random_string: str,
):
    if not (is_docker_available() or we_are_running_in_a_docker_container()):
        pytest.skip(""Docker is not available"")

    result: str = code_executor_component.execute_python_file(python_test_file)
    assert result.replace(""\r"", """") == f""Hello {random_string}!\n""","Point(row=52, column=0)","Point(row=61, column=66)",,classic/forge/forge/components/code_executor/test_code_executor.py
test_execute_python_file_args,function,,"def test_execute_python_file_args(
    code_executor_component: CodeExecutorComponent,
    python_test_args_file: Path,
    random_string: str,
):
    if not (is_docker_available() or we_are_running_in_a_docker_container()):
        pytest.skip(""Docker is not available"")

    random_args = [random_string] * 2
    random_args_string = "" "".join(random_args)
    result = code_executor_component.execute_python_file(
        python_test_args_file, args=random_args
    )
    assert result == f""{random_args_string}\n""","Point(row=64, column=0)","Point(row=77, column=46)",,classic/forge/forge/components/code_executor/test_code_executor.py
test_execute_python_code,function,,"async def test_execute_python_code(
    code_executor_component: CodeExecutorComponent,
    random_code: str,
    random_string: str,
):
    if not (is_docker_available() or we_are_running_in_a_docker_container()):
        pytest.skip(""Docker is not available"")

    result: str = await code_executor_component.execute_python_code(random_code)
    assert result.replace(""\r"", """") == f""Hello {random_string}!\n""","Point(row=81, column=0)","Point(row=90, column=66)",,classic/forge/forge/components/code_executor/test_code_executor.py
test_execute_python_file_invalid,function,,"def test_execute_python_file_invalid(code_executor_component: CodeExecutorComponent):
    with pytest.raises(InvalidArgumentError):
        code_executor_component.execute_python_file(Path(""not_python.txt""))","Point(row=93, column=0)","Point(row=95, column=75)",,classic/forge/forge/components/code_executor/test_code_executor.py
test_execute_python_file_not_found,function,,"def test_execute_python_file_not_found(code_executor_component: CodeExecutorComponent):
    with pytest.raises(
        FileNotFoundError,
        match=r""python: can't open file '([a-zA-Z]:)?[/\\\-\w]*notexist.py': ""
        r""\[Errno 2\] No such file or directory"",
    ):
        code_executor_component.execute_python_file(Path(""notexist.py""))","Point(row=98, column=0)","Point(row=104, column=72)",,classic/forge/forge/components/code_executor/test_code_executor.py
test_execute_shell,function,,"def test_execute_shell(
    code_executor_component: CodeExecutorComponent, random_string: str
):
    code_executor_component.config.shell_command_control = ""allowlist""
    code_executor_component.config.shell_allowlist = [""echo""]
    result = code_executor_component.execute_shell(f""echo 'Hello {random_string}!'"")
    assert f""Hello {random_string}!"" in result","Point(row=107, column=0)","Point(row=113, column=46)",,classic/forge/forge/components/code_executor/test_code_executor.py
test_execute_shell_local_commands_not_allowed,function,,"def test_execute_shell_local_commands_not_allowed(
    code_executor_component: CodeExecutorComponent, random_string: str
):
    with pytest.raises(OperationNotAllowedError, match=""not allowed""):
        code_executor_component.execute_shell(f""echo 'Hello {random_string}!'"")","Point(row=116, column=0)","Point(row=120, column=79)",,classic/forge/forge/components/code_executor/test_code_executor.py
test_execute_shell_denylist_should_deny,function,,"def test_execute_shell_denylist_should_deny(
    code_executor_component: CodeExecutorComponent, random_string: str
):
    code_executor_component.config.shell_command_control = ""denylist""
    code_executor_component.config.shell_denylist = [""echo""]

    with pytest.raises(OperationNotAllowedError, match=""not allowed""):
        code_executor_component.execute_shell(f""echo 'Hello {random_string}!'"")","Point(row=123, column=0)","Point(row=130, column=79)",,classic/forge/forge/components/code_executor/test_code_executor.py
test_execute_shell_denylist_should_allow,function,,"def test_execute_shell_denylist_should_allow(
    code_executor_component: CodeExecutorComponent, random_string: str
):
    code_executor_component.config.shell_command_control = ""denylist""
    code_executor_component.config.shell_denylist = [""cat""]

    result = code_executor_component.execute_shell(f""echo 'Hello {random_string}!'"")
    assert ""Hello"" in result and random_string in result","Point(row=133, column=0)","Point(row=140, column=56)",,classic/forge/forge/components/code_executor/test_code_executor.py
test_execute_shell_allowlist_should_deny,function,,"def test_execute_shell_allowlist_should_deny(
    code_executor_component: CodeExecutorComponent, random_string: str
):
    code_executor_component.config.shell_command_control = ""allowlist""
    code_executor_component.config.shell_allowlist = [""cat""]

    with pytest.raises(OperationNotAllowedError, match=""not allowed""):
        code_executor_component.execute_shell(f""echo 'Hello {random_string}!'"")","Point(row=143, column=0)","Point(row=150, column=79)",,classic/forge/forge/components/code_executor/test_code_executor.py
test_execute_shell_allowlist_should_allow,function,,"def test_execute_shell_allowlist_should_allow(
    code_executor_component: CodeExecutorComponent, random_string: str
):
    code_executor_component.config.shell_command_control = ""allowlist""
    code_executor_component.config.shell_allowlist = [""echo""]

    result = code_executor_component.execute_shell(f""echo 'Hello {random_string}!'"")
    assert ""Hello"" in result and random_string in result","Point(row=153, column=0)","Point(row=160, column=56)",,classic/forge/forge/components/code_executor/test_code_executor.py
mock_clone_from,function,,"def mock_clone_from(mocker):
    return mocker.patch.object(Repo, ""clone_from"")","Point(row=11, column=0)","Point(row=12, column=50)",,classic/forge/forge/components/git_operations/test_git_operations.py
git_ops_component,function,,"def git_ops_component():
    return GitOperationsComponent()","Point(row=16, column=0)","Point(row=17, column=35)",,classic/forge/forge/components/git_operations/test_git_operations.py
test_clone_auto_gpt_repository,function,,"def test_clone_auto_gpt_repository(
    git_ops_component: GitOperationsComponent,
    storage: FileStorage,
    mock_clone_from,
):
    mock_clone_from.return_value = None

    repo = ""github.com/Significant-Gravitas/Auto-GPT.git""
    scheme = ""https://""
    url = scheme + repo
    clone_path = storage.get_path(""auto-gpt-repo"")

    expected_output = f""Cloned {url} to {clone_path}""

    clone_result = git_ops_component.clone_repository(url, clone_path)

    assert clone_result == expected_output
    mock_clone_from.assert_called_once_with(
        url=f""{scheme}{git_ops_component.config.github_username}:{git_ops_component.config.github_api_key}@{repo}"",  # noqa: E501
        to_path=clone_path,
    )","Point(row=20, column=0)","Point(row=40, column=5)",,classic/forge/forge/components/git_operations/test_git_operations.py
test_clone_repository_error,function,,"def test_clone_repository_error(
    git_ops_component: GitOperationsComponent,
    storage: FileStorage,
    mock_clone_from,
):
    url = ""https://github.com/this-repository/does-not-exist.git""
    clone_path = storage.get_path(""does-not-exist"")

    mock_clone_from.side_effect = GitCommandError(
        ""clone"", ""fatal: repository not found"", """"
    )

    with pytest.raises(CommandExecutionError):
        git_ops_component.clone_repository(url, clone_path)","Point(row=43, column=0)","Point(row=56, column=59)",,classic/forge/forge/components/git_operations/test_git_operations.py
GitOperationsConfiguration,class,,"class GitOperationsConfiguration(BaseModel):
    github_username: Optional[str] = UserConfigurable(None, from_env=""GITHUB_USERNAME"")
    github_api_key: Optional[SecretStr] = UserConfigurable(
        None, from_env=""GITHUB_API_KEY"", exclude=True
    )","Point(row=15, column=0)","Point(row=19, column=5)",,classic/forge/forge/components/git_operations/git_operations.py
GitOperationsComponent,class,Provides commands to perform Git operations.,"class GitOperationsComponent(
    CommandProvider, ConfigurableComponent[GitOperationsConfiguration]
):
    """"""Provides commands to perform Git operations.""""""

    config_class = GitOperationsConfiguration

    def __init__(self, config: Optional[GitOperationsConfiguration] = None):
        ConfigurableComponent.__init__(self, config)
        self._enabled = bool(self.config.github_username and self.config.github_api_key)
        self._disabled_reason = ""Configure github_username and github_api_key.""

    def get_commands(self) -> Iterator[Command]:
        yield self.clone_repository

    @command(
        parameters={
            ""url"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The URL of the repository to clone"",
                required=True,
            ),
            ""clone_path"": JSONSchema(
                type=JSONSchema.Type.STRING,
                description=""The path to clone the repository to"",
                required=True,
            ),
        },
    )
    @validate_url
    def clone_repository(self, url: str, clone_path: Path) -> str:
        """"""Clone a GitHub repository locally.

        Args:
            url (str): The URL of the repository to clone.
            clone_path (Path): The path to clone the repository to.

        Returns:
            str: The result of the clone operation.
        """"""
        split_url = url.split(""//"")
        api_key = (
            self.config.github_api_key.get_secret_value()
            if self.config.github_api_key
            else None
        )
        auth_repo_url = f""//{self.config.github_username}:"" f""{api_key}@"".join(
            split_url
        )
        try:
            Repo.clone_from(url=auth_repo_url, to_path=clone_path)
        except Exception as e:
            raise CommandExecutionError(f""Could not clone repo: {e}"")

        return f""""""Cloned {url} to {clone_path}""""""","Point(row=22, column=0)","Point(row=76, column=50)",,classic/forge/forge/components/git_operations/git_operations.py
GitOperationsComponent.__init__,function,,"def __init__(self, config: Optional[GitOperationsConfiguration] = None):
        ConfigurableComponent.__init__(self, config)
        self._enabled = bool(self.config.github_username and self.config.github_api_key)
        self._disabled_reason = ""Configure github_username and github_api_key.""","Point(row=29, column=4)","Point(row=32, column=79)",GitOperationsComponent,classic/forge/forge/components/git_operations/git_operations.py
GitOperationsComponent.get_commands,function,,"def get_commands(self) -> Iterator[Command]:
        yield self.clone_repository","Point(row=34, column=4)","Point(row=35, column=35)",GitOperationsComponent,classic/forge/forge/components/git_operations/git_operations.py
GitOperationsComponent.clone_repository,function,"Clone a GitHub repository locally.

        Args:
            url (str): The URL of the repository to clone.
            clone_path (Path): The path to clone the repository to.

        Returns:
            str: The result of the clone operation.
","def clone_repository(self, url: str, clone_path: Path) -> str:
        """"""Clone a GitHub repository locally.

        Args:
            url (str): The URL of the repository to clone.
            clone_path (Path): The path to clone the repository to.

        Returns:
            str: The result of the clone operation.
        """"""
        split_url = url.split(""//"")
        api_key = (
            self.config.github_api_key.get_secret_value()
            if self.config.github_api_key
            else None
        )
        auth_repo_url = f""//{self.config.github_username}:"" f""{api_key}@"".join(
            split_url
        )
        try:
            Repo.clone_from(url=auth_repo_url, to_path=clone_path)
        except Exception as e:
            raise CommandExecutionError(f""Could not clone repo: {e}"")

        return f""""""Cloned {url} to {clone_path}""""""","Point(row=52, column=4)","Point(row=76, column=50)",GitOperationsComponent,classic/forge/forge/components/git_operations/git_operations.py
extract_hyperlinks,function,"Extract hyperlinks from a BeautifulSoup object

    Args:
        soup (BeautifulSoup): The BeautifulSoup object
        base_url (str): The base URL

    Returns:
        List[Tuple[str, str]]: The extracted hyperlinks
","def extract_hyperlinks(soup: BeautifulSoup, base_url: str) -> list[tuple[str, str]]:
    """"""Extract hyperlinks from a BeautifulSoup object

    Args:
        soup (BeautifulSoup): The BeautifulSoup object
        base_url (str): The base URL

    Returns:
        List[Tuple[str, str]]: The extracted hyperlinks
    """"""
    return [
        (link.text, urljoin(base_url, link[""href""]))
        for link in soup.find_all(""a"", href=True)
    ]","Point(row=7, column=0)","Point(row=20, column=5)",,classic/forge/forge/content_processing/html.py
format_hyperlinks,function,"Format hyperlinks to be displayed to the user

    Args:
        hyperlinks (List[Tuple[str, str]]): The hyperlinks to format

    Returns:
        List[str]: The formatted hyperlinks
","def format_hyperlinks(hyperlinks: list[tuple[str, str]]) -> list[str]:
    """"""Format hyperlinks to be displayed to the user

    Args:
        hyperlinks (List[Tuple[str, str]]): The hyperlinks to format

    Returns:
        List[str]: The formatted hyperlinks
    """"""
    return [f""{link_text.strip()} ({link_url})"" for link_text, link_url in hyperlinks]","Point(row=23, column=0)","Point(row=32, column=86)",,classic/forge/forge/content_processing/html.py
batch,function,"
    Batch data from iterable into slices of length N. The last batch may be shorter.

    Example: `batched('ABCDEFGHIJ', 3)` --> `ABC DEF GHI J`
","def batch(
    sequence: list[T], max_batch_length: int, overlap: int = 0
) -> Iterator[list[T]]:
    """"""
    Batch data from iterable into slices of length N. The last batch may be shorter.

    Example: `batched('ABCDEFGHIJ', 3)` --> `ABC DEF GHI J`
    """"""
    if max_batch_length < 1:
        raise ValueError(""n must be at least one"")
    for i in range(0, len(sequence), max_batch_length - overlap):
        yield sequence[i : i + max_batch_length]","Point(row=19, column=0)","Point(row=30, column=48)",,classic/forge/forge/content_processing/text.py
chunk_content,function,Split content into chunks of approximately equal token length.,"def chunk_content(
    content: str,
    max_chunk_length: int,
    tokenizer: ModelTokenizer,
    with_overlap: bool = True,
) -> Iterator[tuple[str, int]]:
    """"""Split content into chunks of approximately equal token length.""""""

    MAX_OVERLAP = 200  # limit overlap to save tokens

    tokenized_text = tokenizer.encode(content)
    total_length = len(tokenized_text)
    n_chunks = math.ceil(total_length / max_chunk_length)

    chunk_length = math.ceil(total_length / n_chunks)
    overlap = min(max_chunk_length - chunk_length, MAX_OVERLAP) if with_overlap else 0

    for token_batch in batch(tokenized_text, chunk_length + overlap, overlap):
        yield tokenizer.decode(token_batch), len(token_batch)","Point(row=33, column=0)","Point(row=51, column=61)",,classic/forge/forge/content_processing/text.py
summarize_text,function,,"async def summarize_text(
    text: str,
    llm_provider: MultiProvider,
    model_name: ModelName,
    spacy_model: str = ""en_core_web_sm"",
    question: Optional[str] = None,
    instruction: Optional[str] = None,
) -> tuple[str, list[tuple[str, str]]]:
    if question:
        if instruction:
            raise ValueError(
                ""Parameters 'question' and 'instructions' cannot both be set""
            )

        instruction = (
            f'From the text, answer the question: ""{question}"". '
            ""If the answer is not in the text, indicate this clearly ""
            ""and concisely state why the text is not suitable to answer the question.""
        )
    elif not instruction:
        instruction = (
            ""Summarize or describe the text clearly and concisely, ""
            ""whichever seems more appropriate.""
        )

    return await _process_text(  # type: ignore
        text=text,
        instruction=instruction,
        llm_provider=llm_provider,
        model_name=model_name,
        spacy_model=spacy_model,
    )","Point(row=54, column=0)","Point(row=85, column=5)",,classic/forge/forge/content_processing/text.py
extract_information,function,,"async def extract_information(
    source_text: str,
    topics_of_interest: list[str],
    llm_provider: MultiProvider,
    model_name: ModelName,
    spacy_model: str = ""en_core_web_sm"",
) -> list[str]:
    fmt_topics_list = ""\n"".join(f""* {topic}."" for topic in topics_of_interest)
    instruction = (
        ""Extract relevant pieces of information about the following topics:\n""
        f""{fmt_topics_list}\n""
        ""Reword pieces of information if needed to make them self-explanatory. ""
        ""Be concise.\n\n""
        ""Respond with an `Array<string>` in JSON format AND NOTHING ELSE. ""
        'If the text contains no relevant information, return ""[]"".'
    )
    return await _process_text(  # type: ignore
        text=source_text,
        instruction=instruction,
        output_type=list[str],
        llm_provider=llm_provider,
        model_name=model_name,
        spacy_model=spacy_model,
    )","Point(row=88, column=0)","Point(row=111, column=5)",,classic/forge/forge/content_processing/text.py
_process_text,function,"Process text using the OpenAI API for summarization or information extraction

    Params:
        text (str): The text to process.
        instruction (str): Additional instruction for processing.
        llm_provider: LLM provider to use.
        model_name: The name of the llm model to use.
        spacy_model: The spaCy model to use for sentence splitting.
        output_type: `str` for summaries or `list[str]` for piece-wise info extraction.

    Returns:
        For summarization: tuple[str, None | list[(summary, chunk)]]
        For piece-wise information extraction: list[str]
","async def _process_text(
    text: str,
    instruction: str,
    llm_provider: MultiProvider,
    model_name: ModelName,
    spacy_model: str = ""en_core_web_sm"",
    output_type: type[str | list[str]] = str,
) -> tuple[str, list[tuple[str, str]]] | list[str]:
    """"""Process text using the OpenAI API for summarization or information extraction

    Params:
        text (str): The text to process.
        instruction (str): Additional instruction for processing.
        llm_provider: LLM provider to use.
        model_name: The name of the llm model to use.
        spacy_model: The spaCy model to use for sentence splitting.
        output_type: `str` for summaries or `list[str]` for piece-wise info extraction.

    Returns:
        For summarization: tuple[str, None | list[(summary, chunk)]]
        For piece-wise information extraction: list[str]
    """"""
    if not text.strip():
        raise ValueError(""No content"")

    text_tlength = llm_provider.count_tokens(text, model_name)
    logger.debug(f""Text length: {text_tlength} tokens"")

    max_result_tokens = 500
    max_chunk_length = llm_provider.get_token_limit(model_name) - max_result_tokens - 50
    logger.debug(f""Max chunk length: {max_chunk_length} tokens"")

    if text_tlength < max_chunk_length:
        prompt = ChatPrompt(
            messages=[
                ChatMessage.system(
                    ""The user is going to give you a text enclosed in triple quotes. ""
                    f""{instruction}""
                ),
                ChatMessage.user(f'""""""{text}""""""'),
            ]
        )

        logger.debug(f""PROCESSING:\n{prompt}"")

        response = await llm_provider.create_chat_completion(
            model_prompt=prompt.messages,
            model_name=model_name,
            temperature=0.5,
            max_output_tokens=max_result_tokens,
            completion_parser=lambda s: (
                extract_list_from_json(s.content) if output_type is not str else None
            ),
        )

        if isinstance(response.parsed_result, list):
            logger.debug(f""Raw LLM response: {repr(response.response.content)}"")
            fmt_result_bullet_list = ""\n"".join(f""* {r}"" for r in response.parsed_result)
            logger.debug(
                f""\n{'-'*11} EXTRACTION RESULT {'-'*12}\n""
                f""{fmt_result_bullet_list}\n""
                f""{'-'*42}\n""
            )
            return response.parsed_result
        else:
            summary = response.response.content
            logger.debug(f""\n{'-'*16} SUMMARY {'-'*17}\n{summary}\n{'-'*42}\n"")
            return summary.strip(), [(summary, text)]
    else:
        chunks = list(
            split_text(
                text,
                max_chunk_length=max_chunk_length,
                tokenizer=llm_provider.get_tokenizer(model_name),
                spacy_model=spacy_model,
            )
        )

        processed_results = []
        for i, (chunk, _) in enumerate(chunks):
            logger.info(f""Processing chunk {i + 1} / {len(chunks)}"")
            chunk_result = await _process_text(
                text=chunk,
                instruction=instruction,
                output_type=output_type,
                llm_provider=llm_provider,
                model_name=model_name,
                spacy_model=spacy_model,
            )
            processed_results.extend(
                chunk_result if output_type == list[str] else [chunk_result]
            )

        if output_type == list[str]:
            return processed_results
        else:
            summary, _ = await _process_text(
                ""\n\n"".join([result[0] for result in processed_results]),
                instruction=(
                    ""The text consists of multiple partial summaries. ""
                    ""Combine these partial summaries into one.""
                ),
                llm_provider=llm_provider,
                model_name=model_name,
                spacy_model=spacy_model,
            )
            return summary.strip(), [
                (processed_results[i], chunks[i][0]) for i in range(0, len(chunks))
            ]","Point(row=114, column=0)","Point(row=222, column=13)",,classic/forge/forge/content_processing/text.py
split_text,function,"
    Split text into chunks of sentences, with each chunk not exceeding the max length.

    Args:
        text (str): The text to split.
        spacy_model (str): The spaCy model to use for sentence splitting.
        max_chunk_length (int, optional): The maximum length of a chunk.
        tokenizer (ModelTokenizer): Tokenizer to use for determining chunk length.
        with_overlap (bool, optional): Whether to allow overlap between chunks.

    Yields:
        str: The next chunk of text

    Raises:
        ValueError: when a sentence is longer than the maximum length
","def split_text(
    text: str,
    max_chunk_length: int,
    tokenizer: ModelTokenizer,
    spacy_model: str = ""en_core_web_sm"",
    with_overlap: bool = True,
) -> Iterator[tuple[str, int]]:
    """"""
    Split text into chunks of sentences, with each chunk not exceeding the max length.

    Args:
        text (str): The text to split.
        spacy_model (str): The spaCy model to use for sentence splitting.
        max_chunk_length (int, optional): The maximum length of a chunk.
        tokenizer (ModelTokenizer): Tokenizer to use for determining chunk length.
        with_overlap (bool, optional): Whether to allow overlap between chunks.

    Yields:
        str: The next chunk of text

    Raises:
        ValueError: when a sentence is longer than the maximum length
    """"""
    text_length = len(tokenizer.encode(text))

    if text_length < max_chunk_length:
        yield text, text_length
        return

    n_chunks = math.ceil(text_length / max_chunk_length)
    target_chunk_length = math.ceil(text_length / n_chunks)

    nlp: spacy.language.Language = spacy.load(spacy_model)
    nlp.add_pipe(""sentencizer"")
    doc = nlp(text)
    sentences = [sentence.text.strip() for sentence in doc.sents]

    current_chunk: list[str] = []
    current_chunk_length = 0
    last_sentence = None
    last_sentence_length = 0

    i = 0
    while i < len(sentences):
        sentence = sentences[i]
        sentence_length = len(tokenizer.encode(sentence))
        expected_chunk_length = current_chunk_length + 1 + sentence_length

        if (
            expected_chunk_length < max_chunk_length
            # try to create chunks of approximately equal size
            and expected_chunk_length - (sentence_length / 2) < target_chunk_length
        ):
            current_chunk.append(sentence)
            current_chunk_length = expected_chunk_length

        elif sentence_length < max_chunk_length:
            if last_sentence:
                yield "" "".join(current_chunk), current_chunk_length
                current_chunk = []
                current_chunk_length = 0

                if with_overlap:
                    overlap_max_length = max_chunk_length - sentence_length - 1
                    if last_sentence_length < overlap_max_length:
                        current_chunk += [last_sentence]
                        current_chunk_length += last_sentence_length + 1
                    elif overlap_max_length > 5:
                        # add as much from the end of the last sentence as fits
                        current_chunk += [
                            list(
                                chunk_content(
                                    content=last_sentence,
                                    max_chunk_length=overlap_max_length,
                                    tokenizer=tokenizer,
                                )
                            ).pop()[0],
                        ]
                        current_chunk_length += overlap_max_length + 1

            current_chunk += [sentence]
            current_chunk_length += sentence_length

        else:  # sentence longer than maximum length -> chop up and try again
            sentences[i : i + 1] = [
                chunk
                for chunk, _ in chunk_content(sentence, target_chunk_length, tokenizer)
            ]
            continue

        i += 1
        last_sentence = sentence
        last_sentence_length = sentence_length

    if current_chunk:
        yield "" "".join(current_chunk), current_chunk_length","Point(row=225, column=0)","Point(row=320, column=59)",,classic/forge/forge/content_processing/text.py
fixable_json,function,,"def fixable_json(request: pytest.FixtureRequest) -> tuple[str, str]:
    return request.param","Point(row=78, column=0)","Point(row=79, column=24)",,classic/forge/forge/json/test_parsing.py
unfixable_json,function,,"def unfixable_json(request: pytest.FixtureRequest) -> tuple[str, str]:
    return request.param","Point(row=83, column=0)","Point(row=84, column=24)",,classic/forge/forge/json/test_parsing.py
test_json_loads_fixable,function,,"def test_json_loads_fixable(fixable_json: tuple[str, str]):
    assert json_loads(fixable_json[0]) == json.loads(fixable_json[1])","Point(row=87, column=0)","Point(row=88, column=69)",,classic/forge/forge/json/test_parsing.py
test_json_loads_unfixable,function,,"def test_json_loads_unfixable(unfixable_json: tuple[str, str]):
    assert json_loads(unfixable_json[0]) != json.loads(unfixable_json[1])","Point(row=91, column=0)","Point(row=92, column=73)",,classic/forge/forge/json/test_parsing.py
json_loads,function,"Parse a JSON string, tolerating minor syntax issues:
    - Missing, extra and trailing commas
    - Extraneous newlines and whitespace outside of string literals
    - Inconsistent spacing after colons and commas
    - Missing closing brackets or braces
    - Numbers: binary, hex, octal, trailing and prefixed decimal points
    - Different encodings
    - Surrounding markdown code block
    - Comments

    Args:
        json_str: The JSON string to parse.

    Returns:
        The parsed JSON object, same as built-in json.loads.
","def json_loads(json_str: str) -> Any:
    """"""Parse a JSON string, tolerating minor syntax issues:
    - Missing, extra and trailing commas
    - Extraneous newlines and whitespace outside of string literals
    - Inconsistent spacing after colons and commas
    - Missing closing brackets or braces
    - Numbers: binary, hex, octal, trailing and prefixed decimal points
    - Different encodings
    - Surrounding markdown code block
    - Comments

    Args:
        json_str: The JSON string to parse.

    Returns:
        The parsed JSON object, same as built-in json.loads.
    """"""
    # Remove possible code block
    pattern = r""```(?:json|JSON)*([\s\S]*?)```""
    match = re.search(pattern, json_str)

    if match:
        json_str = match.group(1).strip()

    json_result = demjson3.decode(json_str, return_errors=True)
    assert json_result is not None  # by virtue of return_errors=True

    if json_result.errors:
        logger.debug(
            ""JSON parse errors:\n"" + ""\n"".join(str(e) for e in json_result.errors)
        )

    if json_result.object in (demjson3.syntax_error, demjson3.undefined):
        raise ValueError(
            f""Failed to parse JSON string: {json_str}"", *json_result.errors
        )

    return json_result.object","Point(row=9, column=0)","Point(row=46, column=29)",,classic/forge/forge/json/parsing.py
extract_dict_from_json,function,,"def extract_dict_from_json(json_str: str) -> dict[str, Any]:
    # Sometimes the response includes the JSON in a code block with ```
    pattern = r""```(?:json|JSON)*([\s\S]*?)```""
    match = re.search(pattern, json_str)

    if match:
        json_str = match.group(1).strip()
    else:
        # The string may contain JSON.
        json_pattern = r""{[\s\S]*}""
        match = re.search(json_pattern, json_str)

        if match:
            json_str = match.group()

    result = json_loads(json_str)
    if not isinstance(result, dict):
        raise ValueError(
            f""Response '''{json_str}''' evaluated to non-dict value {repr(result)}""
        )
    return result","Point(row=49, column=0)","Point(row=69, column=17)",,classic/forge/forge/json/parsing.py
extract_list_from_json,function,,"def extract_list_from_json(json_str: str) -> list[Any]:
    # Sometimes the response includes the JSON in a code block with ```
    pattern = r""```(?:json|JSON)*([\s\S]*?)```""
    match = re.search(pattern, json_str)

    if match:
        json_str = match.group(1).strip()
    else:
        # The string may contain JSON.
        json_pattern = r""\[[\s\S]*\]""
        match = re.search(json_pattern, json_str)

        if match:
            json_str = match.group()

    result = json_loads(json_str)
    if not isinstance(result, list):
        raise ValueError(
            f""Response '''{json_str}''' evaluated to non-list value {repr(result)}""
        )
    return result","Point(row=72, column=0)","Point(row=92, column=17)",,classic/forge/forge/json/parsing.py
Command,class,"A class representing a command.

    Attributes:
        name (str): The name of the command.
        description (str): A brief description of what the command does.
        parameters (list): The parameters of the function that the command executes.
","class Command(Generic[P, CO]):
    """"""A class representing a command.

    Attributes:
        name (str): The name of the command.
        description (str): A brief description of what the command does.
        parameters (list): The parameters of the function that the command executes.
    """"""

    def __init__(
        self,
        names: list[str],
        description: str,
        method: Callable[Concatenate[_CP, P], CO],
        parameters: list[CommandParameter],
    ):
        # Check if all parameters are provided
        if not self._parameters_match(method, parameters):
            raise ValueError(
                f""Command {names[0]} has different parameters than provided schema""
            )
        self.names = names
        self.description = description
        # Method technically has a `self` parameter, but we can ignore that
        # since Python passes it internally.
        self.method = cast(Callable[P, CO], method)
        self.parameters = parameters

    @property
    def is_async(self) -> bool:
        return inspect.iscoroutinefunction(self.method)

    def _parameters_match(
        self, func: Callable, parameters: list[CommandParameter]
    ) -> bool:
        # Get the function's signature
        signature = inspect.signature(func)
        # Extract parameter names, ignoring 'self' for methods
        func_param_names = [
            param.name
            for param in signature.parameters.values()
            if param.name != ""self""
        ]
        names = [param.name for param in parameters]
        # Check if sorted lists of names/keys are equal
        return sorted(func_param_names) == sorted(names)

    def __call__(self, *args: P.args, **kwargs: P.kwargs) -> CO:
        return self.method(*args, **kwargs)

    def __str__(self) -> str:
        params = [
            f""{param.name}: ""
            + (""%s"" if param.spec.required else ""Optional[%s]"")
            % (param.spec.type.value if param.spec.type else ""Any"")
            for param in self.parameters
        ]
        return (
            f""{self.names[0]}: {self.description.rstrip('.')}. ""
            f""Params: ({', '.join(params)})""
        )

    def __get__(self, instance, owner):
        if instance is None:
            # Accessed on the class, not an instance
            return self
        # Bind the method to the instance
        return Command(
            self.names,
            self.description,
            self.method.__get__(instance, owner),
            self.parameters,
        )","Point(row=15, column=0)","Point(row=87, column=9)",,classic/forge/forge/command/command.py
Command.__init__,function,,"def __init__(
        self,
        names: list[str],
        description: str,
        method: Callable[Concatenate[_CP, P], CO],
        parameters: list[CommandParameter],
    ):
        # Check if all parameters are provided
        if not self._parameters_match(method, parameters):
            raise ValueError(
                f""Command {names[0]} has different parameters than provided schema""
            )
        self.names = names
        self.description = description
        # Method technically has a `self` parameter, but we can ignore that
        # since Python passes it internally.
        self.method = cast(Callable[P, CO], method)
        self.parameters = parameters","Point(row=24, column=4)","Point(row=41, column=36)",Command,classic/forge/forge/command/command.py
Command.is_async,function,,"def is_async(self) -> bool:
        return inspect.iscoroutinefunction(self.method)","Point(row=44, column=4)","Point(row=45, column=55)",Command,classic/forge/forge/command/command.py
Command._parameters_match,function,,"def _parameters_match(
        self, func: Callable, parameters: list[CommandParameter]
    ) -> bool:
        # Get the function's signature
        signature = inspect.signature(func)
        # Extract parameter names, ignoring 'self' for methods
        func_param_names = [
            param.name
            for param in signature.parameters.values()
            if param.name != ""self""
        ]
        names = [param.name for param in parameters]
        # Check if sorted lists of names/keys are equal
        return sorted(func_param_names) == sorted(names)","Point(row=47, column=4)","Point(row=60, column=56)",Command,classic/forge/forge/command/command.py
Command.__call__,function,,"def __call__(self, *args: P.args, **kwargs: P.kwargs) -> CO:
        return self.method(*args, **kwargs)","Point(row=62, column=4)","Point(row=63, column=43)",Command,classic/forge/forge/command/command.py
Command.__str__,function,,"def __str__(self) -> str:
        params = [
            f""{param.name}: ""
            + (""%s"" if param.spec.required else ""Optional[%s]"")
            % (param.spec.type.value if param.spec.type else ""Any"")
            for param in self.parameters
        ]
        return (
            f""{self.names[0]}: {self.description.rstrip('.')}. ""
            f""Params: ({', '.join(params)})""
        )","Point(row=65, column=4)","Point(row=75, column=9)",Command,classic/forge/forge/command/command.py
Command.__get__,function,,"def __get__(self, instance, owner):
        if instance is None:
            # Accessed on the class, not an instance
            return self
        # Bind the method to the instance
        return Command(
            self.names,
            self.description,
            self.method.__get__(instance, owner),
            self.parameters,
        )","Point(row=77, column=4)","Point(row=87, column=9)",Command,classic/forge/forge/command/command.py
command,function,"
    The command decorator is used to make a Command from a function.

    Args:
        names (list[str]): The names of the command.
            If not provided, the function name will be used.
        description (str): A brief description of what the command does.
            If not provided, the docstring until double line break will be used
            (or entire docstring if no double line break is found)
        parameters (dict[str, JSONSchema]): The parameters of the function
            that the command executes.
","def command(
    names: list[str] = [],
    description: Optional[str] = None,
    parameters: dict[str, JSONSchema] = {},
) -> Callable[[Callable[Concatenate[_CP, P], CO]], Command[P, CO]]:
    """"""
    The command decorator is used to make a Command from a function.

    Args:
        names (list[str]): The names of the command.
            If not provided, the function name will be used.
        description (str): A brief description of what the command does.
            If not provided, the docstring until double line break will be used
            (or entire docstring if no double line break is found)
        parameters (dict[str, JSONSchema]): The parameters of the function
            that the command executes.
    """"""

    def decorator(func: Callable[Concatenate[_CP, P], CO]) -> Command[P, CO]:
        doc = func.__doc__ or """"
        # If names is not provided, use the function name
        command_names = names or [func.__name__]
        # If description is not provided, use the first part of the docstring
        if not (command_description := description):
            if not func.__doc__:
                raise ValueError(""Description is required if function has no docstring"")
            # Return the part of the docstring before double line break or everything
            command_description = re.sub(r""\s+"", "" "", doc.split(""\n\n"")[0].strip())

        # Parameters
        typed_parameters = [
            CommandParameter(
                name=param_name,
                spec=spec,
            )
            for param_name, spec in parameters.items()
        ]

        # Wrap func with Command
        command = Command(
            names=command_names,
            description=command_description,
            method=func,
            parameters=typed_parameters,
        )

        return command

    return decorator","Point(row=11, column=0)","Point(row=59, column=20)",,classic/forge/forge/command/decorator.py
command.decorator,function,,"def decorator(func: Callable[Concatenate[_CP, P], CO]) -> Command[P, CO]:
        doc = func.__doc__ or """"
        # If names is not provided, use the function name
        command_names = names or [func.__name__]
        # If description is not provided, use the first part of the docstring
        if not (command_description := description):
            if not func.__doc__:
                raise ValueError(""Description is required if function has no docstring"")
            # Return the part of the docstring before double line break or everything
            command_description = re.sub(r""\s+"", "" "", doc.split(""\n\n"")[0].strip())

        # Parameters
        typed_parameters = [
            CommandParameter(
                name=param_name,
                spec=spec,
            )
            for param_name, spec in parameters.items()
        ]

        # Wrap func with Command
        command = Command(
            names=command_names,
            description=command_description,
            method=func,
            parameters=typed_parameters,
        )

        return command","Point(row=29, column=4)","Point(row=57, column=22)",,classic/forge/forge/command/decorator.py
CommandParameter,class,,"class CommandParameter(BaseModel):
    name: str
    spec: JSONSchema

    def __repr__(self):
        return ""CommandParameter('%s', '%s', '%s', %s)"" % (
            self.name,
            self.spec.type,
            self.spec.description,
            self.spec.required,
        )","Point(row=5, column=0)","Point(row=15, column=9)",,classic/forge/forge/command/parameter.py
CommandParameter.__repr__,function,,"def __repr__(self):
        return ""CommandParameter('%s', '%s', '%s', %s)"" % (
            self.name,
            self.spec.type,
            self.spec.description,
            self.spec.required,
        )","Point(row=9, column=4)","Point(row=15, column=9)",CommandParameter,classic/forge/forge/command/parameter.py
root,function,"
    Root endpoint that returns a welcome message.
","async def root():
    """"""
    Root endpoint that returns a welcome message.
    """"""
    return Response(content=""Welcome to the AutoGPT Forge"")","Point(row=33, column=0)","Point(row=37, column=59)",,classic/forge/forge/agent_protocol/api_router.py
check_server_status,function,"
    Check if the server is running.
","async def check_server_status():
    """"""
    Check if the server is running.
    """"""
    return Response(content=""Server is running."", status_code=200)","Point(row=41, column=0)","Point(row=45, column=66)",,classic/forge/forge/agent_protocol/api_router.py
create_agent_task,function,"
    Creates a new task using the provided TaskRequestBody and returns a Task.

    Args:
        request (Request): FastAPI request object.
        task (TaskRequestBody): The task request containing input data.

    Returns:
        Task: A new task with task_id, input, and additional_input set.

    Example:
        Request (TaskRequestBody defined in schema.py):
            {
                ""input"": ""Write the words you receive to the file 'output.txt'."",
                ""additional_input"": ""python/code""
            }

        Response (Task defined in schema.py):
            {
                ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                ""input"": ""Write the word 'Washington' to a .txt file"",
                ""additional_input"": ""python/code"",
                ""artifacts"": [],
            }
","async def create_agent_task(request: Request, task_request: TaskRequestBody) -> Task:
    """"""
    Creates a new task using the provided TaskRequestBody and returns a Task.

    Args:
        request (Request): FastAPI request object.
        task (TaskRequestBody): The task request containing input data.

    Returns:
        Task: A new task with task_id, input, and additional_input set.

    Example:
        Request (TaskRequestBody defined in schema.py):
            {
                ""input"": ""Write the words you receive to the file 'output.txt'."",
                ""additional_input"": ""python/code""
            }

        Response (Task defined in schema.py):
            {
                ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                ""input"": ""Write the word 'Washington' to a .txt file"",
                ""additional_input"": ""python/code"",
                ""artifacts"": [],
            }
    """"""
    agent: ""ProtocolAgent"" = request[""agent""]

    try:
        task = await agent.create_task(task_request)
        return task
    except Exception:
        logger.exception(f""Error whilst trying to create a task: {task_request}"")
        raise","Point(row=49, column=0)","Point(row=82, column=13)",,classic/forge/forge/agent_protocol/api_router.py
list_agent_tasks,function,"
    Retrieves a paginated list of all tasks.

    Args:
        request (Request): FastAPI request object.
        page (int, optional): Page number for pagination. Default: 1
        page_size (int, optional): Number of tasks per page for pagination. Default: 10

    Returns:
        TaskListResponse: A list of tasks, and pagination details.

    Example:
        Request:
            GET /agent/tasks?page=1&pageSize=10

        Response (TaskListResponse defined in schema.py):
            {
                ""items"": [
                    {
                        ""input"": ""Write the word 'Washington' to a .txt file"",
                        ""additional_input"": null,
                        ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                        ""artifacts"": [],
                        ""steps"": []
                    },
                    ...
                ],
                ""pagination"": {
                    ""total"": 100,
                    ""pages"": 10,
                    ""current"": 1,
                    ""pageSize"": 10
                }
            }
","async def list_agent_tasks(
    request: Request,
    page: int = Query(1, ge=1),
    page_size: int = Query(10, ge=1),
) -> TaskListResponse:
    """"""
    Retrieves a paginated list of all tasks.

    Args:
        request (Request): FastAPI request object.
        page (int, optional): Page number for pagination. Default: 1
        page_size (int, optional): Number of tasks per page for pagination. Default: 10

    Returns:
        TaskListResponse: A list of tasks, and pagination details.

    Example:
        Request:
            GET /agent/tasks?page=1&pageSize=10

        Response (TaskListResponse defined in schema.py):
            {
                ""items"": [
                    {
                        ""input"": ""Write the word 'Washington' to a .txt file"",
                        ""additional_input"": null,
                        ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                        ""artifacts"": [],
                        ""steps"": []
                    },
                    ...
                ],
                ""pagination"": {
                    ""total"": 100,
                    ""pages"": 10,
                    ""current"": 1,
                    ""pageSize"": 10
                }
            }
    """"""
    agent: ""ProtocolAgent"" = request[""agent""]
    try:
        tasks = await agent.list_tasks(page, page_size)
        return tasks
    except Exception:
        logger.exception(""Error whilst trying to list tasks"")
        raise","Point(row=86, column=0)","Point(row=132, column=13)",,classic/forge/forge/agent_protocol/api_router.py
get_agent_task,function,"
    Gets the details of a task by ID.

    Args:
        request (Request): FastAPI request object.
        task_id (str): The ID of the task.

    Returns:
        Task: The task with the given ID.

    Example:
        Request:
            GET /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb

        Response (Task defined in schema.py):
            {
                ""input"": ""Write the word 'Washington' to a .txt file"",
                ""additional_input"": null,
                ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                ""artifacts"": [
                    {
                        ""artifact_id"": ""7a49f31c-f9c6-4346-a22c-e32bc5af4d8e"",
                        ""file_name"": ""output.txt"",
                        ""agent_created"": true,
                        ""relative_path"": ""file://50da533e-3904-4401-8a07-c49adf88b5eb/output.txt""
                    }
                ],
                ""steps"": [
                    {
                        ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                        ""step_id"": ""6bb1801a-fd80-45e8-899a-4dd723cc602e"",
                        ""input"": ""Write the word 'Washington' to a .txt file"",
                        ""additional_input"": ""challenge:write_to_file"",
                        ""name"": ""Write to file"",
                        ""status"": ""completed"",
                        ""output"": ""I am going to use the write_to_file command and write Washington to a file called output.txt <write_to_file('output.txt', 'Washington')>"",
                        ""additional_output"": ""Do you want me to continue?"",
                        ""artifacts"": [
                            {
                                ""artifact_id"": ""7a49f31c-f9c6-4346-a22c-e32bc5af4d8e"",
                                ""file_name"": ""output.txt"",
                                ""agent_created"": true,
                                ""relative_path"": ""file://50da533e-3904-4401-8a07-c49adf88b5eb/output.txt""
                            }
                        ],
                        ""is_last"": true
                    }
                ]
            }
","async def get_agent_task(request: Request, task_id: str) -> Task:
    """"""
    Gets the details of a task by ID.

    Args:
        request (Request): FastAPI request object.
        task_id (str): The ID of the task.

    Returns:
        Task: The task with the given ID.

    Example:
        Request:
            GET /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb

        Response (Task defined in schema.py):
            {
                ""input"": ""Write the word 'Washington' to a .txt file"",
                ""additional_input"": null,
                ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                ""artifacts"": [
                    {
                        ""artifact_id"": ""7a49f31c-f9c6-4346-a22c-e32bc5af4d8e"",
                        ""file_name"": ""output.txt"",
                        ""agent_created"": true,
                        ""relative_path"": ""file://50da533e-3904-4401-8a07-c49adf88b5eb/output.txt""
                    }
                ],
                ""steps"": [
                    {
                        ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                        ""step_id"": ""6bb1801a-fd80-45e8-899a-4dd723cc602e"",
                        ""input"": ""Write the word 'Washington' to a .txt file"",
                        ""additional_input"": ""challenge:write_to_file"",
                        ""name"": ""Write to file"",
                        ""status"": ""completed"",
                        ""output"": ""I am going to use the write_to_file command and write Washington to a file called output.txt <write_to_file('output.txt', 'Washington')>"",
                        ""additional_output"": ""Do you want me to continue?"",
                        ""artifacts"": [
                            {
                                ""artifact_id"": ""7a49f31c-f9c6-4346-a22c-e32bc5af4d8e"",
                                ""file_name"": ""output.txt"",
                                ""agent_created"": true,
                                ""relative_path"": ""file://50da533e-3904-4401-8a07-c49adf88b5eb/output.txt""
                            }
                        ],
                        ""is_last"": true
                    }
                ]
            }
    """"""  # noqa: E501
    agent: ""ProtocolAgent"" = request[""agent""]
    try:
        task = await agent.get_task(task_id)
        return task
    except Exception:
        logger.exception(f""Error whilst trying to get task: {task_id}"")
        raise","Point(row=136, column=0)","Point(row=193, column=13)",,classic/forge/forge/agent_protocol/api_router.py
list_agent_task_steps,function,"
    Retrieves a paginated list of steps associated with a specific task.

    Args:
        request (Request): FastAPI request object.
        task_id (str): The ID of the task.
        page (int, optional): The page number for pagination. Defaults to 1.
        page_size (int, optional): Number of steps per page for pagination. Default: 10.

    Returns:
        TaskStepsListResponse: A list of steps, and pagination details.

    Example:
        Request:
            GET /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/steps?page=1&pageSize=10

        Response (TaskStepsListResponse defined in schema.py):
            {
                ""items"": [
                    {
                        ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                        ""step_id"": ""step1_id"",
                        ...
                    },
                    ...
                ],
                ""pagination"": {
                    ""total"": 100,
                    ""pages"": 10,
                    ""current"": 1,
                    ""pageSize"": 10
                }
            }
","async def list_agent_task_steps(
    request: Request,
    task_id: str,
    page: int = Query(1, ge=1),
    page_size: int = Query(10, ge=1, alias=""pageSize""),
) -> TaskStepsListResponse:
    """"""
    Retrieves a paginated list of steps associated with a specific task.

    Args:
        request (Request): FastAPI request object.
        task_id (str): The ID of the task.
        page (int, optional): The page number for pagination. Defaults to 1.
        page_size (int, optional): Number of steps per page for pagination. Default: 10.

    Returns:
        TaskStepsListResponse: A list of steps, and pagination details.

    Example:
        Request:
            GET /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/steps?page=1&pageSize=10

        Response (TaskStepsListResponse defined in schema.py):
            {
                ""items"": [
                    {
                        ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                        ""step_id"": ""step1_id"",
                        ...
                    },
                    ...
                ],
                ""pagination"": {
                    ""total"": 100,
                    ""pages"": 10,
                    ""current"": 1,
                    ""pageSize"": 10
                }
            }
    """"""  # noqa: E501
    agent: ""ProtocolAgent"" = request[""agent""]
    try:
        steps = await agent.list_steps(task_id, page, page_size)
        return steps
    except Exception:
        logger.exception(""Error whilst trying to list steps"")
        raise","Point(row=201, column=0)","Point(row=247, column=13)",,classic/forge/forge/agent_protocol/api_router.py
execute_agent_task_step,function,"
    Executes the next step for a specified task based on the current task status and
    returns the executed step with additional feedback fields.

    This route is significant because this is where the agent actually performs work.
    The function handles executing the next step for a task based on its current state,
    and it requires careful implementation to ensure all scenarios (like the presence
    or absence of steps or a step marked as `last_step`) are handled correctly.

    Depending on the current state of the task, the following scenarios are possible:
    1. No steps exist for the task.
    2. There is at least one step already for the task, and the task does not have a
       completed step marked as `last_step`.
    3. There is a completed step marked as `last_step` already on the task.

    In each of these scenarios, a step object will be returned with two additional
    fields: `output` and `additional_output`.
    - `output`: Provides the primary response or feedback to the user.
    - `additional_output`: Supplementary information or data. Its specific content is
      not strictly defined and can vary based on the step or agent's implementation.

    Args:
        request (Request): FastAPI request object.
        task_id (str): The ID of the task.
        step (StepRequestBody): The details for executing the step.

    Returns:
        Step: Details of the executed step with additional feedback.

    Example:
        Request:
            POST /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/steps
            {
                ""input"": ""Step input details..."",
                ...
            }

        Response:
            {
                ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                ""step_id"": ""step1_id"",
                ""output"": ""Primary feedback..."",
                ""additional_output"": ""Supplementary details..."",
                ...
            }
","async def execute_agent_task_step(
    request: Request, task_id: str, step_request: Optional[StepRequestBody] = None
) -> Step:
    """"""
    Executes the next step for a specified task based on the current task status and
    returns the executed step with additional feedback fields.

    This route is significant because this is where the agent actually performs work.
    The function handles executing the next step for a task based on its current state,
    and it requires careful implementation to ensure all scenarios (like the presence
    or absence of steps or a step marked as `last_step`) are handled correctly.

    Depending on the current state of the task, the following scenarios are possible:
    1. No steps exist for the task.
    2. There is at least one step already for the task, and the task does not have a
       completed step marked as `last_step`.
    3. There is a completed step marked as `last_step` already on the task.

    In each of these scenarios, a step object will be returned with two additional
    fields: `output` and `additional_output`.
    - `output`: Provides the primary response or feedback to the user.
    - `additional_output`: Supplementary information or data. Its specific content is
      not strictly defined and can vary based on the step or agent's implementation.

    Args:
        request (Request): FastAPI request object.
        task_id (str): The ID of the task.
        step (StepRequestBody): The details for executing the step.

    Returns:
        Step: Details of the executed step with additional feedback.

    Example:
        Request:
            POST /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/steps
            {
                ""input"": ""Step input details..."",
                ...
            }

        Response:
            {
                ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                ""step_id"": ""step1_id"",
                ""output"": ""Primary feedback..."",
                ""additional_output"": ""Supplementary details..."",
                ...
            }
    """"""
    agent: ""ProtocolAgent"" = request[""agent""]
    try:
        # An empty step request represents a yes to continue command
        if not step_request:
            step_request = StepRequestBody(input=""y"")

        step = await agent.execute_step(task_id, step_request)
        return step
    except Exception:
        logger.exception(f""Error whilst trying to execute a task step: {task_id}"")
        raise","Point(row=251, column=0)","Point(row=310, column=13)",,classic/forge/forge/agent_protocol/api_router.py
get_agent_task_step,function,"
    Retrieves the details of a specific step for a given task.

    Args:
        request (Request): FastAPI request object.
        task_id (str): The ID of the task.
        step_id (str): The ID of the step.

    Returns:
        Step: Details of the specific step.

    Example:
        Request:
            GET /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/steps/step1_id

        Response:
            {
                ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                ""step_id"": ""step1_id"",
                ...
            }
","async def get_agent_task_step(request: Request, task_id: str, step_id: str) -> Step:
    """"""
    Retrieves the details of a specific step for a given task.

    Args:
        request (Request): FastAPI request object.
        task_id (str): The ID of the task.
        step_id (str): The ID of the step.

    Returns:
        Step: Details of the specific step.

    Example:
        Request:
            GET /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/steps/step1_id

        Response:
            {
                ""task_id"": ""50da533e-3904-4401-8a07-c49adf88b5eb"",
                ""step_id"": ""step1_id"",
                ...
            }
    """"""
    agent: ""ProtocolAgent"" = request[""agent""]
    try:
        step = await agent.get_step(task_id, step_id)
        return step
    except Exception:
        logger.exception(f""Error whilst trying to get step: {step_id}"")
        raise","Point(row=316, column=0)","Point(row=345, column=13)",,classic/forge/forge/agent_protocol/api_router.py
list_agent_task_artifacts,function,"
    Retrieves a paginated list of artifacts associated with a specific task.

    Args:
        request (Request): FastAPI request object.
        task_id (str): The ID of the task.
        page (int, optional): The page number for pagination. Defaults to 1.
        page_size (int, optional): Number of items per page for pagination. Default: 10.

    Returns:
        TaskArtifactsListResponse: A list of artifacts, and pagination details.

    Example:
        Request:
            GET /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/artifacts?page=1&pageSize=10

        Response (TaskArtifactsListResponse defined in schema.py):
            {
                ""items"": [
                    {""artifact_id"": ""artifact1_id"", ...},
                    {""artifact_id"": ""artifact2_id"", ...},
                    ...
                ],
                ""pagination"": {
                    ""total"": 100,
                    ""pages"": 10,
                    ""current"": 1,
                    ""pageSize"": 10
                }
            }
","async def list_agent_task_artifacts(
    request: Request,
    task_id: str,
    page: int = Query(1, ge=1),
    page_size: int = Query(10, ge=1, alias=""pageSize""),
) -> TaskArtifactsListResponse:
    """"""
    Retrieves a paginated list of artifacts associated with a specific task.

    Args:
        request (Request): FastAPI request object.
        task_id (str): The ID of the task.
        page (int, optional): The page number for pagination. Defaults to 1.
        page_size (int, optional): Number of items per page for pagination. Default: 10.

    Returns:
        TaskArtifactsListResponse: A list of artifacts, and pagination details.

    Example:
        Request:
            GET /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/artifacts?page=1&pageSize=10

        Response (TaskArtifactsListResponse defined in schema.py):
            {
                ""items"": [
                    {""artifact_id"": ""artifact1_id"", ...},
                    {""artifact_id"": ""artifact2_id"", ...},
                    ...
                ],
                ""pagination"": {
                    ""total"": 100,
                    ""pages"": 10,
                    ""current"": 1,
                    ""pageSize"": 10
                }
            }
    """"""  # noqa: E501
    agent: ""ProtocolAgent"" = request[""agent""]
    try:
        artifacts = await agent.list_artifacts(task_id, page, page_size)
        return artifacts
    except Exception:
        logger.exception(""Error whilst trying to list artifacts"")
        raise","Point(row=353, column=0)","Point(row=396, column=13)",,classic/forge/forge/agent_protocol/api_router.py
upload_agent_task_artifacts,function,"
    This endpoint is used to upload an artifact (file) associated with a specific task.

    Args:
        request (Request): The FastAPI request object.
        task_id (str): The ID of the task for which the artifact is being uploaded.
        file (UploadFile): The file being uploaded as an artifact.
        relative_path (str): The relative path for the file. This is a query parameter.

    Returns:
        Artifact: Metadata object for the uploaded artifact, including its ID and path.

    Example:
        Request:
            POST /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/artifacts?relative_path=my_folder/my_other_folder
            File: <uploaded_file>

        Response:
            {
                ""artifact_id"": ""b225e278-8b4c-4f99-a696-8facf19f0e56"",
                ""created_at"": ""2023-01-01T00:00:00Z"",
                ""modified_at"": ""2023-01-01T00:00:00Z"",
                ""agent_created"": false,
                ""relative_path"": ""/my_folder/my_other_folder/"",
                ""file_name"": ""main.py""
            }
","async def upload_agent_task_artifacts(
    request: Request, task_id: str, file: UploadFile, relative_path: str = """"
) -> Artifact:
    """"""
    This endpoint is used to upload an artifact (file) associated with a specific task.

    Args:
        request (Request): The FastAPI request object.
        task_id (str): The ID of the task for which the artifact is being uploaded.
        file (UploadFile): The file being uploaded as an artifact.
        relative_path (str): The relative path for the file. This is a query parameter.

    Returns:
        Artifact: Metadata object for the uploaded artifact, including its ID and path.

    Example:
        Request:
            POST /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/artifacts?relative_path=my_folder/my_other_folder
            File: <uploaded_file>

        Response:
            {
                ""artifact_id"": ""b225e278-8b4c-4f99-a696-8facf19f0e56"",
                ""created_at"": ""2023-01-01T00:00:00Z"",
                ""modified_at"": ""2023-01-01T00:00:00Z"",
                ""agent_created"": false,
                ""relative_path"": ""/my_folder/my_other_folder/"",
                ""file_name"": ""main.py""
            }
    """"""  # noqa: E501
    agent: ""ProtocolAgent"" = request[""agent""]

    if file is None:
        raise HTTPException(status_code=400, detail=""File must be specified"")
    try:
        artifact = await agent.create_artifact(task_id, file, relative_path)
        return artifact
    except Exception:
        logger.exception(f""Error whilst trying to upload artifact: {task_id}"")
        raise","Point(row=402, column=0)","Point(row=441, column=13)",,classic/forge/forge/agent_protocol/api_router.py
download_agent_task_artifact,function,"
    Downloads an artifact associated with a specific task.

    Args:
        request (Request): FastAPI request object.
        task_id (str): The ID of the task.
        artifact_id (str): The ID of the artifact.

    Returns:
        FileResponse: The downloaded artifact file.

    Example:
        Request:
            GET /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/artifacts/artifact1_id

        Response:
            <file_content_of_artifact>
","async def download_agent_task_artifact(
    request: Request, task_id: str, artifact_id: str
) -> StreamingResponse:
    """"""
    Downloads an artifact associated with a specific task.

    Args:
        request (Request): FastAPI request object.
        task_id (str): The ID of the task.
        artifact_id (str): The ID of the artifact.

    Returns:
        FileResponse: The downloaded artifact file.

    Example:
        Request:
            GET /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/artifacts/artifact1_id

        Response:
            <file_content_of_artifact>
    """"""
    agent: ""ProtocolAgent"" = request[""agent""]
    try:
        return await agent.get_artifact(task_id, artifact_id)
    except Exception:
        logger.exception(f""Error whilst trying to download artifact: {task_id}"")
        raise","Point(row=449, column=0)","Point(row=475, column=13)",,classic/forge/forge/agent_protocol/api_router.py
AgentMiddleware,class,"
    Middleware that injects the agent instance into the request scope.
","class AgentMiddleware:
    """"""
    Middleware that injects the agent instance into the request scope.
    """"""

    def __init__(self, app: ASGIApp, agent):
        """"""

        Args:
            app: The FastAPI app - automatically injected by FastAPI.
            agent: The agent instance to inject into the request scope.

        Examples:
            >>> from fastapi import FastAPI, Request
            >>> from agent_protocol.agent import Agent
            >>> from agent_protocol.middlewares import AgentMiddleware
            >>> app = FastAPI()
            >>> @app.get(""/"")
            >>> async def root(request: Request):
            >>>     agent = request[""agent""]
            >>>     task = agent.db.create_task(""Do something."")
            >>>     return {""task_id"": a.task_id}
            >>> agent = Agent()
            >>> app.add_middleware(AgentMiddleware, agent=agent)
        """"""
        self.app = app
        self.agent = agent

    async def __call__(self, scope, receive, send):
        scope[""agent""] = self.agent
        await self.app(scope, receive, send)","Point(row=3, column=0)","Point(row=33, column=44)",,classic/forge/forge/agent_protocol/middlewares.py
AgentMiddleware.__init__,function,"

        Args:
            app: The FastAPI app - automatically injected by FastAPI.
            agent: The agent instance to inject into the request scope.

        Examples:
            >>> from fastapi import FastAPI, Request
            >>> from agent_protocol.agent import Agent
            >>> from agent_protocol.middlewares import AgentMiddleware
            >>> app = FastAPI()
            >>> @app.get(""/"")
            >>> async def root(request: Request):
            >>>     agent = request[""agent""]
            >>>     task = agent.db.create_task(""Do something."")
            >>>     return {""task_id"": a.task_id}
            >>> agent = Agent()
            >>> app.add_middleware(AgentMiddleware, agent=agent)
","def __init__(self, app: ASGIApp, agent):
        """"""

        Args:
            app: The FastAPI app - automatically injected by FastAPI.
            agent: The agent instance to inject into the request scope.

        Examples:
            >>> from fastapi import FastAPI, Request
            >>> from agent_protocol.agent import Agent
            >>> from agent_protocol.middlewares import AgentMiddleware
            >>> app = FastAPI()
            >>> @app.get(""/"")
            >>> async def root(request: Request):
            >>>     agent = request[""agent""]
            >>>     task = agent.db.create_task(""Do something."")
            >>>     return {""task_id"": a.task_id}
            >>> agent = Agent()
            >>> app.add_middleware(AgentMiddleware, agent=agent)
        """"""
        self.app = app
        self.agent = agent","Point(row=8, column=4)","Point(row=29, column=26)",AgentMiddleware,classic/forge/forge/agent_protocol/middlewares.py
AgentMiddleware.__call__,function,,"async def __call__(self, scope, receive, send):
        scope[""agent""] = self.agent
        await self.app(scope, receive, send)","Point(row=31, column=4)","Point(row=33, column=44)",AgentMiddleware,classic/forge/forge/agent_protocol/middlewares.py
agent,function,,"def agent(tmp_project_root: Path):
    db = AgentDB(""sqlite:///test.db"")
    config = FileStorageConfiguration(root=tmp_project_root)
    workspace = LocalFileStorage(config)
    return ProtocolAgent(db, workspace)","Point(row=14, column=0)","Point(row=18, column=39)",,classic/forge/forge/agent_protocol/agent_test.py
file_upload,function,,"def file_upload():
    this_file = Path(__file__)
    file_handle = this_file.open(""rb"")
    yield UploadFile(file_handle, filename=this_file.name)
    file_handle.close()","Point(row=22, column=0)","Point(row=26, column=23)",,classic/forge/forge/agent_protocol/agent_test.py
test_create_task,function,,"async def test_create_task(agent: ProtocolAgent):
    task_request = TaskRequestBody(
        input=""test_input"", additional_input={""input"": ""additional_test_input""}
    )
    task: Task = await agent.create_task(task_request)
    assert task.input == ""test_input""","Point(row=30, column=0)","Point(row=35, column=37)",,classic/forge/forge/agent_protocol/agent_test.py
test_list_tasks,function,,"async def test_list_tasks(agent: ProtocolAgent):
    task_request = TaskRequestBody(
        input=""test_input"", additional_input={""input"": ""additional_test_input""}
    )
    await agent.create_task(task_request)
    tasks = await agent.list_tasks()
    assert isinstance(tasks, TaskListResponse)","Point(row=39, column=0)","Point(row=45, column=46)",,classic/forge/forge/agent_protocol/agent_test.py
test_get_task,function,,"async def test_get_task(agent: ProtocolAgent):
    task_request = TaskRequestBody(
        input=""test_input"", additional_input={""input"": ""additional_test_input""}
    )
    task = await agent.create_task(task_request)
    retrieved_task = await agent.get_task(task.task_id)
    assert retrieved_task.task_id == task.task_id","Point(row=49, column=0)","Point(row=55, column=49)",,classic/forge/forge/agent_protocol/agent_test.py
test_execute_step,function,,"async def test_execute_step(agent: ProtocolAgent):
    task_request = TaskRequestBody(
        input=""test_input"", additional_input={""input"": ""additional_test_input""}
    )
    task = await agent.create_task(task_request)
    step_request = StepRequestBody(
        input=""step_input"", additional_input={""input"": ""additional_test_input""}
    )
    step = await agent.execute_step(task.task_id, step_request)
    assert step.input == ""step_input""
    assert step.additional_input == {""input"": ""additional_test_input""}","Point(row=60, column=0)","Point(row=70, column=70)",,classic/forge/forge/agent_protocol/agent_test.py
test_get_step,function,,"async def test_get_step(agent: ProtocolAgent):
    task_request = TaskRequestBody(
        input=""test_input"", additional_input={""input"": ""additional_test_input""}
    )
    task = await agent.create_task(task_request)
    step_request = StepRequestBody(
        input=""step_input"", additional_input={""input"": ""additional_test_input""}
    )
    step = await agent.execute_step(task.task_id, step_request)
    retrieved_step = await agent.get_step(task.task_id, step.step_id)
    assert retrieved_step.step_id == step.step_id","Point(row=75, column=0)","Point(row=85, column=49)",,classic/forge/forge/agent_protocol/agent_test.py
test_list_artifacts,function,,"async def test_list_artifacts(agent: ProtocolAgent):
    tasks = await agent.list_tasks()
    assert tasks.tasks, ""No tasks in test.db""

    artifacts = await agent.list_artifacts(tasks.tasks[0].task_id)
    assert isinstance(artifacts.artifacts, list)","Point(row=89, column=0)","Point(row=94, column=48)",,classic/forge/forge/agent_protocol/agent_test.py
test_create_artifact,function,,"async def test_create_artifact(agent: ProtocolAgent, file_upload: UploadFile):
    task_request = TaskRequestBody(
        input=""test_input"", additional_input={""input"": ""additional_test_input""}
    )
    task = await agent.create_task(task_request)
    artifact = await agent.create_artifact(
        task_id=task.task_id,
        file=file_upload,
        relative_path=f""a_dir/{file_upload.filename}"",
    )
    assert artifact.file_name == file_upload.filename
    assert artifact.relative_path == f""a_dir/{file_upload.filename}""","Point(row=98, column=0)","Point(row=109, column=68)",,classic/forge/forge/agent_protocol/agent_test.py
test_create_and_get_artifact,function,,"async def test_create_and_get_artifact(agent: ProtocolAgent, file_upload: UploadFile):
    task_request = TaskRequestBody(
        input=""test_input"", additional_input={""input"": ""additional_test_input""}
    )
    task = await agent.create_task(task_request)

    artifact = await agent.create_artifact(
        task_id=task.task_id,
        file=file_upload,
        relative_path=f""b_dir/{file_upload.filename}"",
    )
    await file_upload.seek(0)
    file_upload_content = await file_upload.read()

    retrieved_artifact = await agent.get_artifact(task.task_id, artifact.artifact_id)
    retrieved_artifact_content = bytearray()
    async for b in retrieved_artifact.body_iterator:
        retrieved_artifact_content.extend(b)  # type: ignore
    assert retrieved_artifact_content == file_upload_content","Point(row=113, column=0)","Point(row=131, column=60)",,classic/forge/forge/agent_protocol/agent_test.py
ProtocolAgent,class,,"class ProtocolAgent:
    def __init__(self, database: AgentDB, workspace: FileStorage):
        self.db = database
        self.workspace = workspace

    def get_agent_app(self, router: APIRouter = base_router):
        """"""
        Start the agent server.
        """"""

        app = FastAPI(
            title=""AutoGPT Forge"",
            description=""Modified version of The Agent Protocol."",
            version=""v0.4"",
        )

        # Add CORS middleware
        origins = [
            ""http://localhost:5000"",
            ""http://127.0.0.1:5000"",
            ""http://localhost:8000"",
            ""http://127.0.0.1:8000"",
            ""http://localhost:8080"",
            ""http://127.0.0.1:8080"",
            # Add any other origins you want to whitelist
        ]

        app.add_middleware(
            CORSMiddleware,
            allow_origins=origins,
            allow_credentials=True,
            allow_methods=[""*""],
            allow_headers=[""*""],
        )

        app.include_router(router, prefix=""/ap/v1"")
        script_dir = os.path.dirname(os.path.realpath(__file__))
        frontend_path = pathlib.Path(
            os.path.join(script_dir, ""../../../classic/frontend/build/web"")
        ).resolve()

        if os.path.exists(frontend_path):
            app.mount(""/app"", StaticFiles(directory=frontend_path), name=""app"")

            @app.get(""/"", include_in_schema=False)
            async def root():
                return RedirectResponse(url=""/app/index.html"", status_code=307)

        else:
            logger.warning(
                f""Frontend not found. {frontend_path} does not exist. ""
                ""The frontend will not be served.""
            )
        app.add_middleware(AgentMiddleware, agent=self)

        return app

    def start(self, port):
        uvicorn.run(
            ""forge.app:app"", host=""localhost"", port=port, log_level=""error"", reload=True
        )

    async def create_task(self, task_request: TaskRequestBody) -> Task:
        """"""
        Create a task for the agent.
        """"""
        task = await self.db.create_task(
            input=task_request.input,
            additional_input=task_request.additional_input,
        )
        return task

    async def list_tasks(self, page: int = 1, pageSize: int = 10) -> TaskListResponse:
        """"""
        List all tasks that the agent has created.
        """"""
        tasks, pagination = await self.db.list_tasks(page, pageSize)
        response = TaskListResponse(tasks=tasks, pagination=pagination)
        return response

    async def get_task(self, task_id: str) -> Task:
        """"""
        Get a task by ID.
        """"""
        task = await self.db.get_task(task_id)
        return task

    async def list_steps(
        self, task_id: str, page: int = 1, pageSize: int = 10
    ) -> TaskStepsListResponse:
        """"""
        List the IDs of all steps that the task has created.
        """"""
        steps, pagination = await self.db.list_steps(task_id, page, pageSize)
        response = TaskStepsListResponse(steps=steps, pagination=pagination)
        return response

    async def execute_step(self, task_id: str, step_request: StepRequestBody) -> Step:
        """"""
        Create a step for the task.
        """"""
        raise NotImplementedError

    async def get_step(self, task_id: str, step_id: str) -> Step:
        """"""
        Get a step by ID.
        """"""
        step = await self.db.get_step(task_id, step_id)
        return step

    async def list_artifacts(
        self, task_id: str, page: int = 1, pageSize: int = 10
    ) -> TaskArtifactsListResponse:
        """"""
        List the artifacts that the task has created.
        """"""
        artifacts, pagination = await self.db.list_artifacts(task_id, page, pageSize)
        return TaskArtifactsListResponse(artifacts=artifacts, pagination=pagination)

    async def create_artifact(
        self, task_id: str, file: UploadFile, relative_path: str = """"
    ) -> Artifact:
        """"""
        Create an artifact for the task.
        """"""
        file_name = file.filename or str(uuid4())
        data = b""""
        while contents := file.file.read(1024 * 1024):
            data += contents
        # Check if relative path ends with filename
        if relative_path.endswith(file_name):
            file_path = relative_path
        else:
            file_path = os.path.join(relative_path, file_name)

        await self.workspace.write_file(file_path, data)

        artifact = await self.db.create_artifact(
            task_id=task_id,
            file_name=file_name,
            relative_path=relative_path,
            agent_created=False,
        )
        return artifact

    async def get_artifact(self, task_id: str, artifact_id: str) -> StreamingResponse:
        """"""
        Get an artifact by ID.
        """"""
        artifact = await self.db.get_artifact(artifact_id)
        if artifact.file_name not in artifact.relative_path:
            file_path = os.path.join(artifact.relative_path, artifact.file_name)
        else:
            file_path = artifact.relative_path
        retrieved_artifact = self.workspace.read_file(file_path, binary=True)

        return StreamingResponse(
            BytesIO(retrieved_artifact),
            media_type=""application/octet-stream"",
            headers={
                ""Content-Disposition"": f""attachment; filename={artifact.file_name}""
            },
        )","Point(row=30, column=0)","Point(row=192, column=9)",,classic/forge/forge/agent_protocol/agent.py
ProtocolAgent.__init__,function,,"def __init__(self, database: AgentDB, workspace: FileStorage):
        self.db = database
        self.workspace = workspace","Point(row=31, column=4)","Point(row=33, column=34)",ProtocolAgent,classic/forge/forge/agent_protocol/agent.py
ProtocolAgent.get_agent_app,function,"
        Start the agent server.
","def get_agent_app(self, router: APIRouter = base_router):
        """"""
        Start the agent server.
        """"""

        app = FastAPI(
            title=""AutoGPT Forge"",
            description=""Modified version of The Agent Protocol."",
            version=""v0.4"",
        )

        # Add CORS middleware
        origins = [
            ""http://localhost:5000"",
            ""http://127.0.0.1:5000"",
            ""http://localhost:8000"",
            ""http://127.0.0.1:8000"",
            ""http://localhost:8080"",
            ""http://127.0.0.1:8080"",
            # Add any other origins you want to whitelist
        ]

        app.add_middleware(
            CORSMiddleware,
            allow_origins=origins,
            allow_credentials=True,
            allow_methods=[""*""],
            allow_headers=[""*""],
        )

        app.include_router(router, prefix=""/ap/v1"")
        script_dir = os.path.dirname(os.path.realpath(__file__))
        frontend_path = pathlib.Path(
            os.path.join(script_dir, ""../../../classic/frontend/build/web"")
        ).resolve()

        if os.path.exists(frontend_path):
            app.mount(""/app"", StaticFiles(directory=frontend_path), name=""app"")

            @app.get(""/"", include_in_schema=False)
            async def root():
                return RedirectResponse(url=""/app/index.html"", status_code=307)

        else:
            logger.warning(
                f""Frontend not found. {frontend_path} does not exist. ""
                ""The frontend will not be served.""
            )
        app.add_middleware(AgentMiddleware, agent=self)

        return app","Point(row=35, column=4)","Point(row=85, column=18)",ProtocolAgent,classic/forge/forge/agent_protocol/agent.py
ProtocolAgent.get_agent_app.root,function,,"async def root():
                return RedirectResponse(url=""/app/index.html"", status_code=307)","Point(row=75, column=12)","Point(row=76, column=79)",ProtocolAgent,classic/forge/forge/agent_protocol/agent.py
ProtocolAgent.start,function,,"def start(self, port):
        uvicorn.run(
            ""forge.app:app"", host=""localhost"", port=port, log_level=""error"", reload=True
        )","Point(row=87, column=4)","Point(row=90, column=9)",ProtocolAgent,classic/forge/forge/agent_protocol/agent.py
ProtocolAgent.create_task,function,"
        Create a task for the agent.
","async def create_task(self, task_request: TaskRequestBody) -> Task:
        """"""
        Create a task for the agent.
        """"""
        task = await self.db.create_task(
            input=task_request.input,
            additional_input=task_request.additional_input,
        )
        return task","Point(row=92, column=4)","Point(row=100, column=19)",ProtocolAgent,classic/forge/forge/agent_protocol/agent.py
ProtocolAgent.list_tasks,function,"
        List all tasks that the agent has created.
","async def list_tasks(self, page: int = 1, pageSize: int = 10) -> TaskListResponse:
        """"""
        List all tasks that the agent has created.
        """"""
        tasks, pagination = await self.db.list_tasks(page, pageSize)
        response = TaskListResponse(tasks=tasks, pagination=pagination)
        return response","Point(row=102, column=4)","Point(row=108, column=23)",ProtocolAgent,classic/forge/forge/agent_protocol/agent.py
ProtocolAgent.get_task,function,"
        Get a task by ID.
","async def get_task(self, task_id: str) -> Task:
        """"""
        Get a task by ID.
        """"""
        task = await self.db.get_task(task_id)
        return task","Point(row=110, column=4)","Point(row=115, column=19)",ProtocolAgent,classic/forge/forge/agent_protocol/agent.py
ProtocolAgent.list_steps,function,"
        List the IDs of all steps that the task has created.
","async def list_steps(
        self, task_id: str, page: int = 1, pageSize: int = 10
    ) -> TaskStepsListResponse:
        """"""
        List the IDs of all steps that the task has created.
        """"""
        steps, pagination = await self.db.list_steps(task_id, page, pageSize)
        response = TaskStepsListResponse(steps=steps, pagination=pagination)
        return response","Point(row=117, column=4)","Point(row=125, column=23)",ProtocolAgent,classic/forge/forge/agent_protocol/agent.py
ProtocolAgent.execute_step,function,"
        Create a step for the task.
","async def execute_step(self, task_id: str, step_request: StepRequestBody) -> Step:
        """"""
        Create a step for the task.
        """"""
        raise NotImplementedError","Point(row=127, column=4)","Point(row=131, column=33)",ProtocolAgent,classic/forge/forge/agent_protocol/agent.py
ProtocolAgent.get_step,function,"
        Get a step by ID.
","async def get_step(self, task_id: str, step_id: str) -> Step:
        """"""
        Get a step by ID.
        """"""
        step = await self.db.get_step(task_id, step_id)
        return step","Point(row=133, column=4)","Point(row=138, column=19)",ProtocolAgent,classic/forge/forge/agent_protocol/agent.py
ProtocolAgent.list_artifacts,function,"
        List the artifacts that the task has created.
","async def list_artifacts(
        self, task_id: str, page: int = 1, pageSize: int = 10
    ) -> TaskArtifactsListResponse:
        """"""
        List the artifacts that the task has created.
        """"""
        artifacts, pagination = await self.db.list_artifacts(task_id, page, pageSize)
        return TaskArtifactsListResponse(artifacts=artifacts, pagination=pagination)","Point(row=140, column=4)","Point(row=147, column=84)",ProtocolAgent,classic/forge/forge/agent_protocol/agent.py
ProtocolAgent.create_artifact,function,"
        Create an artifact for the task.
","async def create_artifact(
        self, task_id: str, file: UploadFile, relative_path: str = """"
    ) -> Artifact:
        """"""
        Create an artifact for the task.
        """"""
        file_name = file.filename or str(uuid4())
        data = b""""
        while contents := file.file.read(1024 * 1024):
            data += contents
        # Check if relative path ends with filename
        if relative_path.endswith(file_name):
            file_path = relative_path
        else:
            file_path = os.path.join(relative_path, file_name)

        await self.workspace.write_file(file_path, data)

        artifact = await self.db.create_artifact(
            task_id=task_id,
            file_name=file_name,
            relative_path=relative_path,
            agent_created=False,
        )
        return artifact","Point(row=149, column=4)","Point(row=173, column=23)",ProtocolAgent,classic/forge/forge/agent_protocol/agent.py
ProtocolAgent.get_artifact,function,"
        Get an artifact by ID.
","async def get_artifact(self, task_id: str, artifact_id: str) -> StreamingResponse:
        """"""
        Get an artifact by ID.
        """"""
        artifact = await self.db.get_artifact(artifact_id)
        if artifact.file_name not in artifact.relative_path:
            file_path = os.path.join(artifact.relative_path, artifact.file_name)
        else:
            file_path = artifact.relative_path
        retrieved_artifact = self.workspace.read_file(file_path, binary=True)

        return StreamingResponse(
            BytesIO(retrieved_artifact),
            media_type=""application/octet-stream"",
            headers={
                ""Content-Disposition"": f""attachment; filename={artifact.file_name}""
            },
        )","Point(row=175, column=4)","Point(row=192, column=9)",ProtocolAgent,classic/forge/forge/agent_protocol/agent.py
Base,class,,"class Base(DeclarativeBase):
    type_annotation_map = {
        dict[str, Any]: JSON,
    }","Point(row=32, column=0)","Point(row=35, column=5)",,classic/forge/forge/agent_protocol/database/db.py
TaskModel,class,,"class TaskModel(Base):
    __tablename__ = ""tasks""

    task_id: Mapped[str] = mapped_column(primary_key=True, index=True)
    input: Mapped[str]
    additional_input: Mapped[dict[str, Any]] = mapped_column(default=dict)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(
        DateTime, default=datetime.utcnow, onupdate=datetime.utcnow
    )

    artifacts = relationship(""ArtifactModel"", back_populates=""task"")","Point(row=38, column=0)","Point(row=49, column=68)",,classic/forge/forge/agent_protocol/database/db.py
StepModel,class,,"class StepModel(Base):
    __tablename__ = ""steps""

    step_id: Mapped[str] = mapped_column(primary_key=True, index=True)
    task_id: Mapped[str] = mapped_column(ForeignKey(""tasks.task_id""))
    name: Mapped[str]
    input: Mapped[str]
    status: Mapped[str]
    output: Mapped[Optional[str]]
    is_last: Mapped[bool] = mapped_column(Boolean, default=False)
    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(
        DateTime, default=datetime.utcnow, onupdate=datetime.utcnow
    )

    additional_input: Mapped[dict[str, Any]] = mapped_column(default=dict)
    additional_output: Mapped[Optional[dict[str, Any]]]
    artifacts = relationship(""ArtifactModel"", back_populates=""step"")","Point(row=52, column=0)","Point(row=69, column=68)",,classic/forge/forge/agent_protocol/database/db.py
ArtifactModel,class,,"class ArtifactModel(Base):
    __tablename__ = ""artifacts""

    artifact_id: Mapped[str] = mapped_column(primary_key=True, index=True)
    task_id: Mapped[str] = mapped_column(ForeignKey(""tasks.task_id""))
    step_id: Mapped[Optional[str]] = mapped_column(ForeignKey(""steps.step_id""))
    agent_created: Mapped[bool] = mapped_column(default=False)
    file_name: Mapped[str]
    relative_path: Mapped[str]
    created_at: Mapped[datetime] = mapped_column(default=datetime.utcnow)
    modified_at: Mapped[datetime] = mapped_column(
        default=datetime.utcnow, onupdate=datetime.utcnow
    )

    step = relationship(""StepModel"", back_populates=""artifacts"")
    task = relationship(""TaskModel"", back_populates=""artifacts"")","Point(row=72, column=0)","Point(row=87, column=64)",,classic/forge/forge/agent_protocol/database/db.py
convert_to_task,function,,"def convert_to_task(task_obj: TaskModel, debug_enabled: bool = False) -> Task:
    if debug_enabled:
        logger.debug(f""Converting TaskModel to Task for task_id: {task_obj.task_id}"")
    task_artifacts = [convert_to_artifact(artifact) for artifact in task_obj.artifacts]
    return Task(
        task_id=task_obj.task_id,
        created_at=task_obj.created_at,
        modified_at=task_obj.modified_at,
        input=task_obj.input,
        additional_input=task_obj.additional_input,
        artifacts=task_artifacts,
    )","Point(row=90, column=0)","Point(row=101, column=5)",,classic/forge/forge/agent_protocol/database/db.py
convert_to_step,function,,"def convert_to_step(step_model: StepModel, debug_enabled: bool = False) -> Step:
    if debug_enabled:
        logger.debug(f""Converting StepModel to Step for step_id: {step_model.step_id}"")
    step_artifacts = [
        convert_to_artifact(artifact) for artifact in step_model.artifacts
    ]
    status = (
        StepStatus.completed if step_model.status == ""completed"" else StepStatus.created
    )
    return Step(
        task_id=step_model.task_id,
        step_id=step_model.step_id,
        created_at=step_model.created_at,
        modified_at=step_model.modified_at,
        name=step_model.name,
        input=step_model.input,
        status=status,
        output=step_model.output,
        artifacts=step_artifacts,
        is_last=step_model.is_last == 1,
        additional_input=step_model.additional_input,
        additional_output=step_model.additional_output,
    )","Point(row=104, column=0)","Point(row=126, column=5)",,classic/forge/forge/agent_protocol/database/db.py
convert_to_artifact,function,,"def convert_to_artifact(artifact_model: ArtifactModel) -> Artifact:
    return Artifact(
        artifact_id=artifact_model.artifact_id,
        created_at=artifact_model.created_at,
        modified_at=artifact_model.modified_at,
        agent_created=artifact_model.agent_created,
        relative_path=artifact_model.relative_path,
        file_name=artifact_model.file_name,
    )","Point(row=129, column=0)","Point(row=137, column=5)",,classic/forge/forge/agent_protocol/database/db.py
AgentDB,class,,"class AgentDB:
    def __init__(self, database_string, debug_enabled: bool = False) -> None:
        super().__init__()
        self.debug_enabled = debug_enabled
        if self.debug_enabled:
            logger.debug(
                f""Initializing AgentDB with database_string: {database_string}""
            )
        self.engine = create_engine(database_string)
        Base.metadata.create_all(self.engine)
        self.Session = sessionmaker(bind=self.engine)

    def close(self) -> None:
        self.Session.close_all()
        self.engine.dispose()

    async def create_task(
        self, input: Optional[str], additional_input: Optional[dict] = {}
    ) -> Task:
        if self.debug_enabled:
            logger.debug(""Creating new task"")

        try:
            with self.Session() as session:
                new_task = TaskModel(
                    task_id=str(uuid.uuid4()),
                    input=input,
                    additional_input=additional_input if additional_input else {},
                )
                session.add(new_task)
                session.commit()
                session.refresh(new_task)
                if self.debug_enabled:
                    logger.debug(f""Created new task with task_id: {new_task.task_id}"")
                return convert_to_task(new_task, self.debug_enabled)
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while creating task: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while creating task: {e}"")
            raise

    async def create_step(
        self,
        task_id: str,
        input: StepRequestBody,
        is_last: bool = False,
        additional_input: Optional[Dict[str, Any]] = {},
    ) -> Step:
        if self.debug_enabled:
            logger.debug(f""Creating new step for task_id: {task_id}"")
        try:
            with self.Session() as session:
                new_step = StepModel(
                    task_id=task_id,
                    step_id=str(uuid.uuid4()),
                    name=input.input,
                    input=input.input,
                    status=""created"",
                    is_last=is_last,
                    additional_input=additional_input,
                )
                session.add(new_step)
                session.commit()
                session.refresh(new_step)
                if self.debug_enabled:
                    logger.debug(f""Created new step with step_id: {new_step.step_id}"")
                return convert_to_step(new_step, self.debug_enabled)
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while creating step: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while creating step: {e}"")
            raise

    async def create_artifact(
        self,
        task_id: str,
        file_name: str,
        relative_path: str,
        agent_created: bool = False,
        step_id: str | None = None,
    ) -> Artifact:
        if self.debug_enabled:
            logger.debug(f""Creating new artifact for task_id: {task_id}"")
        try:
            with self.Session() as session:
                if (
                    existing_artifact := session.query(ArtifactModel)
                    .filter_by(
                        task_id=task_id,
                        file_name=file_name,
                        relative_path=relative_path,
                    )
                    .first()
                ):
                    session.close()
                    if self.debug_enabled:
                        logger.debug(
                            f""Artifact {file_name} already exists at {relative_path}/""
                        )
                    return convert_to_artifact(existing_artifact)

                new_artifact = ArtifactModel(
                    artifact_id=str(uuid.uuid4()),
                    task_id=task_id,
                    step_id=step_id,
                    agent_created=agent_created,
                    file_name=file_name,
                    relative_path=relative_path,
                )
                session.add(new_artifact)
                session.commit()
                session.refresh(new_artifact)
                if self.debug_enabled:
                    logger.debug(
                        f""Created new artifact with ID: {new_artifact.artifact_id}""
                    )
                return convert_to_artifact(new_artifact)
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while creating step: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while creating step: {e}"")
            raise

    async def get_task(self, task_id: str) -> Task:
        """"""Get a task by its id""""""
        if self.debug_enabled:
            logger.debug(f""Getting task with task_id: {task_id}"")
        try:
            with self.Session() as session:
                if task_obj := (
                    session.query(TaskModel)
                    .options(joinedload(TaskModel.artifacts))
                    .filter_by(task_id=task_id)
                    .first()
                ):
                    return convert_to_task(task_obj, self.debug_enabled)
                else:
                    logger.error(f""Task not found with task_id: {task_id}"")
                    raise NotFoundError(""Task not found"")
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while getting task: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while getting task: {e}"")
            raise

    async def get_step(self, task_id: str, step_id: str) -> Step:
        if self.debug_enabled:
            logger.debug(f""Getting step with task_id: {task_id} and step_id: {step_id}"")
        try:
            with self.Session() as session:
                if step := (
                    session.query(StepModel)
                    .options(joinedload(StepModel.artifacts))
                    .filter(StepModel.step_id == step_id)
                    .first()
                ):
                    return convert_to_step(step, self.debug_enabled)

                else:
                    logger.error(
                        f""Step not found with task_id: {task_id} and step_id: {step_id}""
                    )
                    raise NotFoundError(""Step not found"")
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while getting step: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while getting step: {e}"")
            raise

    async def get_artifact(self, artifact_id: str) -> Artifact:
        if self.debug_enabled:
            logger.debug(f""Getting artifact with and artifact_id: {artifact_id}"")
        try:
            with self.Session() as session:
                if (
                    artifact_model := session.query(ArtifactModel)
                    .filter_by(artifact_id=artifact_id)
                    .first()
                ):
                    return convert_to_artifact(artifact_model)
                else:
                    logger.error(
                        f""Artifact not found with and artifact_id: {artifact_id}""
                    )
                    raise NotFoundError(""Artifact not found"")
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while getting artifact: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while getting artifact: {e}"")
            raise

    async def update_step(
        self,
        task_id: str,
        step_id: str,
        status: Optional[str] = None,
        output: Optional[str] = None,
        additional_input: Optional[Dict[str, Any]] = None,
        additional_output: Optional[Dict[str, Any]] = None,
    ) -> Step:
        if self.debug_enabled:
            logger.debug(
                f""Updating step with task_id: {task_id} and step_id: {step_id}""
            )
        try:
            with self.Session() as session:
                if (
                    step := session.query(StepModel)
                    .filter_by(task_id=task_id, step_id=step_id)
                    .first()
                ):
                    if status is not None:
                        step.status = status
                    if additional_input is not None:
                        step.additional_input = additional_input
                    if output is not None:
                        step.output = output
                    if additional_output is not None:
                        step.additional_output = additional_output
                    session.commit()
                    return await self.get_step(task_id, step_id)
                else:
                    logger.error(
                        ""Can't update non-existent Step with ""
                        f""task_id: {task_id} and step_id: {step_id}""
                    )
                    raise NotFoundError(""Step not found"")
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while getting step: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while getting step: {e}"")
            raise

    async def update_artifact(
        self,
        artifact_id: str,
        *,
        file_name: str = """",
        relative_path: str = """",
        agent_created: Optional[Literal[True]] = None,
    ) -> Artifact:
        logger.debug(f""Updating artifact with artifact_id: {artifact_id}"")
        with self.Session() as session:
            if (
                artifact := session.query(ArtifactModel)
                .filter_by(artifact_id=artifact_id)
                .first()
            ):
                if file_name:
                    artifact.file_name = file_name
                if relative_path:
                    artifact.relative_path = relative_path
                if agent_created:
                    artifact.agent_created = agent_created
                session.commit()
                return await self.get_artifact(artifact_id)
            else:
                logger.error(f""Artifact not found with artifact_id: {artifact_id}"")
                raise NotFoundError(""Artifact not found"")

    async def list_tasks(
        self, page: int = 1, per_page: int = 10
    ) -> Tuple[List[Task], Pagination]:
        if self.debug_enabled:
            logger.debug(""Listing tasks"")
        try:
            with self.Session() as session:
                tasks = (
                    session.query(TaskModel)
                    .offset((page - 1) * per_page)
                    .limit(per_page)
                    .all()
                )
                total = session.query(TaskModel).count()
                pages = math.ceil(total / per_page)
                pagination = Pagination(
                    total_items=total,
                    total_pages=pages,
                    current_page=page,
                    page_size=per_page,
                )
                return [
                    convert_to_task(task, self.debug_enabled) for task in tasks
                ], pagination
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while listing tasks: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while listing tasks: {e}"")
            raise

    async def list_steps(
        self, task_id: str, page: int = 1, per_page: int = 10
    ) -> Tuple[List[Step], Pagination]:
        if self.debug_enabled:
            logger.debug(f""Listing steps for task_id: {task_id}"")
        try:
            with self.Session() as session:
                steps = (
                    session.query(StepModel)
                    .filter_by(task_id=task_id)
                    .offset((page - 1) * per_page)
                    .limit(per_page)
                    .all()
                )
                total = session.query(StepModel).filter_by(task_id=task_id).count()
                pages = math.ceil(total / per_page)
                pagination = Pagination(
                    total_items=total,
                    total_pages=pages,
                    current_page=page,
                    page_size=per_page,
                )
                return [
                    convert_to_step(step, self.debug_enabled) for step in steps
                ], pagination
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while listing steps: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while listing steps: {e}"")
            raise

    async def list_artifacts(
        self, task_id: str, page: int = 1, per_page: int = 10
    ) -> Tuple[List[Artifact], Pagination]:
        if self.debug_enabled:
            logger.debug(f""Listing artifacts for task_id: {task_id}"")
        try:
            with self.Session() as session:
                artifacts = (
                    session.query(ArtifactModel)
                    .filter_by(task_id=task_id)
                    .offset((page - 1) * per_page)
                    .limit(per_page)
                    .all()
                )
                total = session.query(ArtifactModel).filter_by(task_id=task_id).count()
                pages = math.ceil(total / per_page)
                pagination = Pagination(
                    total_items=total,
                    total_pages=pages,
                    current_page=page,
                    page_size=per_page,
                )
                return [
                    convert_to_artifact(artifact) for artifact in artifacts
                ], pagination
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while listing artifacts: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while listing artifacts: {e}"")
            raise","Point(row=141, column=0)","Point(row=501, column=17)",,classic/forge/forge/agent_protocol/database/db.py
AgentDB.__init__,function,,"def __init__(self, database_string, debug_enabled: bool = False) -> None:
        super().__init__()
        self.debug_enabled = debug_enabled
        if self.debug_enabled:
            logger.debug(
                f""Initializing AgentDB with database_string: {database_string}""
            )
        self.engine = create_engine(database_string)
        Base.metadata.create_all(self.engine)
        self.Session = sessionmaker(bind=self.engine)","Point(row=142, column=4)","Point(row=151, column=53)",AgentDB,classic/forge/forge/agent_protocol/database/db.py
AgentDB.close,function,,"def close(self) -> None:
        self.Session.close_all()
        self.engine.dispose()","Point(row=153, column=4)","Point(row=155, column=29)",AgentDB,classic/forge/forge/agent_protocol/database/db.py
AgentDB.create_task,function,,"async def create_task(
        self, input: Optional[str], additional_input: Optional[dict] = {}
    ) -> Task:
        if self.debug_enabled:
            logger.debug(""Creating new task"")

        try:
            with self.Session() as session:
                new_task = TaskModel(
                    task_id=str(uuid.uuid4()),
                    input=input,
                    additional_input=additional_input if additional_input else {},
                )
                session.add(new_task)
                session.commit()
                session.refresh(new_task)
                if self.debug_enabled:
                    logger.debug(f""Created new task with task_id: {new_task.task_id}"")
                return convert_to_task(new_task, self.debug_enabled)
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while creating task: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while creating task: {e}"")
            raise","Point(row=157, column=4)","Point(row=181, column=17)",AgentDB,classic/forge/forge/agent_protocol/database/db.py
AgentDB.create_step,function,,"async def create_step(
        self,
        task_id: str,
        input: StepRequestBody,
        is_last: bool = False,
        additional_input: Optional[Dict[str, Any]] = {},
    ) -> Step:
        if self.debug_enabled:
            logger.debug(f""Creating new step for task_id: {task_id}"")
        try:
            with self.Session() as session:
                new_step = StepModel(
                    task_id=task_id,
                    step_id=str(uuid.uuid4()),
                    name=input.input,
                    input=input.input,
                    status=""created"",
                    is_last=is_last,
                    additional_input=additional_input,
                )
                session.add(new_step)
                session.commit()
                session.refresh(new_step)
                if self.debug_enabled:
                    logger.debug(f""Created new step with step_id: {new_step.step_id}"")
                return convert_to_step(new_step, self.debug_enabled)
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while creating step: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while creating step: {e}"")
            raise","Point(row=183, column=4)","Point(row=214, column=17)",AgentDB,classic/forge/forge/agent_protocol/database/db.py
AgentDB.create_artifact,function,,"async def create_artifact(
        self,
        task_id: str,
        file_name: str,
        relative_path: str,
        agent_created: bool = False,
        step_id: str | None = None,
    ) -> Artifact:
        if self.debug_enabled:
            logger.debug(f""Creating new artifact for task_id: {task_id}"")
        try:
            with self.Session() as session:
                if (
                    existing_artifact := session.query(ArtifactModel)
                    .filter_by(
                        task_id=task_id,
                        file_name=file_name,
                        relative_path=relative_path,
                    )
                    .first()
                ):
                    session.close()
                    if self.debug_enabled:
                        logger.debug(
                            f""Artifact {file_name} already exists at {relative_path}/""
                        )
                    return convert_to_artifact(existing_artifact)

                new_artifact = ArtifactModel(
                    artifact_id=str(uuid.uuid4()),
                    task_id=task_id,
                    step_id=step_id,
                    agent_created=agent_created,
                    file_name=file_name,
                    relative_path=relative_path,
                )
                session.add(new_artifact)
                session.commit()
                session.refresh(new_artifact)
                if self.debug_enabled:
                    logger.debug(
                        f""Created new artifact with ID: {new_artifact.artifact_id}""
                    )
                return convert_to_artifact(new_artifact)
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while creating step: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while creating step: {e}"")
            raise","Point(row=216, column=4)","Point(row=265, column=17)",AgentDB,classic/forge/forge/agent_protocol/database/db.py
AgentDB.get_task,function,Get a task by its id,"async def get_task(self, task_id: str) -> Task:
        """"""Get a task by its id""""""
        if self.debug_enabled:
            logger.debug(f""Getting task with task_id: {task_id}"")
        try:
            with self.Session() as session:
                if task_obj := (
                    session.query(TaskModel)
                    .options(joinedload(TaskModel.artifacts))
                    .filter_by(task_id=task_id)
                    .first()
                ):
                    return convert_to_task(task_obj, self.debug_enabled)
                else:
                    logger.error(f""Task not found with task_id: {task_id}"")
                    raise NotFoundError(""Task not found"")
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while getting task: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while getting task: {e}"")
            raise","Point(row=267, column=4)","Point(row=288, column=17)",AgentDB,classic/forge/forge/agent_protocol/database/db.py
AgentDB.get_step,function,,"async def get_step(self, task_id: str, step_id: str) -> Step:
        if self.debug_enabled:
            logger.debug(f""Getting step with task_id: {task_id} and step_id: {step_id}"")
        try:
            with self.Session() as session:
                if step := (
                    session.query(StepModel)
                    .options(joinedload(StepModel.artifacts))
                    .filter(StepModel.step_id == step_id)
                    .first()
                ):
                    return convert_to_step(step, self.debug_enabled)

                else:
                    logger.error(
                        f""Step not found with task_id: {task_id} and step_id: {step_id}""
                    )
                    raise NotFoundError(""Step not found"")
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while getting step: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while getting step: {e}"")
            raise","Point(row=290, column=4)","Point(row=313, column=17)",AgentDB,classic/forge/forge/agent_protocol/database/db.py
AgentDB.get_artifact,function,,"async def get_artifact(self, artifact_id: str) -> Artifact:
        if self.debug_enabled:
            logger.debug(f""Getting artifact with and artifact_id: {artifact_id}"")
        try:
            with self.Session() as session:
                if (
                    artifact_model := session.query(ArtifactModel)
                    .filter_by(artifact_id=artifact_id)
                    .first()
                ):
                    return convert_to_artifact(artifact_model)
                else:
                    logger.error(
                        f""Artifact not found with and artifact_id: {artifact_id}""
                    )
                    raise NotFoundError(""Artifact not found"")
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while getting artifact: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while getting artifact: {e}"")
            raise","Point(row=315, column=4)","Point(row=336, column=17)",AgentDB,classic/forge/forge/agent_protocol/database/db.py
AgentDB.update_step,function,,"async def update_step(
        self,
        task_id: str,
        step_id: str,
        status: Optional[str] = None,
        output: Optional[str] = None,
        additional_input: Optional[Dict[str, Any]] = None,
        additional_output: Optional[Dict[str, Any]] = None,
    ) -> Step:
        if self.debug_enabled:
            logger.debug(
                f""Updating step with task_id: {task_id} and step_id: {step_id}""
            )
        try:
            with self.Session() as session:
                if (
                    step := session.query(StepModel)
                    .filter_by(task_id=task_id, step_id=step_id)
                    .first()
                ):
                    if status is not None:
                        step.status = status
                    if additional_input is not None:
                        step.additional_input = additional_input
                    if output is not None:
                        step.output = output
                    if additional_output is not None:
                        step.additional_output = additional_output
                    session.commit()
                    return await self.get_step(task_id, step_id)
                else:
                    logger.error(
                        ""Can't update non-existent Step with ""
                        f""task_id: {task_id} and step_id: {step_id}""
                    )
                    raise NotFoundError(""Step not found"")
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while getting step: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while getting step: {e}"")
            raise","Point(row=338, column=4)","Point(row=379, column=17)",AgentDB,classic/forge/forge/agent_protocol/database/db.py
AgentDB.update_artifact,function,,"async def update_artifact(
        self,
        artifact_id: str,
        *,
        file_name: str = """",
        relative_path: str = """",
        agent_created: Optional[Literal[True]] = None,
    ) -> Artifact:
        logger.debug(f""Updating artifact with artifact_id: {artifact_id}"")
        with self.Session() as session:
            if (
                artifact := session.query(ArtifactModel)
                .filter_by(artifact_id=artifact_id)
                .first()
            ):
                if file_name:
                    artifact.file_name = file_name
                if relative_path:
                    artifact.relative_path = relative_path
                if agent_created:
                    artifact.agent_created = agent_created
                session.commit()
                return await self.get_artifact(artifact_id)
            else:
                logger.error(f""Artifact not found with artifact_id: {artifact_id}"")
                raise NotFoundError(""Artifact not found"")","Point(row=381, column=4)","Point(row=406, column=57)",AgentDB,classic/forge/forge/agent_protocol/database/db.py
AgentDB.list_tasks,function,,"async def list_tasks(
        self, page: int = 1, per_page: int = 10
    ) -> Tuple[List[Task], Pagination]:
        if self.debug_enabled:
            logger.debug(""Listing tasks"")
        try:
            with self.Session() as session:
                tasks = (
                    session.query(TaskModel)
                    .offset((page - 1) * per_page)
                    .limit(per_page)
                    .all()
                )
                total = session.query(TaskModel).count()
                pages = math.ceil(total / per_page)
                pagination = Pagination(
                    total_items=total,
                    total_pages=pages,
                    current_page=page,
                    page_size=per_page,
                )
                return [
                    convert_to_task(task, self.debug_enabled) for task in tasks
                ], pagination
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while listing tasks: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while listing tasks: {e}"")
            raise","Point(row=408, column=4)","Point(row=437, column=17)",AgentDB,classic/forge/forge/agent_protocol/database/db.py
AgentDB.list_steps,function,,"async def list_steps(
        self, task_id: str, page: int = 1, per_page: int = 10
    ) -> Tuple[List[Step], Pagination]:
        if self.debug_enabled:
            logger.debug(f""Listing steps for task_id: {task_id}"")
        try:
            with self.Session() as session:
                steps = (
                    session.query(StepModel)
                    .filter_by(task_id=task_id)
                    .offset((page - 1) * per_page)
                    .limit(per_page)
                    .all()
                )
                total = session.query(StepModel).filter_by(task_id=task_id).count()
                pages = math.ceil(total / per_page)
                pagination = Pagination(
                    total_items=total,
                    total_pages=pages,
                    current_page=page,
                    page_size=per_page,
                )
                return [
                    convert_to_step(step, self.debug_enabled) for step in steps
                ], pagination
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while listing steps: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while listing steps: {e}"")
            raise","Point(row=439, column=4)","Point(row=469, column=17)",AgentDB,classic/forge/forge/agent_protocol/database/db.py
AgentDB.list_artifacts,function,,"async def list_artifacts(
        self, task_id: str, page: int = 1, per_page: int = 10
    ) -> Tuple[List[Artifact], Pagination]:
        if self.debug_enabled:
            logger.debug(f""Listing artifacts for task_id: {task_id}"")
        try:
            with self.Session() as session:
                artifacts = (
                    session.query(ArtifactModel)
                    .filter_by(task_id=task_id)
                    .offset((page - 1) * per_page)
                    .limit(per_page)
                    .all()
                )
                total = session.query(ArtifactModel).filter_by(task_id=task_id).count()
                pages = math.ceil(total / per_page)
                pagination = Pagination(
                    total_items=total,
                    total_pages=pages,
                    current_page=page,
                    page_size=per_page,
                )
                return [
                    convert_to_artifact(artifact) for artifact in artifacts
                ], pagination
        except SQLAlchemyError as e:
            logger.error(f""SQLAlchemy error while listing artifacts: {e}"")
            raise
        except Exception as e:
            logger.error(f""Unexpected error while listing artifacts: {e}"")
            raise","Point(row=471, column=4)","Point(row=501, column=17)",AgentDB,classic/forge/forge/agent_protocol/database/db.py
agent_db,function,,"def agent_db():
    db = AgentDB(TEST_DB_URL)
    yield db
    db.close()
    os.remove(TEST_DB_FILENAME)","Point(row=29, column=0)","Point(row=33, column=31)",,classic/forge/forge/agent_protocol/database/db_test.py
raw_db_connection,function,,"def raw_db_connection(agent_db: AgentDB):
    connection = sqlite3.connect(TEST_DB_FILENAME)
    yield connection
    connection.close()","Point(row=37, column=0)","Point(row=40, column=22)",,classic/forge/forge/agent_protocol/database/db_test.py
test_table_creation,function,,"def test_table_creation(raw_db_connection: sqlite3.Connection):
    cursor = raw_db_connection.cursor()

    # Test for tasks table existence
    cursor.execute(""SELECT name FROM sqlite_master WHERE type='table' AND name='tasks'"")
    assert cursor.fetchone() is not None

    # Test for steps table existence
    cursor.execute(""SELECT name FROM sqlite_master WHERE type='table' AND name='steps'"")
    assert cursor.fetchone() is not None

    # Test for artifacts table existence
    cursor.execute(
        ""SELECT name FROM sqlite_master WHERE type='table' AND name='artifacts'""
    )
    assert cursor.fetchone() is not None","Point(row=43, column=0)","Point(row=58, column=40)",,classic/forge/forge/agent_protocol/database/db_test.py
test_task_schema,function,,"async def test_task_schema():
    now = datetime.now()
    task = Task(
        task_id=""50da533e-3904-4401-8a07-c49adf88b5eb"",
        input=""Write the words you receive to the file 'output.txt'."",
        created_at=now,
        modified_at=now,
        artifacts=[
            Artifact(
                artifact_id=""b225e278-8b4c-4f99-a696-8facf19f0e56"",
                agent_created=True,
                file_name=""main.py"",
                relative_path=""python/code/"",
                created_at=now,
                modified_at=now,
            )
        ],
    )
    assert task.task_id == ""50da533e-3904-4401-8a07-c49adf88b5eb""
    assert task.input == ""Write the words you receive to the file 'output.txt'.""
    assert len(task.artifacts) == 1
    assert task.artifacts[0].artifact_id == ""b225e278-8b4c-4f99-a696-8facf19f0e56""","Point(row=62, column=0)","Point(row=83, column=82)",,classic/forge/forge/agent_protocol/database/db_test.py
test_step_schema,function,,"async def test_step_schema():
    now = datetime.now()
    step = Step(
        task_id=""50da533e-3904-4401-8a07-c49adf88b5eb"",
        step_id=""6bb1801a-fd80-45e8-899a-4dd723cc602e"",
        created_at=now,
        modified_at=now,
        name=""Write to file"",
        input=""Write the words you receive to the file 'output.txt'."",
        status=StepStatus.created,
        output=(
            ""I am going to use the write_to_file command and write Washington ""
            ""to a file called output.txt <write_to_file('output.txt', 'Washington')>""
        ),
        artifacts=[
            Artifact(
                artifact_id=""b225e278-8b4c-4f99-a696-8facf19f0e56"",
                file_name=""main.py"",
                relative_path=""python/code/"",
                created_at=now,
                modified_at=now,
                agent_created=True,
            )
        ],
        is_last=False,
    )
    assert step.task_id == ""50da533e-3904-4401-8a07-c49adf88b5eb""
    assert step.step_id == ""6bb1801a-fd80-45e8-899a-4dd723cc602e""
    assert step.name == ""Write to file""
    assert step.status == StepStatus.created
    assert step.output == (
        ""I am going to use the write_to_file command and write Washington ""
        ""to a file called output.txt <write_to_file('output.txt', 'Washington')>""
    )
    assert len(step.artifacts) == 1
    assert step.artifacts[0].artifact_id == ""b225e278-8b4c-4f99-a696-8facf19f0e56""
    assert step.is_last is False","Point(row=87, column=0)","Point(row=123, column=32)",,classic/forge/forge/agent_protocol/database/db_test.py
test_convert_to_task,function,,"async def test_convert_to_task():
    now = datetime.now()
    task_model = TaskModel(
        task_id=""50da533e-3904-4401-8a07-c49adf88b5eb"",
        created_at=now,
        modified_at=now,
        input=""Write the words you receive to the file 'output.txt'."",
        additional_input={},
        artifacts=[
            ArtifactModel(
                artifact_id=""b225e278-8b4c-4f99-a696-8facf19f0e56"",
                created_at=now,
                modified_at=now,
                relative_path=""file:///path/to/main.py"",
                agent_created=True,
                file_name=""main.py"",
            )
        ],
    )
    task = convert_to_task(task_model)
    assert task.task_id == ""50da533e-3904-4401-8a07-c49adf88b5eb""
    assert task.input == ""Write the words you receive to the file 'output.txt'.""
    assert len(task.artifacts) == 1
    assert task.artifacts[0].artifact_id == ""b225e278-8b4c-4f99-a696-8facf19f0e56""","Point(row=127, column=0)","Point(row=150, column=82)",,classic/forge/forge/agent_protocol/database/db_test.py
test_convert_to_step,function,,"async def test_convert_to_step():
    now = datetime.now()
    step_model = StepModel(
        task_id=""50da533e-3904-4401-8a07-c49adf88b5eb"",
        step_id=""6bb1801a-fd80-45e8-899a-4dd723cc602e"",
        created_at=now,
        modified_at=now,
        name=""Write to file"",
        status=""created"",
        input=""Write the words you receive to the file 'output.txt'."",
        additional_input={},
        artifacts=[
            ArtifactModel(
                artifact_id=""b225e278-8b4c-4f99-a696-8facf19f0e56"",
                created_at=now,
                modified_at=now,
                relative_path=""file:///path/to/main.py"",
                agent_created=True,
                file_name=""main.py"",
            )
        ],
        is_last=False,
    )
    step = convert_to_step(step_model)
    assert step.task_id == ""50da533e-3904-4401-8a07-c49adf88b5eb""
    assert step.step_id == ""6bb1801a-fd80-45e8-899a-4dd723cc602e""
    assert step.name == ""Write to file""
    assert step.status == StepStatus.created
    assert len(step.artifacts) == 1
    assert step.artifacts[0].artifact_id == ""b225e278-8b4c-4f99-a696-8facf19f0e56""
    assert step.is_last is False","Point(row=154, column=0)","Point(row=184, column=32)",,classic/forge/forge/agent_protocol/database/db_test.py
test_convert_to_artifact,function,,"async def test_convert_to_artifact():
    now = datetime.now()
    artifact_model = ArtifactModel(
        artifact_id=""b225e278-8b4c-4f99-a696-8facf19f0e56"",
        created_at=now,
        modified_at=now,
        relative_path=""file:///path/to/main.py"",
        agent_created=True,
        file_name=""main.py"",
    )
    artifact = convert_to_artifact(artifact_model)
    assert artifact.artifact_id == ""b225e278-8b4c-4f99-a696-8facf19f0e56""
    assert artifact.relative_path == ""file:///path/to/main.py""
    assert artifact.agent_created is True","Point(row=188, column=0)","Point(row=201, column=41)",,classic/forge/forge/agent_protocol/database/db_test.py
test_create_task,function,,"async def test_create_task(agent_db: AgentDB):
    task = await agent_db.create_task(""task_input"")
    assert task.input == ""task_input""","Point(row=205, column=0)","Point(row=207, column=37)",,classic/forge/forge/agent_protocol/database/db_test.py
test_create_and_get_task,function,,"async def test_create_and_get_task(agent_db: AgentDB):
    task = await agent_db.create_task(""test_input"")
    fetched_task = await agent_db.get_task(task.task_id)
    assert fetched_task.input == ""test_input""","Point(row=211, column=0)","Point(row=214, column=45)",,classic/forge/forge/agent_protocol/database/db_test.py
test_get_task_not_found,function,,"async def test_get_task_not_found(agent_db: AgentDB):
    with pytest.raises(DataNotFoundError):
        await agent_db.get_task(""9999"")","Point(row=218, column=0)","Point(row=220, column=39)",,classic/forge/forge/agent_protocol/database/db_test.py
test_create_and_get_step,function,,"async def test_create_and_get_step(agent_db: AgentDB):
    task = await agent_db.create_task(""task_input"")
    step_input = {""type"": ""python/code""}
    request = StepRequestBody(input=""test_input debug"", additional_input=step_input)
    step = await agent_db.create_step(task.task_id, request)
    step = await agent_db.get_step(task.task_id, step.step_id)
    assert step.input == ""test_input debug""","Point(row=224, column=0)","Point(row=230, column=43)",,classic/forge/forge/agent_protocol/database/db_test.py
test_updating_step,function,,"async def test_updating_step(agent_db: AgentDB):
    created_task = await agent_db.create_task(""task_input"")
    step_input = {""type"": ""python/code""}
    request = StepRequestBody(input=""test_input debug"", additional_input=step_input)
    created_step = await agent_db.create_step(created_task.task_id, request)
    await agent_db.update_step(created_task.task_id, created_step.step_id, ""completed"")

    step = await agent_db.get_step(created_task.task_id, created_step.step_id)
    assert step.status.value == ""completed""","Point(row=234, column=0)","Point(row=242, column=43)",,classic/forge/forge/agent_protocol/database/db_test.py
test_get_step_not_found,function,,"async def test_get_step_not_found(agent_db: AgentDB):
    with pytest.raises(DataNotFoundError):
        await agent_db.get_step(""9999"", ""9999"")","Point(row=246, column=0)","Point(row=248, column=47)",,classic/forge/forge/agent_protocol/database/db_test.py
test_get_artifact,function,,"async def test_get_artifact(agent_db: AgentDB):
    # Given: A task and its corresponding artifact
    task = await agent_db.create_task(""test_input debug"")
    step_input = {""type"": ""python/code""}
    requst = StepRequestBody(input=""test_input debug"", additional_input=step_input)

    step = await agent_db.create_step(task.task_id, requst)

    # Create an artifact
    artifact = await agent_db.create_artifact(
        task_id=task.task_id,
        file_name=""test_get_artifact_sample_file.txt"",
        relative_path=""file:///path/to/test_get_artifact_sample_file.txt"",
        agent_created=True,
        step_id=step.step_id,
    )

    # When: The artifact is fetched by its ID
    fetched_artifact = await agent_db.get_artifact(artifact.artifact_id)

    # Then: The fetched artifact matches the original
    assert fetched_artifact.artifact_id == artifact.artifact_id
    assert (
        fetched_artifact.relative_path
        == ""file:///path/to/test_get_artifact_sample_file.txt""
    )","Point(row=252, column=0)","Point(row=277, column=5)",,classic/forge/forge/agent_protocol/database/db_test.py
test_list_tasks,function,,"async def test_list_tasks(agent_db: AgentDB):
    # Given: Multiple tasks in the database
    task1 = await agent_db.create_task(""test_input_1"")
    task2 = await agent_db.create_task(""test_input_2"")

    # When: All tasks are fetched
    fetched_tasks, pagination = await agent_db.list_tasks()

    # Then: The fetched tasks list includes the created tasks
    task_ids = [task.task_id for task in fetched_tasks]
    assert task1.task_id in task_ids
    assert task2.task_id in task_ids","Point(row=281, column=0)","Point(row=292, column=36)",,classic/forge/forge/agent_protocol/database/db_test.py
test_list_steps,function,,"async def test_list_steps(agent_db: AgentDB):
    step_input = {""type"": ""python/code""}
    request = StepRequestBody(input=""test_input debug"", additional_input=step_input)

    # Given: A task and multiple steps for that task
    task = await agent_db.create_task(""test_input"")
    step1 = await agent_db.create_step(task.task_id, request)
    request = StepRequestBody(input=""step two"")
    step2 = await agent_db.create_step(task.task_id, request)

    # When: All steps for the task are fetched
    fetched_steps, pagination = await agent_db.list_steps(task.task_id)

    # Then: The fetched steps list includes the created steps
    step_ids = [step.step_id for step in fetched_steps]
    assert step1.step_id in step_ids
    assert step2.step_id in step_ids","Point(row=296, column=0)","Point(row=312, column=36)",,classic/forge/forge/agent_protocol/database/db_test.py
TaskRequestBody,class,,"class TaskRequestBody(BaseModel):
    input: str = Field(
        min_length=1,
        description=""Input prompt for the task."",
        examples=[""Write the words you receive to the file 'output.txt'.""],
    )
    additional_input: dict[str, Any] = Field(default_factory=dict)","Point(row=12, column=0)","Point(row=18, column=66)",,classic/forge/forge/agent_protocol/models/task.py
Task,class,,"class Task(TaskRequestBody):
    created_at: datetime = Field(
        description=""The creation datetime of the task."",
        examples=[""2023-01-01T00:00:00Z""],
    )
    modified_at: datetime = Field(
        description=""The modification datetime of the task."",
        examples=[""2023-01-01T00:00:00Z""],
    )
    task_id: str = Field(
        description=""The ID of the task."",
        examples=[""50da533e-3904-4401-8a07-c49adf88b5eb""],
    )
    artifacts: list[Artifact] = Field(
        default_factory=list,
        description=""A list of artifacts that the task has produced."",
        examples=[
            ""7a49f31c-f9c6-4346-a22c-e32bc5af4d8e"",
            ""ab7b4091-2560-4692-a4fe-d831ea3ca7d6"",
        ],
    )

    model_config = ConfigDict(
        json_encoders={datetime: lambda v: v.isoformat()},
    )","Point(row=21, column=0)","Point(row=45, column=5)",,classic/forge/forge/agent_protocol/models/task.py
StepRequestBody,class,,"class StepRequestBody(BaseModel):
    name: Optional[str] = Field(
        default=None,
        description=""The name of the task step."",
        examples=[""Write to file""],
    )
    input: str = Field(
        description=""Input prompt for the step."", examples=[""Washington""]
    )
    additional_input: dict[str, Any] = Field(default_factory=dict)","Point(row=48, column=0)","Point(row=57, column=66)",,classic/forge/forge/agent_protocol/models/task.py
StepStatus,class,,"class StepStatus(Enum):
    created = ""created""
    running = ""running""
    completed = ""completed""","Point(row=60, column=0)","Point(row=63, column=27)",,classic/forge/forge/agent_protocol/models/task.py
Step,class,,"class Step(StepRequestBody):
    created_at: datetime = Field(
        description=""The creation datetime of the task."",
        examples=[
            ""2023-01-01T00:00:00Z"",
        ],
    )
    modified_at: datetime = Field(
        description=""The modification datetime of the task."",
        examples=[
            ""2023-01-01T00:00:00Z"",
        ],
    )
    task_id: str = Field(
        description=""The ID of the task this step belongs to."",
        examples=[
            ""50da533e-3904-4401-8a07-c49adf88b5eb"",
        ],
    )
    step_id: str = Field(
        description=""The ID of the task step."",
        examples=[
            ""6bb1801a-fd80-45e8-899a-4dd723cc602e"",
        ],
    )
    name: Optional[str] = Field(
        default=None,
        description=""The name of the task step."",
        examples=[""Write to file""],
    )
    status: StepStatus = Field(
        description=""The status of the task step."", examples=[""created""]
    )
    output: Optional[str] = Field(
        default=None,
        description=""Output of the task step."",
        examples=[
            ""I am going to use the write_to_file command and write Washington ""
            ""to a file called output.txt <write_to_file('output.txt', 'Washington')""
        ],
    )
    additional_output: Optional[dict[str, Any]] = None
    artifacts: list[Artifact] = Field(
        default_factory=list,
        description=""A list of artifacts that the step has produced."",
    )
    is_last: bool = Field(
        description=""Whether this is the last step in the task."", examples=[True]
    )

    model_config = ConfigDict(
        json_encoders={datetime: lambda v: v.isoformat()},
    )","Point(row=66, column=0)","Point(row=118, column=5)",,classic/forge/forge/agent_protocol/models/task.py
TaskListResponse,class,,"class TaskListResponse(BaseModel):
    tasks: Optional[List[Task]] = None
    pagination: Optional[Pagination] = None","Point(row=121, column=0)","Point(row=123, column=43)",,classic/forge/forge/agent_protocol/models/task.py
TaskStepsListResponse,class,,"class TaskStepsListResponse(BaseModel):
    steps: Optional[List[Step]] = None
    pagination: Optional[Pagination] = None","Point(row=126, column=0)","Point(row=128, column=43)",,classic/forge/forge/agent_protocol/models/task.py
TaskArtifactsListResponse,class,,"class TaskArtifactsListResponse(BaseModel):
    artifacts: Optional[List[Artifact]] = None
    pagination: Optional[Pagination] = None","Point(row=131, column=0)","Point(row=133, column=43)",,classic/forge/forge/agent_protocol/models/task.py
Artifact,class,,"class Artifact(BaseModel):
    created_at: datetime = Field(
        description=""The creation datetime of the task."",
        examples=[""2023-01-01T00:00:00Z""],
    )
    modified_at: datetime = Field(
        description=""The modification datetime of the task."",
        examples=[""2023-01-01T00:00:00Z""],
    )
    artifact_id: str = Field(
        description=""ID of the artifact."",
        examples=[""b225e278-8b4c-4f99-a696-8facf19f0e56""],
    )
    agent_created: bool = Field(
        description=""Whether the artifact has been created by the agent."",
        examples=[False],
    )
    relative_path: str = Field(
        description=""Relative path of the artifact in the agents workspace."",
        examples=[""/my_folder/my_other_folder/""],
    )
    file_name: str = Field(
        description=""Filename of the artifact."",
        examples=[""main.py""],
    )

    model_config = ConfigDict(
        json_encoders={datetime: lambda v: v.isoformat()},
    )","Point(row=5, column=0)","Point(row=33, column=5)",,classic/forge/forge/agent_protocol/models/artifact.py
Pagination,class,,"class Pagination(BaseModel):
    total_items: int = Field(description=""Total number of items."", examples=[42])
    total_pages: int = Field(description=""Total number of pages."", examples=[97])
    current_page: int = Field(description=""Current_page page number."", examples=[1])
    page_size: int = Field(description=""Number of items per page."", examples=[25])","Point(row=3, column=0)","Point(row=7, column=82)",,classic/forge/forge/agent_protocol/models/pagination.py
test_remove_color_codes,function,,"def test_remove_color_codes(raw_text, clean_text):
    assert remove_color_codes(raw_text) == clean_text","Point(row=34, column=0)","Point(row=35, column=53)",,classic/forge/forge/logging/test_utils.py
LogFormatName,class,,"class LogFormatName(str, enum.Enum):
    SIMPLE = ""simple""
    DEBUG = ""debug""
    STRUCTURED = ""structured_google_cloud""","Point(row=35, column=0)","Point(row=38, column=42)",,classic/forge/forge/logging/config.py
LoggingConfig,class,,"class LoggingConfig(SystemConfiguration):
    level: int = UserConfigurable(
        default=logging.INFO,
        from_env=lambda: logging.getLevelName(os.getenv(""LOG_LEVEL"", ""INFO"")),
    )

    # Console output
    log_format: LogFormatName = UserConfigurable(
        default=LogFormatName.SIMPLE, from_env=""LOG_FORMAT""
    )
    plain_console_output: bool = UserConfigurable(
        default=False,
        from_env=lambda: os.getenv(""PLAIN_OUTPUT"", ""False"") == ""True"",
    )

    # File output
    log_dir: Path = LOG_DIR
    log_file_format: Optional[LogFormatName] = UserConfigurable(
        default=LogFormatName.SIMPLE,
        from_env=lambda: os.getenv(  # type: ignore
            ""LOG_FILE_FORMAT"", os.getenv(""LOG_FORMAT"", ""simple"")
        ),
    )","Point(row=47, column=0)","Point(row=69, column=5)",,classic/forge/forge/logging/config.py
configure_logging,function,"Configure the native logging module, based on the environment config and any
    specified overrides.

    Arguments override values specified in the environment.
    Overrides are also applied to `config`, if passed.

    Should be usable as `configure_logging(**config.logging.dict())`, where
    `config.logging` is a `LoggingConfig` object.
","def configure_logging(
    debug: bool = False,
    level: Optional[int | str] = None,
    log_dir: Optional[Path] = None,
    log_format: Optional[LogFormatName | str] = None,
    log_file_format: Optional[LogFormatName | str] = None,
    plain_console_output: Optional[bool] = None,
    config: Optional[LoggingConfig] = None,
    tts_config: Optional[TTSConfig] = None,
) -> None:
    """"""Configure the native logging module, based on the environment config and any
    specified overrides.

    Arguments override values specified in the environment.
    Overrides are also applied to `config`, if passed.

    Should be usable as `configure_logging(**config.logging.dict())`, where
    `config.logging` is a `LoggingConfig` object.
    """"""
    if debug and level:
        raise ValueError(""Only one of either 'debug' and 'level' arguments may be set"")

    # Parse arguments
    if isinstance(level, str):
        if type(_level := logging.getLevelName(level.upper())) is int:
            level = _level
        else:
            raise ValueError(f""Unknown log level '{level}'"")
    if isinstance(log_format, str):
        if log_format in LogFormatName._value2member_map_:
            log_format = LogFormatName(log_format)
        elif not isinstance(log_format, LogFormatName):
            raise ValueError(f""Unknown log format '{log_format}'"")
    if isinstance(log_file_format, str):
        if log_file_format in LogFormatName._value2member_map_:
            log_file_format = LogFormatName(log_file_format)
        elif not isinstance(log_file_format, LogFormatName):
            raise ValueError(f""Unknown log format '{log_format}'"")

    config = config or LoggingConfig.from_env()

    # Aggregate env config + arguments
    config.level = logging.DEBUG if debug else level or config.level
    config.log_dir = log_dir or config.log_dir
    config.log_format = log_format or (
        LogFormatName.DEBUG if debug else config.log_format
    )
    config.log_file_format = log_file_format or log_format or config.log_file_format
    config.plain_console_output = (
        plain_console_output
        if plain_console_output is not None
        else config.plain_console_output
    )

    # Structured logging is used for cloud environments,
    # where logging to a file makes no sense.
    if config.log_format == LogFormatName.STRUCTURED:
        config.plain_console_output = True
        config.log_file_format = None

    # create log directory if it doesn't exist
    if not config.log_dir.exists():
        config.log_dir.mkdir()

    log_handlers: list[logging.Handler] = []

    if config.log_format in (LogFormatName.DEBUG, LogFormatName.SIMPLE):
        console_format_template = TEXT_LOG_FORMAT_MAP[config.log_format]
        console_formatter = ForgeFormatter(console_format_template)
    else:
        console_formatter = StructuredLoggingFormatter()
        console_format_template = SIMPLE_LOG_FORMAT

    # Console output handlers
    stdout = logging.StreamHandler(stream=sys.stdout)
    stdout.setLevel(config.level)
    stdout.addFilter(BelowLevelFilter(logging.WARNING))
    stdout.setFormatter(console_formatter)
    stderr = logging.StreamHandler()
    stderr.setLevel(logging.WARNING)
    stderr.setFormatter(console_formatter)
    log_handlers += [stdout, stderr]

    # File output handlers
    if config.log_file_format is not None:
        if config.level < logging.ERROR:
            file_output_format_template = TEXT_LOG_FORMAT_MAP[config.log_file_format]
            file_output_formatter = ForgeFormatter(
                file_output_format_template, no_color=True
            )

            # INFO log file handler
            activity_log_handler = logging.FileHandler(
                config.log_dir / LOG_FILE, ""a"", ""utf-8""
            )
            activity_log_handler.setLevel(config.level)
            activity_log_handler.setFormatter(file_output_formatter)
            log_handlers += [activity_log_handler]

        # ERROR log file handler
        error_log_handler = logging.FileHandler(
            config.log_dir / ERROR_LOG_FILE, ""a"", ""utf-8""
        )
        error_log_handler.setLevel(logging.ERROR)
        error_log_handler.setFormatter(ForgeFormatter(DEBUG_LOG_FORMAT, no_color=True))
        log_handlers += [error_log_handler]

    # Configure the root logger
    logging.basicConfig(
        format=console_format_template,
        level=config.level,
        handlers=log_handlers,
    )

    # Speech output
    speech_output_logger = logging.getLogger(SPEECH_OUTPUT_LOGGER)
    speech_output_logger.setLevel(logging.INFO)
    if tts_config:
        speech_output_logger.addHandler(TTSHandler(tts_config))
    speech_output_logger.propagate = False

    # JSON logger with better formatting
    json_logger = logging.getLogger(""JSON_LOGGER"")
    json_logger.setLevel(logging.DEBUG)
    json_logger.propagate = False

    # Disable debug logging from OpenAI library
    openai_logger.setLevel(logging.WARNING)","Point(row=72, column=0)","Point(row=199, column=43)",,classic/forge/forge/logging/config.py
FancyConsoleFormatter,class,"
    A custom logging formatter designed for console output.

    This formatter enhances the standard logging output with color coding. The color
    coding is based on the level of the log message, making it easier to distinguish
    between different types of messages in the console output.

    The color for each level is defined in the LEVEL_COLOR_MAP class attribute.
","class FancyConsoleFormatter(logging.Formatter):
    """"""
    A custom logging formatter designed for console output.

    This formatter enhances the standard logging output with color coding. The color
    coding is based on the level of the log message, making it easier to distinguish
    between different types of messages in the console output.

    The color for each level is defined in the LEVEL_COLOR_MAP class attribute.
    """"""

    # level -> (level & text color, title color)
    LEVEL_COLOR_MAP = {
        logging.DEBUG: Fore.LIGHTBLACK_EX,
        logging.INFO: Fore.BLUE,
        logging.WARNING: Fore.YELLOW,
        logging.ERROR: Fore.RED,
        logging.CRITICAL: Fore.RED + Style.BRIGHT,
    }

    def format(self, record: logging.LogRecord) -> str:
        # Make sure `msg` is a string
        if not hasattr(record, ""msg""):
            record.msg = """"
        elif not type(record.msg) is str:
            record.msg = str(record.msg)

        # Determine default color based on error level
        level_color = """"
        if record.levelno in self.LEVEL_COLOR_MAP:
            level_color = self.LEVEL_COLOR_MAP[record.levelno]
            record.levelname = f""{level_color}{record.levelname}{Style.RESET_ALL}""

        # Determine color for message
        color = getattr(record, ""color"", level_color)
        color_is_specified = hasattr(record, ""color"")

        # Don't color INFO messages unless the color is explicitly specified.
        if color and (record.levelno != logging.INFO or color_is_specified):
            record.msg = f""{color}{record.msg}{Style.RESET_ALL}""

        return super().format(record)","Point(row=8, column=0)","Point(row=49, column=37)",,classic/forge/forge/logging/formatters.py
FancyConsoleFormatter.format,function,,"def format(self, record: logging.LogRecord) -> str:
        # Make sure `msg` is a string
        if not hasattr(record, ""msg""):
            record.msg = """"
        elif not type(record.msg) is str:
            record.msg = str(record.msg)

        # Determine default color based on error level
        level_color = """"
        if record.levelno in self.LEVEL_COLOR_MAP:
            level_color = self.LEVEL_COLOR_MAP[record.levelno]
            record.levelname = f""{level_color}{record.levelname}{Style.RESET_ALL}""

        # Determine color for message
        color = getattr(record, ""color"", level_color)
        color_is_specified = hasattr(record, ""color"")

        # Don't color INFO messages unless the color is explicitly specified.
        if color and (record.levelno != logging.INFO or color_is_specified):
            record.msg = f""{color}{record.msg}{Style.RESET_ALL}""

        return super().format(record)","Point(row=28, column=4)","Point(row=49, column=37)",FancyConsoleFormatter,classic/forge/forge/logging/formatters.py
ForgeFormatter,class,,"class ForgeFormatter(FancyConsoleFormatter):
    def __init__(self, *args, no_color: bool = False, **kwargs):
        super().__init__(*args, **kwargs)
        self.no_color = no_color

    def format(self, record: logging.LogRecord) -> str:
        # Make sure `msg` is a string
        if not hasattr(record, ""msg""):
            record.msg = """"
        elif not type(record.msg) is str:
            record.msg = str(record.msg)

        # Strip color from the message to prevent color spoofing
        if record.msg and not getattr(record, ""preserve_color"", False):
            record.msg = remove_color_codes(record.msg)

        # Determine color for title
        title = getattr(record, ""title"", """")
        title_color = getattr(record, ""title_color"", """") or self.LEVEL_COLOR_MAP.get(
            record.levelno, """"
        )
        if title and title_color:
            title = f""{title_color + Style.BRIGHT}{title}{Style.RESET_ALL}""
        # Make sure record.title is set, and padded with a space if not empty
        record.title = f""{title} "" if title else """"

        if self.no_color:
            return remove_color_codes(super().format(record))
        else:
            return super().format(record)","Point(row=52, column=0)","Point(row=81, column=41)",,classic/forge/forge/logging/formatters.py
ForgeFormatter.__init__,function,,"def __init__(self, *args, no_color: bool = False, **kwargs):
        super().__init__(*args, **kwargs)
        self.no_color = no_color","Point(row=53, column=4)","Point(row=55, column=32)",ForgeFormatter,classic/forge/forge/logging/formatters.py
ForgeFormatter.format,function,,"def format(self, record: logging.LogRecord) -> str:
        # Make sure `msg` is a string
        if not hasattr(record, ""msg""):
            record.msg = """"
        elif not type(record.msg) is str:
            record.msg = str(record.msg)

        # Strip color from the message to prevent color spoofing
        if record.msg and not getattr(record, ""preserve_color"", False):
            record.msg = remove_color_codes(record.msg)

        # Determine color for title
        title = getattr(record, ""title"", """")
        title_color = getattr(record, ""title_color"", """") or self.LEVEL_COLOR_MAP.get(
            record.levelno, """"
        )
        if title and title_color:
            title = f""{title_color + Style.BRIGHT}{title}{Style.RESET_ALL}""
        # Make sure record.title is set, and padded with a space if not empty
        record.title = f""{title} "" if title else """"

        if self.no_color:
            return remove_color_codes(super().format(record))
        else:
            return super().format(record)","Point(row=57, column=4)","Point(row=81, column=41)",ForgeFormatter,classic/forge/forge/logging/formatters.py
StructuredLoggingFormatter,class,,"class StructuredLoggingFormatter(StructuredLogHandler, logging.Formatter):
    def __init__(self):
        # Set up CloudLoggingFilter to add diagnostic info to the log records
        self.cloud_logging_filter = CloudLoggingFilter()

        # Init StructuredLogHandler
        super().__init__()

    def format(self, record: logging.LogRecord) -> str:
        self.cloud_logging_filter.filter(record)
        return super().format(record)","Point(row=84, column=0)","Point(row=94, column=37)",,classic/forge/forge/logging/formatters.py
StructuredLoggingFormatter.__init__,function,,"def __init__(self):
        # Set up CloudLoggingFilter to add diagnostic info to the log records
        self.cloud_logging_filter = CloudLoggingFilter()

        # Init StructuredLogHandler
        super().__init__()","Point(row=85, column=4)","Point(row=90, column=26)",StructuredLoggingFormatter,classic/forge/forge/logging/formatters.py
StructuredLoggingFormatter.format,function,,"def format(self, record: logging.LogRecord) -> str:
        self.cloud_logging_filter.filter(record)
        return super().format(record)","Point(row=92, column=4)","Point(row=94, column=37)",StructuredLoggingFormatter,classic/forge/forge/logging/formatters.py
TTSHandler,class,Output messages to the configured TTS engine (if any),"class TTSHandler(logging.Handler):
    """"""Output messages to the configured TTS engine (if any)""""""

    def __init__(self, config: TTSConfig):
        super().__init__()
        self.config = config
        self.tts_provider = TextToSpeechProvider(config)

    def format(self, record: logging.LogRecord) -> str:
        if getattr(record, ""title"", """"):
            msg = f""{getattr(record, 'title')} {record.msg}""
        else:
            msg = f""{record.msg}""

        return remove_color_codes(msg)

    def emit(self, record: logging.LogRecord) -> None:
        if not self.config.speak_mode:
            return

        message = self.format(record)
        self.tts_provider.say(message)","Point(row=13, column=0)","Point(row=34, column=38)",,classic/forge/forge/logging/handlers.py
TTSHandler.__init__,function,,"def __init__(self, config: TTSConfig):
        super().__init__()
        self.config = config
        self.tts_provider = TextToSpeechProvider(config)","Point(row=16, column=4)","Point(row=19, column=56)",TTSHandler,classic/forge/forge/logging/handlers.py
TTSHandler.format,function,,"def format(self, record: logging.LogRecord) -> str:
        if getattr(record, ""title"", """"):
            msg = f""{getattr(record, 'title')} {record.msg}""
        else:
            msg = f""{record.msg}""

        return remove_color_codes(msg)","Point(row=21, column=4)","Point(row=27, column=38)",TTSHandler,classic/forge/forge/logging/handlers.py
TTSHandler.emit,function,,"def emit(self, record: logging.LogRecord) -> None:
        if not self.config.speak_mode:
            return

        message = self.format(record)
        self.tts_provider.say(message)","Point(row=29, column=4)","Point(row=34, column=38)",TTSHandler,classic/forge/forge/logging/handlers.py
JsonFileHandler,class,,"class JsonFileHandler(logging.FileHandler):
    def format(self, record: logging.LogRecord) -> str:
        record.json_data = json.loads(record.getMessage())
        return json.dumps(getattr(record, ""json_data""), ensure_ascii=False, indent=4)

    def emit(self, record: logging.LogRecord) -> None:
        with open(self.baseFilename, ""w"", encoding=""utf-8"") as f:
            f.write(self.format(record))","Point(row=37, column=0)","Point(row=44, column=40)",,classic/forge/forge/logging/handlers.py
JsonFileHandler.format,function,,"def format(self, record: logging.LogRecord) -> str:
        record.json_data = json.loads(record.getMessage())
        return json.dumps(getattr(record, ""json_data""), ensure_ascii=False, indent=4)","Point(row=38, column=4)","Point(row=40, column=85)",JsonFileHandler,classic/forge/forge/logging/handlers.py
JsonFileHandler.emit,function,,"def emit(self, record: logging.LogRecord) -> None:
        with open(self.baseFilename, ""w"", encoding=""utf-8"") as f:
            f.write(self.format(record))","Point(row=42, column=4)","Point(row=44, column=40)",JsonFileHandler,classic/forge/forge/logging/handlers.py
remove_color_codes,function,,"def remove_color_codes(s: str) -> str:
    return re.sub(r""\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])"", """", s)","Point(row=7, column=0)","Point(row=8, column=66)",,classic/forge/forge/logging/utils.py
fmt_kwargs,function,,"def fmt_kwargs(kwargs: dict) -> str:
    return "", "".join(f""{n}={repr(v)}"" for n, v in kwargs.items())","Point(row=11, column=0)","Point(row=12, column=65)",,classic/forge/forge/logging/utils.py
print_attribute,function,,"def print_attribute(
    title: str, value: Any, title_color: str = Fore.GREEN, value_color: str = """"
) -> None:
    logger = logging.getLogger()
    logger.info(
        str(value),
        extra={
            ""title"": f""{title.rstrip(':')}:"",
            ""title_color"": title_color,
            ""color"": value_color,
        },
    )","Point(row=15, column=0)","Point(row=26, column=5)",,classic/forge/forge/logging/utils.py
speak,function,,"def speak(message: str, level: int = logging.INFO) -> None:
    from .config import SPEECH_OUTPUT_LOGGER

    logging.getLogger(SPEECH_OUTPUT_LOGGER).log(level, message)","Point(row=29, column=0)","Point(row=32, column=63)",,classic/forge/forge/logging/utils.py
BelowLevelFilter,class,Filter for logging levels below a certain threshold.,"class BelowLevelFilter(logging.Filter):
    """"""Filter for logging levels below a certain threshold.""""""

    def __init__(self, below_level: int):
        super().__init__()
        self.below_level = below_level

    def filter(self, record: logging.LogRecord):
        return record.levelno < self.below_level","Point(row=3, column=0)","Point(row=11, column=48)",,classic/forge/forge/logging/filters.py
BelowLevelFilter.__init__,function,,"def __init__(self, below_level: int):
        super().__init__()
        self.below_level = below_level","Point(row=6, column=4)","Point(row=8, column=38)",BelowLevelFilter,classic/forge/forge/logging/filters.py
BelowLevelFilter.filter,function,,"def filter(self, record: logging.LogRecord):
        return record.levelno < self.below_level","Point(row=10, column=4)","Point(row=11, column=48)",BelowLevelFilter,classic/forge/forge/logging/filters.py
get_environment_variables,function,Retrieve and return necessary environment variables.,"def get_environment_variables() -> Tuple[str, str, str, str, str]:
    """"""Retrieve and return necessary environment variables.""""""
    try:
        with open(os.environ[""GITHUB_EVENT_PATH""]) as f:
            event = json.load(f)

        sha = event[""pull_request""][""head""][""sha""]

        return (
            os.environ[""GITHUB_API_URL""],
            os.environ[""GITHUB_REPOSITORY""],
            sha,
            os.environ[""GITHUB_TOKEN""],
            os.environ[""GITHUB_RUN_ID""],
        )
    except KeyError as e:
        print(f""Error: Missing required environment variable or event data: {e}"")
        sys.exit(1)","Point(row=9, column=0)","Point(row=26, column=19)",,.github/workflows/scripts/check_actions_status.py
make_api_request,function,Make an API request and return the JSON response.,"def make_api_request(url: str, headers: Dict[str, str]) -> Dict:
    """"""Make an API request and return the JSON response.""""""
    try:
        print(""Making API request to:"", url)
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        print(f""Error: API request failed. {e}"")
        sys.exit(1)","Point(row=29, column=0)","Point(row=38, column=19)",,.github/workflows/scripts/check_actions_status.py
process_check_runs,function,Process check runs and return their status.,"def process_check_runs(check_runs: List[Dict]) -> Tuple[bool, bool]:
    """"""Process check runs and return their status.""""""
    runs_in_progress = False
    all_others_passed = True

    for run in check_runs:
        if str(run[""name""]) != ""Check PR Status"":
            status = run[""status""]
            conclusion = run[""conclusion""]

            if status == ""completed"":
                if conclusion not in [""success"", ""skipped"", ""neutral""]:
                    all_others_passed = False
                    print(
                        f""Check run {run['name']} (ID: {run['id']}) has conclusion: {conclusion}""
                    )
            else:
                runs_in_progress = True
                print(f""Check run {run['name']} (ID: {run['id']}) is still {status}."")
                all_others_passed = False
        else:
            print(
                f""Skipping check run {run['name']} (ID: {run['id']}) as it is the current run.""
            )

    return runs_in_progress, all_others_passed","Point(row=41, column=0)","Point(row=66, column=46)",,.github/workflows/scripts/check_actions_status.py
main,function,,"def main():
    api_url, repo, sha, github_token, current_run_id = get_environment_variables()

    endpoint = f""{api_url}/repos/{repo}/commits/{sha}/check-runs""
    headers = {
        ""Accept"": ""application/vnd.github.v3+json"",
    }
    if github_token:
        headers[""Authorization""] = f""token {github_token}""

    print(f""Current run ID: {current_run_id}"")

    while True:
        data = make_api_request(endpoint, headers)

        check_runs = data[""check_runs""]

        print(""Processing check runs..."")

        print(check_runs)

        runs_in_progress, all_others_passed = process_check_runs(check_runs)

        if not runs_in_progress:
            break

        print(
            ""Some check runs are still in progress. ""
            f""Waiting {CHECK_INTERVAL} seconds before checking again...""
        )
        time.sleep(CHECK_INTERVAL)

    if all_others_passed:
        print(""All other completed check runs have passed. This check passes."")
        sys.exit(0)
    else:
        print(""Some check runs have failed or have not completed. This check fails."")
        sys.exit(1)","Point(row=69, column=0)","Point(row=106, column=19)",,.github/workflows/scripts/check_actions_status.py
wait_for_postgres,function,,"def wait_for_postgres(max_retries=5, delay=5):
    for _ in range(max_retries):
        try:
            result = subprocess.run(
                [
                    ""docker"",
                    ""compose"",
                    ""-f"",
                    ""docker-compose.test.yaml"",
                    ""exec"",
                    ""postgres-test"",
                    ""pg_isready"",
                    ""-U"",
                    ""agpt_user"",
                    ""-d"",
                    ""agpt_local"",
                ],
                check=True,
                capture_output=True,
                text=True,
            )
            if ""accepting connections"" in result.stdout:
                print(""PostgreSQL is ready."")
                return True
        except subprocess.CalledProcessError:
            print(f""PostgreSQL is not ready yet. Retrying in {delay} seconds..."")
            time.sleep(delay)
    print(""Failed to connect to PostgreSQL."")
    return False","Point(row=5, column=0)","Point(row=33, column=16)",,autogpt_platform/backend/run_tests.py
run_command,function,,"def run_command(command, check=True):
    try:
        subprocess.run(command, check=check)
    except subprocess.CalledProcessError as e:
        print(f""Command failed: {e}"")
        sys.exit(1)","Point(row=36, column=0)","Point(row=41, column=19)",,autogpt_platform/backend/run_tests.py
test,function,,"def test():
    # Start PostgreSQL with Docker Compose
    run_command(
        [
            ""docker"",
            ""compose"",
            ""-f"",
            ""docker-compose.test.yaml"",
            ""up"",
            ""-d"",
            ""postgres-test"",
        ]
    )

    if not wait_for_postgres():
        run_command([""docker"", ""compose"", ""-f"", ""docker-compose.test.yaml"", ""down""])
        sys.exit(1)

    # Run Prisma migrations
    run_command([""prisma"", ""migrate"", ""dev""])

    # Run the tests
    result = subprocess.run([""pytest""] + sys.argv[1:], check=False)

    run_command([""docker"", ""compose"", ""-f"", ""docker-compose.test.yaml"", ""down""])

    sys.exit(result.returncode)","Point(row=44, column=0)","Point(row=70, column=31)",,autogpt_platform/backend/run_tests.py
run,function,,"def run(*command: str) -> None:
    print(f"">>>>> Running poetry run {' '.join(command)}"")
    subprocess.run([""poetry"", ""run""] + list(command), cwd=directory, check=True)","Point(row=6, column=0)","Point(row=8, column=80)",,autogpt_platform/backend/linter.py
lint,function,,"def lint():
    try:
        run(""ruff"", ""check"", ""."", ""--exit-zero"")
        run(""isort"", ""--diff"", ""--check"", ""--profile"", ""black"", ""."")
        run(""black"", ""--diff"", ""--check"", ""."")
        run(""pyright"")
    except subprocess.CalledProcessError as e:
        print(""Lint failed, try running `poetry run format` to fix the issues: "", e)
        raise e","Point(row=11, column=0)","Point(row=19, column=15)",,autogpt_platform/backend/linter.py
format,function,,"def format():
    run(""ruff"", ""check"", ""--fix"", ""."")
    run(""isort"", ""--profile"", ""black"", ""."")
    run(""black"", ""."")
    run(""pyright"", ""."")","Point(row=22, column=0)","Point(row=26, column=23)",,autogpt_platform/backend/linter.py
server,function,,"async def server():
    async with SpinTestServer() as server:
        yield server","Point(row=19, column=0)","Point(row=21, column=20)",,autogpt_platform/backend/test/conftest.py
graph_cleanup,function,,"async def graph_cleanup(server):
    created_graph_ids = []
    original_create_graph = server.agent_server.test_create_graph

    async def create_graph_wrapper(*args, **kwargs):
        created_graph = await original_create_graph(*args, **kwargs)
        # Extract user_id correctly
        user_id = kwargs.get(""user_id"", args[2] if len(args) > 2 else None)
        created_graph_ids.append((created_graph.id, user_id))
        return created_graph

    try:
        server.agent_server.test_create_graph = create_graph_wrapper
        yield  # This runs the test function
    finally:
        server.agent_server.test_create_graph = original_create_graph

        # Delete the created graphs and assert they were deleted
        for graph_id, user_id in created_graph_ids:
            if user_id:
                resp = await server.agent_server.test_delete_graph(graph_id, user_id)
                num_deleted = resp[""version_counts""]
                assert num_deleted > 0, f""Graph {graph_id} was not deleted.""","Point(row=25, column=0)","Point(row=47, column=76)",,autogpt_platform/backend/test/conftest.py
graph_cleanup.create_graph_wrapper,function,,"async def create_graph_wrapper(*args, **kwargs):
        created_graph = await original_create_graph(*args, **kwargs)
        # Extract user_id correctly
        user_id = kwargs.get(""user_id"", args[2] if len(args) > 2 else None)
        created_graph_ids.append((created_graph.id, user_id))
        return created_graph","Point(row=29, column=4)","Point(row=34, column=28)",,autogpt_platform/backend/test/conftest.py
ServiceTest,class,,"class ServiceTest(AppService):
    def __init__(self):
        super().__init__()

    @classmethod
    def get_port(cls) -> int:
        return TEST_SERVICE_PORT

    @expose
    def add(self, a: int, b: int) -> int:
        return a + b

    @expose
    def subtract(self, a: int, b: int) -> int:
        return a - b

    @expose
    def fun_with_async(self, a: int, b: int) -> int:
        async def add_async(a: int, b: int) -> int:
            return a + b

        return self.run_and_wait(add_async(a, b))","Point(row=7, column=0)","Point(row=28, column=49)",,autogpt_platform/backend/test/util/test_service.py
ServiceTest.__init__,function,,"def __init__(self):
        super().__init__()","Point(row=8, column=4)","Point(row=9, column=26)",ServiceTest,autogpt_platform/backend/test/util/test_service.py
ServiceTest.get_port,function,,"def get_port(cls) -> int:
        return TEST_SERVICE_PORT","Point(row=12, column=4)","Point(row=13, column=32)",ServiceTest,autogpt_platform/backend/test/util/test_service.py
ServiceTest.add,function,,"def add(self, a: int, b: int) -> int:
        return a + b","Point(row=16, column=4)","Point(row=17, column=20)",ServiceTest,autogpt_platform/backend/test/util/test_service.py
ServiceTest.subtract,function,,"def subtract(self, a: int, b: int) -> int:
        return a - b","Point(row=20, column=4)","Point(row=21, column=20)",ServiceTest,autogpt_platform/backend/test/util/test_service.py
ServiceTest.fun_with_async,function,,"def fun_with_async(self, a: int, b: int) -> int:
        async def add_async(a: int, b: int) -> int:
            return a + b

        return self.run_and_wait(add_async(a, b))","Point(row=24, column=4)","Point(row=28, column=49)",ServiceTest,autogpt_platform/backend/test/util/test_service.py
ServiceTest.fun_with_async.add_async,function,,"async def add_async(a: int, b: int) -> int:
            return a + b","Point(row=25, column=8)","Point(row=26, column=24)",ServiceTest,autogpt_platform/backend/test/util/test_service.py
test_service_creation,function,,"async def test_service_creation(server):
    with ServiceTest():
        client = get_service_client(ServiceTest)
        assert client.add(5, 3) == 8
        assert client.subtract(10, 4) == 6
        assert client.fun_with_async(5, 3) == 8","Point(row=32, column=0)","Point(row=37, column=47)",,autogpt_platform/backend/test/util/test_service.py
test_type_conversion,function,,"def test_type_conversion():
    assert convert(5.5, int) == 5
    assert convert(""5.5"", int) == 5
    assert convert([1, 2, 3], int) == 3

    assert convert(""5.5"", float) == 5.5
    assert convert(5, float) == 5.0

    assert convert(""True"", bool) is True
    assert convert(""False"", bool) is False

    assert convert(5, str) == ""5""
    assert convert({""a"": 1, ""b"": 2}, str) == '{""a"": 1, ""b"": 2}'
    assert convert([1, 2, 3], str) == ""[1, 2, 3]""

    assert convert(""5"", list) == [""5""]
    assert convert((1, 2, 3), list) == [1, 2, 3]
    assert convert({1, 2, 3}, list) == [1, 2, 3]

    assert convert(""5"", dict) == {""value"": 5}
    assert convert('{""a"": 1, ""b"": 2}', dict) == {""a"": 1, ""b"": 2}
    assert convert([1, 2, 3], dict) == {0: 1, 1: 2, 2: 3}
    assert convert((1, 2, 3), dict) == {0: 1, 1: 2, 2: 3}

    from typing import List

    assert convert(""5"", List[int]) == [5]
    assert convert(""[5,4,2]"", List[int]) == [5, 4, 2]
    assert convert([5, 4, 2], List[str]) == [""5"", ""4"", ""2""]","Point(row=3, column=0)","Point(row=31, column=59)",,autogpt_platform/backend/test/util/test_type.py
test_validate_url,function,,"def test_validate_url():
    with pytest.raises(ValueError):
        validate_url(""localhost"", [])

    with pytest.raises(ValueError):
        validate_url(""192.168.1.1"", [])

    with pytest.raises(ValueError):
        validate_url(""127.0.0.1"", [])

    with pytest.raises(ValueError):
        validate_url(""0.0.0.0"", [])

    validate_url(""google.com"", [])
    validate_url(""github.com"", [])
    validate_url(""http://github.com"", [])","Point(row=5, column=0)","Point(row=20, column=41)",,autogpt_platform/backend/test/util/test_request.py
example_function,function,,"def example_function(a: int, b: int, c: int) -> int:
    time.sleep(0.5)
    return a + b + c","Point(row=6, column=0)","Point(row=8, column=20)",,autogpt_platform/backend/test/util/decorator.py
example_function_with_error,function,,"def example_function_with_error(a: int, b: int, c: int) -> int:
    raise ValueError(""This is a test error"")","Point(row=12, column=0)","Point(row=13, column=44)",,autogpt_platform/backend/test/util/decorator.py
test_timer_decorator,function,,"def test_timer_decorator():
    info, res = example_function(1, 2, 3)
    assert info.cpu_time >= 0
    assert info.wall_time >= 0.4
    assert res == 6","Point(row=16, column=0)","Point(row=20, column=19)",,autogpt_platform/backend/test/util/decorator.py
test_error_decorator,function,,"def test_error_decorator():
    res = example_function_with_error(1, 2, 3)
    assert res is None","Point(row=23, column=0)","Point(row=25, column=22)",,autogpt_platform/backend/test/util/decorator.py
connection_manager,function,,"def connection_manager() -> ConnectionManager:
    return ConnectionManager()","Point(row=12, column=0)","Point(row=13, column=30)",,autogpt_platform/backend/test/server/test_con_manager.py
mock_websocket,function,,"def mock_websocket() -> AsyncMock:
    websocket: AsyncMock = AsyncMock(spec=WebSocket)
    websocket.send_text = AsyncMock()
    return websocket","Point(row=17, column=0)","Point(row=20, column=20)",,autogpt_platform/backend/test/server/test_con_manager.py
test_connect,function,,"async def test_connect(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    await connection_manager.connect(mock_websocket)
    assert mock_websocket in connection_manager.active_connections
    mock_websocket.accept.assert_called_once()","Point(row=24, column=0)","Point(row=29, column=46)",,autogpt_platform/backend/test/server/test_con_manager.py
test_disconnect,function,,"def test_disconnect(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    connection_manager.active_connections.add(mock_websocket)
    connection_manager.subscriptions[""test_graph""] = {mock_websocket}

    connection_manager.disconnect(mock_websocket)

    assert mock_websocket not in connection_manager.active_connections
    assert mock_websocket not in connection_manager.subscriptions[""test_graph""]","Point(row=32, column=0)","Point(row=41, column=79)",,autogpt_platform/backend/test/server/test_con_manager.py
test_subscribe,function,,"async def test_subscribe(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    await connection_manager.subscribe(""test_graph"", mock_websocket)
    assert mock_websocket in connection_manager.subscriptions[""test_graph""]","Point(row=45, column=0)","Point(row=49, column=75)",,autogpt_platform/backend/test/server/test_con_manager.py
test_unsubscribe,function,,"async def test_unsubscribe(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    connection_manager.subscriptions[""test_graph""] = {mock_websocket}

    await connection_manager.unsubscribe(""test_graph"", mock_websocket)

    assert ""test_graph"" not in connection_manager.subscriptions","Point(row=53, column=0)","Point(row=60, column=63)",,autogpt_platform/backend/test/server/test_con_manager.py
test_send_execution_result,function,,"async def test_send_execution_result(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    connection_manager.subscriptions[""test_graph""] = {mock_websocket}
    result: ExecutionResult = ExecutionResult(
        graph_id=""test_graph"",
        graph_version=1,
        graph_exec_id=""test_exec_id"",
        node_exec_id=""test_node_exec_id"",
        node_id=""test_node_id"",
        status=ExecutionStatus.COMPLETED,
        input_data={""input1"": ""value1""},
        output_data={""output1"": [""result1""]},
        add_time=datetime.now(tz=timezone.utc),
        queue_time=None,
        start_time=datetime.now(tz=timezone.utc),
        end_time=datetime.now(tz=timezone.utc),
    )

    await connection_manager.send_execution_result(result)

    mock_websocket.send_text.assert_called_once_with(
        WsMessage(
            method=Methods.EXECUTION_EVENT,
            channel=""test_graph"",
            data=result.model_dump(),
        ).model_dump_json()
    )","Point(row=64, column=0)","Point(row=91, column=5)",,autogpt_platform/backend/test/server/test_con_manager.py
test_send_execution_result_no_subscribers,function,,"async def test_send_execution_result_no_subscribers(
    connection_manager: ConnectionManager, mock_websocket: AsyncMock
) -> None:
    result: ExecutionResult = ExecutionResult(
        graph_id=""test_graph"",
        graph_version=1,
        graph_exec_id=""test_exec_id"",
        node_exec_id=""test_node_exec_id"",
        node_id=""test_node_id"",
        status=ExecutionStatus.COMPLETED,
        input_data={""input1"": ""value1""},
        output_data={""output1"": [""result1""]},
        add_time=datetime.now(),
        queue_time=None,
        start_time=datetime.now(),
        end_time=datetime.now(),
    )

    await connection_manager.send_execution_result(result)

    mock_websocket.send_text.assert_not_called()","Point(row=95, column=0)","Point(row=115, column=48)",,autogpt_platform/backend/test/server/test_con_manager.py
mock_websocket,function,,"def mock_websocket() -> AsyncMock:
    return AsyncMock(spec=WebSocket)","Point(row=17, column=0)","Point(row=18, column=36)",,autogpt_platform/backend/test/server/test_ws_api.py
mock_manager,function,,"def mock_manager() -> AsyncMock:
    return AsyncMock(spec=ConnectionManager)","Point(row=22, column=0)","Point(row=23, column=44)",,autogpt_platform/backend/test/server/test_ws_api.py
test_websocket_router_subscribe,function,,"async def test_websocket_router_subscribe(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    mock_websocket.receive_text.side_effect = [
        WsMessage(
            method=Methods.SUBSCRIBE, data={""graph_id"": ""test_graph""}
        ).model_dump_json(),
        WebSocketDisconnect(),
    ]

    await websocket_router(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager)
    )

    mock_manager.connect.assert_called_once_with(mock_websocket)
    mock_manager.subscribe.assert_called_once_with(""test_graph"", mock_websocket)
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""subscribe""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":true' in mock_websocket.send_text.call_args[0][0]
    mock_manager.disconnect.assert_called_once_with(mock_websocket)","Point(row=27, column=0)","Point(row=46, column=67)",,autogpt_platform/backend/test/server/test_ws_api.py
test_websocket_router_unsubscribe,function,,"async def test_websocket_router_unsubscribe(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    mock_websocket.receive_text.side_effect = [
        WsMessage(
            method=Methods.UNSUBSCRIBE, data={""graph_id"": ""test_graph""}
        ).model_dump_json(),
        WebSocketDisconnect(),
    ]

    await websocket_router(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager)
    )

    mock_manager.connect.assert_called_once_with(mock_websocket)
    mock_manager.unsubscribe.assert_called_once_with(""test_graph"", mock_websocket)
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""unsubscribe""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":true' in mock_websocket.send_text.call_args[0][0]
    mock_manager.disconnect.assert_called_once_with(mock_websocket)","Point(row=50, column=0)","Point(row=69, column=67)",,autogpt_platform/backend/test/server/test_ws_api.py
test_websocket_router_invalid_method,function,,"async def test_websocket_router_invalid_method(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    mock_websocket.receive_text.side_effect = [
        WsMessage(method=Methods.EXECUTION_EVENT).model_dump_json(),
        WebSocketDisconnect(),
    ]

    await websocket_router(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager)
    )

    mock_manager.connect.assert_called_once_with(mock_websocket)
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""error""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":false' in mock_websocket.send_text.call_args[0][0]
    mock_manager.disconnect.assert_called_once_with(mock_websocket)","Point(row=73, column=0)","Point(row=89, column=67)",,autogpt_platform/backend/test/server/test_ws_api.py
test_handle_subscribe_success,function,,"async def test_handle_subscribe_success(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    message = WsMessage(method=Methods.SUBSCRIBE, data={""graph_id"": ""test_graph""})

    await handle_subscribe(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager), message
    )

    mock_manager.subscribe.assert_called_once_with(""test_graph"", mock_websocket)
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""subscribe""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":true' in mock_websocket.send_text.call_args[0][0]","Point(row=93, column=0)","Point(row=105, column=71)",,autogpt_platform/backend/test/server/test_ws_api.py
test_handle_subscribe_missing_data,function,,"async def test_handle_subscribe_missing_data(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    message = WsMessage(method=Methods.SUBSCRIBE)

    await handle_subscribe(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager), message
    )

    mock_manager.subscribe.assert_not_called()
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""error""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":false' in mock_websocket.send_text.call_args[0][0]","Point(row=109, column=0)","Point(row=121, column=72)",,autogpt_platform/backend/test/server/test_ws_api.py
test_handle_unsubscribe_success,function,,"async def test_handle_unsubscribe_success(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    message = WsMessage(method=Methods.UNSUBSCRIBE, data={""graph_id"": ""test_graph""})

    await handle_unsubscribe(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager), message
    )

    mock_manager.unsubscribe.assert_called_once_with(""test_graph"", mock_websocket)
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""unsubscribe""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":true' in mock_websocket.send_text.call_args[0][0]","Point(row=125, column=0)","Point(row=137, column=71)",,autogpt_platform/backend/test/server/test_ws_api.py
test_handle_unsubscribe_missing_data,function,,"async def test_handle_unsubscribe_missing_data(
    mock_websocket: AsyncMock, mock_manager: AsyncMock
) -> None:
    message = WsMessage(method=Methods.UNSUBSCRIBE)

    await handle_unsubscribe(
        cast(WebSocket, mock_websocket), cast(ConnectionManager, mock_manager), message
    )

    mock_manager.unsubscribe.assert_not_called()
    mock_websocket.send_text.assert_called_once()
    assert '""method"":""error""' in mock_websocket.send_text.call_args[0][0]
    assert '""success"":false' in mock_websocket.send_text.call_args[0][0]","Point(row=141, column=0)","Point(row=153, column=72)",,autogpt_platform/backend/test/server/test_ws_api.py
test_agent_schedule,function,,"async def test_agent_schedule(server: SpinTestServer):
    await db.connect()
    test_user = await create_test_user()
    test_graph = await server.agent_server.test_create_graph(
        create_graph=CreateGraph(graph=create_test_graph()),
        is_template=False,
        user_id=test_user.id,
    )

    scheduler = get_service_client(ExecutionScheduler)
    schedules = scheduler.get_execution_schedules(test_graph.id, test_user.id)
    assert len(schedules) == 0

    schedule_id = scheduler.add_execution_schedule(
        graph_id=test_graph.id,
        user_id=test_user.id,
        graph_version=1,
        cron=""0 0 * * *"",
        input_data={""input"": ""data""},
    )
    assert schedule_id

    schedules = scheduler.get_execution_schedules(test_graph.id, test_user.id)
    assert len(schedules) == 1
    assert schedules[schedule_id] == ""0 0 * * *""

    scheduler.update_schedule(schedule_id, is_enabled=False, user_id=test_user.id)
    schedules = scheduler.get_execution_schedules(test_graph.id, user_id=test_user.id)
    assert len(schedules) == 0","Point(row=11, column=0)","Point(row=39, column=30)",,autogpt_platform/backend/test/executor/test_scheduler.py
create_graph,function,,"async def create_graph(s: SpinTestServer, g: graph.Graph, u: User) -> graph.Graph:
    logger.info(f""Creating graph for user {u.id}"")
    return await s.agent_server.test_create_graph(CreateGraph(graph=g), u.id)","Point(row=16, column=0)","Point(row=18, column=77)",,autogpt_platform/backend/test/executor/test_manager.py
execute_graph,function,,"async def execute_graph(
    agent_server: AgentServer,
    test_graph: graph.Graph,
    test_user: User,
    input_data: dict,
    num_execs: int = 4,
) -> str:
    logger.info(f""Executing graph {test_graph.id} for user {test_user.id}"")
    logger.info(f""Input data: {input_data}"")

    # --- Test adding new executions --- #
    response = await agent_server.test_execute_graph(
        test_graph.id, input_data, test_user.id
    )
    graph_exec_id = response[""id""]
    logger.info(f""Created execution with ID: {graph_exec_id}"")

    # Execution queue should be empty
    logger.info(""Waiting for execution to complete..."")
    result = await wait_execution(test_user.id, test_graph.id, graph_exec_id)
    logger.info(f""Execution completed with {len(result)} results"")
    assert result and len(result) == num_execs
    return graph_exec_id","Point(row=21, column=0)","Point(row=43, column=24)",,autogpt_platform/backend/test/executor/test_manager.py
assert_sample_graph_executions,function,,"async def assert_sample_graph_executions(
    agent_server: AgentServer,
    test_graph: graph.Graph,
    test_user: User,
    graph_exec_id: str,
):
    logger.info(f""Checking execution results for graph {test_graph.id}"")
    executions = await agent_server.test_get_graph_run_node_execution_results(
        test_graph.id,
        graph_exec_id,
        test_user.id,
    )

    output_list = [{""result"": [""Hello""]}, {""result"": [""World""]}]
    input_list = [
        {
            ""name"": ""input_1"",
            ""value"": ""Hello"",
        },
        {
            ""name"": ""input_2"",
            ""value"": ""World"",
        },
    ]

    # Executing StoreValueBlock
    exec = executions[0]
    logger.info(f""Checking first StoreValueBlock execution: {exec}"")
    assert exec.status == execution.ExecutionStatus.COMPLETED
    assert exec.graph_exec_id == graph_exec_id
    assert (
        exec.output_data in output_list
    ), f""Output data: {exec.output_data} and {output_list}""
    assert (
        exec.input_data in input_list
    ), f""Input data: {exec.input_data} and {input_list}""
    assert exec.node_id in [test_graph.nodes[0].id, test_graph.nodes[1].id]

    # Executing StoreValueBlock
    exec = executions[1]
    logger.info(f""Checking second StoreValueBlock execution: {exec}"")
    assert exec.status == execution.ExecutionStatus.COMPLETED
    assert exec.graph_exec_id == graph_exec_id
    assert (
        exec.output_data in output_list
    ), f""Output data: {exec.output_data} and {output_list}""
    assert (
        exec.input_data in input_list
    ), f""Input data: {exec.input_data} and {input_list}""
    assert exec.node_id in [test_graph.nodes[0].id, test_graph.nodes[1].id]

    # Executing FillTextTemplateBlock
    exec = executions[2]
    logger.info(f""Checking FillTextTemplateBlock execution: {exec}"")
    assert exec.status == execution.ExecutionStatus.COMPLETED
    assert exec.graph_exec_id == graph_exec_id
    assert exec.output_data == {""output"": [""Hello, World!!!""]}
    assert exec.input_data == {
        ""format"": ""{a}, {b}{c}"",
        ""values"": {""a"": ""Hello"", ""b"": ""World"", ""c"": ""!!!""},
        ""values_#_a"": ""Hello"",
        ""values_#_b"": ""World"",
        ""values_#_c"": ""!!!"",
    }
    assert exec.node_id == test_graph.nodes[2].id

    # Executing PrintToConsoleBlock
    exec = executions[3]
    logger.info(f""Checking PrintToConsoleBlock execution: {exec}"")
    assert exec.status == execution.ExecutionStatus.COMPLETED
    assert exec.graph_exec_id == graph_exec_id
    assert exec.output_data == {""status"": [""printed""]}
    assert exec.input_data == {""text"": ""Hello, World!!!""}
    assert exec.node_id == test_graph.nodes[3].id","Point(row=46, column=0)","Point(row=119, column=49)",,autogpt_platform/backend/test/executor/test_manager.py
test_agent_execution,function,,"async def test_agent_execution(server: SpinTestServer):
    logger.info(""Starting test_agent_execution"")
    test_user = await create_test_user()
    test_graph = await create_graph(server, create_test_graph(), test_user)
    data = {""input_1"": ""Hello"", ""input_2"": ""World""}
    graph_exec_id = await execute_graph(
        server.agent_server,
        test_graph,
        test_user,
        data,
        4,
    )
    await assert_sample_graph_executions(
        server.agent_server, test_graph, test_user, graph_exec_id
    )
    logger.info(""Completed test_agent_execution"")","Point(row=123, column=0)","Point(row=138, column=49)",,autogpt_platform/backend/test/executor/test_manager.py
test_input_pin_always_waited,function,"
    This test is asserting that the input pin should always be waited for the execution,
    even when default value on that pin is defined, the value has to be ignored.

    Test scenario:
    StoreValueBlock1
                \\ input
                     >------- FindInDictionaryBlock | input_default: key: """", input: {}
                // key
    StoreValueBlock2
","async def test_input_pin_always_waited(server: SpinTestServer):
    """"""
    This test is asserting that the input pin should always be waited for the execution,
    even when default value on that pin is defined, the value has to be ignored.

    Test scenario:
    StoreValueBlock1
                \\ input
                     >------- FindInDictionaryBlock | input_default: key: """", input: {}
                // key
    StoreValueBlock2
    """"""
    logger.info(""Starting test_input_pin_always_waited"")
    nodes = [
        graph.Node(
            block_id=StoreValueBlock().id,
            input_default={""input"": {""key1"": ""value1"", ""key2"": ""value2""}},
        ),
        graph.Node(
            block_id=StoreValueBlock().id,
            input_default={""input"": ""key2""},
        ),
        graph.Node(
            block_id=FindInDictionaryBlock().id,
            input_default={""key"": """", ""input"": {}},
        ),
    ]
    links = [
        graph.Link(
            source_id=nodes[0].id,
            sink_id=nodes[2].id,
            source_name=""output"",
            sink_name=""input"",
        ),
        graph.Link(
            source_id=nodes[1].id,
            sink_id=nodes[2].id,
            source_name=""output"",
            sink_name=""key"",
        ),
    ]
    test_graph = graph.Graph(
        name=""TestGraph"",
        description=""Test graph"",
        nodes=nodes,
        links=links,
    )
    test_user = await create_test_user()
    test_graph = await create_graph(server, test_graph, test_user)
    graph_exec_id = await execute_graph(
        server.agent_server, test_graph, test_user, {}, 3
    )

    logger.info(""Checking execution results"")
    executions = await server.agent_server.test_get_graph_run_node_execution_results(
        test_graph.id, graph_exec_id, test_user.id
    )
    assert len(executions) == 3
    # FindInDictionaryBlock should wait for the input pin to be provided,
    # Hence executing extraction of ""key"" from {""key1"": ""value1"", ""key2"": ""value2""}
    assert executions[2].status == execution.ExecutionStatus.COMPLETED
    assert executions[2].output_data == {""output"": [""value2""]}
    logger.info(""Completed test_input_pin_always_waited"")","Point(row=142, column=0)","Point(row=204, column=57)",,autogpt_platform/backend/test/executor/test_manager.py
test_static_input_link_on_graph,function,"
    This test is asserting the behaviour of static input link, e.g: reusable input link.

    Test scenario:
    *StoreValueBlock1*===a=========\\
    *StoreValueBlock2*===a=====\\  ||
    *StoreValueBlock3*===a===*MathBlock*====b / static====*StoreValueBlock5*
    *StoreValueBlock4*=========================================//

    In this test, there will be three input waiting in the MathBlock input pin `a`.
    And later, another output is produced on input pin `b`, which is a static link,
    this input will complete the input of those three incomplete executions.
","async def test_static_input_link_on_graph(server: SpinTestServer):
    """"""
    This test is asserting the behaviour of static input link, e.g: reusable input link.

    Test scenario:
    *StoreValueBlock1*===a=========\\
    *StoreValueBlock2*===a=====\\  ||
    *StoreValueBlock3*===a===*MathBlock*====b / static====*StoreValueBlock5*
    *StoreValueBlock4*=========================================//

    In this test, there will be three input waiting in the MathBlock input pin `a`.
    And later, another output is produced on input pin `b`, which is a static link,
    this input will complete the input of those three incomplete executions.
    """"""
    logger.info(""Starting test_static_input_link_on_graph"")
    nodes = [
        graph.Node(block_id=StoreValueBlock().id, input_default={""input"": 4}),  # a
        graph.Node(block_id=StoreValueBlock().id, input_default={""input"": 4}),  # a
        graph.Node(block_id=StoreValueBlock().id, input_default={""input"": 4}),  # a
        graph.Node(block_id=StoreValueBlock().id, input_default={""input"": 5}),  # b
        graph.Node(block_id=StoreValueBlock().id),
        graph.Node(
            block_id=CalculatorBlock().id,
            input_default={""operation"": Operation.ADD.value},
        ),
    ]
    links = [
        graph.Link(
            source_id=nodes[0].id,
            sink_id=nodes[5].id,
            source_name=""output"",
            sink_name=""a"",
        ),
        graph.Link(
            source_id=nodes[1].id,
            sink_id=nodes[5].id,
            source_name=""output"",
            sink_name=""a"",
        ),
        graph.Link(
            source_id=nodes[2].id,
            sink_id=nodes[5].id,
            source_name=""output"",
            sink_name=""a"",
        ),
        graph.Link(
            source_id=nodes[3].id,
            sink_id=nodes[4].id,
            source_name=""output"",
            sink_name=""input"",
        ),
        graph.Link(
            source_id=nodes[4].id,
            sink_id=nodes[5].id,
            source_name=""output"",
            sink_name=""b"",
            is_static=True,  # This is the static link to test.
        ),
    ]
    test_graph = graph.Graph(
        name=""TestGraph"",
        description=""Test graph"",
        nodes=nodes,
        links=links,
    )
    test_user = await create_test_user()
    test_graph = await create_graph(server, test_graph, test_user)
    graph_exec_id = await execute_graph(
        server.agent_server, test_graph, test_user, {}, 8
    )
    logger.info(""Checking execution results"")
    executions = await server.agent_server.test_get_graph_run_node_execution_results(
        test_graph.id, graph_exec_id, test_user.id
    )
    assert len(executions) == 8
    # The last 3 executions will be a+b=4+5=9
    for i, exec_data in enumerate(executions[-3:]):
        logger.info(f""Checking execution {i+1} of last 3: {exec_data}"")
        assert exec_data.status == execution.ExecutionStatus.COMPLETED
        assert exec_data.output_data == {""result"": [9]}
    logger.info(""Completed test_static_input_link_on_graph"")","Point(row=208, column=0)","Point(row=288, column=60)",,autogpt_platform/backend/test/executor/test_manager.py
test_block_credit_usage,function,,"async def test_block_credit_usage(server: SpinTestServer):
    current_credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)

    spending_amount_1 = await user_credit.spend_credits(
        DEFAULT_USER_ID,
        current_credit,
        AITextGeneratorBlock().id,
        {""model"": ""gpt-4-turbo""},
        0.0,
        0.0,
        validate_balance=False,
    )
    assert spending_amount_1 > 0

    spending_amount_2 = await user_credit.spend_credits(
        DEFAULT_USER_ID,
        current_credit,
        AITextGeneratorBlock().id,
        {""model"": ""gpt-4-turbo"", ""api_key"": ""owned_api_key""},
        0.0,
        0.0,
        validate_balance=False,
    )
    assert spending_amount_2 == 0

    new_credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)
    assert new_credit == current_credit - spending_amount_1 - spending_amount_2","Point(row=15, column=0)","Point(row=41, column=79)",,autogpt_platform/backend/test/data/test_credit.py
test_block_credit_top_up,function,,"async def test_block_credit_top_up(server: SpinTestServer):
    current_credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)

    await user_credit.top_up_credits(DEFAULT_USER_ID, 100)

    new_credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)
    assert new_credit == current_credit + 100","Point(row=45, column=0)","Point(row=51, column=45)",,autogpt_platform/backend/test/data/test_credit.py
test_block_credit_reset,function,,"async def test_block_credit_reset(server: SpinTestServer):
    month1 = datetime(2022, 1, 15)
    month2 = datetime(2022, 2, 15)

    user_credit.time_now = lambda: month2
    month2credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)

    # Month 1 result should only affect month 1
    user_credit.time_now = lambda: month1
    month1credit = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)
    await user_credit.top_up_credits(DEFAULT_USER_ID, 100)
    assert await user_credit.get_or_refill_credit(DEFAULT_USER_ID) == month1credit + 100

    # Month 2 balance is unaffected
    user_credit.time_now = lambda: month2
    assert await user_credit.get_or_refill_credit(DEFAULT_USER_ID) == month2credit","Point(row=55, column=0)","Point(row=70, column=82)",,autogpt_platform/backend/test/data/test_credit.py
test_credit_refill,function,,"async def test_credit_refill(server: SpinTestServer):
    # Clear all transactions within the month
    await UserBlockCredit.prisma().update_many(
        where={
            ""userId"": DEFAULT_USER_ID,
            ""createdAt"": {
                ""gte"": datetime(2022, 2, 1),
                ""lt"": datetime(2022, 3, 1),
            },
        },
        data={""isActive"": False},
    )
    user_credit.time_now = lambda: datetime(2022, 2, 15)

    balance = await user_credit.get_or_refill_credit(DEFAULT_USER_ID)
    assert balance == REFILL_VALUE","Point(row=74, column=0)","Point(row=89, column=34)",,autogpt_platform/backend/test/data/test_credit.py
test_graph_creation,function,"
    Test the creation of a graph with nodes and links.

    This test ensures that:
    1. A graph can be successfully created with valid connections.
    2. The created graph has the correct structure and properties.

    Args:
        server (SpinTestServer): The test server instance.
","async def test_graph_creation(server: SpinTestServer):
    """"""
    Test the creation of a graph with nodes and links.

    This test ensures that:
    1. A graph can be successfully created with valid connections.
    2. The created graph has the correct structure and properties.

    Args:
        server (SpinTestServer): The test server instance.
    """"""
    value_block = StoreValueBlock().id
    input_block = AgentInputBlock().id

    graph = Graph(
        id=""test_graph"",
        name=""TestGraph"",
        description=""Test graph"",
        nodes=[
            Node(id=""node_1"", block_id=value_block),
            Node(id=""node_2"", block_id=input_block, input_default={""name"": ""input""}),
            Node(id=""node_3"", block_id=value_block),
        ],
        links=[
            Link(
                source_id=""node_1"",
                sink_id=""node_2"",
                source_name=""output"",
                sink_name=""name"",
            ),
        ],
    )
    create_graph = CreateGraph(graph=graph)
    created_graph = await server.agent_server.test_create_graph(
        create_graph, DEFAULT_USER_ID
    )

    assert UUID(created_graph.id)
    assert created_graph.name == ""TestGraph""

    assert len(created_graph.nodes) == 3
    assert UUID(created_graph.nodes[0].id)
    assert UUID(created_graph.nodes[1].id)
    assert UUID(created_graph.nodes[2].id)

    nodes = created_graph.nodes
    links = created_graph.links
    assert len(links) == 1
    assert links[0].source_id != links[0].sink_id
    assert links[0].source_id in {nodes[0].id, nodes[1].id, nodes[2].id}
    assert links[0].sink_id in {nodes[0].id, nodes[1].id, nodes[2].id}","Point(row=15, column=0)","Point(row=65, column=70)",,autogpt_platform/backend/test/data/test_graph.py
test_get_input_schema,function,"
    Test the get_input_schema method of a created graph.

    This test ensures that:
    1. A graph can be created with a single node.
    2. The input schema of the created graph is correctly generated.
    3. The input schema contains the expected input name and node id.

    Args:
        server (SpinTestServer): The test server instance.
","async def test_get_input_schema(server: SpinTestServer):
    """"""
    Test the get_input_schema method of a created graph.

    This test ensures that:
    1. A graph can be created with a single node.
    2. The input schema of the created graph is correctly generated.
    3. The input schema contains the expected input name and node id.

    Args:
        server (SpinTestServer): The test server instance.
    """"""
    value_block = StoreValueBlock().id
    input_block = AgentInputBlock().id
    output_block = AgentOutputBlock().id

    graph = Graph(
        name=""TestInputSchema"",
        description=""Test input schema"",
        nodes=[
            Node(
                id=""node_0_a"",
                block_id=input_block,
                input_default={""name"": ""in_key_a"", ""title"": ""Key A"", ""value"": ""A""},
                metadata={""id"": ""node_0_a""},
            ),
            Node(
                id=""node_0_b"",
                block_id=input_block,
                input_default={""name"": ""in_key_b"", ""advanced"": True},
                metadata={""id"": ""node_0_b""},
            ),
            Node(id=""node_1"", block_id=value_block, metadata={""id"": ""node_1""}),
            Node(
                id=""node_2"",
                block_id=output_block,
                input_default={
                    ""name"": ""out_key"",
                    ""description"": ""This is an output key"",
                },
                metadata={""id"": ""node_2""},
            ),
        ],
        links=[
            Link(
                source_id=""node_0_a"",
                sink_id=""node_1"",
                source_name=""result"",
                sink_name=""input"",
            ),
            Link(
                source_id=""node_0_b"",
                sink_id=""node_1"",
                source_name=""result"",
                sink_name=""input"",
            ),
            Link(
                source_id=""node_1"",
                sink_id=""node_2"",
                source_name=""output"",
                sink_name=""value"",
            ),
        ],
    )

    create_graph = CreateGraph(graph=graph)
    created_graph = await server.agent_server.test_create_graph(
        create_graph, DEFAULT_USER_ID
    )

    class ExpectedInputSchema(BlockSchema):
        in_key_a: Any = SchemaField(title=""Key A"", default=""A"", advanced=False)
        in_key_b: Any = SchemaField(title=""in_key_b"", advanced=True)

    class ExpectedOutputSchema(BlockSchema):
        out_key: Any = SchemaField(
            description=""This is an output key"",
            title=""out_key"",
            advanced=False,
        )

    input_schema = created_graph.input_schema
    input_schema[""title""] = ""ExpectedInputSchema""
    assert input_schema == ExpectedInputSchema.jsonschema()

    output_schema = created_graph.output_schema
    output_schema[""title""] = ""ExpectedOutputSchema""
    assert output_schema == ExpectedOutputSchema.jsonschema()","Point(row=69, column=0)","Point(row=156, column=61)",,autogpt_platform/backend/test/data/test_graph.py
ExpectedInputSchema,class,,"class ExpectedInputSchema(BlockSchema):
        in_key_a: Any = SchemaField(title=""Key A"", default=""A"", advanced=False)
        in_key_b: Any = SchemaField(title=""in_key_b"", advanced=True)","Point(row=139, column=4)","Point(row=141, column=68)",,autogpt_platform/backend/test/data/test_graph.py
ExpectedOutputSchema,class,,"class ExpectedOutputSchema(BlockSchema):
        out_key: Any = SchemaField(
            description=""This is an output key"",
            title=""out_key"",
            advanced=False,
        )","Point(row=143, column=4)","Point(row=148, column=9)",,autogpt_platform/backend/test/data/test_graph.py
test_available_blocks,function,,"def test_available_blocks(block: Type[Block]):
    execute_block_test(block())","Point(row=9, column=0)","Point(row=10, column=31)",,autogpt_platform/backend/test/block/test_block.py
main,function,"
    Run all the processes required for the AutoGPT-server REST API.
","def main():
    """"""
    Run all the processes required for the AutoGPT-server REST API.
    """"""
    run_processes(
        DatabaseManager(),
        ExecutionManager(),
    )","Point(row=4, column=0)","Point(row=11, column=5)",,autogpt_platform/backend/backend/exec.py
main,function,"
    Run all the processes required for the AutoGPT-server REST API.
","def main():
    """"""
    Run all the processes required for the AutoGPT-server REST API.
    """"""
    run_processes(
        ExecutionScheduler(),
        AgentServer(),
    )","Point(row=5, column=0)","Point(row=12, column=5)",,autogpt_platform/backend/backend/rest.py
main,function,"
    Run all the processes required for the AutoGPT-server WebSocket API.
","def main():
    """"""
    Run all the processes required for the AutoGPT-server WebSocket API.
    """"""
    run_processes(WebsocketServer())","Point(row=4, column=0)","Point(row=8, column=36)",,autogpt_platform/backend/backend/ws.py
get_pid_path,function,,"def get_pid_path() -> pathlib.Path:
    home_dir = pathlib.Path.home()
    new_dir = home_dir / "".config"" / ""agpt""
    file_path = new_dir / ""running.tmp""
    return file_path","Point(row=14, column=0)","Point(row=18, column=20)",,autogpt_platform/backend/backend/cli.py
get_pid,function,,"def get_pid() -> int | None:
    file_path = get_pid_path()
    if not file_path.exists():
        return None

    os.makedirs(file_path.parent, exist_ok=True)
    with open(file_path, ""r"", encoding=""utf-8"") as file:
        pid = file.read()
    try:
        return int(pid)
    except ValueError:
        return None","Point(row=21, column=0)","Point(row=32, column=19)",,autogpt_platform/backend/backend/cli.py
write_pid,function,,"def write_pid(pid: int):
    file_path = get_pid_path()
    os.makedirs(file_path.parent, exist_ok=True)
    with open(file_path, ""w"", encoding=""utf-8"") as file:
        file.write(str(pid))","Point(row=35, column=0)","Point(row=39, column=28)",,autogpt_platform/backend/backend/cli.py
MainApp,class,,"class MainApp(AppProcess):
    def run(self):
        app.main(silent=True)","Point(row=42, column=0)","Point(row=44, column=29)",,autogpt_platform/backend/backend/cli.py
MainApp.run,function,,"def run(self):
        app.main(silent=True)","Point(row=43, column=4)","Point(row=44, column=29)",MainApp,autogpt_platform/backend/backend/cli.py
main,function,AutoGPT Server CLI Tool,"def main():
    """"""AutoGPT Server CLI Tool""""""
    pass","Point(row=48, column=0)","Point(row=50, column=8)",,autogpt_platform/backend/backend/cli.py
start,function,"
    Starts the server in the background and saves the PID
","def start():
    """"""
    Starts the server in the background and saves the PID
    """"""
    # Define the path for the new directory and file
    pid = get_pid()
    if pid and psutil.pid_exists(pid):
        print(""Server is already running"")
        exit(1)
    elif pid:
        print(""PID does not exist deleting file"")
        os.remove(get_pid_path())

    print(""Starting server"")
    pid = MainApp().start(background=True, silent=True)
    print(f""Server running in process: {pid}"")

    write_pid(pid)
    print(""done"")
    os._exit(status=0)","Point(row=54, column=0)","Point(row=73, column=22)",,autogpt_platform/backend/backend/cli.py
stop,function,"
    Stops the server
","def stop():
    """"""
    Stops the server
    """"""
    pid = get_pid()
    if not pid:
        print(""Server is not running"")
        return

    os.remove(get_pid_path())
    process = psutil.Process(int(pid))
    for child in process.children(recursive=True):
        child.terminate()
    process.terminate()

    print(""Server Stopped"")","Point(row=77, column=0)","Point(row=92, column=27)",,autogpt_platform/backend/backend/cli.py
test,function,"
    Group for test commands
","def test():
    """"""
    Group for test commands
    """"""
    pass","Point(row=96, column=0)","Point(row=100, column=8)",,autogpt_platform/backend/backend/cli.py
reddit,function,"
    Create an event graph
","def reddit(server_address: str):
    """"""
    Create an event graph
    """"""
    import requests

    from backend.usecases.reddit_marketing import create_test_graph

    test_graph = create_test_graph()
    url = f""{server_address}/graphs""
    headers = {""Content-Type"": ""application/json""}
    data = test_graph.model_dump_json()

    response = requests.post(url, headers=headers, data=data)

    graph_id = response.json()[""id""]
    print(f""Graph created with ID: {graph_id}"")","Point(row=105, column=0)","Point(row=121, column=47)",,autogpt_platform/backend/backend/cli.py
populate_db,function,"
    Create an event graph
","def populate_db(server_address: str):
    """"""
    Create an event graph
    """"""
    import requests

    from backend.usecases.sample import create_test_graph

    test_graph = create_test_graph()
    url = f""{server_address}/graphs""
    headers = {""Content-Type"": ""application/json""}
    data = test_graph.model_dump_json()

    response = requests.post(url, headers=headers, data=data)

    graph_id = response.json()[""id""]

    if response.status_code == 200:
        execute_url = f""{server_address}/graphs/{response.json()['id']}/execute""
        text = ""Hello, World!""
        input_data = {""input"": text}
        response = requests.post(execute_url, headers=headers, json=input_data)

        schedule_url = f""{server_address}/graphs/{graph_id}/schedules""
        data = {
            ""graph_id"": graph_id,
            ""cron"": ""*/5 * * * *"",
            ""input_data"": {""input"": ""Hello, World!""},
        }
        response = requests.post(schedule_url, headers=headers, json=data)

    print(""Database populated with: \n- graph\n- execution\n- schedule"")","Point(row=126, column=0)","Point(row=157, column=72)",,autogpt_platform/backend/backend/cli.py
graph,function,"
    Create an event graph
","def graph(server_address: str):
    """"""
    Create an event graph
    """"""
    import requests

    from backend.usecases.sample import create_test_graph

    url = f""{server_address}/graphs""
    headers = {""Content-Type"": ""application/json""}
    data = create_test_graph().model_dump_json()
    response = requests.post(url, headers=headers, data=data)

    if response.status_code == 200:
        print(response.json()[""id""])
        execute_url = f""{server_address}/graphs/{response.json()['id']}/execute""
        text = ""Hello, World!""
        input_data = {""input"": text}
        response = requests.post(execute_url, headers=headers, json=input_data)

    else:
        print(""Failed to send graph"")
        print(f""Response: {response.text}"")","Point(row=162, column=0)","Point(row=184, column=43)",,autogpt_platform/backend/backend/cli.py
execute,function,"
    Create an event graph
","def execute(graph_id: str, content: dict):
    """"""
    Create an event graph
    """"""
    import requests

    headers = {""Content-Type"": ""application/json""}

    execute_url = f""http://0.0.0.0:8000/graphs/{graph_id}/execute""
    requests.post(execute_url, headers=headers, json=content)","Point(row=190, column=0)","Point(row=199, column=61)",,autogpt_platform/backend/backend/cli.py
event,function,"
    Send an event to the running server
","def event():
    """"""
    Send an event to the running server
    """"""
    print(""Event sent"")","Point(row=203, column=0)","Point(row=207, column=23)",,autogpt_platform/backend/backend/cli.py
websocket,function,"
    Tests the websocket connection.
","def websocket(server_address: str, graph_id: str):
    """"""
    Tests the websocket connection.
    """"""
    import asyncio

    import websockets.asyncio.client

    from backend.server.ws_api import ExecutionSubscription, Methods, WsMessage

    async def send_message(server_address: str):
        uri = f""ws://{server_address}""
        async with websockets.asyncio.client.connect(uri) as websocket:
            try:
                msg = WsMessage(
                    method=Methods.SUBSCRIBE,
                    data=ExecutionSubscription(graph_id=graph_id).model_dump(),
                ).model_dump_json()
                await websocket.send(msg)
                print(f""Sending: {msg}"")
                while True:
                    response = await websocket.recv()
                    print(f""Response from server: {response}"")
            except InterruptedError:
                exit(0)

    asyncio.run(send_message(server_address))
    print(""Testing WS"")","Point(row=213, column=0)","Point(row=240, column=23)",,autogpt_platform/backend/backend/cli.py
websocket.send_message,function,,"async def send_message(server_address: str):
        uri = f""ws://{server_address}""
        async with websockets.asyncio.client.connect(uri) as websocket:
            try:
                msg = WsMessage(
                    method=Methods.SUBSCRIBE,
                    data=ExecutionSubscription(graph_id=graph_id).model_dump(),
                ).model_dump_json()
                await websocket.send(msg)
                print(f""Sending: {msg}"")
                while True:
                    response = await websocket.recv()
                    print(f""Response from server: {response}"")
            except InterruptedError:
                exit(0)","Point(row=223, column=4)","Point(row=237, column=23)",,autogpt_platform/backend/backend/cli.py
run_processes,function,"
    Execute all processes in the app. The last process is run in the foreground.
","def run_processes(*processes: ""AppProcess"", **kwargs):
    """"""
    Execute all processes in the app. The last process is run in the foreground.
    """"""
    try:
        for process in processes[:-1]:
            process.start(background=True, **kwargs)

        # Run the last process in the foreground
        processes[-1].start(background=False, **kwargs)
    finally:
        for process in processes:
            process.stop()","Point(row=6, column=0)","Point(row=18, column=26)",,autogpt_platform/backend/backend/app.py
main,function,"
    Run all the processes required for the AutoGPT-server (REST and WebSocket APIs).
","def main(**kwargs):
    """"""
    Run all the processes required for the AutoGPT-server (REST and WebSocket APIs).
    """"""

    from backend.executor import DatabaseManager, ExecutionManager, ExecutionScheduler
    from backend.server.rest_api import AgentServer
    from backend.server.ws_api import WebsocketServer

    run_processes(
        DatabaseManager(),
        ExecutionManager(),
        ExecutionScheduler(),
        WebsocketServer(),
        AgentServer(),
        **kwargs,
    )","Point(row=21, column=0)","Point(row=37, column=5)",,autogpt_platform/backend/backend/app.py
StepThroughItemsBlock,class,,"class StepThroughItemsBlock(Block):
    class Input(BlockSchema):
        items: list | dict = SchemaField(
            description=""The list or dictionary of items to iterate over"",
            placeholder=""[1, 2, 3, 4, 5] or {'key1': 'value1', 'key2': 'value2'}"",
        )

    class Output(BlockSchema):
        item: Any = SchemaField(description=""The current item in the iteration"")
        key: Any = SchemaField(
            description=""The key or index of the current item in the iteration"",
        )

    def __init__(self):
        super().__init__(
            id=""f66a3543-28d3-4ab5-8945-9b336371e2ce"",
            input_schema=StepThroughItemsBlock.Input,
            output_schema=StepThroughItemsBlock.Output,
            categories={BlockCategory.LOGIC},
            description=""Iterates over a list or dictionary and outputs each item."",
            test_input={""items"": [1, 2, 3, {""key1"": ""value1"", ""key2"": ""value2""}]},
            test_output=[
                (""item"", 1),
                (""key"", 0),
                (""item"", 2),
                (""key"", 1),
                (""item"", 3),
                (""key"", 2),
                (""item"", {""key1"": ""value1"", ""key2"": ""value2""}),
                (""key"", 3),
            ],
            test_mock={},
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        items = input_data.items
        if isinstance(items, dict):
            # If items is a dictionary, iterate over its values
            for item in items.values():
                yield ""item"", item
                yield ""key"", item
        else:
            # If items is a list, iterate over the list
            for index, item in enumerate(items):
                yield ""item"", item
                yield ""key"", index","Point(row=6, column=0)","Point(row=51, column=34)",,autogpt_platform/backend/backend/blocks/iteration.py
Input,class,,"class Input(BlockSchema):
        items: list | dict = SchemaField(
            description=""The list or dictionary of items to iterate over"",
            placeholder=""[1, 2, 3, 4, 5] or {'key1': 'value1', 'key2': 'value2'}"",
        )","Point(row=7, column=4)","Point(row=11, column=9)",,autogpt_platform/backend/backend/blocks/iteration.py
Output,class,,"class Output(BlockSchema):
        item: Any = SchemaField(description=""The current item in the iteration"")
        key: Any = SchemaField(
            description=""The key or index of the current item in the iteration"",
        )","Point(row=13, column=4)","Point(row=17, column=9)",,autogpt_platform/backend/backend/blocks/iteration.py
StepThroughItemsBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""f66a3543-28d3-4ab5-8945-9b336371e2ce"",
            input_schema=StepThroughItemsBlock.Input,
            output_schema=StepThroughItemsBlock.Output,
            categories={BlockCategory.LOGIC},
            description=""Iterates over a list or dictionary and outputs each item."",
            test_input={""items"": [1, 2, 3, {""key1"": ""value1"", ""key2"": ""value2""}]},
            test_output=[
                (""item"", 1),
                (""key"", 0),
                (""item"", 2),
                (""key"", 1),
                (""item"", 3),
                (""key"", 2),
                (""item"", {""key1"": ""value1"", ""key2"": ""value2""}),
                (""key"", 3),
            ],
            test_mock={},
        )","Point(row=19, column=4)","Point(row=38, column=9)",StepThroughItemsBlock,autogpt_platform/backend/backend/blocks/iteration.py
StepThroughItemsBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        items = input_data.items
        if isinstance(items, dict):
            # If items is a dictionary, iterate over its values
            for item in items.values():
                yield ""item"", item
                yield ""key"", item
        else:
            # If items is a list, iterate over the list
            for index, item in enumerate(items):
                yield ""item"", item
                yield ""key"", index","Point(row=40, column=4)","Point(row=51, column=34)",StepThroughItemsBlock,autogpt_platform/backend/backend/blocks/iteration.py
AudioTrack,class,,"class AudioTrack(str, Enum):
    OBSERVER = (""Observer"",)
    FUTURISTIC_BEAT = (""Futuristic Beat"",)
    SCIENCE_DOCUMENTARY = (""Science Documentary"",)
    HOTLINE = (""Hotline"",)
    BLADERUNNER_2049 = (""Bladerunner 2049"",)
    A_FUTURE = (""A Future"",)
    ELYSIAN_EMBERS = (""Elysian Embers"",)
    INSPIRING_CINEMATIC = (""Inspiring Cinematic"",)
    BLADERUNNER_REMIX = (""Bladerunner Remix"",)
    IZZAMUZZIC = (""Izzamuzzic"",)
    NAS = (""Nas"",)
    PARIS_ELSE = (""Paris - Else"",)
    SNOWFALL = (""Snowfall"",)
    BURLESQUE = (""Burlesque"",)
    CORNY_CANDY = (""Corny Candy"",)
    HIGHWAY_NOCTURNE = (""Highway Nocturne"",)
    I_DONT_THINK_SO = (""I Don't Think So"",)
    LOSING_YOUR_MARBLES = (""Losing Your Marbles"",)
    REFRESHER = (""Refresher"",)
    TOURIST = (""Tourist"",)
    TWIN_TYCHES = (""Twin Tyches"",)

    @property
    def audio_url(self):
        audio_urls = {
            AudioTrack.OBSERVER: ""https://cdn.tfrv.xyz/audio/observer.mp3"",
            AudioTrack.FUTURISTIC_BEAT: ""https://cdn.tfrv.xyz/audio/_futuristic-beat.mp3"",
            AudioTrack.SCIENCE_DOCUMENTARY: ""https://cdn.tfrv.xyz/audio/_science-documentary.mp3"",
            AudioTrack.HOTLINE: ""https://cdn.tfrv.xyz/audio/_hotline.mp3"",
            AudioTrack.BLADERUNNER_2049: ""https://cdn.tfrv.xyz/audio/_bladerunner-2049.mp3"",
            AudioTrack.A_FUTURE: ""https://cdn.tfrv.xyz/audio/a-future.mp3"",
            AudioTrack.ELYSIAN_EMBERS: ""https://cdn.tfrv.xyz/audio/elysian-embers.mp3"",
            AudioTrack.INSPIRING_CINEMATIC: ""https://cdn.tfrv.xyz/audio/inspiring-cinematic-ambient.mp3"",
            AudioTrack.BLADERUNNER_REMIX: ""https://cdn.tfrv.xyz/audio/bladerunner-remix.mp3"",
            AudioTrack.IZZAMUZZIC: ""https://cdn.tfrv.xyz/audio/_izzamuzzic.mp3"",
            AudioTrack.NAS: ""https://cdn.tfrv.xyz/audio/_nas.mp3"",
            AudioTrack.PARIS_ELSE: ""https://cdn.tfrv.xyz/audio/_paris-else.mp3"",
            AudioTrack.SNOWFALL: ""https://cdn.tfrv.xyz/audio/_snowfall.mp3"",
            AudioTrack.BURLESQUE: ""https://cdn.tfrv.xyz/audio/burlesque.mp3"",
            AudioTrack.CORNY_CANDY: ""https://cdn.tfrv.xyz/audio/corny-candy.mp3"",
            AudioTrack.HIGHWAY_NOCTURNE: ""https://cdn.tfrv.xyz/audio/highway-nocturne.mp3"",
            AudioTrack.I_DONT_THINK_SO: ""https://cdn.tfrv.xyz/audio/i-dont-think-so.mp3"",
            AudioTrack.LOSING_YOUR_MARBLES: ""https://cdn.tfrv.xyz/audio/losing-your-marbles.mp3"",
            AudioTrack.REFRESHER: ""https://cdn.tfrv.xyz/audio/refresher.mp3"",
            AudioTrack.TOURIST: ""https://cdn.tfrv.xyz/audio/tourist.mp3"",
            AudioTrack.TWIN_TYCHES: ""https://cdn.tfrv.xyz/audio/twin-tynches.mp3"",
        }
        return audio_urls[self]","Point(row=27, column=0)","Point(row=75, column=31)",,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
AudioTrack.audio_url,function,,"def audio_url(self):
        audio_urls = {
            AudioTrack.OBSERVER: ""https://cdn.tfrv.xyz/audio/observer.mp3"",
            AudioTrack.FUTURISTIC_BEAT: ""https://cdn.tfrv.xyz/audio/_futuristic-beat.mp3"",
            AudioTrack.SCIENCE_DOCUMENTARY: ""https://cdn.tfrv.xyz/audio/_science-documentary.mp3"",
            AudioTrack.HOTLINE: ""https://cdn.tfrv.xyz/audio/_hotline.mp3"",
            AudioTrack.BLADERUNNER_2049: ""https://cdn.tfrv.xyz/audio/_bladerunner-2049.mp3"",
            AudioTrack.A_FUTURE: ""https://cdn.tfrv.xyz/audio/a-future.mp3"",
            AudioTrack.ELYSIAN_EMBERS: ""https://cdn.tfrv.xyz/audio/elysian-embers.mp3"",
            AudioTrack.INSPIRING_CINEMATIC: ""https://cdn.tfrv.xyz/audio/inspiring-cinematic-ambient.mp3"",
            AudioTrack.BLADERUNNER_REMIX: ""https://cdn.tfrv.xyz/audio/bladerunner-remix.mp3"",
            AudioTrack.IZZAMUZZIC: ""https://cdn.tfrv.xyz/audio/_izzamuzzic.mp3"",
            AudioTrack.NAS: ""https://cdn.tfrv.xyz/audio/_nas.mp3"",
            AudioTrack.PARIS_ELSE: ""https://cdn.tfrv.xyz/audio/_paris-else.mp3"",
            AudioTrack.SNOWFALL: ""https://cdn.tfrv.xyz/audio/_snowfall.mp3"",
            AudioTrack.BURLESQUE: ""https://cdn.tfrv.xyz/audio/burlesque.mp3"",
            AudioTrack.CORNY_CANDY: ""https://cdn.tfrv.xyz/audio/corny-candy.mp3"",
            AudioTrack.HIGHWAY_NOCTURNE: ""https://cdn.tfrv.xyz/audio/highway-nocturne.mp3"",
            AudioTrack.I_DONT_THINK_SO: ""https://cdn.tfrv.xyz/audio/i-dont-think-so.mp3"",
            AudioTrack.LOSING_YOUR_MARBLES: ""https://cdn.tfrv.xyz/audio/losing-your-marbles.mp3"",
            AudioTrack.REFRESHER: ""https://cdn.tfrv.xyz/audio/refresher.mp3"",
            AudioTrack.TOURIST: ""https://cdn.tfrv.xyz/audio/tourist.mp3"",
            AudioTrack.TWIN_TYCHES: ""https://cdn.tfrv.xyz/audio/twin-tynches.mp3"",
        }
        return audio_urls[self]","Point(row=51, column=4)","Point(row=75, column=31)",AudioTrack,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
GenerationPreset,class,,"class GenerationPreset(str, Enum):
    LEONARDO = (""Default"",)
    ANIME = (""Anime"",)
    REALISM = (""Realist"",)
    ILLUSTRATION = (""Illustration"",)
    SKETCH_COLOR = (""Sketch Color"",)
    SKETCH_BW = (""Sketch B&W"",)
    PIXAR = (""Pixar"",)
    INK = (""Japanese Ink"",)
    RENDER_3D = (""3D Render"",)
    LEGO = (""Lego"",)
    SCIFI = (""Sci-Fi"",)
    RECRO_CARTOON = (""Retro Cartoon"",)
    PIXEL_ART = (""Pixel Art"",)
    CREATIVE = (""Creative"",)
    PHOTOGRAPHY = (""Photography"",)
    RAYTRACED = (""Raytraced"",)
    ENVIRONMENT = (""Environment"",)
    FANTASY = (""Fantasy"",)
    ANIME_SR = (""Anime Realism"",)
    MOVIE = (""Movie"",)
    STYLIZED_ILLUSTRATION = (""Stylized Illustration"",)
    MANGA = (""Manga"",)","Point(row=78, column=0)","Point(row=100, column=22)",,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
Voice,class,,"class Voice(str, Enum):
    LILY = ""Lily""
    DANIEL = ""Daniel""
    BRIAN = ""Brian""
    JESSICA = ""Jessica""
    CHARLOTTE = ""Charlotte""
    CALLUM = ""Callum""

    @property
    def voice_id(self):
        voice_id_map = {
            Voice.LILY: ""pFZP5JQG7iQjIQuC4Bku"",
            Voice.DANIEL: ""onwK4e9ZLuTAKqWW03F9"",
            Voice.BRIAN: ""nPczCjzI2devNBz1zQrb"",
            Voice.JESSICA: ""cgSgspJ2msm6clMCkdW9"",
            Voice.CHARLOTTE: ""XB0fDUnXU5powFXDhCwa"",
            Voice.CALLUM: ""N2lVS1w4EtoT3dr4eOWO"",
        }
        return voice_id_map[self]

    def __str__(self):
        return self.value","Point(row=103, column=0)","Point(row=124, column=25)",,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
Voice.voice_id,function,,"def voice_id(self):
        voice_id_map = {
            Voice.LILY: ""pFZP5JQG7iQjIQuC4Bku"",
            Voice.DANIEL: ""onwK4e9ZLuTAKqWW03F9"",
            Voice.BRIAN: ""nPczCjzI2devNBz1zQrb"",
            Voice.JESSICA: ""cgSgspJ2msm6clMCkdW9"",
            Voice.CHARLOTTE: ""XB0fDUnXU5powFXDhCwa"",
            Voice.CALLUM: ""N2lVS1w4EtoT3dr4eOWO"",
        }
        return voice_id_map[self]","Point(row=112, column=4)","Point(row=121, column=33)",Voice,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
Voice.__str__,function,,"def __str__(self):
        return self.value","Point(row=123, column=4)","Point(row=124, column=25)",Voice,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
VisualMediaType,class,,"class VisualMediaType(str, Enum):
    STOCK_VIDEOS = (""stockVideo"",)
    MOVING_AI_IMAGES = (""movingImage"",)
    AI_VIDEO = (""aiVideo"",)","Point(row=127, column=0)","Point(row=130, column=27)",,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
AIShortformVideoCreatorBlock,class,,"class AIShortformVideoCreatorBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[Literal[""revid""], Literal[""api_key""]] = (
            CredentialsField(
                provider=""revid"",
                supported_credential_types={""api_key""},
                description=""The revid.ai integration can be used with ""
                ""any API key with sufficient permissions for the blocks it is used on."",
            )
        )
        script: str = SchemaField(
            description=""""""1. Use short and punctuated sentences\n\n2. Use linebreaks to create a new clip\n\n3. Text outside of brackets is spoken by the AI, and [text between brackets] will be used to guide the visual generation. For example, [close-up of a cat] will show a close-up of a cat."""""",
            placeholder=""[close-up of a cat] Meow!"",
        )
        ratio: str = SchemaField(
            description=""Aspect ratio of the video"", default=""9 / 16""
        )
        resolution: str = SchemaField(
            description=""Resolution of the video"", default=""720p""
        )
        frame_rate: int = SchemaField(description=""Frame rate of the video"", default=60)
        generation_preset: GenerationPreset = SchemaField(
            description=""Generation preset for visual style - only effects AI generated visuals"",
            default=GenerationPreset.LEONARDO,
            placeholder=GenerationPreset.LEONARDO,
        )
        background_music: AudioTrack = SchemaField(
            description=""Background music track"",
            default=AudioTrack.HIGHWAY_NOCTURNE,
            placeholder=AudioTrack.HIGHWAY_NOCTURNE,
        )
        voice: Voice = SchemaField(
            description=""AI voice to use for narration"",
            default=Voice.LILY,
            placeholder=Voice.LILY,
        )
        video_style: VisualMediaType = SchemaField(
            description=""Type of visual media to use for the video"",
            default=VisualMediaType.STOCK_VIDEOS,
            placeholder=VisualMediaType.STOCK_VIDEOS,
        )

    class Output(BlockSchema):
        video_url: str = SchemaField(description=""The URL of the created video"")
        error: str = SchemaField(description=""Error message if the request failed"")

    def __init__(self):
        super().__init__(
            id=""361697fb-0c4f-4feb-aed3-8320c88c771b"",
            description=""Creates a shortform video using revid.ai"",
            categories={BlockCategory.SOCIAL, BlockCategory.AI},
            input_schema=AIShortformVideoCreatorBlock.Input,
            output_schema=AIShortformVideoCreatorBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""script"": ""[close-up of a cat] Meow!"",
                ""ratio"": ""9 / 16"",
                ""resolution"": ""720p"",
                ""frame_rate"": 60,
                ""generation_preset"": GenerationPreset.LEONARDO,
                ""background_music"": AudioTrack.HIGHWAY_NOCTURNE,
                ""voice"": Voice.LILY,
                ""video_style"": VisualMediaType.STOCK_VIDEOS,
            },
            test_output=(
                ""video_url"",
                ""https://example.com/video.mp4"",
            ),
            test_mock={
                ""create_webhook"": lambda: (
                    ""test_uuid"",
                    ""https://webhook.site/test_uuid"",
                ),
                ""create_video"": lambda api_key, payload: {""pid"": ""test_pid""},
                ""wait_for_video"": lambda api_key, pid, webhook_token, max_wait_time=1000: ""https://example.com/video.mp4"",
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def create_webhook(self):
        url = ""https://webhook.site/token""
        headers = {""Accept"": ""application/json"", ""Content-Type"": ""application/json""}
        response = requests.post(url, headers=headers)
        webhook_data = response.json()
        return webhook_data[""uuid""], f""https://webhook.site/{webhook_data['uuid']}""

    def create_video(self, api_key: SecretStr, payload: dict) -> dict:
        url = ""https://www.revid.ai/api/public/v2/render""
        headers = {""key"": api_key.get_secret_value()}
        response = requests.post(url, json=payload, headers=headers)
        logger.debug(
            f""API Response Status Code: {response.status_code}, Content: {response.text}""
        )
        return response.json()

    def check_video_status(self, api_key: SecretStr, pid: str) -> dict:
        url = f""https://www.revid.ai/api/public/v2/status?pid={pid}""
        headers = {""key"": api_key.get_secret_value()}
        response = requests.get(url, headers=headers)
        return response.json()

    def wait_for_video(
        self,
        api_key: SecretStr,
        pid: str,
        webhook_token: str,
        max_wait_time: int = 1000,
    ) -> str:
        start_time = time.time()
        while time.time() - start_time < max_wait_time:
            status = self.check_video_status(api_key, pid)
            logger.debug(f""Video status: {status}"")

            if status.get(""status"") == ""ready"" and ""videoUrl"" in status:
                return status[""videoUrl""]
            elif status.get(""status"") == ""error"":
                error_message = status.get(""error"", ""Unknown error occurred"")
                logger.error(f""Video creation failed: {error_message}"")
                raise ValueError(f""Video creation failed: {error_message}"")
            elif status.get(""status"") in [""FAILED"", ""CANCELED""]:
                logger.error(f""Video creation failed: {status.get('message')}"")
                raise ValueError(f""Video creation failed: {status.get('message')}"")

            time.sleep(10)

        logger.error(""Video creation timed out"")
        raise TimeoutError(""Video creation timed out"")

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        # Create a new Webhook.site URL
        webhook_token, webhook_url = self.create_webhook()
        logger.debug(f""Webhook URL: {webhook_url}"")

        audio_url = input_data.background_music.audio_url

        payload = {
            ""frameRate"": input_data.frame_rate,
            ""resolution"": input_data.resolution,
            ""frameDurationMultiplier"": 18,
            ""webhook"": webhook_url,
            ""creationParams"": {
                ""mediaType"": input_data.video_style,
                ""captionPresetName"": ""Wrap 1"",
                ""selectedVoice"": input_data.voice.voice_id,
                ""hasEnhancedGeneration"": True,
                ""generationPreset"": input_data.generation_preset.name,
                ""selectedAudio"": input_data.background_music,
                ""origin"": ""/create"",
                ""inputText"": input_data.script,
                ""flowType"": ""text-to-video"",
                ""slug"": ""create-tiktok-video"",
                ""hasToGenerateVoice"": True,
                ""hasToTranscript"": False,
                ""hasToSearchMedia"": True,
                ""hasAvatar"": False,
                ""hasWebsiteRecorder"": False,
                ""hasTextSmallAtBottom"": False,
                ""ratio"": input_data.ratio,
                ""sourceType"": ""contentScraping"",
                ""selectedStoryStyle"": {""value"": ""custom"", ""label"": ""Custom""},
                ""hasToGenerateVideos"": input_data.video_style
                != VisualMediaType.STOCK_VIDEOS,
                ""audioUrl"": audio_url,
            },
        }

        logger.debug(""Creating video..."")
        response = self.create_video(credentials.api_key, payload)
        pid = response.get(""pid"")

        if not pid:
            logger.error(
                f""Failed to create video: No project ID returned. API Response: {response}""
            )
            raise RuntimeError(""Failed to create video: No project ID returned"")
        else:
            logger.debug(
                f""Video created with project ID: {pid}. Waiting for completion...""
            )
            video_url = self.wait_for_video(credentials.api_key, pid, webhook_token)
            logger.debug(f""Video ready: {video_url}"")
            yield ""video_url"", video_url","Point(row=136, column=0)","Point(row=319, column=40)",,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
Input,class,,"class Input(BlockSchema):
        credentials: CredentialsMetaInput[Literal[""revid""], Literal[""api_key""]] = (
            CredentialsField(
                provider=""revid"",
                supported_credential_types={""api_key""},
                description=""The revid.ai integration can be used with ""
                ""any API key with sufficient permissions for the blocks it is used on."",
            )
        )
        script: str = SchemaField(
            description=""""""1. Use short and punctuated sentences\n\n2. Use linebreaks to create a new clip\n\n3. Text outside of brackets is spoken by the AI, and [text between brackets] will be used to guide the visual generation. For example, [close-up of a cat] will show a close-up of a cat."""""",
            placeholder=""[close-up of a cat] Meow!"",
        )
        ratio: str = SchemaField(
            description=""Aspect ratio of the video"", default=""9 / 16""
        )
        resolution: str = SchemaField(
            description=""Resolution of the video"", default=""720p""
        )
        frame_rate: int = SchemaField(description=""Frame rate of the video"", default=60)
        generation_preset: GenerationPreset = SchemaField(
            description=""Generation preset for visual style - only effects AI generated visuals"",
            default=GenerationPreset.LEONARDO,
            placeholder=GenerationPreset.LEONARDO,
        )
        background_music: AudioTrack = SchemaField(
            description=""Background music track"",
            default=AudioTrack.HIGHWAY_NOCTURNE,
            placeholder=AudioTrack.HIGHWAY_NOCTURNE,
        )
        voice: Voice = SchemaField(
            description=""AI voice to use for narration"",
            default=Voice.LILY,
            placeholder=Voice.LILY,
        )
        video_style: VisualMediaType = SchemaField(
            description=""Type of visual media to use for the video"",
            default=VisualMediaType.STOCK_VIDEOS,
            placeholder=VisualMediaType.STOCK_VIDEOS,
        )","Point(row=137, column=4)","Point(row=176, column=9)",,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
Output,class,,"class Output(BlockSchema):
        video_url: str = SchemaField(description=""The URL of the created video"")
        error: str = SchemaField(description=""Error message if the request failed"")","Point(row=178, column=4)","Point(row=180, column=83)",,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
AIShortformVideoCreatorBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""361697fb-0c4f-4feb-aed3-8320c88c771b"",
            description=""Creates a shortform video using revid.ai"",
            categories={BlockCategory.SOCIAL, BlockCategory.AI},
            input_schema=AIShortformVideoCreatorBlock.Input,
            output_schema=AIShortformVideoCreatorBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""script"": ""[close-up of a cat] Meow!"",
                ""ratio"": ""9 / 16"",
                ""resolution"": ""720p"",
                ""frame_rate"": 60,
                ""generation_preset"": GenerationPreset.LEONARDO,
                ""background_music"": AudioTrack.HIGHWAY_NOCTURNE,
                ""voice"": Voice.LILY,
                ""video_style"": VisualMediaType.STOCK_VIDEOS,
            },
            test_output=(
                ""video_url"",
                ""https://example.com/video.mp4"",
            ),
            test_mock={
                ""create_webhook"": lambda: (
                    ""test_uuid"",
                    ""https://webhook.site/test_uuid"",
                ),
                ""create_video"": lambda api_key, payload: {""pid"": ""test_pid""},
                ""wait_for_video"": lambda api_key, pid, webhook_token, max_wait_time=1000: ""https://example.com/video.mp4"",
            },
            test_credentials=TEST_CREDENTIALS,
        )","Point(row=182, column=4)","Point(row=213, column=9)",AIShortformVideoCreatorBlock,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
AIShortformVideoCreatorBlock.create_webhook,function,,"def create_webhook(self):
        url = ""https://webhook.site/token""
        headers = {""Accept"": ""application/json"", ""Content-Type"": ""application/json""}
        response = requests.post(url, headers=headers)
        webhook_data = response.json()
        return webhook_data[""uuid""], f""https://webhook.site/{webhook_data['uuid']}""","Point(row=215, column=4)","Point(row=220, column=83)",AIShortformVideoCreatorBlock,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
AIShortformVideoCreatorBlock.create_video,function,,"def create_video(self, api_key: SecretStr, payload: dict) -> dict:
        url = ""https://www.revid.ai/api/public/v2/render""
        headers = {""key"": api_key.get_secret_value()}
        response = requests.post(url, json=payload, headers=headers)
        logger.debug(
            f""API Response Status Code: {response.status_code}, Content: {response.text}""
        )
        return response.json()","Point(row=222, column=4)","Point(row=229, column=30)",AIShortformVideoCreatorBlock,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
AIShortformVideoCreatorBlock.check_video_status,function,,"def check_video_status(self, api_key: SecretStr, pid: str) -> dict:
        url = f""https://www.revid.ai/api/public/v2/status?pid={pid}""
        headers = {""key"": api_key.get_secret_value()}
        response = requests.get(url, headers=headers)
        return response.json()","Point(row=231, column=4)","Point(row=235, column=30)",AIShortformVideoCreatorBlock,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
AIShortformVideoCreatorBlock.wait_for_video,function,,"def wait_for_video(
        self,
        api_key: SecretStr,
        pid: str,
        webhook_token: str,
        max_wait_time: int = 1000,
    ) -> str:
        start_time = time.time()
        while time.time() - start_time < max_wait_time:
            status = self.check_video_status(api_key, pid)
            logger.debug(f""Video status: {status}"")

            if status.get(""status"") == ""ready"" and ""videoUrl"" in status:
                return status[""videoUrl""]
            elif status.get(""status"") == ""error"":
                error_message = status.get(""error"", ""Unknown error occurred"")
                logger.error(f""Video creation failed: {error_message}"")
                raise ValueError(f""Video creation failed: {error_message}"")
            elif status.get(""status"") in [""FAILED"", ""CANCELED""]:
                logger.error(f""Video creation failed: {status.get('message')}"")
                raise ValueError(f""Video creation failed: {status.get('message')}"")

            time.sleep(10)

        logger.error(""Video creation timed out"")
        raise TimeoutError(""Video creation timed out"")","Point(row=237, column=4)","Point(row=262, column=54)",AIShortformVideoCreatorBlock,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
AIShortformVideoCreatorBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        # Create a new Webhook.site URL
        webhook_token, webhook_url = self.create_webhook()
        logger.debug(f""Webhook URL: {webhook_url}"")

        audio_url = input_data.background_music.audio_url

        payload = {
            ""frameRate"": input_data.frame_rate,
            ""resolution"": input_data.resolution,
            ""frameDurationMultiplier"": 18,
            ""webhook"": webhook_url,
            ""creationParams"": {
                ""mediaType"": input_data.video_style,
                ""captionPresetName"": ""Wrap 1"",
                ""selectedVoice"": input_data.voice.voice_id,
                ""hasEnhancedGeneration"": True,
                ""generationPreset"": input_data.generation_preset.name,
                ""selectedAudio"": input_data.background_music,
                ""origin"": ""/create"",
                ""inputText"": input_data.script,
                ""flowType"": ""text-to-video"",
                ""slug"": ""create-tiktok-video"",
                ""hasToGenerateVoice"": True,
                ""hasToTranscript"": False,
                ""hasToSearchMedia"": True,
                ""hasAvatar"": False,
                ""hasWebsiteRecorder"": False,
                ""hasTextSmallAtBottom"": False,
                ""ratio"": input_data.ratio,
                ""sourceType"": ""contentScraping"",
                ""selectedStoryStyle"": {""value"": ""custom"", ""label"": ""Custom""},
                ""hasToGenerateVideos"": input_data.video_style
                != VisualMediaType.STOCK_VIDEOS,
                ""audioUrl"": audio_url,
            },
        }

        logger.debug(""Creating video..."")
        response = self.create_video(credentials.api_key, payload)
        pid = response.get(""pid"")

        if not pid:
            logger.error(
                f""Failed to create video: No project ID returned. API Response: {response}""
            )
            raise RuntimeError(""Failed to create video: No project ID returned"")
        else:
            logger.debug(
                f""Video created with project ID: {pid}. Waiting for completion...""
            )
            video_url = self.wait_for_video(credentials.api_key, pid, webhook_token)
            logger.debug(f""Video ready: {video_url}"")
            yield ""video_url"", video_url","Point(row=264, column=4)","Point(row=319, column=40)",AIShortformVideoCreatorBlock,autogpt_platform/backend/backend/blocks/ai_shortform_video_block.py
Place,class,,"class Place(BaseModel):
    name: str
    address: str
    phone: str
    rating: float
    reviews: int
    website: str","Point(row=24, column=0)","Point(row=30, column=16)",,autogpt_platform/backend/backend/blocks/google_maps.py
GoogleMapsSearchBlock,class,,"class GoogleMapsSearchBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[""google_maps""], Literal[""api_key""]
        ] = CredentialsField(
            provider=""google_maps"",
            supported_credential_types={""api_key""},
            description=""Google Maps API Key"",
        )
        query: str = SchemaField(
            description=""Search query for local businesses"",
            placeholder=""e.g., 'restaurants in New York'"",
        )
        radius: int = SchemaField(
            description=""Search radius in meters (max 50000)"",
            default=5000,
            ge=1,
            le=50000,
        )
        max_results: int = SchemaField(
            description=""Maximum number of results to return (max 60)"",
            default=20,
            ge=1,
            le=60,
        )

    class Output(BlockSchema):
        place: Place = SchemaField(description=""Place found"")
        error: str = SchemaField(description=""Error message if the search failed"")

    def __init__(self):
        super().__init__(
            id=""f47ac10b-58cc-4372-a567-0e02b2c3d479"",
            description=""This block searches for local businesses using Google Maps API."",
            categories={BlockCategory.SEARCH},
            input_schema=GoogleMapsSearchBlock.Input,
            output_schema=GoogleMapsSearchBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""query"": ""restaurants in new york"",
                ""radius"": 5000,
                ""max_results"": 5,
            },
            test_output=[
                (
                    ""place"",
                    {
                        ""name"": ""Test Restaurant"",
                        ""address"": ""123 Test St, New York, NY 10001"",
                        ""phone"": ""+1 (555) 123-4567"",
                        ""rating"": 4.5,
                        ""reviews"": 100,
                        ""website"": ""https://testrestaurant.com"",
                    },
                ),
            ],
            test_mock={
                ""search_places"": lambda *args, **kwargs: [
                    {
                        ""name"": ""Test Restaurant"",
                        ""address"": ""123 Test St, New York, NY 10001"",
                        ""phone"": ""+1 (555) 123-4567"",
                        ""rating"": 4.5,
                        ""reviews"": 100,
                        ""website"": ""https://testrestaurant.com"",
                    }
                ]
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        places = self.search_places(
            credentials.api_key,
            input_data.query,
            input_data.radius,
            input_data.max_results,
        )
        for place in places:
            yield ""place"", place

    def search_places(self, api_key: SecretStr, query, radius, max_results):
        client = googlemaps.Client(key=api_key.get_secret_value())
        return self._search_places(client, query, radius, max_results)

    def _search_places(self, client, query, radius, max_results):
        results = []
        next_page_token = None
        while len(results) < max_results:
            response = client.places(
                query=query,
                radius=radius,
                page_token=next_page_token,
            )
            for place in response[""results""]:
                if len(results) >= max_results:
                    break
                place_details = client.place(place[""place_id""])[""result""]
                results.append(
                    Place(
                        name=place_details.get(""name"", """"),
                        address=place_details.get(""formatted_address"", """"),
                        phone=place_details.get(""formatted_phone_number"", """"),
                        rating=place_details.get(""rating"", 0),
                        reviews=place_details.get(""user_ratings_total"", 0),
                        website=place_details.get(""website"", """"),
                    )
                )
            next_page_token = response.get(""next_page_token"")
            if not next_page_token:
                break
        return results","Point(row=33, column=0)","Point(row=146, column=22)",,autogpt_platform/backend/backend/blocks/google_maps.py
Input,class,,"class Input(BlockSchema):
        credentials: CredentialsMetaInput[
            Literal[""google_maps""], Literal[""api_key""]
        ] = CredentialsField(
            provider=""google_maps"",
            supported_credential_types={""api_key""},
            description=""Google Maps API Key"",
        )
        query: str = SchemaField(
            description=""Search query for local businesses"",
            placeholder=""e.g., 'restaurants in New York'"",
        )
        radius: int = SchemaField(
            description=""Search radius in meters (max 50000)"",
            default=5000,
            ge=1,
            le=50000,
        )
        max_results: int = SchemaField(
            description=""Maximum number of results to return (max 60)"",
            default=20,
            ge=1,
            le=60,
        )","Point(row=34, column=4)","Point(row=57, column=9)",,autogpt_platform/backend/backend/blocks/google_maps.py
Output,class,,"class Output(BlockSchema):
        place: Place = SchemaField(description=""Place found"")
        error: str = SchemaField(description=""Error message if the search failed"")","Point(row=59, column=4)","Point(row=61, column=82)",,autogpt_platform/backend/backend/blocks/google_maps.py
GoogleMapsSearchBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""f47ac10b-58cc-4372-a567-0e02b2c3d479"",
            description=""This block searches for local businesses using Google Maps API."",
            categories={BlockCategory.SEARCH},
            input_schema=GoogleMapsSearchBlock.Input,
            output_schema=GoogleMapsSearchBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""query"": ""restaurants in new york"",
                ""radius"": 5000,
                ""max_results"": 5,
            },
            test_output=[
                (
                    ""place"",
                    {
                        ""name"": ""Test Restaurant"",
                        ""address"": ""123 Test St, New York, NY 10001"",
                        ""phone"": ""+1 (555) 123-4567"",
                        ""rating"": 4.5,
                        ""reviews"": 100,
                        ""website"": ""https://testrestaurant.com"",
                    },
                ),
            ],
            test_mock={
                ""search_places"": lambda *args, **kwargs: [
                    {
                        ""name"": ""Test Restaurant"",
                        ""address"": ""123 Test St, New York, NY 10001"",
                        ""phone"": ""+1 (555) 123-4567"",
                        ""rating"": 4.5,
                        ""reviews"": 100,
                        ""website"": ""https://testrestaurant.com"",
                    }
                ]
            },
            test_credentials=TEST_CREDENTIALS,
        )","Point(row=63, column=4)","Point(row=102, column=9)",GoogleMapsSearchBlock,autogpt_platform/backend/backend/blocks/google_maps.py
GoogleMapsSearchBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        places = self.search_places(
            credentials.api_key,
            input_data.query,
            input_data.radius,
            input_data.max_results,
        )
        for place in places:
            yield ""place"", place","Point(row=104, column=4)","Point(row=114, column=32)",GoogleMapsSearchBlock,autogpt_platform/backend/backend/blocks/google_maps.py
GoogleMapsSearchBlock.search_places,function,,"def search_places(self, api_key: SecretStr, query, radius, max_results):
        client = googlemaps.Client(key=api_key.get_secret_value())
        return self._search_places(client, query, radius, max_results)","Point(row=116, column=4)","Point(row=118, column=70)",GoogleMapsSearchBlock,autogpt_platform/backend/backend/blocks/google_maps.py
GoogleMapsSearchBlock._search_places,function,,"def _search_places(self, client, query, radius, max_results):
        results = []
        next_page_token = None
        while len(results) < max_results:
            response = client.places(
                query=query,
                radius=radius,
                page_token=next_page_token,
            )
            for place in response[""results""]:
                if len(results) >= max_results:
                    break
                place_details = client.place(place[""place_id""])[""result""]
                results.append(
                    Place(
                        name=place_details.get(""name"", """"),
                        address=place_details.get(""formatted_address"", """"),
                        phone=place_details.get(""formatted_phone_number"", """"),
                        rating=place_details.get(""rating"", 0),
                        reviews=place_details.get(""user_ratings_total"", 0),
                        website=place_details.get(""website"", """"),
                    )
                )
            next_page_token = response.get(""next_page_token"")
            if not next_page_token:
                break
        return results","Point(row=120, column=4)","Point(row=146, column=22)",GoogleMapsSearchBlock,autogpt_platform/backend/backend/blocks/google_maps.py
UnrealTextToSpeechBlock,class,,"class UnrealTextToSpeechBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(
            description=""The text to be converted to speech"",
            placeholder=""Enter the text you want to convert to speech"",
        )
        voice_id: str = SchemaField(
            description=""The voice ID to use for text-to-speech conversion"",
            placeholder=""Scarlett"",
            default=""Scarlett"",
        )
        credentials: CredentialsMetaInput[
            Literal[""unreal_speech""], Literal[""api_key""]
        ] = CredentialsField(
            provider=""unreal_speech"",
            supported_credential_types={""api_key""},
            description=""The Unreal Speech integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )

    class Output(BlockSchema):
        mp3_url: str = SchemaField(description=""The URL of the generated MP3 file"")
        error: str = SchemaField(description=""Error message if the API call failed"")

    def __init__(self):
        super().__init__(
            id=""4ff1ff6d-cc40-4caa-ae69-011daa20c378"",
            description=""Converts text to speech using the Unreal Speech API"",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input_schema=UnrealTextToSpeechBlock.Input,
            output_schema=UnrealTextToSpeechBlock.Output,
            test_input={
                ""text"": ""This is a test of the text to speech API."",
                ""voice_id"": ""Scarlett"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[(""mp3_url"", ""https://example.com/test.mp3"")],
            test_mock={
                ""call_unreal_speech_api"": lambda *args, **kwargs: {
                    ""OutputUri"": ""https://example.com/test.mp3""
                }
            },
            test_credentials=TEST_CREDENTIALS,
        )

    @staticmethod
    def call_unreal_speech_api(
        api_key: SecretStr, text: str, voice_id: str
    ) -> dict[str, Any]:
        url = ""https://api.v7.unrealspeech.com/speech""
        headers = {
            ""Authorization"": f""Bearer {api_key.get_secret_value()}"",
            ""Content-Type"": ""application/json"",
        }
        data = {
            ""Text"": text,
            ""VoiceId"": voice_id,
            ""Bitrate"": ""192k"",
            ""Speed"": ""0"",
            ""Pitch"": ""1"",
            ""TimestampType"": ""sentence"",
        }

        response = requests.post(url, headers=headers, json=data)
        return response.json()

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        api_response = self.call_unreal_speech_api(
            credentials.api_key,
            input_data.text,
            input_data.voice_id,
        )
        yield ""mp3_url"", api_response[""OutputUri""]","Point(row=24, column=0)","Point(row=98, column=50)",,autogpt_platform/backend/backend/blocks/text_to_speech_block.py
Input,class,,"class Input(BlockSchema):
        text: str = SchemaField(
            description=""The text to be converted to speech"",
            placeholder=""Enter the text you want to convert to speech"",
        )
        voice_id: str = SchemaField(
            description=""The voice ID to use for text-to-speech conversion"",
            placeholder=""Scarlett"",
            default=""Scarlett"",
        )
        credentials: CredentialsMetaInput[
            Literal[""unreal_speech""], Literal[""api_key""]
        ] = CredentialsField(
            provider=""unreal_speech"",
            supported_credential_types={""api_key""},
            description=""The Unreal Speech integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )","Point(row=25, column=4)","Point(row=42, column=9)",,autogpt_platform/backend/backend/blocks/text_to_speech_block.py
Output,class,,"class Output(BlockSchema):
        mp3_url: str = SchemaField(description=""The URL of the generated MP3 file"")
        error: str = SchemaField(description=""Error message if the API call failed"")","Point(row=44, column=4)","Point(row=46, column=84)",,autogpt_platform/backend/backend/blocks/text_to_speech_block.py
UnrealTextToSpeechBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""4ff1ff6d-cc40-4caa-ae69-011daa20c378"",
            description=""Converts text to speech using the Unreal Speech API"",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input_schema=UnrealTextToSpeechBlock.Input,
            output_schema=UnrealTextToSpeechBlock.Output,
            test_input={
                ""text"": ""This is a test of the text to speech API."",
                ""voice_id"": ""Scarlett"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[(""mp3_url"", ""https://example.com/test.mp3"")],
            test_mock={
                ""call_unreal_speech_api"": lambda *args, **kwargs: {
                    ""OutputUri"": ""https://example.com/test.mp3""
                }
            },
            test_credentials=TEST_CREDENTIALS,
        )","Point(row=48, column=4)","Point(row=67, column=9)",UnrealTextToSpeechBlock,autogpt_platform/backend/backend/blocks/text_to_speech_block.py
UnrealTextToSpeechBlock.call_unreal_speech_api,function,,"def call_unreal_speech_api(
        api_key: SecretStr, text: str, voice_id: str
    ) -> dict[str, Any]:
        url = ""https://api.v7.unrealspeech.com/speech""
        headers = {
            ""Authorization"": f""Bearer {api_key.get_secret_value()}"",
            ""Content-Type"": ""application/json"",
        }
        data = {
            ""Text"": text,
            ""VoiceId"": voice_id,
            ""Bitrate"": ""192k"",
            ""Speed"": ""0"",
            ""Pitch"": ""1"",
            ""TimestampType"": ""sentence"",
        }

        response = requests.post(url, headers=headers, json=data)
        return response.json()","Point(row=70, column=4)","Point(row=88, column=30)",UnrealTextToSpeechBlock,autogpt_platform/backend/backend/blocks/text_to_speech_block.py
UnrealTextToSpeechBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        api_response = self.call_unreal_speech_api(
            credentials.api_key,
            input_data.text,
            input_data.voice_id,
        )
        yield ""mp3_url"", api_response[""OutputUri""]","Point(row=90, column=4)","Point(row=98, column=50)",UnrealTextToSpeechBlock,autogpt_platform/backend/backend/blocks/text_to_speech_block.py
RSSEntry,class,,"class RSSEntry(pydantic.BaseModel):
    title: str
    link: str
    description: str
    pub_date: datetime
    author: str
    categories: list[str]","Point(row=11, column=0)","Point(row=17, column=25)",,autogpt_platform/backend/backend/blocks/rss.py
ReadRSSFeedBlock,class,,"class ReadRSSFeedBlock(Block):
    class Input(BlockSchema):
        rss_url: str = SchemaField(
            description=""The URL of the RSS feed to read"",
            placeholder=""https://example.com/rss"",
        )
        time_period: int = SchemaField(
            description=""The time period to check in minutes relative to the run block runtime, e.g. 60 would check for new entries in the last hour."",
            placeholder=""1440"",
            default=1440,
        )
        polling_rate: int = SchemaField(
            description=""The number of seconds to wait between polling attempts."",
            placeholder=""300"",
        )
        run_continuously: bool = SchemaField(
            description=""Whether to run the block continuously or just once."",
            default=True,
        )

    class Output(BlockSchema):
        entry: RSSEntry = SchemaField(description=""The RSS item"")

    def __init__(self):
        super().__init__(
            id=""5ebe6768-8e5d-41e3-9134-1c7bd89a8d52"",
            input_schema=ReadRSSFeedBlock.Input,
            output_schema=ReadRSSFeedBlock.Output,
            description=""Reads RSS feed entries from a given URL."",
            categories={BlockCategory.INPUT},
            test_input={
                ""rss_url"": ""https://example.com/rss"",
                ""time_period"": 10_000_000,
                ""polling_rate"": 1,
                ""run_continuously"": False,
            },
            test_output=[
                (
                    ""entry"",
                    RSSEntry(
                        title=""Example RSS Item"",
                        link=""https://example.com/article"",
                        description=""This is an example RSS item description."",
                        pub_date=datetime(2023, 6, 23, 12, 30, 0, tzinfo=timezone.utc),
                        author=""John Doe"",
                        categories=[""Technology"", ""News""],
                    ),
                ),
            ],
            test_mock={
                ""parse_feed"": lambda *args, **kwargs: {
                    ""entries"": [
                        {
                            ""title"": ""Example RSS Item"",
                            ""link"": ""https://example.com/article"",
                            ""summary"": ""This is an example RSS item description."",
                            ""published_parsed"": (2023, 6, 23, 12, 30, 0, 4, 174, 0),
                            ""author"": ""John Doe"",
                            ""tags"": [{""term"": ""Technology""}, {""term"": ""News""}],
                        }
                    ]
                }
            },
        )

    @staticmethod
    def parse_feed(url: str) -> dict[str, Any]:
        return feedparser.parse(url)  # type: ignore

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        keep_going = True
        start_time = datetime.now(timezone.utc) - timedelta(
            minutes=input_data.time_period
        )
        while keep_going:
            keep_going = input_data.run_continuously

            feed = self.parse_feed(input_data.rss_url)

            for entry in feed[""entries""]:
                pub_date = datetime(*entry[""published_parsed""][:6], tzinfo=timezone.utc)

                if pub_date > start_time:
                    yield (
                        ""entry"",
                        RSSEntry(
                            title=entry[""title""],
                            link=entry[""link""],
                            description=entry.get(""summary"", """"),
                            pub_date=pub_date,
                            author=entry.get(""author"", """"),
                            categories=[tag[""term""] for tag in entry.get(""tags"", [])],
                        ),
                    )

            time.sleep(input_data.polling_rate)","Point(row=20, column=0)","Point(row=115, column=47)",,autogpt_platform/backend/backend/blocks/rss.py
Input,class,,"class Input(BlockSchema):
        rss_url: str = SchemaField(
            description=""The URL of the RSS feed to read"",
            placeholder=""https://example.com/rss"",
        )
        time_period: int = SchemaField(
            description=""The time period to check in minutes relative to the run block runtime, e.g. 60 would check for new entries in the last hour."",
            placeholder=""1440"",
            default=1440,
        )
        polling_rate: int = SchemaField(
            description=""The number of seconds to wait between polling attempts."",
            placeholder=""300"",
        )
        run_continuously: bool = SchemaField(
            description=""Whether to run the block continuously or just once."",
            default=True,
        )","Point(row=21, column=4)","Point(row=38, column=9)",,autogpt_platform/backend/backend/blocks/rss.py
Output,class,,"class Output(BlockSchema):
        entry: RSSEntry = SchemaField(description=""The RSS item"")","Point(row=40, column=4)","Point(row=41, column=65)",,autogpt_platform/backend/backend/blocks/rss.py
ReadRSSFeedBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""5ebe6768-8e5d-41e3-9134-1c7bd89a8d52"",
            input_schema=ReadRSSFeedBlock.Input,
            output_schema=ReadRSSFeedBlock.Output,
            description=""Reads RSS feed entries from a given URL."",
            categories={BlockCategory.INPUT},
            test_input={
                ""rss_url"": ""https://example.com/rss"",
                ""time_period"": 10_000_000,
                ""polling_rate"": 1,
                ""run_continuously"": False,
            },
            test_output=[
                (
                    ""entry"",
                    RSSEntry(
                        title=""Example RSS Item"",
                        link=""https://example.com/article"",
                        description=""This is an example RSS item description."",
                        pub_date=datetime(2023, 6, 23, 12, 30, 0, tzinfo=timezone.utc),
                        author=""John Doe"",
                        categories=[""Technology"", ""News""],
                    ),
                ),
            ],
            test_mock={
                ""parse_feed"": lambda *args, **kwargs: {
                    ""entries"": [
                        {
                            ""title"": ""Example RSS Item"",
                            ""link"": ""https://example.com/article"",
                            ""summary"": ""This is an example RSS item description."",
                            ""published_parsed"": (2023, 6, 23, 12, 30, 0, 4, 174, 0),
                            ""author"": ""John Doe"",
                            ""tags"": [{""term"": ""Technology""}, {""term"": ""News""}],
                        }
                    ]
                }
            },
        )","Point(row=43, column=4)","Point(row=83, column=9)",ReadRSSFeedBlock,autogpt_platform/backend/backend/blocks/rss.py
ReadRSSFeedBlock.parse_feed,function,,"def parse_feed(url: str) -> dict[str, Any]:
        return feedparser.parse(url)  # type: ignore","Point(row=86, column=4)","Point(row=87, column=52)",ReadRSSFeedBlock,autogpt_platform/backend/backend/blocks/rss.py
ReadRSSFeedBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        keep_going = True
        start_time = datetime.now(timezone.utc) - timedelta(
            minutes=input_data.time_period
        )
        while keep_going:
            keep_going = input_data.run_continuously

            feed = self.parse_feed(input_data.rss_url)

            for entry in feed[""entries""]:
                pub_date = datetime(*entry[""published_parsed""][:6], tzinfo=timezone.utc)

                if pub_date > start_time:
                    yield (
                        ""entry"",
                        RSSEntry(
                            title=entry[""title""],
                            link=entry[""link""],
                            description=entry.get(""summary"", """"),
                            pub_date=pub_date,
                            author=entry.get(""author"", """"),
                            categories=[tag[""term""] for tag in entry.get(""tags"", [])],
                        ),
                    )

            time.sleep(input_data.polling_rate)","Point(row=89, column=4)","Point(row=115, column=47)",ReadRSSFeedBlock,autogpt_platform/backend/backend/blocks/rss.py
CreateTalkingAvatarVideoBlock,class,,"class CreateTalkingAvatarVideoBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[Literal[""d_id""], Literal[""api_key""]] = (
            CredentialsField(
                provider=""d_id"",
                supported_credential_types={""api_key""},
                description=""The D-ID integration can be used with ""
                ""any API key with sufficient permissions for the blocks it is used on."",
            )
        )
        script_input: str = SchemaField(
            description=""The text input for the script"",
            placeholder=""Welcome to AutoGPT"",
        )
        provider: Literal[""microsoft"", ""elevenlabs"", ""amazon""] = SchemaField(
            description=""The voice provider to use"", default=""microsoft""
        )
        voice_id: str = SchemaField(
            description=""The voice ID to use, get list of voices [here](https://docs.agpt.co/server/d_id)"",
            default=""en-US-JennyNeural"",
        )
        presenter_id: str = SchemaField(
            description=""The presenter ID to use"", default=""amy-Aq6OmGZnMt""
        )
        driver_id: str = SchemaField(
            description=""The driver ID to use"", default=""Vcq0R4a8F0""
        )
        result_format: Literal[""mp4"", ""gif"", ""wav""] = SchemaField(
            description=""The desired result format"", default=""mp4""
        )
        crop_type: Literal[""wide"", ""square"", ""vertical""] = SchemaField(
            description=""The crop type for the presenter"", default=""wide""
        )
        subtitles: bool = SchemaField(
            description=""Whether to include subtitles"", default=False
        )
        ssml: bool = SchemaField(description=""Whether the input is SSML"", default=False)
        max_polling_attempts: int = SchemaField(
            description=""Maximum number of polling attempts"", default=30, ge=5
        )
        polling_interval: int = SchemaField(
            description=""Interval between polling attempts in seconds"", default=10, ge=5
        )

    class Output(BlockSchema):
        video_url: str = SchemaField(description=""The URL of the created video"")
        error: str = SchemaField(description=""Error message if the request failed"")

    def __init__(self):
        super().__init__(
            id=""98c6f503-8c47-4b1c-a96d-351fc7c87dab"",
            description=""This block integrates with D-ID to create video clips and retrieve their URLs."",
            categories={BlockCategory.AI},
            input_schema=CreateTalkingAvatarVideoBlock.Input,
            output_schema=CreateTalkingAvatarVideoBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""script_input"": ""Welcome to AutoGPT"",
                ""voice_id"": ""en-US-JennyNeural"",
                ""presenter_id"": ""amy-Aq6OmGZnMt"",
                ""driver_id"": ""Vcq0R4a8F0"",
                ""result_format"": ""mp4"",
                ""crop_type"": ""wide"",
                ""subtitles"": False,
                ""ssml"": False,
                ""max_polling_attempts"": 5,
                ""polling_interval"": 5,
            },
            test_output=[
                (
                    ""video_url"",
                    ""https://d-id.com/api/clips/abcd1234-5678-efgh-ijkl-mnopqrstuvwx/video"",
                ),
            ],
            test_mock={
                ""create_clip"": lambda *args, **kwargs: {
                    ""id"": ""abcd1234-5678-efgh-ijkl-mnopqrstuvwx"",
                    ""status"": ""created"",
                },
                ""get_clip_status"": lambda *args, **kwargs: {
                    ""status"": ""done"",
                    ""result_url"": ""https://d-id.com/api/clips/abcd1234-5678-efgh-ijkl-mnopqrstuvwx/video"",
                },
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def create_clip(self, api_key: SecretStr, payload: dict) -> dict:
        url = ""https://api.d-id.com/clips""
        headers = {
            ""accept"": ""application/json"",
            ""content-type"": ""application/json"",
            ""authorization"": f""Basic {api_key.get_secret_value()}"",
        }
        response = requests.post(url, json=payload, headers=headers)
        return response.json()

    def get_clip_status(self, api_key: SecretStr, clip_id: str) -> dict:
        url = f""https://api.d-id.com/clips/{clip_id}""
        headers = {
            ""accept"": ""application/json"",
            ""authorization"": f""Basic {api_key.get_secret_value()}"",
        }
        response = requests.get(url, headers=headers)
        return response.json()

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        # Create the clip
        payload = {
            ""script"": {
                ""type"": ""text"",
                ""subtitles"": str(input_data.subtitles).lower(),
                ""provider"": {
                    ""type"": input_data.provider,
                    ""voice_id"": input_data.voice_id,
                },
                ""ssml"": str(input_data.ssml).lower(),
                ""input"": input_data.script_input,
            },
            ""config"": {""result_format"": input_data.result_format},
            ""presenter_config"": {""crop"": {""type"": input_data.crop_type}},
            ""presenter_id"": input_data.presenter_id,
            ""driver_id"": input_data.driver_id,
        }

        response = self.create_clip(credentials.api_key, payload)
        clip_id = response[""id""]

        # Poll for clip status
        for _ in range(input_data.max_polling_attempts):
            status_response = self.get_clip_status(credentials.api_key, clip_id)
            if status_response[""status""] == ""done"":
                yield ""video_url"", status_response[""result_url""]
                return
            elif status_response[""status""] == ""error"":
                raise RuntimeError(
                    f""Clip creation failed: {status_response.get('error', 'Unknown error')}""
                )

            time.sleep(input_data.polling_interval)

        raise TimeoutError(""Clip creation timed out"")","Point(row=25, column=0)","Point(row=168, column=53)",,autogpt_platform/backend/backend/blocks/talking_head.py
Input,class,,"class Input(BlockSchema):
        credentials: CredentialsMetaInput[Literal[""d_id""], Literal[""api_key""]] = (
            CredentialsField(
                provider=""d_id"",
                supported_credential_types={""api_key""},
                description=""The D-ID integration can be used with ""
                ""any API key with sufficient permissions for the blocks it is used on."",
            )
        )
        script_input: str = SchemaField(
            description=""The text input for the script"",
            placeholder=""Welcome to AutoGPT"",
        )
        provider: Literal[""microsoft"", ""elevenlabs"", ""amazon""] = SchemaField(
            description=""The voice provider to use"", default=""microsoft""
        )
        voice_id: str = SchemaField(
            description=""The voice ID to use, get list of voices [here](https://docs.agpt.co/server/d_id)"",
            default=""en-US-JennyNeural"",
        )
        presenter_id: str = SchemaField(
            description=""The presenter ID to use"", default=""amy-Aq6OmGZnMt""
        )
        driver_id: str = SchemaField(
            description=""The driver ID to use"", default=""Vcq0R4a8F0""
        )
        result_format: Literal[""mp4"", ""gif"", ""wav""] = SchemaField(
            description=""The desired result format"", default=""mp4""
        )
        crop_type: Literal[""wide"", ""square"", ""vertical""] = SchemaField(
            description=""The crop type for the presenter"", default=""wide""
        )
        subtitles: bool = SchemaField(
            description=""Whether to include subtitles"", default=False
        )
        ssml: bool = SchemaField(description=""Whether the input is SSML"", default=False)
        max_polling_attempts: int = SchemaField(
            description=""Maximum number of polling attempts"", default=30, ge=5
        )
        polling_interval: int = SchemaField(
            description=""Interval between polling attempts in seconds"", default=10, ge=5
        )","Point(row=26, column=4)","Point(row=67, column=9)",,autogpt_platform/backend/backend/blocks/talking_head.py
Output,class,,"class Output(BlockSchema):
        video_url: str = SchemaField(description=""The URL of the created video"")
        error: str = SchemaField(description=""Error message if the request failed"")","Point(row=69, column=4)","Point(row=71, column=83)",,autogpt_platform/backend/backend/blocks/talking_head.py
CreateTalkingAvatarVideoBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""98c6f503-8c47-4b1c-a96d-351fc7c87dab"",
            description=""This block integrates with D-ID to create video clips and retrieve their URLs."",
            categories={BlockCategory.AI},
            input_schema=CreateTalkingAvatarVideoBlock.Input,
            output_schema=CreateTalkingAvatarVideoBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""script_input"": ""Welcome to AutoGPT"",
                ""voice_id"": ""en-US-JennyNeural"",
                ""presenter_id"": ""amy-Aq6OmGZnMt"",
                ""driver_id"": ""Vcq0R4a8F0"",
                ""result_format"": ""mp4"",
                ""crop_type"": ""wide"",
                ""subtitles"": False,
                ""ssml"": False,
                ""max_polling_attempts"": 5,
                ""polling_interval"": 5,
            },
            test_output=[
                (
                    ""video_url"",
                    ""https://d-id.com/api/clips/abcd1234-5678-efgh-ijkl-mnopqrstuvwx/video"",
                ),
            ],
            test_mock={
                ""create_clip"": lambda *args, **kwargs: {
                    ""id"": ""abcd1234-5678-efgh-ijkl-mnopqrstuvwx"",
                    ""status"": ""created"",
                },
                ""get_clip_status"": lambda *args, **kwargs: {
                    ""status"": ""done"",
                    ""result_url"": ""https://d-id.com/api/clips/abcd1234-5678-efgh-ijkl-mnopqrstuvwx/video"",
                },
            },
            test_credentials=TEST_CREDENTIALS,
        )","Point(row=73, column=4)","Point(row=110, column=9)",CreateTalkingAvatarVideoBlock,autogpt_platform/backend/backend/blocks/talking_head.py
CreateTalkingAvatarVideoBlock.create_clip,function,,"def create_clip(self, api_key: SecretStr, payload: dict) -> dict:
        url = ""https://api.d-id.com/clips""
        headers = {
            ""accept"": ""application/json"",
            ""content-type"": ""application/json"",
            ""authorization"": f""Basic {api_key.get_secret_value()}"",
        }
        response = requests.post(url, json=payload, headers=headers)
        return response.json()","Point(row=112, column=4)","Point(row=120, column=30)",CreateTalkingAvatarVideoBlock,autogpt_platform/backend/backend/blocks/talking_head.py
CreateTalkingAvatarVideoBlock.get_clip_status,function,,"def get_clip_status(self, api_key: SecretStr, clip_id: str) -> dict:
        url = f""https://api.d-id.com/clips/{clip_id}""
        headers = {
            ""accept"": ""application/json"",
            ""authorization"": f""Basic {api_key.get_secret_value()}"",
        }
        response = requests.get(url, headers=headers)
        return response.json()","Point(row=122, column=4)","Point(row=129, column=30)",CreateTalkingAvatarVideoBlock,autogpt_platform/backend/backend/blocks/talking_head.py
CreateTalkingAvatarVideoBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        # Create the clip
        payload = {
            ""script"": {
                ""type"": ""text"",
                ""subtitles"": str(input_data.subtitles).lower(),
                ""provider"": {
                    ""type"": input_data.provider,
                    ""voice_id"": input_data.voice_id,
                },
                ""ssml"": str(input_data.ssml).lower(),
                ""input"": input_data.script_input,
            },
            ""config"": {""result_format"": input_data.result_format},
            ""presenter_config"": {""crop"": {""type"": input_data.crop_type}},
            ""presenter_id"": input_data.presenter_id,
            ""driver_id"": input_data.driver_id,
        }

        response = self.create_clip(credentials.api_key, payload)
        clip_id = response[""id""]

        # Poll for clip status
        for _ in range(input_data.max_polling_attempts):
            status_response = self.get_clip_status(credentials.api_key, clip_id)
            if status_response[""status""] == ""done"":
                yield ""video_url"", status_response[""result_url""]
                return
            elif status_response[""status""] == ""error"":
                raise RuntimeError(
                    f""Clip creation failed: {status_response.get('error', 'Unknown error')}""
                )

            time.sleep(input_data.polling_interval)

        raise TimeoutError(""Clip creation timed out"")","Point(row=131, column=4)","Point(row=168, column=53)",CreateTalkingAvatarVideoBlock,autogpt_platform/backend/backend/blocks/talking_head.py
TranscribeYoutubeVideoBlock,class,,"class TranscribeYoutubeVideoBlock(Block):
    class Input(BlockSchema):
        youtube_url: str = SchemaField(
            title=""YouTube URL"",
            description=""The URL of the YouTube video to transcribe"",
            placeholder=""https://www.youtube.com/watch?v=dQw4w9WgXcQ"",
        )

    class Output(BlockSchema):
        video_id: str = SchemaField(description=""The extracted YouTube video ID"")
        transcript: str = SchemaField(description=""The transcribed text of the video"")
        error: str = SchemaField(
            description=""Any error message if the transcription fails""
        )

    def __init__(self):
        super().__init__(
            id=""f3a8f7e1-4b1d-4e5f-9f2a-7c3d5a2e6b4c"",
            input_schema=TranscribeYoutubeVideoBlock.Input,
            output_schema=TranscribeYoutubeVideoBlock.Output,
            description=""Transcribes a YouTube video."",
            categories={BlockCategory.SOCIAL},
            test_input={""youtube_url"": ""https://www.youtube.com/watch?v=dQw4w9WgXcQ""},
            test_output=[
                (""video_id"", ""dQw4w9WgXcQ""),
                (
                    ""transcript"",
                    ""Never gonna give you up\nNever gonna let you down"",
                ),
            ],
            test_mock={
                ""get_transcript"": lambda video_id: [
                    {""text"": ""Never gonna give you up""},
                    {""text"": ""Never gonna let you down""},
                ],
            },
        )

    @staticmethod
    def extract_video_id(url: str) -> str:
        parsed_url = urlparse(url)
        if parsed_url.netloc == ""youtu.be"":
            return parsed_url.path[1:]
        if parsed_url.netloc in (""www.youtube.com"", ""youtube.com""):
            if parsed_url.path == ""/watch"":
                p = parse_qs(parsed_url.query)
                return p[""v""][0]
            if parsed_url.path[:7] == ""/embed/"":
                return parsed_url.path.split(""/"")[2]
            if parsed_url.path[:3] == ""/v/"":
                return parsed_url.path.split(""/"")[2]
        raise ValueError(f""Invalid YouTube URL: {url}"")

    @staticmethod
    def get_transcript(video_id: str):
        return YouTubeTranscriptApi.get_transcript(video_id)

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        video_id = self.extract_video_id(input_data.youtube_url)
        yield ""video_id"", video_id

        transcript = self.get_transcript(video_id)
        formatter = TextFormatter()
        transcript_text = formatter.format_transcript(transcript)

        yield ""transcript"", transcript_text","Point(row=9, column=0)","Point(row=74, column=43)",,autogpt_platform/backend/backend/blocks/youtube.py
Input,class,,"class Input(BlockSchema):
        youtube_url: str = SchemaField(
            title=""YouTube URL"",
            description=""The URL of the YouTube video to transcribe"",
            placeholder=""https://www.youtube.com/watch?v=dQw4w9WgXcQ"",
        )","Point(row=10, column=4)","Point(row=15, column=9)",,autogpt_platform/backend/backend/blocks/youtube.py
Output,class,,"class Output(BlockSchema):
        video_id: str = SchemaField(description=""The extracted YouTube video ID"")
        transcript: str = SchemaField(description=""The transcribed text of the video"")
        error: str = SchemaField(
            description=""Any error message if the transcription fails""
        )","Point(row=17, column=4)","Point(row=22, column=9)",,autogpt_platform/backend/backend/blocks/youtube.py
TranscribeYoutubeVideoBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""f3a8f7e1-4b1d-4e5f-9f2a-7c3d5a2e6b4c"",
            input_schema=TranscribeYoutubeVideoBlock.Input,
            output_schema=TranscribeYoutubeVideoBlock.Output,
            description=""Transcribes a YouTube video."",
            categories={BlockCategory.SOCIAL},
            test_input={""youtube_url"": ""https://www.youtube.com/watch?v=dQw4w9WgXcQ""},
            test_output=[
                (""video_id"", ""dQw4w9WgXcQ""),
                (
                    ""transcript"",
                    ""Never gonna give you up\nNever gonna let you down"",
                ),
            ],
            test_mock={
                ""get_transcript"": lambda video_id: [
                    {""text"": ""Never gonna give you up""},
                    {""text"": ""Never gonna let you down""},
                ],
            },
        )","Point(row=24, column=4)","Point(row=45, column=9)",TranscribeYoutubeVideoBlock,autogpt_platform/backend/backend/blocks/youtube.py
TranscribeYoutubeVideoBlock.extract_video_id,function,,"def extract_video_id(url: str) -> str:
        parsed_url = urlparse(url)
        if parsed_url.netloc == ""youtu.be"":
            return parsed_url.path[1:]
        if parsed_url.netloc in (""www.youtube.com"", ""youtube.com""):
            if parsed_url.path == ""/watch"":
                p = parse_qs(parsed_url.query)
                return p[""v""][0]
            if parsed_url.path[:7] == ""/embed/"":
                return parsed_url.path.split(""/"")[2]
            if parsed_url.path[:3] == ""/v/"":
                return parsed_url.path.split(""/"")[2]
        raise ValueError(f""Invalid YouTube URL: {url}"")","Point(row=48, column=4)","Point(row=60, column=55)",TranscribeYoutubeVideoBlock,autogpt_platform/backend/backend/blocks/youtube.py
TranscribeYoutubeVideoBlock.get_transcript,function,,"def get_transcript(video_id: str):
        return YouTubeTranscriptApi.get_transcript(video_id)","Point(row=63, column=4)","Point(row=64, column=60)",TranscribeYoutubeVideoBlock,autogpt_platform/backend/backend/blocks/youtube.py
TranscribeYoutubeVideoBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        video_id = self.extract_video_id(input_data.youtube_url)
        yield ""video_id"", video_id

        transcript = self.get_transcript(video_id)
        formatter = TextFormatter()
        transcript_text = formatter.format_transcript(transcript)

        yield ""transcript"", transcript_text","Point(row=66, column=4)","Point(row=74, column=43)",TranscribeYoutubeVideoBlock,autogpt_platform/backend/backend/blocks/youtube.py
PineconeCredentialsField,function,"
    Creates a Pinecone credentials input on a block.

","def PineconeCredentialsField() -> PineconeCredentialsInput:
    """"""
    Creates a Pinecone credentials input on a block.

    """"""
    return CredentialsField(
        provider=""pinecone"",
        supported_credential_types={""api_key""},
        description=""The Pinecone integration can be used with an API Key."",
    )","Point(row=16, column=0)","Point(row=25, column=5)",,autogpt_platform/backend/backend/blocks/pinecone.py
PineconeInitBlock,class,,"class PineconeInitBlock(Block):
    class Input(BlockSchema):
        credentials: PineconeCredentialsInput = PineconeCredentialsField()
        index_name: str = SchemaField(description=""Name of the Pinecone index"")
        dimension: int = SchemaField(
            description=""Dimension of the vectors"", default=768
        )
        metric: str = SchemaField(
            description=""Distance metric for the index"", default=""cosine""
        )
        cloud: str = SchemaField(
            description=""Cloud provider for serverless"", default=""aws""
        )
        region: str = SchemaField(
            description=""Region for serverless"", default=""us-east-1""
        )

    class Output(BlockSchema):
        index: str = SchemaField(description=""Name of the initialized Pinecone index"")
        message: str = SchemaField(description=""Status message"")

    def __init__(self):
        super().__init__(
            id=""48d8fdab-8f03-41f3-8407-8107ba11ec9b"",
            description=""Initializes a Pinecone index"",
            categories={BlockCategory.LOGIC},
            input_schema=PineconeInitBlock.Input,
            output_schema=PineconeInitBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        pc = Pinecone(api_key=credentials.api_key.get_secret_value())

        try:
            existing_indexes = pc.list_indexes()
            if input_data.index_name not in [index.name for index in existing_indexes]:
                pc.create_index(
                    name=input_data.index_name,
                    dimension=input_data.dimension,
                    metric=input_data.metric,
                    spec=ServerlessSpec(
                        cloud=input_data.cloud, region=input_data.region
                    ),
                )
                message = f""Created new index: {input_data.index_name}""
            else:
                message = f""Using existing index: {input_data.index_name}""

            yield ""index"", input_data.index_name
            yield ""message"", message
        except Exception as e:
            yield ""message"", f""Error initializing Pinecone index: {str(e)}""","Point(row=28, column=0)","Point(row=81, column=75)",,autogpt_platform/backend/backend/blocks/pinecone.py
Input,class,,"class Input(BlockSchema):
        credentials: PineconeCredentialsInput = PineconeCredentialsField()
        index_name: str = SchemaField(description=""Name of the Pinecone index"")
        dimension: int = SchemaField(
            description=""Dimension of the vectors"", default=768
        )
        metric: str = SchemaField(
            description=""Distance metric for the index"", default=""cosine""
        )
        cloud: str = SchemaField(
            description=""Cloud provider for serverless"", default=""aws""
        )
        region: str = SchemaField(
            description=""Region for serverless"", default=""us-east-1""
        )","Point(row=29, column=4)","Point(row=43, column=9)",,autogpt_platform/backend/backend/blocks/pinecone.py
Output,class,,"class Output(BlockSchema):
        index: str = SchemaField(description=""Name of the initialized Pinecone index"")
        message: str = SchemaField(description=""Status message"")","Point(row=45, column=4)","Point(row=47, column=64)",,autogpt_platform/backend/backend/blocks/pinecone.py
PineconeInitBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""48d8fdab-8f03-41f3-8407-8107ba11ec9b"",
            description=""Initializes a Pinecone index"",
            categories={BlockCategory.LOGIC},
            input_schema=PineconeInitBlock.Input,
            output_schema=PineconeInitBlock.Output,
        )","Point(row=49, column=4)","Point(row=56, column=9)",PineconeInitBlock,autogpt_platform/backend/backend/blocks/pinecone.py
PineconeInitBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        pc = Pinecone(api_key=credentials.api_key.get_secret_value())

        try:
            existing_indexes = pc.list_indexes()
            if input_data.index_name not in [index.name for index in existing_indexes]:
                pc.create_index(
                    name=input_data.index_name,
                    dimension=input_data.dimension,
                    metric=input_data.metric,
                    spec=ServerlessSpec(
                        cloud=input_data.cloud, region=input_data.region
                    ),
                )
                message = f""Created new index: {input_data.index_name}""
            else:
                message = f""Using existing index: {input_data.index_name}""

            yield ""index"", input_data.index_name
            yield ""message"", message
        except Exception as e:
            yield ""message"", f""Error initializing Pinecone index: {str(e)}""","Point(row=58, column=4)","Point(row=81, column=75)",PineconeInitBlock,autogpt_platform/backend/backend/blocks/pinecone.py
PineconeQueryBlock,class,,"class PineconeQueryBlock(Block):
    class Input(BlockSchema):
        credentials: PineconeCredentialsInput = PineconeCredentialsField()
        query_vector: list = SchemaField(description=""Query vector"")
        namespace: str = SchemaField(
            description=""Namespace to query in Pinecone"", default=""""
        )
        top_k: int = SchemaField(
            description=""Number of top results to return"", default=3
        )
        include_values: bool = SchemaField(
            description=""Whether to include vector values in the response"",
            default=False,
        )
        include_metadata: bool = SchemaField(
            description=""Whether to include metadata in the response"", default=True
        )
        host: str = SchemaField(description=""Host for pinecone"", default="""")
        idx_name: str = SchemaField(description=""Index name for pinecone"")

    class Output(BlockSchema):
        results: Any = SchemaField(description=""Query results from Pinecone"")
        combined_results: Any = SchemaField(
            description=""Combined results from Pinecone""
        )

    def __init__(self):
        super().__init__(
            id=""9ad93d0f-91b4-4c9c-8eb1-82e26b4a01c5"",
            description=""Queries a Pinecone index"",
            categories={BlockCategory.LOGIC},
            input_schema=PineconeQueryBlock.Input,
            output_schema=PineconeQueryBlock.Output,
        )

    def run(
        self,
        input_data: Input,
        *,
        credentials: APIKeyCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            # Create a new client instance
            pc = Pinecone(api_key=credentials.api_key.get_secret_value())

            # Get the index
            idx = pc.Index(input_data.idx_name)

            # Ensure query_vector is in correct format
            query_vector = input_data.query_vector
            if isinstance(query_vector, list) and len(query_vector) > 0:
                if isinstance(query_vector[0], list):
                    query_vector = query_vector[0]

            results = idx.query(
                namespace=input_data.namespace,
                vector=query_vector,
                top_k=input_data.top_k,
                include_values=input_data.include_values,
                include_metadata=input_data.include_metadata,
            ).to_dict()
            combined_text = """"
            if results[""matches""]:
                texts = [
                    match[""metadata""][""text""]
                    for match in results[""matches""]
                    if match.get(""metadata"", {}).get(""text"")
                ]
                combined_text = ""\n\n"".join(texts)

            # Return both the raw matches and combined text
            yield ""results"", {
                ""matches"": results[""matches""],
                ""combined_text"": combined_text,
            }
            yield ""combined_results"", combined_text

        except Exception as e:
            error_msg = f""Error querying Pinecone: {str(e)}""
            raise RuntimeError(error_msg) from e","Point(row=84, column=0)","Point(row=164, column=48)",,autogpt_platform/backend/backend/blocks/pinecone.py
Input,class,,"class Input(BlockSchema):
        credentials: PineconeCredentialsInput = PineconeCredentialsField()
        query_vector: list = SchemaField(description=""Query vector"")
        namespace: str = SchemaField(
            description=""Namespace to query in Pinecone"", default=""""
        )
        top_k: int = SchemaField(
            description=""Number of top results to return"", default=3
        )
        include_values: bool = SchemaField(
            description=""Whether to include vector values in the response"",
            default=False,
        )
        include_metadata: bool = SchemaField(
            description=""Whether to include metadata in the response"", default=True
        )
        host: str = SchemaField(description=""Host for pinecone"", default="""")
        idx_name: str = SchemaField(description=""Index name for pinecone"")","Point(row=85, column=4)","Point(row=102, column=74)",,autogpt_platform/backend/backend/blocks/pinecone.py
Output,class,,"class Output(BlockSchema):
        results: Any = SchemaField(description=""Query results from Pinecone"")
        combined_results: Any = SchemaField(
            description=""Combined results from Pinecone""
        )","Point(row=104, column=4)","Point(row=108, column=9)",,autogpt_platform/backend/backend/blocks/pinecone.py
PineconeQueryBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""9ad93d0f-91b4-4c9c-8eb1-82e26b4a01c5"",
            description=""Queries a Pinecone index"",
            categories={BlockCategory.LOGIC},
            input_schema=PineconeQueryBlock.Input,
            output_schema=PineconeQueryBlock.Output,
        )","Point(row=110, column=4)","Point(row=117, column=9)",PineconeQueryBlock,autogpt_platform/backend/backend/blocks/pinecone.py
PineconeQueryBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: APIKeyCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            # Create a new client instance
            pc = Pinecone(api_key=credentials.api_key.get_secret_value())

            # Get the index
            idx = pc.Index(input_data.idx_name)

            # Ensure query_vector is in correct format
            query_vector = input_data.query_vector
            if isinstance(query_vector, list) and len(query_vector) > 0:
                if isinstance(query_vector[0], list):
                    query_vector = query_vector[0]

            results = idx.query(
                namespace=input_data.namespace,
                vector=query_vector,
                top_k=input_data.top_k,
                include_values=input_data.include_values,
                include_metadata=input_data.include_metadata,
            ).to_dict()
            combined_text = """"
            if results[""matches""]:
                texts = [
                    match[""metadata""][""text""]
                    for match in results[""matches""]
                    if match.get(""metadata"", {}).get(""text"")
                ]
                combined_text = ""\n\n"".join(texts)

            # Return both the raw matches and combined text
            yield ""results"", {
                ""matches"": results[""matches""],
                ""combined_text"": combined_text,
            }
            yield ""combined_results"", combined_text

        except Exception as e:
            error_msg = f""Error querying Pinecone: {str(e)}""
            raise RuntimeError(error_msg) from e","Point(row=119, column=4)","Point(row=164, column=48)",PineconeQueryBlock,autogpt_platform/backend/backend/blocks/pinecone.py
PineconeInsertBlock,class,,"class PineconeInsertBlock(Block):
    class Input(BlockSchema):
        credentials: PineconeCredentialsInput = PineconeCredentialsField()
        index: str = SchemaField(description=""Initialized Pinecone index"")
        chunks: list = SchemaField(description=""List of text chunks to ingest"")
        embeddings: list = SchemaField(
            description=""List of embeddings corresponding to the chunks""
        )
        namespace: str = SchemaField(
            description=""Namespace to use in Pinecone"", default=""""
        )
        metadata: dict = SchemaField(
            description=""Additional metadata to store with each vector"", default={}
        )

    class Output(BlockSchema):
        upsert_response: str = SchemaField(
            description=""Response from Pinecone upsert operation""
        )

    def __init__(self):
        super().__init__(
            id=""477f2168-cd91-475a-8146-9499a5982434"",
            description=""Upload data to a Pinecone index"",
            categories={BlockCategory.LOGIC},
            input_schema=PineconeInsertBlock.Input,
            output_schema=PineconeInsertBlock.Output,
        )

    def run(
        self,
        input_data: Input,
        *,
        credentials: APIKeyCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            # Create a new client instance
            pc = Pinecone(api_key=credentials.api_key.get_secret_value())

            # Get the index
            idx = pc.Index(input_data.index)

            vectors = []
            for chunk, embedding in zip(input_data.chunks, input_data.embeddings):
                vector_metadata = input_data.metadata.copy()
                vector_metadata[""text""] = chunk
                vectors.append(
                    {
                        ""id"": str(uuid.uuid4()),
                        ""values"": embedding,
                        ""metadata"": vector_metadata,
                    }
                )
            idx.upsert(vectors=vectors, namespace=input_data.namespace)

            yield ""upsert_response"", ""successfully upserted""

        except Exception as e:
            error_msg = f""Error uploading to Pinecone: {str(e)}""
            raise RuntimeError(error_msg) from e","Point(row=167, column=0)","Point(row=227, column=48)",,autogpt_platform/backend/backend/blocks/pinecone.py
Input,class,,"class Input(BlockSchema):
        credentials: PineconeCredentialsInput = PineconeCredentialsField()
        index: str = SchemaField(description=""Initialized Pinecone index"")
        chunks: list = SchemaField(description=""List of text chunks to ingest"")
        embeddings: list = SchemaField(
            description=""List of embeddings corresponding to the chunks""
        )
        namespace: str = SchemaField(
            description=""Namespace to use in Pinecone"", default=""""
        )
        metadata: dict = SchemaField(
            description=""Additional metadata to store with each vector"", default={}
        )","Point(row=168, column=4)","Point(row=180, column=9)",,autogpt_platform/backend/backend/blocks/pinecone.py
Output,class,,"class Output(BlockSchema):
        upsert_response: str = SchemaField(
            description=""Response from Pinecone upsert operation""
        )","Point(row=182, column=4)","Point(row=185, column=9)",,autogpt_platform/backend/backend/blocks/pinecone.py
PineconeInsertBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""477f2168-cd91-475a-8146-9499a5982434"",
            description=""Upload data to a Pinecone index"",
            categories={BlockCategory.LOGIC},
            input_schema=PineconeInsertBlock.Input,
            output_schema=PineconeInsertBlock.Output,
        )","Point(row=187, column=4)","Point(row=194, column=9)",PineconeInsertBlock,autogpt_platform/backend/backend/blocks/pinecone.py
PineconeInsertBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: APIKeyCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            # Create a new client instance
            pc = Pinecone(api_key=credentials.api_key.get_secret_value())

            # Get the index
            idx = pc.Index(input_data.index)

            vectors = []
            for chunk, embedding in zip(input_data.chunks, input_data.embeddings):
                vector_metadata = input_data.metadata.copy()
                vector_metadata[""text""] = chunk
                vectors.append(
                    {
                        ""id"": str(uuid.uuid4()),
                        ""values"": embedding,
                        ""metadata"": vector_metadata,
                    }
                )
            idx.upsert(vectors=vectors, namespace=input_data.namespace)

            yield ""upsert_response"", ""successfully upserted""

        except Exception as e:
            error_msg = f""Error uploading to Pinecone: {str(e)}""
            raise RuntimeError(error_msg) from e","Point(row=196, column=4)","Point(row=227, column=48)",PineconeInsertBlock,autogpt_platform/backend/backend/blocks/pinecone.py
DiscordCredentialsField,function,,"def DiscordCredentialsField() -> DiscordCredentials:
    return CredentialsField(
        description=""Discord bot token"",
        provider=""discord"",
        supported_credential_types={""api_key""},
    )","Point(row=14, column=0)","Point(row=19, column=5)",,autogpt_platform/backend/backend/blocks/discord.py
ReadDiscordMessagesBlock,class,,"class ReadDiscordMessagesBlock(Block):
    class Input(BlockSchema):
        credentials: DiscordCredentials = DiscordCredentialsField()

    class Output(BlockSchema):
        message_content: str = SchemaField(
            description=""The content of the message received""
        )
        channel_name: str = SchemaField(
            description=""The name of the channel the message was received from""
        )
        username: str = SchemaField(
            description=""The username of the user who sent the message""
        )

    def __init__(self):
        super().__init__(
            id=""df06086a-d5ac-4abb-9996-2ad0acb2eff7"",
            input_schema=ReadDiscordMessagesBlock.Input,  # Assign input schema
            output_schema=ReadDiscordMessagesBlock.Output,  # Assign output schema
            description=""Reads messages from a Discord channel using a bot token."",
            categories={BlockCategory.SOCIAL},
            test_input={
                ""continuous_read"": False,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""message_content"",
                    ""Hello!\n\nFile from user: example.txt\nContent: This is the content of the file."",
                ),
                (""channel_name"", ""general""),
                (""username"", ""test_user""),
            ],
            test_mock={
                ""run_bot"": lambda token: asyncio.Future()  # Create a Future object for mocking
            },
        )

    async def run_bot(self, token: SecretStr):
        intents = discord.Intents.default()
        intents.message_content = True

        client = discord.Client(intents=intents)

        self.output_data = None
        self.channel_name = None
        self.username = None

        @client.event
        async def on_ready():
            print(f""Logged in as {client.user}"")

        @client.event
        async def on_message(message):
            if message.author == client.user:
                return

            self.output_data = message.content
            self.channel_name = message.channel.name
            self.username = message.author.name

            if message.attachments:
                attachment = message.attachments[0]  # Process the first attachment
                if attachment.filename.endswith(("".txt"", "".py"")):
                    async with aiohttp.ClientSession() as session:
                        async with session.get(attachment.url) as response:
                            file_content = await response.text()
                            self.output_data += f""\n\nFile from user: {attachment.filename}\nContent: {file_content}""

            await client.close()

        await client.start(token.get_secret_value())

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        while True:
            for output_name, output_value in self.__run(input_data, credentials):
                yield output_name, output_value
            break

    def __run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:
        try:
            loop = asyncio.get_event_loop()
            future = self.run_bot(credentials.api_key)

            # If it's a Future (mock), set the result
            if isinstance(future, asyncio.Future):
                future.set_result(
                    {
                        ""output_data"": ""Hello!\n\nFile from user: example.txt\nContent: This is the content of the file."",
                        ""channel_name"": ""general"",
                        ""username"": ""test_user"",
                    }
                )

            result = loop.run_until_complete(future)

            # For testing purposes, use the mocked result
            if isinstance(result, dict):
                self.output_data = result.get(""output_data"")
                self.channel_name = result.get(""channel_name"")
                self.username = result.get(""username"")

            if (
                self.output_data is None
                or self.channel_name is None
                or self.username is None
            ):
                raise ValueError(""No message, channel name, or username received."")

            yield ""message_content"", self.output_data
            yield ""channel_name"", self.channel_name
            yield ""username"", self.username

        except discord.errors.LoginFailure as login_err:
            raise ValueError(f""Login error occurred: {login_err}"")
        except Exception as e:
            raise ValueError(f""An error occurred: {e}"")","Point(row=37, column=0)","Point(row=157, column=55)",,autogpt_platform/backend/backend/blocks/discord.py
Input,class,,"class Input(BlockSchema):
        credentials: DiscordCredentials = DiscordCredentialsField()","Point(row=38, column=4)","Point(row=39, column=67)",,autogpt_platform/backend/backend/blocks/discord.py
Output,class,,"class Output(BlockSchema):
        message_content: str = SchemaField(
            description=""The content of the message received""
        )
        channel_name: str = SchemaField(
            description=""The name of the channel the message was received from""
        )
        username: str = SchemaField(
            description=""The username of the user who sent the message""
        )","Point(row=41, column=4)","Point(row=50, column=9)",,autogpt_platform/backend/backend/blocks/discord.py
ReadDiscordMessagesBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""df06086a-d5ac-4abb-9996-2ad0acb2eff7"",
            input_schema=ReadDiscordMessagesBlock.Input,  # Assign input schema
            output_schema=ReadDiscordMessagesBlock.Output,  # Assign output schema
            description=""Reads messages from a Discord channel using a bot token."",
            categories={BlockCategory.SOCIAL},
            test_input={
                ""continuous_read"": False,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""message_content"",
                    ""Hello!\n\nFile from user: example.txt\nContent: This is the content of the file."",
                ),
                (""channel_name"", ""general""),
                (""username"", ""test_user""),
            ],
            test_mock={
                ""run_bot"": lambda token: asyncio.Future()  # Create a Future object for mocking
            },
        )","Point(row=52, column=4)","Point(row=75, column=9)",ReadDiscordMessagesBlock,autogpt_platform/backend/backend/blocks/discord.py
ReadDiscordMessagesBlock.run_bot,function,,"async def run_bot(self, token: SecretStr):
        intents = discord.Intents.default()
        intents.message_content = True

        client = discord.Client(intents=intents)

        self.output_data = None
        self.channel_name = None
        self.username = None

        @client.event
        async def on_ready():
            print(f""Logged in as {client.user}"")

        @client.event
        async def on_message(message):
            if message.author == client.user:
                return

            self.output_data = message.content
            self.channel_name = message.channel.name
            self.username = message.author.name

            if message.attachments:
                attachment = message.attachments[0]  # Process the first attachment
                if attachment.filename.endswith(("".txt"", "".py"")):
                    async with aiohttp.ClientSession() as session:
                        async with session.get(attachment.url) as response:
                            file_content = await response.text()
                            self.output_data += f""\n\nFile from user: {attachment.filename}\nContent: {file_content}""

            await client.close()

        await client.start(token.get_secret_value())","Point(row=77, column=4)","Point(row=110, column=52)",ReadDiscordMessagesBlock,autogpt_platform/backend/backend/blocks/discord.py
ReadDiscordMessagesBlock.run_bot.on_ready,function,,"async def on_ready():
            print(f""Logged in as {client.user}"")","Point(row=88, column=8)","Point(row=89, column=48)",ReadDiscordMessagesBlock,autogpt_platform/backend/backend/blocks/discord.py
ReadDiscordMessagesBlock.run_bot.on_message,function,,"async def on_message(message):
            if message.author == client.user:
                return

            self.output_data = message.content
            self.channel_name = message.channel.name
            self.username = message.author.name

            if message.attachments:
                attachment = message.attachments[0]  # Process the first attachment
                if attachment.filename.endswith(("".txt"", "".py"")):
                    async with aiohttp.ClientSession() as session:
                        async with session.get(attachment.url) as response:
                            file_content = await response.text()
                            self.output_data += f""\n\nFile from user: {attachment.filename}\nContent: {file_content}""

            await client.close()","Point(row=92, column=8)","Point(row=108, column=32)",ReadDiscordMessagesBlock,autogpt_platform/backend/backend/blocks/discord.py
ReadDiscordMessagesBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        while True:
            for output_name, output_value in self.__run(input_data, credentials):
                yield output_name, output_value
            break","Point(row=112, column=4)","Point(row=118, column=17)",ReadDiscordMessagesBlock,autogpt_platform/backend/backend/blocks/discord.py
ReadDiscordMessagesBlock.__run,function,,"def __run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:
        try:
            loop = asyncio.get_event_loop()
            future = self.run_bot(credentials.api_key)

            # If it's a Future (mock), set the result
            if isinstance(future, asyncio.Future):
                future.set_result(
                    {
                        ""output_data"": ""Hello!\n\nFile from user: example.txt\nContent: This is the content of the file."",
                        ""channel_name"": ""general"",
                        ""username"": ""test_user"",
                    }
                )

            result = loop.run_until_complete(future)

            # For testing purposes, use the mocked result
            if isinstance(result, dict):
                self.output_data = result.get(""output_data"")
                self.channel_name = result.get(""channel_name"")
                self.username = result.get(""username"")

            if (
                self.output_data is None
                or self.channel_name is None
                or self.username is None
            ):
                raise ValueError(""No message, channel name, or username received."")

            yield ""message_content"", self.output_data
            yield ""channel_name"", self.channel_name
            yield ""username"", self.username

        except discord.errors.LoginFailure as login_err:
            raise ValueError(f""Login error occurred: {login_err}"")
        except Exception as e:
            raise ValueError(f""An error occurred: {e}"")","Point(row=120, column=4)","Point(row=157, column=55)",ReadDiscordMessagesBlock,autogpt_platform/backend/backend/blocks/discord.py
SendDiscordMessageBlock,class,,"class SendDiscordMessageBlock(Block):
    class Input(BlockSchema):
        credentials: DiscordCredentials = DiscordCredentialsField()
        message_content: str = SchemaField(
            description=""The content of the message received""
        )
        channel_name: str = SchemaField(
            description=""The name of the channel the message was received from""
        )

    class Output(BlockSchema):
        status: str = SchemaField(
            description=""The status of the operation (e.g., 'Message sent', 'Error')""
        )

    def __init__(self):
        super().__init__(
            id=""d0822ab5-9f8a-44a3-8971-531dd0178b6b"",
            input_schema=SendDiscordMessageBlock.Input,  # Assign input schema
            output_schema=SendDiscordMessageBlock.Output,  # Assign output schema
            description=""Sends a message to a Discord channel using a bot token."",
            categories={BlockCategory.SOCIAL},
            test_input={
                ""channel_name"": ""general"",
                ""message_content"": ""Hello, Discord!"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[(""status"", ""Message sent"")],
            test_mock={
                ""send_message"": lambda token, channel_name, message_content: asyncio.Future()
            },
            test_credentials=TEST_CREDENTIALS,
        )

    async def send_message(self, token: str, channel_name: str, message_content: str):
        intents = discord.Intents.default()
        intents.guilds = True  # Required for fetching guild/channel information
        client = discord.Client(intents=intents)

        @client.event
        async def on_ready():
            print(f""Logged in as {client.user}"")
            for guild in client.guilds:
                for channel in guild.text_channels:
                    if channel.name == channel_name:
                        # Split message into chunks if it exceeds 2000 characters
                        for chunk in self.chunk_message(message_content):
                            await channel.send(chunk)
                        self.output_data = ""Message sent""
                        await client.close()
                        return

            self.output_data = ""Channel not found""
            await client.close()

        await client.start(token)

    def chunk_message(self, message: str, limit: int = 2000) -> list:
        """"""Splits a message into chunks not exceeding the Discord limit.""""""
        return [message[i : i + limit] for i in range(0, len(message), limit)]

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            loop = asyncio.get_event_loop()
            future = self.send_message(
                credentials.api_key.get_secret_value(),
                input_data.channel_name,
                input_data.message_content,
            )

            # If it's a Future (mock), set the result
            if isinstance(future, asyncio.Future):
                future.set_result(""Message sent"")

            result = loop.run_until_complete(future)

            # For testing purposes, use the mocked result
            if isinstance(result, str):
                self.output_data = result

            if self.output_data is None:
                raise ValueError(""No status message received."")

            yield ""status"", self.output_data

        except discord.errors.LoginFailure as login_err:
            raise ValueError(f""Login error occurred: {login_err}"")
        except Exception as e:
            raise ValueError(f""An error occurred: {e}"")","Point(row=160, column=0)","Point(row=250, column=55)",,autogpt_platform/backend/backend/blocks/discord.py
Input,class,,"class Input(BlockSchema):
        credentials: DiscordCredentials = DiscordCredentialsField()
        message_content: str = SchemaField(
            description=""The content of the message received""
        )
        channel_name: str = SchemaField(
            description=""The name of the channel the message was received from""
        )","Point(row=161, column=4)","Point(row=168, column=9)",,autogpt_platform/backend/backend/blocks/discord.py
Output,class,,"class Output(BlockSchema):
        status: str = SchemaField(
            description=""The status of the operation (e.g., 'Message sent', 'Error')""
        )","Point(row=170, column=4)","Point(row=173, column=9)",,autogpt_platform/backend/backend/blocks/discord.py
SendDiscordMessageBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""d0822ab5-9f8a-44a3-8971-531dd0178b6b"",
            input_schema=SendDiscordMessageBlock.Input,  # Assign input schema
            output_schema=SendDiscordMessageBlock.Output,  # Assign output schema
            description=""Sends a message to a Discord channel using a bot token."",
            categories={BlockCategory.SOCIAL},
            test_input={
                ""channel_name"": ""general"",
                ""message_content"": ""Hello, Discord!"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[(""status"", ""Message sent"")],
            test_mock={
                ""send_message"": lambda token, channel_name, message_content: asyncio.Future()
            },
            test_credentials=TEST_CREDENTIALS,
        )","Point(row=175, column=4)","Point(row=192, column=9)",SendDiscordMessageBlock,autogpt_platform/backend/backend/blocks/discord.py
SendDiscordMessageBlock.send_message,function,,"async def send_message(self, token: str, channel_name: str, message_content: str):
        intents = discord.Intents.default()
        intents.guilds = True  # Required for fetching guild/channel information
        client = discord.Client(intents=intents)

        @client.event
        async def on_ready():
            print(f""Logged in as {client.user}"")
            for guild in client.guilds:
                for channel in guild.text_channels:
                    if channel.name == channel_name:
                        # Split message into chunks if it exceeds 2000 characters
                        for chunk in self.chunk_message(message_content):
                            await channel.send(chunk)
                        self.output_data = ""Message sent""
                        await client.close()
                        return

            self.output_data = ""Channel not found""
            await client.close()

        await client.start(token)","Point(row=194, column=4)","Point(row=215, column=33)",SendDiscordMessageBlock,autogpt_platform/backend/backend/blocks/discord.py
SendDiscordMessageBlock.send_message.on_ready,function,,"async def on_ready():
            print(f""Logged in as {client.user}"")
            for guild in client.guilds:
                for channel in guild.text_channels:
                    if channel.name == channel_name:
                        # Split message into chunks if it exceeds 2000 characters
                        for chunk in self.chunk_message(message_content):
                            await channel.send(chunk)
                        self.output_data = ""Message sent""
                        await client.close()
                        return

            self.output_data = ""Channel not found""
            await client.close()","Point(row=200, column=8)","Point(row=213, column=32)",SendDiscordMessageBlock,autogpt_platform/backend/backend/blocks/discord.py
SendDiscordMessageBlock.chunk_message,function,Splits a message into chunks not exceeding the Discord limit.,"def chunk_message(self, message: str, limit: int = 2000) -> list:
        """"""Splits a message into chunks not exceeding the Discord limit.""""""
        return [message[i : i + limit] for i in range(0, len(message), limit)]","Point(row=217, column=4)","Point(row=219, column=78)",SendDiscordMessageBlock,autogpt_platform/backend/backend/blocks/discord.py
SendDiscordMessageBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        try:
            loop = asyncio.get_event_loop()
            future = self.send_message(
                credentials.api_key.get_secret_value(),
                input_data.channel_name,
                input_data.message_content,
            )

            # If it's a Future (mock), set the result
            if isinstance(future, asyncio.Future):
                future.set_result(""Message sent"")

            result = loop.run_until_complete(future)

            # For testing purposes, use the mocked result
            if isinstance(result, str):
                self.output_data = result

            if self.output_data is None:
                raise ValueError(""No status message received."")

            yield ""status"", self.output_data

        except discord.errors.LoginFailure as login_err:
            raise ValueError(f""Login error occurred: {login_err}"")
        except Exception as e:
            raise ValueError(f""An error occurred: {e}"")","Point(row=221, column=4)","Point(row=250, column=55)",SendDiscordMessageBlock,autogpt_platform/backend/backend/blocks/discord.py
PublishToMediumStatus,class,,"class PublishToMediumStatus(str, Enum):
    PUBLIC = ""public""
    DRAFT = ""draft""
    UNLISTED = ""unlisted""","Point(row=31, column=0)","Point(row=34, column=25)",,autogpt_platform/backend/backend/blocks/medium.py
PublishToMediumBlock,class,,"class PublishToMediumBlock(Block):
    class Input(BlockSchema):
        author_id: BlockSecret = SecretField(
            key=""medium_author_id"",
            description=""""""The Medium AuthorID of the user. You can get this by calling the /me endpoint of the Medium API.\n\ncurl -H ""Authorization: Bearer YOUR_ACCESS_TOKEN"" https://api.medium.com/v1/me"" the response will contain the authorId field."""""",
            placeholder=""Enter the author's Medium AuthorID"",
        )
        title: str = SchemaField(
            description=""The title of your Medium post"",
            placeholder=""Enter your post title"",
        )
        content: str = SchemaField(
            description=""The main content of your Medium post"",
            placeholder=""Enter your post content"",
        )
        content_format: str = SchemaField(
            description=""The format of the content: 'html' or 'markdown'"",
            placeholder=""html"",
        )
        tags: List[str] = SchemaField(
            description=""List of tags for your Medium post (up to 5)"",
            placeholder=""['technology', 'AI', 'blogging']"",
        )
        canonical_url: str | None = SchemaField(
            default=None,
            description=""The original home of this content, if it was originally published elsewhere"",
            placeholder=""https://yourblog.com/original-post"",
        )
        publish_status: PublishToMediumStatus = SchemaField(
            description=""The publish status"",
            placeholder=PublishToMediumStatus.DRAFT,
        )
        license: str = SchemaField(
            default=""all-rights-reserved"",
            description=""The license of the post: 'all-rights-reserved', 'cc-40-by', 'cc-40-by-sa', 'cc-40-by-nd', 'cc-40-by-nc', 'cc-40-by-nc-nd', 'cc-40-by-nc-sa', 'cc-40-zero', 'public-domain'"",
            placeholder=""all-rights-reserved"",
        )
        notify_followers: bool = SchemaField(
            default=False,
            description=""Whether to notify followers that the user has published"",
            placeholder=""False"",
        )
        credentials: CredentialsMetaInput[Literal[""medium""], Literal[""api_key""]] = (
            CredentialsField(
                provider=""medium"",
                supported_credential_types={""api_key""},
                description=""The Medium integration can be used with any API key with sufficient permissions for the blocks it is used on."",
            )
        )

    class Output(BlockSchema):
        post_id: str = SchemaField(description=""The ID of the created Medium post"")
        post_url: str = SchemaField(description=""The URL of the created Medium post"")
        published_at: int = SchemaField(
            description=""The timestamp when the post was published""
        )
        error: str = SchemaField(
            description=""Error message if the post creation failed""
        )

    def __init__(self):
        super().__init__(
            id=""3f7b2dcb-4a78-4e3f-b0f1-88132e1b89df"",
            input_schema=PublishToMediumBlock.Input,
            output_schema=PublishToMediumBlock.Output,
            description=""Publishes a post to Medium."",
            categories={BlockCategory.SOCIAL},
            test_input={
                ""author_id"": ""1234567890abcdef"",
                ""title"": ""Test Post"",
                ""content"": ""<h1>Test Content</h1><p>This is a test post.</p>"",
                ""content_format"": ""html"",
                ""tags"": [""test"", ""automation""],
                ""license"": ""all-rights-reserved"",
                ""notify_followers"": False,
                ""publish_status"": PublishToMediumStatus.DRAFT.value,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[
                (""post_id"", ""e6f36a""),
                (""post_url"", ""https://medium.com/@username/test-post-e6f36a""),
                (""published_at"", 1626282600),
            ],
            test_mock={
                ""create_post"": lambda *args, **kwargs: {
                    ""data"": {
                        ""id"": ""e6f36a"",
                        ""url"": ""https://medium.com/@username/test-post-e6f36a"",
                        ""authorId"": ""1234567890abcdef"",
                        ""publishedAt"": 1626282600,
                    }
                }
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def create_post(
        self,
        api_key: SecretStr,
        author_id,
        title,
        content,
        content_format,
        tags,
        canonical_url,
        publish_status,
        license,
        notify_followers,
    ):
        headers = {
            ""Authorization"": f""Bearer {api_key.get_secret_value()}"",
            ""Content-Type"": ""application/json"",
            ""Accept"": ""application/json"",
        }

        data = {
            ""title"": title,
            ""content"": content,
            ""contentFormat"": content_format,
            ""tags"": tags,
            ""canonicalUrl"": canonical_url,
            ""publishStatus"": publish_status,
            ""license"": license,
            ""notifyFollowers"": notify_followers,
        }

        response = requests.post(
            f""https://api.medium.com/v1/users/{author_id}/posts"",
            headers=headers,
            json=data,
        )

        return response.json()

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        response = self.create_post(
            credentials.api_key,
            input_data.author_id.get_secret_value(),
            input_data.title,
            input_data.content,
            input_data.content_format,
            input_data.tags,
            input_data.canonical_url,
            input_data.publish_status,
            input_data.license,
            input_data.notify_followers,
        )

        if ""data"" in response:
            yield ""post_id"", response[""data""][""id""]
            yield ""post_url"", response[""data""][""url""]
            yield ""published_at"", response[""data""][""publishedAt""]
        else:
            error_message = response.get(""errors"", [{}])[0].get(
                ""message"", ""Unknown error occurred""
            )
            raise RuntimeError(f""Failed to create Medium post: {error_message}"")","Point(row=37, column=0)","Point(row=195, column=80)",,autogpt_platform/backend/backend/blocks/medium.py
Input,class,,"class Input(BlockSchema):
        author_id: BlockSecret = SecretField(
            key=""medium_author_id"",
            description=""""""The Medium AuthorID of the user. You can get this by calling the /me endpoint of the Medium API.\n\ncurl -H ""Authorization: Bearer YOUR_ACCESS_TOKEN"" https://api.medium.com/v1/me"" the response will contain the authorId field."""""",
            placeholder=""Enter the author's Medium AuthorID"",
        )
        title: str = SchemaField(
            description=""The title of your Medium post"",
            placeholder=""Enter your post title"",
        )
        content: str = SchemaField(
            description=""The main content of your Medium post"",
            placeholder=""Enter your post content"",
        )
        content_format: str = SchemaField(
            description=""The format of the content: 'html' or 'markdown'"",
            placeholder=""html"",
        )
        tags: List[str] = SchemaField(
            description=""List of tags for your Medium post (up to 5)"",
            placeholder=""['technology', 'AI', 'blogging']"",
        )
        canonical_url: str | None = SchemaField(
            default=None,
            description=""The original home of this content, if it was originally published elsewhere"",
            placeholder=""https://yourblog.com/original-post"",
        )
        publish_status: PublishToMediumStatus = SchemaField(
            description=""The publish status"",
            placeholder=PublishToMediumStatus.DRAFT,
        )
        license: str = SchemaField(
            default=""all-rights-reserved"",
            description=""The license of the post: 'all-rights-reserved', 'cc-40-by', 'cc-40-by-sa', 'cc-40-by-nd', 'cc-40-by-nc', 'cc-40-by-nc-nd', 'cc-40-by-nc-sa', 'cc-40-zero', 'public-domain'"",
            placeholder=""all-rights-reserved"",
        )
        notify_followers: bool = SchemaField(
            default=False,
            description=""Whether to notify followers that the user has published"",
            placeholder=""False"",
        )
        credentials: CredentialsMetaInput[Literal[""medium""], Literal[""api_key""]] = (
            CredentialsField(
                provider=""medium"",
                supported_credential_types={""api_key""},
                description=""The Medium integration can be used with any API key with sufficient permissions for the blocks it is used on."",
            )
        )","Point(row=38, column=4)","Point(row=85, column=9)",,autogpt_platform/backend/backend/blocks/medium.py
Output,class,,"class Output(BlockSchema):
        post_id: str = SchemaField(description=""The ID of the created Medium post"")
        post_url: str = SchemaField(description=""The URL of the created Medium post"")
        published_at: int = SchemaField(
            description=""The timestamp when the post was published""
        )
        error: str = SchemaField(
            description=""Error message if the post creation failed""
        )","Point(row=87, column=4)","Point(row=95, column=9)",,autogpt_platform/backend/backend/blocks/medium.py
PublishToMediumBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""3f7b2dcb-4a78-4e3f-b0f1-88132e1b89df"",
            input_schema=PublishToMediumBlock.Input,
            output_schema=PublishToMediumBlock.Output,
            description=""Publishes a post to Medium."",
            categories={BlockCategory.SOCIAL},
            test_input={
                ""author_id"": ""1234567890abcdef"",
                ""title"": ""Test Post"",
                ""content"": ""<h1>Test Content</h1><p>This is a test post.</p>"",
                ""content_format"": ""html"",
                ""tags"": [""test"", ""automation""],
                ""license"": ""all-rights-reserved"",
                ""notify_followers"": False,
                ""publish_status"": PublishToMediumStatus.DRAFT.value,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[
                (""post_id"", ""e6f36a""),
                (""post_url"", ""https://medium.com/@username/test-post-e6f36a""),
                (""published_at"", 1626282600),
            ],
            test_mock={
                ""create_post"": lambda *args, **kwargs: {
                    ""data"": {
                        ""id"": ""e6f36a"",
                        ""url"": ""https://medium.com/@username/test-post-e6f36a"",
                        ""authorId"": ""1234567890abcdef"",
                        ""publishedAt"": 1626282600,
                    }
                }
            },
            test_credentials=TEST_CREDENTIALS,
        )","Point(row=97, column=4)","Point(row=131, column=9)",PublishToMediumBlock,autogpt_platform/backend/backend/blocks/medium.py
PublishToMediumBlock.create_post,function,,"def create_post(
        self,
        api_key: SecretStr,
        author_id,
        title,
        content,
        content_format,
        tags,
        canonical_url,
        publish_status,
        license,
        notify_followers,
    ):
        headers = {
            ""Authorization"": f""Bearer {api_key.get_secret_value()}"",
            ""Content-Type"": ""application/json"",
            ""Accept"": ""application/json"",
        }

        data = {
            ""title"": title,
            ""content"": content,
            ""contentFormat"": content_format,
            ""tags"": tags,
            ""canonicalUrl"": canonical_url,
            ""publishStatus"": publish_status,
            ""license"": license,
            ""notifyFollowers"": notify_followers,
        }

        response = requests.post(
            f""https://api.medium.com/v1/users/{author_id}/posts"",
            headers=headers,
            json=data,
        )

        return response.json()","Point(row=133, column=4)","Point(row=169, column=30)",PublishToMediumBlock,autogpt_platform/backend/backend/blocks/medium.py
PublishToMediumBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        response = self.create_post(
            credentials.api_key,
            input_data.author_id.get_secret_value(),
            input_data.title,
            input_data.content,
            input_data.content_format,
            input_data.tags,
            input_data.canonical_url,
            input_data.publish_status,
            input_data.license,
            input_data.notify_followers,
        )

        if ""data"" in response:
            yield ""post_id"", response[""data""][""id""]
            yield ""post_url"", response[""data""][""url""]
            yield ""published_at"", response[""data""][""publishedAt""]
        else:
            error_message = response.get(""errors"", [{}])[0].get(
                ""message"", ""Unknown error occurred""
            )
            raise RuntimeError(f""Failed to create Medium post: {error_message}"")","Point(row=171, column=4)","Point(row=195, column=80)",PublishToMediumBlock,autogpt_platform/backend/backend/blocks/medium.py
all_subclasses,function,,"def all_subclasses(cls: Type[T]) -> list[Type[T]]:
    subclasses = cls.__subclasses__()
    for subclass in subclasses:
        subclasses += all_subclasses(subclass)
    return subclasses","Point(row=33, column=0)","Point(row=37, column=21)",,autogpt_platform/backend/backend/blocks/__init__.py
AICredentialsField,function,,"def AICredentialsField() -> AICredentials:
    return CredentialsField(
        description=""API key for the LLM provider."",
        provider=""llm"",
        supported_credential_types={""api_key""},
    )","Point(row=49, column=0)","Point(row=54, column=5)",,autogpt_platform/backend/backend/blocks/llm.py
ModelMetadata,class,,"class ModelMetadata(NamedTuple):
    provider: str
    context_window: int
    cost_factor: int","Point(row=57, column=0)","Point(row=60, column=20)",,autogpt_platform/backend/backend/blocks/llm.py
LlmModelMeta,class,,"class LlmModelMeta(EnumMeta):
    @property
    def __members__(
        self: type[""_EnumMemberT""],
    ) -> MappingProxyType[str, ""_EnumMemberT""]:
        if Settings().config.behave_as == BehaveAs.LOCAL:
            members = super().__members__
            return members
        else:
            removed_providers = [""ollama""]
            existing_members = super().__members__
            members = {
                name: member
                for name, member in existing_members.items()
                if LlmModel[name].provider not in removed_providers
            }
            return MappingProxyType(members)","Point(row=63, column=0)","Point(row=79, column=44)",,autogpt_platform/backend/backend/blocks/llm.py
LlmModelMeta.__members__,function,,"def __members__(
        self: type[""_EnumMemberT""],
    ) -> MappingProxyType[str, ""_EnumMemberT""]:
        if Settings().config.behave_as == BehaveAs.LOCAL:
            members = super().__members__
            return members
        else:
            removed_providers = [""ollama""]
            existing_members = super().__members__
            members = {
                name: member
                for name, member in existing_members.items()
                if LlmModel[name].provider not in removed_providers
            }
            return MappingProxyType(members)","Point(row=65, column=4)","Point(row=79, column=44)",LlmModelMeta,autogpt_platform/backend/backend/blocks/llm.py
LlmModel,class,,"class LlmModel(str, Enum, metaclass=LlmModelMeta):
    # OpenAI models
    O1_PREVIEW = ""o1-preview""
    O1_MINI = ""o1-mini""
    GPT4O_MINI = ""gpt-4o-mini""
    GPT4O = ""gpt-4o""
    GPT4_TURBO = ""gpt-4-turbo""
    GPT3_5_TURBO = ""gpt-3.5-turbo""
    # Anthropic models
    CLAUDE_3_5_SONNET = ""claude-3-5-sonnet-latest""
    CLAUDE_3_HAIKU = ""claude-3-haiku-20240307""
    # Groq models
    LLAMA3_8B = ""llama3-8b-8192""
    LLAMA3_70B = ""llama3-70b-8192""
    MIXTRAL_8X7B = ""mixtral-8x7b-32768""
    GEMMA_7B = ""gemma-7b-it""
    GEMMA2_9B = ""gemma2-9b-it""
    # New Groq models (Preview)
    LLAMA3_1_405B = ""llama-3.1-405b-reasoning""
    LLAMA3_1_70B = ""llama-3.1-70b-versatile""
    LLAMA3_1_8B = ""llama-3.1-8b-instant""
    # Ollama models
    OLLAMA_LLAMA3_8B = ""llama3""
    OLLAMA_LLAMA3_405B = ""llama3.1:405b""

    @property
    def metadata(self) -> ModelMetadata:
        return MODEL_METADATA[self]

    @property
    def provider(self) -> str:
        return self.metadata.provider

    @property
    def context_window(self) -> int:
        return self.metadata.context_window

    @property
    def cost_factor(self) -> int:
        return self.metadata.cost_factor","Point(row=82, column=0)","Point(row=121, column=40)",,autogpt_platform/backend/backend/blocks/llm.py
LlmModel.metadata,function,,"def metadata(self) -> ModelMetadata:
        return MODEL_METADATA[self]","Point(row=108, column=4)","Point(row=109, column=35)",LlmModel,autogpt_platform/backend/backend/blocks/llm.py
LlmModel.provider,function,,"def provider(self) -> str:
        return self.metadata.provider","Point(row=112, column=4)","Point(row=113, column=37)",LlmModel,autogpt_platform/backend/backend/blocks/llm.py
LlmModel.context_window,function,,"def context_window(self) -> int:
        return self.metadata.context_window","Point(row=116, column=4)","Point(row=117, column=43)",LlmModel,autogpt_platform/backend/backend/blocks/llm.py
LlmModel.cost_factor,function,,"def cost_factor(self) -> int:
        return self.metadata.cost_factor","Point(row=120, column=4)","Point(row=121, column=40)",LlmModel,autogpt_platform/backend/backend/blocks/llm.py
MessageRole,class,,"class MessageRole(str, Enum):
    SYSTEM = ""system""
    USER = ""user""
    ASSISTANT = ""assistant""","Point(row=151, column=0)","Point(row=154, column=27)",,autogpt_platform/backend/backend/blocks/llm.py
Message,class,,"class Message(BlockSchema):
    role: MessageRole
    content: str","Point(row=157, column=0)","Point(row=159, column=16)",,autogpt_platform/backend/backend/blocks/llm.py
AIStructuredResponseGeneratorBlock,class,,"class AIStructuredResponseGeneratorBlock(Block):
    class Input(BlockSchema):
        prompt: str = SchemaField(
            description=""The prompt to send to the language model."",
            placeholder=""Enter your prompt here..."",
        )
        expected_format: dict[str, str] = SchemaField(
            description=""Expected format of the response. If provided, the response will be validated against this format. ""
            ""The keys should be the expected fields in the response, and the values should be the description of the field."",
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for answering the prompt."",
            advanced=False,
        )
        credentials: AICredentials = AICredentialsField()
        sys_prompt: str = SchemaField(
            title=""System Prompt"",
            default="""",
            description=""The system prompt to provide additional context to the model."",
        )
        conversation_history: list[Message] = SchemaField(
            default=[],
            description=""The conversation history to provide context for the prompt."",
        )
        retry: int = SchemaField(
            title=""Retry Count"",
            default=3,
            description=""Number of times to retry the LLM call if the response does not match the expected format."",
        )
        prompt_values: dict[str, str] = SchemaField(
            advanced=False, default={}, description=""Values used to fill in the prompt.""
        )
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )

    class Output(BlockSchema):
        response: dict[str, Any] = SchemaField(
            description=""The response object generated by the language model.""
        )
        error: str = SchemaField(description=""Error message if the API call failed."")

    def __init__(self):
        super().__init__(
            id=""ed55ac19-356e-4243-a6cb-bc599e9b716f"",
            description=""Call a Large Language Model (LLM) to generate formatted object based on the given prompt."",
            categories={BlockCategory.AI},
            input_schema=AIStructuredResponseGeneratorBlock.Input,
            output_schema=AIStructuredResponseGeneratorBlock.Output,
            test_input={
                ""model"": LlmModel.GPT4_TURBO,
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""expected_format"": {
                    ""key1"": ""value1"",
                    ""key2"": ""value2"",
                },
                ""prompt"": ""User prompt"",
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(""response"", {""key1"": ""key1Value"", ""key2"": ""key2Value""}),
            test_mock={
                ""llm_call"": lambda *args, **kwargs: (
                    json.dumps(
                        {
                            ""key1"": ""key1Value"",
                            ""key2"": ""key2Value"",
                        }
                    ),
                    0,
                    0,
                )
            },
        )

    @staticmethod
    def llm_call(
        credentials: APIKeyCredentials,
        llm_model: LlmModel,
        prompt: list[dict],
        json_format: bool,
        max_tokens: int | None = None,
    ) -> tuple[str, int, int]:
        """"""
        Args:
            api_key: API key for the LLM provider.
            llm_model: The LLM model to use.
            prompt: The prompt to send to the LLM.
            json_format: Whether the response should be in JSON format.
            max_tokens: The maximum number of tokens to generate in the chat completion.

        Returns:
            The response from the LLM.
            The number of tokens used in the prompt.
            The number of tokens used in the completion.
        """"""
        provider = llm_model.metadata.provider

        if provider == ""openai"":
            oai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())
            response_format = None

            if llm_model in [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]:
                sys_messages = [p[""content""] for p in prompt if p[""role""] == ""system""]
                usr_messages = [p[""content""] for p in prompt if p[""role""] != ""system""]
                prompt = [
                    {""role"": ""user"", ""content"": ""\n"".join(sys_messages)},
                    {""role"": ""user"", ""content"": ""\n"".join(usr_messages)},
                ]
            elif json_format:
                response_format = {""type"": ""json_object""}

            response = oai_client.chat.completions.create(
                model=llm_model.value,
                messages=prompt,  # type: ignore
                response_format=response_format,  # type: ignore
                max_completion_tokens=max_tokens,
            )

            return (
                response.choices[0].message.content or """",
                response.usage.prompt_tokens if response.usage else 0,
                response.usage.completion_tokens if response.usage else 0,
            )
        elif provider == ""anthropic"":
            system_messages = [p[""content""] for p in prompt if p[""role""] == ""system""]
            sysprompt = "" "".join(system_messages)

            messages = []
            last_role = None
            for p in prompt:
                if p[""role""] in [""user"", ""assistant""]:
                    if p[""role""] != last_role:
                        messages.append({""role"": p[""role""], ""content"": p[""content""]})
                        last_role = p[""role""]
                    else:
                        # If the role is the same as the last one, combine the content
                        messages[-1][""content""] += ""\n"" + p[""content""]

            client = anthropic.Anthropic(api_key=credentials.api_key.get_secret_value())
            try:
                resp = client.messages.create(
                    model=llm_model.value,
                    system=sysprompt,
                    messages=messages,
                    max_tokens=max_tokens or 8192,
                )

                if not resp.content:
                    raise ValueError(""No content returned from Anthropic."")

                return (
                    (
                        resp.content[0].name
                        if isinstance(resp.content[0], anthropic.types.ToolUseBlock)
                        else resp.content[0].text
                    ),
                    resp.usage.input_tokens,
                    resp.usage.output_tokens,
                )
            except anthropic.APIError as e:
                error_message = f""Anthropic API error: {str(e)}""
                logger.error(error_message)
                raise ValueError(error_message)
        elif provider == ""groq"":
            client = Groq(api_key=credentials.api_key.get_secret_value())
            response_format = {""type"": ""json_object""} if json_format else None
            response = client.chat.completions.create(
                model=llm_model.value,
                messages=prompt,  # type: ignore
                response_format=response_format,  # type: ignore
                max_tokens=max_tokens,
            )
            return (
                response.choices[0].message.content or """",
                response.usage.prompt_tokens if response.usage else 0,
                response.usage.completion_tokens if response.usage else 0,
            )
        elif provider == ""ollama"":
            sys_messages = [p[""content""] for p in prompt if p[""role""] == ""system""]
            usr_messages = [p[""content""] for p in prompt if p[""role""] != ""system""]
            response = ollama.generate(
                model=llm_model.value,
                prompt=f""{sys_messages}\n\n{usr_messages}"",
                stream=False,
            )
            return (
                response.get(""response"") or """",
                response.get(""prompt_eval_count"") or 0,
                response.get(""eval_count"") or 0,
            )
        else:
            raise ValueError(f""Unsupported LLM provider: {provider}"")

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        logger.debug(f""Calling LLM with input data: {input_data}"")
        prompt = [p.model_dump() for p in input_data.conversation_history]

        def trim_prompt(s: str) -> str:
            lines = s.strip().split(""\n"")
            return ""\n"".join([line.strip().lstrip(""|"") for line in lines])

        values = input_data.prompt_values
        if values:
            input_data.prompt = input_data.prompt.format(**values)
            input_data.sys_prompt = input_data.sys_prompt.format(**values)

        if input_data.sys_prompt:
            prompt.append({""role"": ""system"", ""content"": input_data.sys_prompt})

        if input_data.expected_format:
            expected_format = [
                f'""{k}"": ""{v}""' for k, v in input_data.expected_format.items()
            ]
            format_prompt = "",\n  "".join(expected_format)
            sys_prompt = trim_prompt(
                f""""""
                  |Reply strictly only in the following JSON format:
                  |{{
                  |  {format_prompt}
                  |}}
                """"""
            )
            prompt.append({""role"": ""system"", ""content"": sys_prompt})

        if input_data.prompt:
            prompt.append({""role"": ""user"", ""content"": input_data.prompt})

        def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:
            try:
                parsed = json.loads(resp)
                if not isinstance(parsed, dict):
                    return {}, f""Expected a dictionary, but got {type(parsed)}""
                miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())
                if miss_keys:
                    return parsed, f""Missing keys: {miss_keys}""
                return parsed, None
            except JSONDecodeError as e:
                return {}, f""JSON decode error: {e}""

        logger.info(f""LLM request: {prompt}"")
        retry_prompt = """"
        llm_model = input_data.model

        for retry_count in range(input_data.retry):
            try:
                response_text, input_token, output_token = self.llm_call(
                    credentials=credentials,
                    llm_model=llm_model,
                    prompt=prompt,
                    json_format=bool(input_data.expected_format),
                    max_tokens=input_data.max_tokens,
                )
                self.merge_stats(
                    {
                        ""input_token_count"": input_token,
                        ""output_token_count"": output_token,
                    }
                )
                logger.info(f""LLM attempt-{retry_count} response: {response_text}"")

                if input_data.expected_format:
                    parsed_dict, parsed_error = parse_response(response_text)
                    if not parsed_error:
                        yield ""response"", {
                            k: (
                                json.loads(v)
                                if isinstance(v, str)
                                and v.startswith(""["")
                                and v.endswith(""]"")
                                else ("", "".join(v) if isinstance(v, list) else v)
                            )
                            for k, v in parsed_dict.items()
                        }
                        return
                else:
                    yield ""response"", {""response"": response_text}
                    return

                retry_prompt = trim_prompt(
                    f""""""
                  |This is your previous error response:
                  |--
                  |{response_text}
                  |--
                  |
                  |And this is the error:
                  |--
                  |{parsed_error}
                  |--
                """"""
                )
                prompt.append({""role"": ""user"", ""content"": retry_prompt})
            except Exception as e:
                logger.exception(f""Error calling LLM: {e}"")
                retry_prompt = f""Error calling LLM: {e}""
            finally:
                self.merge_stats(
                    {
                        ""llm_call_count"": retry_count + 1,
                        ""llm_retry_count"": retry_count,
                    }
                )

        raise RuntimeError(retry_prompt)","Point(row=162, column=0)","Point(row=471, column=40)",,autogpt_platform/backend/backend/blocks/llm.py
Input,class,,"class Input(BlockSchema):
        prompt: str = SchemaField(
            description=""The prompt to send to the language model."",
            placeholder=""Enter your prompt here..."",
        )
        expected_format: dict[str, str] = SchemaField(
            description=""Expected format of the response. If provided, the response will be validated against this format. ""
            ""The keys should be the expected fields in the response, and the values should be the description of the field."",
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for answering the prompt."",
            advanced=False,
        )
        credentials: AICredentials = AICredentialsField()
        sys_prompt: str = SchemaField(
            title=""System Prompt"",
            default="""",
            description=""The system prompt to provide additional context to the model."",
        )
        conversation_history: list[Message] = SchemaField(
            default=[],
            description=""The conversation history to provide context for the prompt."",
        )
        retry: int = SchemaField(
            title=""Retry Count"",
            default=3,
            description=""Number of times to retry the LLM call if the response does not match the expected format."",
        )
        prompt_values: dict[str, str] = SchemaField(
            advanced=False, default={}, description=""Values used to fill in the prompt.""
        )
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )","Point(row=163, column=4)","Point(row=200, column=9)",,autogpt_platform/backend/backend/blocks/llm.py
Output,class,,"class Output(BlockSchema):
        response: dict[str, Any] = SchemaField(
            description=""The response object generated by the language model.""
        )
        error: str = SchemaField(description=""Error message if the API call failed."")","Point(row=202, column=4)","Point(row=206, column=85)",,autogpt_platform/backend/backend/blocks/llm.py
AIStructuredResponseGeneratorBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""ed55ac19-356e-4243-a6cb-bc599e9b716f"",
            description=""Call a Large Language Model (LLM) to generate formatted object based on the given prompt."",
            categories={BlockCategory.AI},
            input_schema=AIStructuredResponseGeneratorBlock.Input,
            output_schema=AIStructuredResponseGeneratorBlock.Output,
            test_input={
                ""model"": LlmModel.GPT4_TURBO,
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""expected_format"": {
                    ""key1"": ""value1"",
                    ""key2"": ""value2"",
                },
                ""prompt"": ""User prompt"",
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(""response"", {""key1"": ""key1Value"", ""key2"": ""key2Value""}),
            test_mock={
                ""llm_call"": lambda *args, **kwargs: (
                    json.dumps(
                        {
                            ""key1"": ""key1Value"",
                            ""key2"": ""key2Value"",
                        }
                    ),
                    0,
                    0,
                )
            },
        )","Point(row=208, column=4)","Point(row=238, column=9)",AIStructuredResponseGeneratorBlock,autogpt_platform/backend/backend/blocks/llm.py
AIStructuredResponseGeneratorBlock.llm_call,function,"
        Args:
            api_key: API key for the LLM provider.
            llm_model: The LLM model to use.
            prompt: The prompt to send to the LLM.
            json_format: Whether the response should be in JSON format.
            max_tokens: The maximum number of tokens to generate in the chat completion.

        Returns:
            The response from the LLM.
            The number of tokens used in the prompt.
            The number of tokens used in the completion.
","def llm_call(
        credentials: APIKeyCredentials,
        llm_model: LlmModel,
        prompt: list[dict],
        json_format: bool,
        max_tokens: int | None = None,
    ) -> tuple[str, int, int]:
        """"""
        Args:
            api_key: API key for the LLM provider.
            llm_model: The LLM model to use.
            prompt: The prompt to send to the LLM.
            json_format: Whether the response should be in JSON format.
            max_tokens: The maximum number of tokens to generate in the chat completion.

        Returns:
            The response from the LLM.
            The number of tokens used in the prompt.
            The number of tokens used in the completion.
        """"""
        provider = llm_model.metadata.provider

        if provider == ""openai"":
            oai_client = openai.OpenAI(api_key=credentials.api_key.get_secret_value())
            response_format = None

            if llm_model in [LlmModel.O1_MINI, LlmModel.O1_PREVIEW]:
                sys_messages = [p[""content""] for p in prompt if p[""role""] == ""system""]
                usr_messages = [p[""content""] for p in prompt if p[""role""] != ""system""]
                prompt = [
                    {""role"": ""user"", ""content"": ""\n"".join(sys_messages)},
                    {""role"": ""user"", ""content"": ""\n"".join(usr_messages)},
                ]
            elif json_format:
                response_format = {""type"": ""json_object""}

            response = oai_client.chat.completions.create(
                model=llm_model.value,
                messages=prompt,  # type: ignore
                response_format=response_format,  # type: ignore
                max_completion_tokens=max_tokens,
            )

            return (
                response.choices[0].message.content or """",
                response.usage.prompt_tokens if response.usage else 0,
                response.usage.completion_tokens if response.usage else 0,
            )
        elif provider == ""anthropic"":
            system_messages = [p[""content""] for p in prompt if p[""role""] == ""system""]
            sysprompt = "" "".join(system_messages)

            messages = []
            last_role = None
            for p in prompt:
                if p[""role""] in [""user"", ""assistant""]:
                    if p[""role""] != last_role:
                        messages.append({""role"": p[""role""], ""content"": p[""content""]})
                        last_role = p[""role""]
                    else:
                        # If the role is the same as the last one, combine the content
                        messages[-1][""content""] += ""\n"" + p[""content""]

            client = anthropic.Anthropic(api_key=credentials.api_key.get_secret_value())
            try:
                resp = client.messages.create(
                    model=llm_model.value,
                    system=sysprompt,
                    messages=messages,
                    max_tokens=max_tokens or 8192,
                )

                if not resp.content:
                    raise ValueError(""No content returned from Anthropic."")

                return (
                    (
                        resp.content[0].name
                        if isinstance(resp.content[0], anthropic.types.ToolUseBlock)
                        else resp.content[0].text
                    ),
                    resp.usage.input_tokens,
                    resp.usage.output_tokens,
                )
            except anthropic.APIError as e:
                error_message = f""Anthropic API error: {str(e)}""
                logger.error(error_message)
                raise ValueError(error_message)
        elif provider == ""groq"":
            client = Groq(api_key=credentials.api_key.get_secret_value())
            response_format = {""type"": ""json_object""} if json_format else None
            response = client.chat.completions.create(
                model=llm_model.value,
                messages=prompt,  # type: ignore
                response_format=response_format,  # type: ignore
                max_tokens=max_tokens,
            )
            return (
                response.choices[0].message.content or """",
                response.usage.prompt_tokens if response.usage else 0,
                response.usage.completion_tokens if response.usage else 0,
            )
        elif provider == ""ollama"":
            sys_messages = [p[""content""] for p in prompt if p[""role""] == ""system""]
            usr_messages = [p[""content""] for p in prompt if p[""role""] != ""system""]
            response = ollama.generate(
                model=llm_model.value,
                prompt=f""{sys_messages}\n\n{usr_messages}"",
                stream=False,
            )
            return (
                response.get(""response"") or """",
                response.get(""prompt_eval_count"") or 0,
                response.get(""eval_count"") or 0,
            )
        else:
            raise ValueError(f""Unsupported LLM provider: {provider}"")","Point(row=241, column=4)","Point(row=357, column=69)",AIStructuredResponseGeneratorBlock,autogpt_platform/backend/backend/blocks/llm.py
AIStructuredResponseGeneratorBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        logger.debug(f""Calling LLM with input data: {input_data}"")
        prompt = [p.model_dump() for p in input_data.conversation_history]

        def trim_prompt(s: str) -> str:
            lines = s.strip().split(""\n"")
            return ""\n"".join([line.strip().lstrip(""|"") for line in lines])

        values = input_data.prompt_values
        if values:
            input_data.prompt = input_data.prompt.format(**values)
            input_data.sys_prompt = input_data.sys_prompt.format(**values)

        if input_data.sys_prompt:
            prompt.append({""role"": ""system"", ""content"": input_data.sys_prompt})

        if input_data.expected_format:
            expected_format = [
                f'""{k}"": ""{v}""' for k, v in input_data.expected_format.items()
            ]
            format_prompt = "",\n  "".join(expected_format)
            sys_prompt = trim_prompt(
                f""""""
                  |Reply strictly only in the following JSON format:
                  |{{
                  |  {format_prompt}
                  |}}
                """"""
            )
            prompt.append({""role"": ""system"", ""content"": sys_prompt})

        if input_data.prompt:
            prompt.append({""role"": ""user"", ""content"": input_data.prompt})

        def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:
            try:
                parsed = json.loads(resp)
                if not isinstance(parsed, dict):
                    return {}, f""Expected a dictionary, but got {type(parsed)}""
                miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())
                if miss_keys:
                    return parsed, f""Missing keys: {miss_keys}""
                return parsed, None
            except JSONDecodeError as e:
                return {}, f""JSON decode error: {e}""

        logger.info(f""LLM request: {prompt}"")
        retry_prompt = """"
        llm_model = input_data.model

        for retry_count in range(input_data.retry):
            try:
                response_text, input_token, output_token = self.llm_call(
                    credentials=credentials,
                    llm_model=llm_model,
                    prompt=prompt,
                    json_format=bool(input_data.expected_format),
                    max_tokens=input_data.max_tokens,
                )
                self.merge_stats(
                    {
                        ""input_token_count"": input_token,
                        ""output_token_count"": output_token,
                    }
                )
                logger.info(f""LLM attempt-{retry_count} response: {response_text}"")

                if input_data.expected_format:
                    parsed_dict, parsed_error = parse_response(response_text)
                    if not parsed_error:
                        yield ""response"", {
                            k: (
                                json.loads(v)
                                if isinstance(v, str)
                                and v.startswith(""["")
                                and v.endswith(""]"")
                                else ("", "".join(v) if isinstance(v, list) else v)
                            )
                            for k, v in parsed_dict.items()
                        }
                        return
                else:
                    yield ""response"", {""response"": response_text}
                    return

                retry_prompt = trim_prompt(
                    f""""""
                  |This is your previous error response:
                  |--
                  |{response_text}
                  |--
                  |
                  |And this is the error:
                  |--
                  |{parsed_error}
                  |--
                """"""
                )
                prompt.append({""role"": ""user"", ""content"": retry_prompt})
            except Exception as e:
                logger.exception(f""Error calling LLM: {e}"")
                retry_prompt = f""Error calling LLM: {e}""
            finally:
                self.merge_stats(
                    {
                        ""llm_call_count"": retry_count + 1,
                        ""llm_retry_count"": retry_count,
                    }
                )

        raise RuntimeError(retry_prompt)","Point(row=359, column=4)","Point(row=471, column=40)",AIStructuredResponseGeneratorBlock,autogpt_platform/backend/backend/blocks/llm.py
AIStructuredResponseGeneratorBlock.run.trim_prompt,function,,"def trim_prompt(s: str) -> str:
            lines = s.strip().split(""\n"")
            return ""\n"".join([line.strip().lstrip(""|"") for line in lines])","Point(row=365, column=8)","Point(row=367, column=74)",AIStructuredResponseGeneratorBlock,autogpt_platform/backend/backend/blocks/llm.py
AIStructuredResponseGeneratorBlock.run.parse_response,function,,"def parse_response(resp: str) -> tuple[dict[str, Any], str | None]:
            try:
                parsed = json.loads(resp)
                if not isinstance(parsed, dict):
                    return {}, f""Expected a dictionary, but got {type(parsed)}""
                miss_keys = set(input_data.expected_format.keys()) - set(parsed.keys())
                if miss_keys:
                    return parsed, f""Missing keys: {miss_keys}""
                return parsed, None
            except JSONDecodeError as e:
                return {}, f""JSON decode error: {e}""","Point(row=395, column=8)","Point(row=405, column=52)",AIStructuredResponseGeneratorBlock,autogpt_platform/backend/backend/blocks/llm.py
AITextGeneratorBlock,class,,"class AITextGeneratorBlock(Block):
    class Input(BlockSchema):
        prompt: str = SchemaField(
            description=""The prompt to send to the language model."",
            placeholder=""Enter your prompt here..."",
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for answering the prompt."",
            advanced=False,
        )
        credentials: AICredentials = AICredentialsField()
        sys_prompt: str = SchemaField(
            title=""System Prompt"",
            default="""",
            description=""The system prompt to provide additional context to the model."",
        )
        retry: int = SchemaField(
            title=""Retry Count"",
            default=3,
            description=""Number of times to retry the LLM call if the response does not match the expected format."",
        )
        prompt_values: dict[str, str] = SchemaField(
            advanced=False, default={}, description=""Values used to fill in the prompt.""
        )
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )

    class Output(BlockSchema):
        response: str = SchemaField(
            description=""The response generated by the language model.""
        )
        error: str = SchemaField(description=""Error message if the API call failed."")

    def __init__(self):
        super().__init__(
            id=""1f292d4a-41a4-4977-9684-7c8d560b9f91"",
            description=""Call a Large Language Model (LLM) to generate a string based on the given prompt."",
            categories={BlockCategory.AI},
            input_schema=AITextGeneratorBlock.Input,
            output_schema=AITextGeneratorBlock.Output,
            test_input={
                ""prompt"": ""User prompt"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(""response"", ""Response text""),
            test_mock={""llm_call"": lambda *args, **kwargs: ""Response text""},
        )

    def llm_call(
        self,
        input_data: AIStructuredResponseGeneratorBlock.Input,
        credentials: APIKeyCredentials,
    ) -> str:
        block = AIStructuredResponseGeneratorBlock()
        response = block.run_once(input_data, ""response"", credentials=credentials)
        self.merge_stats(block.execution_stats)
        return response[""response""]

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        object_input_data = AIStructuredResponseGeneratorBlock.Input(
            **{attr: getattr(input_data, attr) for attr in input_data.model_fields},
            expected_format={},
        )
        yield ""response"", self.llm_call(object_input_data, credentials)","Point(row=474, column=0)","Point(row=545, column=71)",,autogpt_platform/backend/backend/blocks/llm.py
Input,class,,"class Input(BlockSchema):
        prompt: str = SchemaField(
            description=""The prompt to send to the language model."",
            placeholder=""Enter your prompt here..."",
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for answering the prompt."",
            advanced=False,
        )
        credentials: AICredentials = AICredentialsField()
        sys_prompt: str = SchemaField(
            title=""System Prompt"",
            default="""",
            description=""The system prompt to provide additional context to the model."",
        )
        retry: int = SchemaField(
            title=""Retry Count"",
            default=3,
            description=""Number of times to retry the LLM call if the response does not match the expected format."",
        )
        prompt_values: dict[str, str] = SchemaField(
            advanced=False, default={}, description=""Values used to fill in the prompt.""
        )
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )","Point(row=475, column=4)","Point(row=504, column=9)",,autogpt_platform/backend/backend/blocks/llm.py
Output,class,,"class Output(BlockSchema):
        response: str = SchemaField(
            description=""The response generated by the language model.""
        )
        error: str = SchemaField(description=""Error message if the API call failed."")","Point(row=506, column=4)","Point(row=510, column=85)",,autogpt_platform/backend/backend/blocks/llm.py
AITextGeneratorBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""1f292d4a-41a4-4977-9684-7c8d560b9f91"",
            description=""Call a Large Language Model (LLM) to generate a string based on the given prompt."",
            categories={BlockCategory.AI},
            input_schema=AITextGeneratorBlock.Input,
            output_schema=AITextGeneratorBlock.Output,
            test_input={
                ""prompt"": ""User prompt"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(""response"", ""Response text""),
            test_mock={""llm_call"": lambda *args, **kwargs: ""Response text""},
        )","Point(row=512, column=4)","Point(row=526, column=9)",AITextGeneratorBlock,autogpt_platform/backend/backend/blocks/llm.py
AITextGeneratorBlock.llm_call,function,,"def llm_call(
        self,
        input_data: AIStructuredResponseGeneratorBlock.Input,
        credentials: APIKeyCredentials,
    ) -> str:
        block = AIStructuredResponseGeneratorBlock()
        response = block.run_once(input_data, ""response"", credentials=credentials)
        self.merge_stats(block.execution_stats)
        return response[""response""]","Point(row=528, column=4)","Point(row=536, column=35)",AITextGeneratorBlock,autogpt_platform/backend/backend/blocks/llm.py
AITextGeneratorBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        object_input_data = AIStructuredResponseGeneratorBlock.Input(
            **{attr: getattr(input_data, attr) for attr in input_data.model_fields},
            expected_format={},
        )
        yield ""response"", self.llm_call(object_input_data, credentials)","Point(row=538, column=4)","Point(row=545, column=71)",AITextGeneratorBlock,autogpt_platform/backend/backend/blocks/llm.py
SummaryStyle,class,,"class SummaryStyle(Enum):
    CONCISE = ""concise""
    DETAILED = ""detailed""
    BULLET_POINTS = ""bullet points""
    NUMBERED_LIST = ""numbered list""","Point(row=548, column=0)","Point(row=552, column=35)",,autogpt_platform/backend/backend/blocks/llm.py
AITextSummarizerBlock,class,,"class AITextSummarizerBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(
            description=""The text to summarize."",
            placeholder=""Enter the text to summarize here..."",
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for summarizing the text."",
        )
        focus: str = SchemaField(
            title=""Focus"",
            default=""general information"",
            description=""The topic to focus on in the summary"",
        )
        style: SummaryStyle = SchemaField(
            title=""Summary Style"",
            default=SummaryStyle.CONCISE,
            description=""The style of the summary to generate."",
        )
        credentials: AICredentials = AICredentialsField()
        # TODO: Make this dynamic
        max_tokens: int = SchemaField(
            title=""Max Tokens"",
            default=4096,
            description=""The maximum number of tokens to generate in the chat completion."",
            ge=1,
        )
        chunk_overlap: int = SchemaField(
            title=""Chunk Overlap"",
            default=100,
            description=""The number of overlapping tokens between chunks to maintain context."",
            ge=0,
        )

    class Output(BlockSchema):
        summary: str = SchemaField(description=""The final summary of the text."")
        error: str = SchemaField(description=""Error message if the API call failed."")

    def __init__(self):
        super().__init__(
            id=""a0a69be1-4528-491c-a85a-a4ab6873e3f0"",
            description=""Utilize a Large Language Model (LLM) to summarize a long text."",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input_schema=AITextSummarizerBlock.Input,
            output_schema=AITextSummarizerBlock.Output,
            test_input={
                ""text"": ""Lorem ipsum..."" * 100,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(""summary"", ""Final summary of a long text""),
            test_mock={
                ""llm_call"": lambda input_data, credentials: (
                    {""final_summary"": ""Final summary of a long text""}
                    if ""final_summary"" in input_data.expected_format
                    else {""summary"": ""Summary of a chunk of text""}
                )
            },
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        for output in self._run(input_data, credentials):
            yield output

    def _run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:
        chunks = self._split_text(
            input_data.text, input_data.max_tokens, input_data.chunk_overlap
        )
        summaries = []

        for chunk in chunks:
            chunk_summary = self._summarize_chunk(chunk, input_data, credentials)
            summaries.append(chunk_summary)

        final_summary = self._combine_summaries(summaries, input_data, credentials)
        yield ""summary"", final_summary

    @staticmethod
    def _split_text(text: str, max_tokens: int, overlap: int) -> list[str]:
        words = text.split()
        chunks = []
        chunk_size = max_tokens - overlap

        for i in range(0, len(words), chunk_size):
            chunk = "" "".join(words[i : i + max_tokens])
            chunks.append(chunk)

        return chunks

    def llm_call(
        self,
        input_data: AIStructuredResponseGeneratorBlock.Input,
        credentials: APIKeyCredentials,
    ) -> dict:
        block = AIStructuredResponseGeneratorBlock()
        response = block.run_once(input_data, ""response"", credentials=credentials)
        self.merge_stats(block.execution_stats)
        return response

    def _summarize_chunk(
        self, chunk: str, input_data: Input, credentials: APIKeyCredentials
    ) -> str:
        prompt = f""Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\n\n```{chunk}```""

        llm_response = self.llm_call(
            AIStructuredResponseGeneratorBlock.Input(
                prompt=prompt,
                credentials=input_data.credentials,
                model=input_data.model,
                expected_format={""summary"": ""The summary of the given text.""},
            ),
            credentials=credentials,
        )

        return llm_response[""summary""]

    def _combine_summaries(
        self, summaries: list[str], input_data: Input, credentials: APIKeyCredentials
    ) -> str:
        combined_text = ""\n\n"".join(summaries)

        if len(combined_text.split()) <= input_data.max_tokens:
            prompt = f""Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\n\n ```{combined_text}```\n\n Just respond with the final_summary in the format specified.""

            llm_response = self.llm_call(
                AIStructuredResponseGeneratorBlock.Input(
                    prompt=prompt,
                    credentials=input_data.credentials,
                    model=input_data.model,
                    expected_format={
                        ""final_summary"": ""The final summary of all provided summaries.""
                    },
                ),
                credentials=credentials,
            )

            return llm_response[""final_summary""]
        else:
            # If combined summaries are still too long, recursively summarize
            return self._run(
                AITextSummarizerBlock.Input(
                    text=combined_text,
                    credentials=input_data.credentials,
                    model=input_data.model,
                    max_tokens=input_data.max_tokens,
                    chunk_overlap=input_data.chunk_overlap,
                ),
                credentials=credentials,
            ).send(None)[
                1
            ]  # Get the first yielded value","Point(row=555, column=0)","Point(row=709, column=44)",,autogpt_platform/backend/backend/blocks/llm.py
Input,class,,"class Input(BlockSchema):
        text: str = SchemaField(
            description=""The text to summarize."",
            placeholder=""Enter the text to summarize here..."",
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for summarizing the text."",
        )
        focus: str = SchemaField(
            title=""Focus"",
            default=""general information"",
            description=""The topic to focus on in the summary"",
        )
        style: SummaryStyle = SchemaField(
            title=""Summary Style"",
            default=SummaryStyle.CONCISE,
            description=""The style of the summary to generate."",
        )
        credentials: AICredentials = AICredentialsField()
        # TODO: Make this dynamic
        max_tokens: int = SchemaField(
            title=""Max Tokens"",
            default=4096,
            description=""The maximum number of tokens to generate in the chat completion."",
            ge=1,
        )
        chunk_overlap: int = SchemaField(
            title=""Chunk Overlap"",
            default=100,
            description=""The number of overlapping tokens between chunks to maintain context."",
            ge=0,
        )","Point(row=556, column=4)","Point(row=589, column=9)",,autogpt_platform/backend/backend/blocks/llm.py
Output,class,,"class Output(BlockSchema):
        summary: str = SchemaField(description=""The final summary of the text."")
        error: str = SchemaField(description=""Error message if the API call failed."")","Point(row=591, column=4)","Point(row=593, column=85)",,autogpt_platform/backend/backend/blocks/llm.py
AITextSummarizerBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""a0a69be1-4528-491c-a85a-a4ab6873e3f0"",
            description=""Utilize a Large Language Model (LLM) to summarize a long text."",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input_schema=AITextSummarizerBlock.Input,
            output_schema=AITextSummarizerBlock.Output,
            test_input={
                ""text"": ""Lorem ipsum..."" * 100,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(""summary"", ""Final summary of a long text""),
            test_mock={
                ""llm_call"": lambda input_data, credentials: (
                    {""final_summary"": ""Final summary of a long text""}
                    if ""final_summary"" in input_data.expected_format
                    else {""summary"": ""Summary of a chunk of text""}
                )
            },
        )","Point(row=595, column=4)","Point(row=615, column=9)",AITextSummarizerBlock,autogpt_platform/backend/backend/blocks/llm.py
AITextSummarizerBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        for output in self._run(input_data, credentials):
            yield output","Point(row=617, column=4)","Point(row=621, column=24)",AITextSummarizerBlock,autogpt_platform/backend/backend/blocks/llm.py
AITextSummarizerBlock._run,function,,"def _run(self, input_data: Input, credentials: APIKeyCredentials) -> BlockOutput:
        chunks = self._split_text(
            input_data.text, input_data.max_tokens, input_data.chunk_overlap
        )
        summaries = []

        for chunk in chunks:
            chunk_summary = self._summarize_chunk(chunk, input_data, credentials)
            summaries.append(chunk_summary)

        final_summary = self._combine_summaries(summaries, input_data, credentials)
        yield ""summary"", final_summary","Point(row=623, column=4)","Point(row=634, column=38)",AITextSummarizerBlock,autogpt_platform/backend/backend/blocks/llm.py
AITextSummarizerBlock._split_text,function,,"def _split_text(text: str, max_tokens: int, overlap: int) -> list[str]:
        words = text.split()
        chunks = []
        chunk_size = max_tokens - overlap

        for i in range(0, len(words), chunk_size):
            chunk = "" "".join(words[i : i + max_tokens])
            chunks.append(chunk)

        return chunks","Point(row=637, column=4)","Point(row=646, column=21)",AITextSummarizerBlock,autogpt_platform/backend/backend/blocks/llm.py
AITextSummarizerBlock.llm_call,function,,"def llm_call(
        self,
        input_data: AIStructuredResponseGeneratorBlock.Input,
        credentials: APIKeyCredentials,
    ) -> dict:
        block = AIStructuredResponseGeneratorBlock()
        response = block.run_once(input_data, ""response"", credentials=credentials)
        self.merge_stats(block.execution_stats)
        return response","Point(row=648, column=4)","Point(row=656, column=23)",AITextSummarizerBlock,autogpt_platform/backend/backend/blocks/llm.py
AITextSummarizerBlock._summarize_chunk,function,,"def _summarize_chunk(
        self, chunk: str, input_data: Input, credentials: APIKeyCredentials
    ) -> str:
        prompt = f""Summarize the following text in a {input_data.style} form. Focus your summary on the topic of `{input_data.focus}` if present, otherwise just provide a general summary:\n\n```{chunk}```""

        llm_response = self.llm_call(
            AIStructuredResponseGeneratorBlock.Input(
                prompt=prompt,
                credentials=input_data.credentials,
                model=input_data.model,
                expected_format={""summary"": ""The summary of the given text.""},
            ),
            credentials=credentials,
        )

        return llm_response[""summary""]","Point(row=658, column=4)","Point(row=673, column=38)",AITextSummarizerBlock,autogpt_platform/backend/backend/blocks/llm.py
AITextSummarizerBlock._combine_summaries,function,,"def _combine_summaries(
        self, summaries: list[str], input_data: Input, credentials: APIKeyCredentials
    ) -> str:
        combined_text = ""\n\n"".join(summaries)

        if len(combined_text.split()) <= input_data.max_tokens:
            prompt = f""Provide a final summary of the following section summaries in a {input_data.style} form, focus your summary on the topic of `{input_data.focus}` if present:\n\n ```{combined_text}```\n\n Just respond with the final_summary in the format specified.""

            llm_response = self.llm_call(
                AIStructuredResponseGeneratorBlock.Input(
                    prompt=prompt,
                    credentials=input_data.credentials,
                    model=input_data.model,
                    expected_format={
                        ""final_summary"": ""The final summary of all provided summaries.""
                    },
                ),
                credentials=credentials,
            )

            return llm_response[""final_summary""]
        else:
            # If combined summaries are still too long, recursively summarize
            return self._run(
                AITextSummarizerBlock.Input(
                    text=combined_text,
                    credentials=input_data.credentials,
                    model=input_data.model,
                    max_tokens=input_data.max_tokens,
                    chunk_overlap=input_data.chunk_overlap,
                ),
                credentials=credentials,
            ).send(None)[
                1
            ]  # Get the first yielded value","Point(row=675, column=4)","Point(row=709, column=44)",AITextSummarizerBlock,autogpt_platform/backend/backend/blocks/llm.py
AIConversationBlock,class,,"class AIConversationBlock(Block):
    class Input(BlockSchema):
        messages: List[Message] = SchemaField(
            description=""List of messages in the conversation."", min_length=1
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for the conversation."",
        )
        credentials: AICredentials = AICredentialsField()
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )

    class Output(BlockSchema):
        response: str = SchemaField(
            description=""The model's response to the conversation.""
        )
        error: str = SchemaField(description=""Error message if the API call failed."")

    def __init__(self):
        super().__init__(
            id=""32a87eab-381e-4dd4-bdb8-4c47151be35a"",
            description=""Advanced LLM call that takes a list of messages and sends them to the language model."",
            categories={BlockCategory.AI},
            input_schema=AIConversationBlock.Input,
            output_schema=AIConversationBlock.Output,
            test_input={
                ""messages"": [
                    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
                    {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
                    {
                        ""role"": ""assistant"",
                        ""content"": ""The Los Angeles Dodgers won the World Series in 2020."",
                    },
                    {""role"": ""user"", ""content"": ""Where was it played?""},
                ],
                ""model"": LlmModel.GPT4_TURBO,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(
                ""response"",
                ""The 2020 World Series was played at Globe Life Field in Arlington, Texas."",
            ),
            test_mock={
                ""llm_call"": lambda *args, **kwargs: ""The 2020 World Series was played at Globe Life Field in Arlington, Texas.""
            },
        )

    def llm_call(
        self,
        input_data: AIStructuredResponseGeneratorBlock.Input,
        credentials: APIKeyCredentials,
    ) -> str:
        block = AIStructuredResponseGeneratorBlock()
        response = block.run_once(input_data, ""response"", credentials=credentials)
        self.merge_stats(block.execution_stats)
        return response[""response""]

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        response = self.llm_call(
            AIStructuredResponseGeneratorBlock.Input(
                prompt="""",
                credentials=input_data.credentials,
                model=input_data.model,
                conversation_history=input_data.messages,
                max_tokens=input_data.max_tokens,
                expected_format={},
            ),
            credentials=credentials,
        )

        yield ""response"", response","Point(row=712, column=0)","Point(row=790, column=34)",,autogpt_platform/backend/backend/blocks/llm.py
Input,class,,"class Input(BlockSchema):
        messages: List[Message] = SchemaField(
            description=""List of messages in the conversation."", min_length=1
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for the conversation."",
        )
        credentials: AICredentials = AICredentialsField()
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )","Point(row=713, column=4)","Point(row=727, column=9)",,autogpt_platform/backend/backend/blocks/llm.py
Output,class,,"class Output(BlockSchema):
        response: str = SchemaField(
            description=""The model's response to the conversation.""
        )
        error: str = SchemaField(description=""Error message if the API call failed."")","Point(row=729, column=4)","Point(row=733, column=85)",,autogpt_platform/backend/backend/blocks/llm.py
AIConversationBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""32a87eab-381e-4dd4-bdb8-4c47151be35a"",
            description=""Advanced LLM call that takes a list of messages and sends them to the language model."",
            categories={BlockCategory.AI},
            input_schema=AIConversationBlock.Input,
            output_schema=AIConversationBlock.Output,
            test_input={
                ""messages"": [
                    {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
                    {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
                    {
                        ""role"": ""assistant"",
                        ""content"": ""The Los Angeles Dodgers won the World Series in 2020."",
                    },
                    {""role"": ""user"", ""content"": ""Where was it played?""},
                ],
                ""model"": LlmModel.GPT4_TURBO,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(
                ""response"",
                ""The 2020 World Series was played at Globe Life Field in Arlington, Texas."",
            ),
            test_mock={
                ""llm_call"": lambda *args, **kwargs: ""The 2020 World Series was played at Globe Life Field in Arlington, Texas.""
            },
        )","Point(row=735, column=4)","Point(row=763, column=9)",AIConversationBlock,autogpt_platform/backend/backend/blocks/llm.py
AIConversationBlock.llm_call,function,,"def llm_call(
        self,
        input_data: AIStructuredResponseGeneratorBlock.Input,
        credentials: APIKeyCredentials,
    ) -> str:
        block = AIStructuredResponseGeneratorBlock()
        response = block.run_once(input_data, ""response"", credentials=credentials)
        self.merge_stats(block.execution_stats)
        return response[""response""]","Point(row=765, column=4)","Point(row=773, column=35)",AIConversationBlock,autogpt_platform/backend/backend/blocks/llm.py
AIConversationBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        response = self.llm_call(
            AIStructuredResponseGeneratorBlock.Input(
                prompt="""",
                credentials=input_data.credentials,
                model=input_data.model,
                conversation_history=input_data.messages,
                max_tokens=input_data.max_tokens,
                expected_format={},
            ),
            credentials=credentials,
        )

        yield ""response"", response","Point(row=775, column=4)","Point(row=790, column=34)",AIConversationBlock,autogpt_platform/backend/backend/blocks/llm.py
AIListGeneratorBlock,class,,"class AIListGeneratorBlock(Block):
    class Input(BlockSchema):
        focus: str | None = SchemaField(
            description=""The focus of the list to generate."",
            placeholder=""The top 5 most interesting news stories in the data."",
            default=None,
            advanced=False,
        )
        source_data: str | None = SchemaField(
            description=""The data to generate the list from."",
            placeholder=""News Today: Humans land on Mars: Today humans landed on mars. -- AI wins Nobel Prize: AI wins Nobel Prize for solving world hunger. -- New AI Model: A new AI model has been released."",
            default=None,
            advanced=False,
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for generating the list."",
            advanced=True,
        )
        credentials: AICredentials = AICredentialsField()
        max_retries: int = SchemaField(
            default=3,
            description=""Maximum number of retries for generating a valid list."",
            ge=1,
            le=5,
        )
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )

    class Output(BlockSchema):
        generated_list: List[str] = SchemaField(description=""The generated list."")
        list_item: str = SchemaField(
            description=""Each individual item in the list."",
        )
        error: str = SchemaField(
            description=""Error message if the list generation failed.""
        )

    def __init__(self):
        super().__init__(
            id=""9c0b0450-d199-458b-a731-072189dd6593"",
            description=""Generate a Python list based on the given prompt using a Large Language Model (LLM)."",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input_schema=AIListGeneratorBlock.Input,
            output_schema=AIListGeneratorBlock.Output,
            test_input={
                ""focus"": ""planets"",
                ""source_data"": (
                    ""Zylora Prime is a glowing jungle world with bioluminescent plants, ""
                    ""while Kharon-9 is a harsh desert planet with underground cities. ""
                    ""Vortexia's constant storms power floating cities, and Oceara is a water-covered world home to ""
                    ""intelligent marine life. On icy Draknos, ancient ruins lie buried beneath its frozen landscape, ""
                    ""drawing explorers to uncover its mysteries. Each planet showcases the limitless possibilities of ""
                    ""fictional worlds.""
                ),
                ""model"": LlmModel.GPT4_TURBO,
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""max_retries"": 3,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""generated_list"",
                    [""Zylora Prime"", ""Kharon-9"", ""Vortexia"", ""Oceara"", ""Draknos""],
                ),
                (""list_item"", ""Zylora Prime""),
                (""list_item"", ""Kharon-9""),
                (""list_item"", ""Vortexia""),
                (""list_item"", ""Oceara""),
                (""list_item"", ""Draknos""),
            ],
            test_mock={
                ""llm_call"": lambda input_data, credentials: {
                    ""response"": ""['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']""
                },
            },
        )

    @staticmethod
    def llm_call(
        input_data: AIStructuredResponseGeneratorBlock.Input,
        credentials: APIKeyCredentials,
    ) -> dict[str, str]:
        llm_block = AIStructuredResponseGeneratorBlock()
        response = llm_block.run_once(input_data, ""response"", credentials=credentials)
        return response

    @staticmethod
    def string_to_list(string):
        """"""
        Converts a string representation of a list into an actual Python list object.
        """"""
        logger.debug(f""Converting string to list. Input string: {string}"")
        try:
            # Use ast.literal_eval to safely evaluate the string
            python_list = ast.literal_eval(string)
            if isinstance(python_list, list):
                logger.debug(f""Successfully converted string to list: {python_list}"")
                return python_list
            else:
                logger.error(f""The provided string '{string}' is not a valid list"")
                raise ValueError(f""The provided string '{string}' is not a valid list."")
        except (SyntaxError, ValueError) as e:
            logger.error(f""Failed to convert string to list: {e}"")
            raise ValueError(""Invalid list format. Could not convert to list."")

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        logger.debug(f""Starting AIListGeneratorBlock.run with input data: {input_data}"")

        # Check for API key
        api_key_check = credentials.api_key.get_secret_value()
        if not api_key_check:
            raise ValueError(""No LLM API key provided."")

        # Prepare the system prompt
        sys_prompt = """"""You are a Python list generator. Your task is to generate a Python list based on the user's prompt. 
            |Respond ONLY with a valid python list. 
            |The list can contain strings, numbers, or nested lists as appropriate. 
            |Do not include any explanations or additional text.

            |Valid Example string formats:

            |Example 1:
            |```
            |['1', '2', '3', '4']
            |```

            |Example 2:
            |```
            |[['1', '2'], ['3', '4'], ['5', '6']]
            |```

            |Example 3:
            |```
            |['1', ['2', '3'], ['4', ['5', '6']]]
            |```

            |Example 4:
            |```
            |['a', 'b', 'c']
            |```

            |Example 5:
            |```
            |['1', '2.5', 'string', 'True', ['False', 'None']]
            |```

            |Do not include any explanations or additional text, just respond with the list in the format specified above.
            """"""
        # If a focus is provided, add it to the prompt
        if input_data.focus:
            prompt = f""Generate a list with the following focus:\n<focus>\n\n{input_data.focus}</focus>""
        else:
            # If there's source data
            if input_data.source_data:
                prompt = ""Extract the main focus of the source data to a list.\ni.e if the source data is a news website, the focus would be the news stories rather than the social links in the footer.""
            else:
                # No focus or source data provided, generat a random list
                prompt = ""Generate a random list.""

        # If the source data is provided, add it to the prompt
        if input_data.source_data:
            prompt += f""\n\nUse the following source data to generate the list from:\n\n<source_data>\n\n{input_data.source_data}</source_data>\n\nDo not invent fictional data that is not present in the source data.""
        # Else, tell the LLM to synthesize the data
        else:
            prompt += ""\n\nInvent the data to generate the list from.""

        for attempt in range(input_data.max_retries):
            try:
                logger.debug(""Calling LLM"")
                llm_response = self.llm_call(
                    AIStructuredResponseGeneratorBlock.Input(
                        sys_prompt=sys_prompt,
                        prompt=prompt,
                        credentials=input_data.credentials,
                        model=input_data.model,
                        expected_format={},  # Do not use structured response
                    ),
                    credentials=credentials,
                )

                logger.debug(f""LLM response: {llm_response}"")

                # Extract Response string
                response_string = llm_response[""response""]
                logger.debug(f""Response string: {response_string}"")

                # Convert the string to a Python list
                logger.debug(""Converting string to Python list"")
                parsed_list = self.string_to_list(response_string)
                logger.debug(f""Parsed list: {parsed_list}"")

                # If we reach here, we have a valid Python list
                logger.debug(""Successfully generated a valid Python list"")
                yield ""generated_list"", parsed_list

                # Yield each item in the list
                for item in parsed_list:
                    yield ""list_item"", item
                return

            except Exception as e:
                logger.error(f""Error in attempt {attempt + 1}: {str(e)}"")
                if attempt == input_data.max_retries - 1:
                    logger.error(
                        f""Failed to generate a valid Python list after {input_data.max_retries} attempts""
                    )
                    raise RuntimeError(
                        f""Failed to generate a valid Python list after {input_data.max_retries} attempts. Last error: {str(e)}""
                    )
                else:
                    # Add a retry prompt
                    logger.debug(""Preparing retry prompt"")
                    prompt = f""""""
                    The previous attempt failed due to `{e}`
                    Generate a valid Python list based on the original prompt.
                    Remember to respond ONLY with a valid Python list as per the format specified earlier.
                    Original prompt: 
                    ```{prompt}```
                    
                    Respond only with the list in the format specified with no commentary or apologies.
                    """"""
                    logger.debug(f""Retry prompt: {prompt}"")

        logger.debug(""AIListGeneratorBlock.run completed"")","Point(row=793, column=0)","Point(row=1023, column=58)",,autogpt_platform/backend/backend/blocks/llm.py
Input,class,,"class Input(BlockSchema):
        focus: str | None = SchemaField(
            description=""The focus of the list to generate."",
            placeholder=""The top 5 most interesting news stories in the data."",
            default=None,
            advanced=False,
        )
        source_data: str | None = SchemaField(
            description=""The data to generate the list from."",
            placeholder=""News Today: Humans land on Mars: Today humans landed on mars. -- AI wins Nobel Prize: AI wins Nobel Prize for solving world hunger. -- New AI Model: A new AI model has been released."",
            default=None,
            advanced=False,
        )
        model: LlmModel = SchemaField(
            title=""LLM Model"",
            default=LlmModel.GPT4_TURBO,
            description=""The language model to use for generating the list."",
            advanced=True,
        )
        credentials: AICredentials = AICredentialsField()
        max_retries: int = SchemaField(
            default=3,
            description=""Maximum number of retries for generating a valid list."",
            ge=1,
            le=5,
        )
        max_tokens: int | None = SchemaField(
            advanced=True,
            default=None,
            description=""The maximum number of tokens to generate in the chat completion."",
        )","Point(row=794, column=4)","Point(row=824, column=9)",,autogpt_platform/backend/backend/blocks/llm.py
Output,class,,"class Output(BlockSchema):
        generated_list: List[str] = SchemaField(description=""The generated list."")
        list_item: str = SchemaField(
            description=""Each individual item in the list."",
        )
        error: str = SchemaField(
            description=""Error message if the list generation failed.""
        )","Point(row=826, column=4)","Point(row=833, column=9)",,autogpt_platform/backend/backend/blocks/llm.py
AIListGeneratorBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""9c0b0450-d199-458b-a731-072189dd6593"",
            description=""Generate a Python list based on the given prompt using a Large Language Model (LLM)."",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input_schema=AIListGeneratorBlock.Input,
            output_schema=AIListGeneratorBlock.Output,
            test_input={
                ""focus"": ""planets"",
                ""source_data"": (
                    ""Zylora Prime is a glowing jungle world with bioluminescent plants, ""
                    ""while Kharon-9 is a harsh desert planet with underground cities. ""
                    ""Vortexia's constant storms power floating cities, and Oceara is a water-covered world home to ""
                    ""intelligent marine life. On icy Draknos, ancient ruins lie buried beneath its frozen landscape, ""
                    ""drawing explorers to uncover its mysteries. Each planet showcases the limitless possibilities of ""
                    ""fictional worlds.""
                ),
                ""model"": LlmModel.GPT4_TURBO,
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""max_retries"": 3,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""generated_list"",
                    [""Zylora Prime"", ""Kharon-9"", ""Vortexia"", ""Oceara"", ""Draknos""],
                ),
                (""list_item"", ""Zylora Prime""),
                (""list_item"", ""Kharon-9""),
                (""list_item"", ""Vortexia""),
                (""list_item"", ""Oceara""),
                (""list_item"", ""Draknos""),
            ],
            test_mock={
                ""llm_call"": lambda input_data, credentials: {
                    ""response"": ""['Zylora Prime', 'Kharon-9', 'Vortexia', 'Oceara', 'Draknos']""
                },
            },
        )","Point(row=835, column=4)","Point(row=873, column=9)",AIListGeneratorBlock,autogpt_platform/backend/backend/blocks/llm.py
AIListGeneratorBlock.llm_call,function,,"def llm_call(
        input_data: AIStructuredResponseGeneratorBlock.Input,
        credentials: APIKeyCredentials,
    ) -> dict[str, str]:
        llm_block = AIStructuredResponseGeneratorBlock()
        response = llm_block.run_once(input_data, ""response"", credentials=credentials)
        return response","Point(row=876, column=4)","Point(row=882, column=23)",AIListGeneratorBlock,autogpt_platform/backend/backend/blocks/llm.py
AIListGeneratorBlock.string_to_list,function,"
        Converts a string representation of a list into an actual Python list object.
","def string_to_list(string):
        """"""
        Converts a string representation of a list into an actual Python list object.
        """"""
        logger.debug(f""Converting string to list. Input string: {string}"")
        try:
            # Use ast.literal_eval to safely evaluate the string
            python_list = ast.literal_eval(string)
            if isinstance(python_list, list):
                logger.debug(f""Successfully converted string to list: {python_list}"")
                return python_list
            else:
                logger.error(f""The provided string '{string}' is not a valid list"")
                raise ValueError(f""The provided string '{string}' is not a valid list."")
        except (SyntaxError, ValueError) as e:
            logger.error(f""Failed to convert string to list: {e}"")
            raise ValueError(""Invalid list format. Could not convert to list."")","Point(row=885, column=4)","Point(row=901, column=79)",AIListGeneratorBlock,autogpt_platform/backend/backend/blocks/llm.py
AIListGeneratorBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        logger.debug(f""Starting AIListGeneratorBlock.run with input data: {input_data}"")

        # Check for API key
        api_key_check = credentials.api_key.get_secret_value()
        if not api_key_check:
            raise ValueError(""No LLM API key provided."")

        # Prepare the system prompt
        sys_prompt = """"""You are a Python list generator. Your task is to generate a Python list based on the user's prompt. 
            |Respond ONLY with a valid python list. 
            |The list can contain strings, numbers, or nested lists as appropriate. 
            |Do not include any explanations or additional text.

            |Valid Example string formats:

            |Example 1:
            |```
            |['1', '2', '3', '4']
            |```

            |Example 2:
            |```
            |[['1', '2'], ['3', '4'], ['5', '6']]
            |```

            |Example 3:
            |```
            |['1', ['2', '3'], ['4', ['5', '6']]]
            |```

            |Example 4:
            |```
            |['a', 'b', 'c']
            |```

            |Example 5:
            |```
            |['1', '2.5', 'string', 'True', ['False', 'None']]
            |```

            |Do not include any explanations or additional text, just respond with the list in the format specified above.
            """"""
        # If a focus is provided, add it to the prompt
        if input_data.focus:
            prompt = f""Generate a list with the following focus:\n<focus>\n\n{input_data.focus}</focus>""
        else:
            # If there's source data
            if input_data.source_data:
                prompt = ""Extract the main focus of the source data to a list.\ni.e if the source data is a news website, the focus would be the news stories rather than the social links in the footer.""
            else:
                # No focus or source data provided, generat a random list
                prompt = ""Generate a random list.""

        # If the source data is provided, add it to the prompt
        if input_data.source_data:
            prompt += f""\n\nUse the following source data to generate the list from:\n\n<source_data>\n\n{input_data.source_data}</source_data>\n\nDo not invent fictional data that is not present in the source data.""
        # Else, tell the LLM to synthesize the data
        else:
            prompt += ""\n\nInvent the data to generate the list from.""

        for attempt in range(input_data.max_retries):
            try:
                logger.debug(""Calling LLM"")
                llm_response = self.llm_call(
                    AIStructuredResponseGeneratorBlock.Input(
                        sys_prompt=sys_prompt,
                        prompt=prompt,
                        credentials=input_data.credentials,
                        model=input_data.model,
                        expected_format={},  # Do not use structured response
                    ),
                    credentials=credentials,
                )

                logger.debug(f""LLM response: {llm_response}"")

                # Extract Response string
                response_string = llm_response[""response""]
                logger.debug(f""Response string: {response_string}"")

                # Convert the string to a Python list
                logger.debug(""Converting string to Python list"")
                parsed_list = self.string_to_list(response_string)
                logger.debug(f""Parsed list: {parsed_list}"")

                # If we reach here, we have a valid Python list
                logger.debug(""Successfully generated a valid Python list"")
                yield ""generated_list"", parsed_list

                # Yield each item in the list
                for item in parsed_list:
                    yield ""list_item"", item
                return

            except Exception as e:
                logger.error(f""Error in attempt {attempt + 1}: {str(e)}"")
                if attempt == input_data.max_retries - 1:
                    logger.error(
                        f""Failed to generate a valid Python list after {input_data.max_retries} attempts""
                    )
                    raise RuntimeError(
                        f""Failed to generate a valid Python list after {input_data.max_retries} attempts. Last error: {str(e)}""
                    )
                else:
                    # Add a retry prompt
                    logger.debug(""Preparing retry prompt"")
                    prompt = f""""""
                    The previous attempt failed due to `{e}`
                    Generate a valid Python list based on the original prompt.
                    Remember to respond ONLY with a valid Python list as per the format specified earlier.
                    Original prompt: 
                    ```{prompt}```
                    
                    Respond only with the list in the format specified with no commentary or apologies.
                    """"""
                    logger.debug(f""Retry prompt: {prompt}"")

        logger.debug(""AIListGeneratorBlock.run completed"")","Point(row=903, column=4)","Point(row=1023, column=58)",AIListGeneratorBlock,autogpt_platform/backend/backend/blocks/llm.py
EmailCredentials,class,,"class EmailCredentials(BaseModel):
    smtp_server: str = SchemaField(
        default=""smtp.gmail.com"", description=""SMTP server address""
    )
    smtp_port: int = SchemaField(default=25, description=""SMTP port number"")
    smtp_username: BlockSecret = SecretField(key=""smtp_username"")
    smtp_password: BlockSecret = SecretField(key=""smtp_password"")

    model_config = ConfigDict(title=""Email Credentials"")","Point(row=10, column=0)","Point(row=18, column=56)",,autogpt_platform/backend/backend/blocks/email_block.py
SendEmailBlock,class,,"class SendEmailBlock(Block):
    class Input(BlockSchema):
        to_email: str = SchemaField(
            description=""Recipient email address"", placeholder=""recipient@example.com""
        )
        subject: str = SchemaField(
            description=""Subject of the email"", placeholder=""Enter the email subject""
        )
        body: str = SchemaField(
            description=""Body of the email"", placeholder=""Enter the email body""
        )
        creds: EmailCredentials = SchemaField(
            description=""SMTP credentials"",
            default=EmailCredentials(),
        )

    class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the email sending operation"")
        error: str = SchemaField(
            description=""Error message if the email sending failed""
        )

    def __init__(self):
        super().__init__(
            disabled=True,
            id=""4335878a-394e-4e67-adf2-919877ff49ae"",
            description=""This block sends an email using the provided SMTP credentials."",
            categories={BlockCategory.OUTPUT},
            input_schema=SendEmailBlock.Input,
            output_schema=SendEmailBlock.Output,
            test_input={
                ""to_email"": ""recipient@example.com"",
                ""subject"": ""Test Email"",
                ""body"": ""This is a test email."",
                ""creds"": {
                    ""smtp_server"": ""smtp.gmail.com"",
                    ""smtp_port"": 25,
                    ""smtp_username"": ""your-email@gmail.com"",
                    ""smtp_password"": ""your-gmail-password"",
                },
            },
            test_output=[(""status"", ""Email sent successfully"")],
            test_mock={""send_email"": lambda *args, **kwargs: ""Email sent successfully""},
        )

    @staticmethod
    def send_email(
        creds: EmailCredentials, to_email: str, subject: str, body: str
    ) -> str:
        smtp_server = creds.smtp_server
        smtp_port = creds.smtp_port
        smtp_username = creds.smtp_username.get_secret_value()
        smtp_password = creds.smtp_password.get_secret_value()

        msg = MIMEMultipart()
        msg[""From""] = smtp_username
        msg[""To""] = to_email
        msg[""Subject""] = subject
        msg.attach(MIMEText(body, ""plain""))

        with smtplib.SMTP(smtp_server, smtp_port) as server:
            server.starttls()
            server.login(smtp_username, smtp_password)
            server.sendmail(smtp_username, to_email, msg.as_string())

        return ""Email sent successfully""

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""status"", self.send_email(
            input_data.creds,
            input_data.to_email,
            input_data.subject,
            input_data.body,
        )","Point(row=21, column=0)","Point(row=94, column=9)",,autogpt_platform/backend/backend/blocks/email_block.py
Input,class,,"class Input(BlockSchema):
        to_email: str = SchemaField(
            description=""Recipient email address"", placeholder=""recipient@example.com""
        )
        subject: str = SchemaField(
            description=""Subject of the email"", placeholder=""Enter the email subject""
        )
        body: str = SchemaField(
            description=""Body of the email"", placeholder=""Enter the email body""
        )
        creds: EmailCredentials = SchemaField(
            description=""SMTP credentials"",
            default=EmailCredentials(),
        )","Point(row=22, column=4)","Point(row=35, column=9)",,autogpt_platform/backend/backend/blocks/email_block.py
Output,class,,"class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the email sending operation"")
        error: str = SchemaField(
            description=""Error message if the email sending failed""
        )","Point(row=37, column=4)","Point(row=41, column=9)",,autogpt_platform/backend/backend/blocks/email_block.py
SendEmailBlock.__init__,function,,"def __init__(self):
        super().__init__(
            disabled=True,
            id=""4335878a-394e-4e67-adf2-919877ff49ae"",
            description=""This block sends an email using the provided SMTP credentials."",
            categories={BlockCategory.OUTPUT},
            input_schema=SendEmailBlock.Input,
            output_schema=SendEmailBlock.Output,
            test_input={
                ""to_email"": ""recipient@example.com"",
                ""subject"": ""Test Email"",
                ""body"": ""This is a test email."",
                ""creds"": {
                    ""smtp_server"": ""smtp.gmail.com"",
                    ""smtp_port"": 25,
                    ""smtp_username"": ""your-email@gmail.com"",
                    ""smtp_password"": ""your-gmail-password"",
                },
            },
            test_output=[(""status"", ""Email sent successfully"")],
            test_mock={""send_email"": lambda *args, **kwargs: ""Email sent successfully""},
        )","Point(row=43, column=4)","Point(row=64, column=9)",SendEmailBlock,autogpt_platform/backend/backend/blocks/email_block.py
SendEmailBlock.send_email,function,,"def send_email(
        creds: EmailCredentials, to_email: str, subject: str, body: str
    ) -> str:
        smtp_server = creds.smtp_server
        smtp_port = creds.smtp_port
        smtp_username = creds.smtp_username.get_secret_value()
        smtp_password = creds.smtp_password.get_secret_value()

        msg = MIMEMultipart()
        msg[""From""] = smtp_username
        msg[""To""] = to_email
        msg[""Subject""] = subject
        msg.attach(MIMEText(body, ""plain""))

        with smtplib.SMTP(smtp_server, smtp_port) as server:
            server.starttls()
            server.login(smtp_username, smtp_password)
            server.sendmail(smtp_username, to_email, msg.as_string())

        return ""Email sent successfully""","Point(row=67, column=4)","Point(row=86, column=40)",SendEmailBlock,autogpt_platform/backend/backend/blocks/email_block.py
SendEmailBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""status"", self.send_email(
            input_data.creds,
            input_data.to_email,
            input_data.subject,
            input_data.body,
        )","Point(row=88, column=4)","Point(row=94, column=9)",SendEmailBlock,autogpt_platform/backend/backend/blocks/email_block.py
ReplicateFluxModelName,class,,"class ReplicateFluxModelName(str, Enum):
    FLUX_SCHNELL = (""Flux Schnell"",)
    FLUX_PRO = (""Flux Pro"",)
    FLUX_PRO1_1 = (""Flux Pro 1.1"",)

    @property
    def api_name(self):
        api_names = {
            ReplicateFluxModelName.FLUX_SCHNELL: ""black-forest-labs/flux-schnell"",
            ReplicateFluxModelName.FLUX_PRO: ""black-forest-labs/flux-pro"",
            ReplicateFluxModelName.FLUX_PRO1_1: ""black-forest-labs/flux-1.1-pro"",
        }
        return api_names[self]","Point(row=28, column=0)","Point(row=40, column=30)",,autogpt_platform/backend/backend/blocks/replicate_flux_advanced.py
ReplicateFluxModelName.api_name,function,,"def api_name(self):
        api_names = {
            ReplicateFluxModelName.FLUX_SCHNELL: ""black-forest-labs/flux-schnell"",
            ReplicateFluxModelName.FLUX_PRO: ""black-forest-labs/flux-pro"",
            ReplicateFluxModelName.FLUX_PRO1_1: ""black-forest-labs/flux-1.1-pro"",
        }
        return api_names[self]","Point(row=34, column=4)","Point(row=40, column=30)",ReplicateFluxModelName,autogpt_platform/backend/backend/blocks/replicate_flux_advanced.py
ImageType,class,,"class ImageType(str, Enum):
    WEBP = ""webp""
    JPG = ""jpg""
    PNG = ""png""","Point(row=44, column=0)","Point(row=47, column=15)",,autogpt_platform/backend/backend/blocks/replicate_flux_advanced.py
ReplicateFluxAdvancedModelBlock,class,,"class ReplicateFluxAdvancedModelBlock(Block):
    class Input(BlockSchema):
        credentials: CredentialsMetaInput[Literal[""replicate""], Literal[""api_key""]] = (
            CredentialsField(
                provider=""replicate"",
                supported_credential_types={""api_key""},
                description=""The Replicate integration can be used with ""
                ""any API key with sufficient permissions for the blocks it is used on."",
            )
        )
        prompt: str = SchemaField(
            description=""Text prompt for image generation"",
            placeholder=""e.g., 'A futuristic cityscape at sunset'"",
            title=""Prompt"",
        )
        replicate_model_name: ReplicateFluxModelName = SchemaField(
            description=""The name of the Image Generation Model, i.e Flux Schnell"",
            default=ReplicateFluxModelName.FLUX_SCHNELL,
            title=""Image Generation Model"",
            advanced=False,
        )
        seed: int | None = SchemaField(
            description=""Random seed. Set for reproducible generation"",
            default=None,
            title=""Seed"",
        )
        steps: int = SchemaField(
            description=""Number of diffusion steps"",
            default=25,
            title=""Steps"",
        )
        guidance: float = SchemaField(
            description=(
                ""Controls the balance between adherence to the text prompt and image quality/diversity. ""
                ""Higher values make the output more closely match the prompt but may reduce overall image quality.""
            ),
            default=3,
            title=""Guidance"",
        )
        interval: float = SchemaField(
            description=(
                ""Interval is a setting that increases the variance in possible outputs. ""
                ""Setting this value low will ensure strong prompt following with more consistent outputs.""
            ),
            default=2,
            title=""Interval"",
        )
        aspect_ratio: str = SchemaField(
            description=""Aspect ratio for the generated image"",
            default=""1:1"",
            title=""Aspect Ratio"",
            placeholder=""Choose from: 1:1, 16:9, 2:3, 3:2, 4:5, 5:4, 9:16"",
        )
        output_format: ImageType = SchemaField(
            description=""File format of the output image"",
            default=ImageType.WEBP,
            title=""Output Format"",
        )
        output_quality: int = SchemaField(
            description=(
                ""Quality when saving the output images, from 0 to 100. ""
                ""Not relevant for .png outputs""
            ),
            default=80,
            title=""Output Quality"",
        )
        safety_tolerance: int = SchemaField(
            description=""Safety tolerance, 1 is most strict and 5 is most permissive"",
            default=2,
            title=""Safety Tolerance"",
        )

    class Output(BlockSchema):
        result: str = SchemaField(description=""Generated output"")
        error: str = SchemaField(description=""Error message if the model run failed"")

    def __init__(self):
        super().__init__(
            id=""90f8c45e-e983-4644-aa0b-b4ebe2f531bc"",
            description=""This block runs Flux models on Replicate with advanced settings."",
            categories={BlockCategory.AI},
            input_schema=ReplicateFluxAdvancedModelBlock.Input,
            output_schema=ReplicateFluxAdvancedModelBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""replicate_model_name"": ReplicateFluxModelName.FLUX_SCHNELL,
                ""prompt"": ""A beautiful landscape painting of a serene lake at sunrise"",
                ""seed"": None,
                ""steps"": 25,
                ""guidance"": 3.0,
                ""interval"": 2.0,
                ""aspect_ratio"": ""1:1"",
                ""output_format"": ImageType.PNG,
                ""output_quality"": 80,
                ""safety_tolerance"": 2,
            },
            test_output=[
                (
                    ""result"",
                    ""https://replicate.com/output/generated-image-url.jpg"",
                ),
            ],
            test_mock={
                ""run_model"": lambda api_key, model_name, prompt, seed, steps, guidance, interval, aspect_ratio, output_format, output_quality, safety_tolerance: ""https://replicate.com/output/generated-image-url.jpg"",
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        # If the seed is not provided, generate a random seed
        seed = input_data.seed
        if seed is None:
            seed = int.from_bytes(os.urandom(4), ""big"")

        # Run the model using the provided inputs
        result = self.run_model(
            api_key=credentials.api_key,
            model_name=input_data.replicate_model_name.api_name,
            prompt=input_data.prompt,
            seed=seed,
            steps=input_data.steps,
            guidance=input_data.guidance,
            interval=input_data.interval,
            aspect_ratio=input_data.aspect_ratio,
            output_format=input_data.output_format,
            output_quality=input_data.output_quality,
            safety_tolerance=input_data.safety_tolerance,
        )
        yield ""result"", result

    def run_model(
        self,
        api_key: SecretStr,
        model_name,
        prompt,
        seed,
        steps,
        guidance,
        interval,
        aspect_ratio,
        output_format,
        output_quality,
        safety_tolerance,
    ):
        # Initialize Replicate client with the API key
        client = replicate.Client(api_token=api_key.get_secret_value())

        # Run the model with additional parameters
        output: FileOutput | list[FileOutput] = client.run(  # type: ignore This is because they changed the return type, and didn't update the type hint! It should be overloaded depending on the value of `use_file_output` to `FileOutput | list[FileOutput]` but it's `Any | Iterator[Any]`
            f""{model_name}"",
            input={
                ""prompt"": prompt,
                ""seed"": seed,
                ""steps"": steps,
                ""guidance"": guidance,
                ""interval"": interval,
                ""aspect_ratio"": aspect_ratio,
                ""output_format"": output_format,
                ""output_quality"": output_quality,
                ""safety_tolerance"": safety_tolerance,
            },
            wait=False,  # don't arbitrarily return data:octect/stream or sometimes url depending on the model???? what is this api
        )

        # Check if output is a list or a string and extract accordingly; otherwise, assign a default message
        if isinstance(output, list) and len(output) > 0:
            if isinstance(output[0], FileOutput):
                result_url = output[0].url  # If output is a list, get the first element
            else:
                result_url = output[
                    0
                ]  # If output is a list and not a FileOutput, get the first element. Should never happen, but just in case.
        elif isinstance(output, FileOutput):
            result_url = output.url  # If output is a FileOutput, use the url
        elif isinstance(output, str):
            result_url = output  # If output is a string (for some reason due to their janky type hinting), use it directly
        else:
            result_url = (
                ""No output received""  # Fallback message if output is not as expected
            )

        return result_url","Point(row=50, column=0)","Point(row=233, column=25)",,autogpt_platform/backend/backend/blocks/replicate_flux_advanced.py
Input,class,,"class Input(BlockSchema):
        credentials: CredentialsMetaInput[Literal[""replicate""], Literal[""api_key""]] = (
            CredentialsField(
                provider=""replicate"",
                supported_credential_types={""api_key""},
                description=""The Replicate integration can be used with ""
                ""any API key with sufficient permissions for the blocks it is used on."",
            )
        )
        prompt: str = SchemaField(
            description=""Text prompt for image generation"",
            placeholder=""e.g., 'A futuristic cityscape at sunset'"",
            title=""Prompt"",
        )
        replicate_model_name: ReplicateFluxModelName = SchemaField(
            description=""The name of the Image Generation Model, i.e Flux Schnell"",
            default=ReplicateFluxModelName.FLUX_SCHNELL,
            title=""Image Generation Model"",
            advanced=False,
        )
        seed: int | None = SchemaField(
            description=""Random seed. Set for reproducible generation"",
            default=None,
            title=""Seed"",
        )
        steps: int = SchemaField(
            description=""Number of diffusion steps"",
            default=25,
            title=""Steps"",
        )
        guidance: float = SchemaField(
            description=(
                ""Controls the balance between adherence to the text prompt and image quality/diversity. ""
                ""Higher values make the output more closely match the prompt but may reduce overall image quality.""
            ),
            default=3,
            title=""Guidance"",
        )
        interval: float = SchemaField(
            description=(
                ""Interval is a setting that increases the variance in possible outputs. ""
                ""Setting this value low will ensure strong prompt following with more consistent outputs.""
            ),
            default=2,
            title=""Interval"",
        )
        aspect_ratio: str = SchemaField(
            description=""Aspect ratio for the generated image"",
            default=""1:1"",
            title=""Aspect Ratio"",
            placeholder=""Choose from: 1:1, 16:9, 2:3, 3:2, 4:5, 5:4, 9:16"",
        )
        output_format: ImageType = SchemaField(
            description=""File format of the output image"",
            default=ImageType.WEBP,
            title=""Output Format"",
        )
        output_quality: int = SchemaField(
            description=(
                ""Quality when saving the output images, from 0 to 100. ""
                ""Not relevant for .png outputs""
            ),
            default=80,
            title=""Output Quality"",
        )
        safety_tolerance: int = SchemaField(
            description=""Safety tolerance, 1 is most strict and 5 is most permissive"",
            default=2,
            title=""Safety Tolerance"",
        )","Point(row=51, column=4)","Point(row=120, column=9)",,autogpt_platform/backend/backend/blocks/replicate_flux_advanced.py
Output,class,,"class Output(BlockSchema):
        result: str = SchemaField(description=""Generated output"")
        error: str = SchemaField(description=""Error message if the model run failed"")","Point(row=122, column=4)","Point(row=124, column=85)",,autogpt_platform/backend/backend/blocks/replicate_flux_advanced.py
ReplicateFluxAdvancedModelBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""90f8c45e-e983-4644-aa0b-b4ebe2f531bc"",
            description=""This block runs Flux models on Replicate with advanced settings."",
            categories={BlockCategory.AI},
            input_schema=ReplicateFluxAdvancedModelBlock.Input,
            output_schema=ReplicateFluxAdvancedModelBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""replicate_model_name"": ReplicateFluxModelName.FLUX_SCHNELL,
                ""prompt"": ""A beautiful landscape painting of a serene lake at sunrise"",
                ""seed"": None,
                ""steps"": 25,
                ""guidance"": 3.0,
                ""interval"": 2.0,
                ""aspect_ratio"": ""1:1"",
                ""output_format"": ImageType.PNG,
                ""output_quality"": 80,
                ""safety_tolerance"": 2,
            },
            test_output=[
                (
                    ""result"",
                    ""https://replicate.com/output/generated-image-url.jpg"",
                ),
            ],
            test_mock={
                ""run_model"": lambda api_key, model_name, prompt, seed, steps, guidance, interval, aspect_ratio, output_format, output_quality, safety_tolerance: ""https://replicate.com/output/generated-image-url.jpg"",
            },
            test_credentials=TEST_CREDENTIALS,
        )","Point(row=126, column=4)","Point(row=156, column=9)",ReplicateFluxAdvancedModelBlock,autogpt_platform/backend/backend/blocks/replicate_flux_advanced.py
ReplicateFluxAdvancedModelBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        # If the seed is not provided, generate a random seed
        seed = input_data.seed
        if seed is None:
            seed = int.from_bytes(os.urandom(4), ""big"")

        # Run the model using the provided inputs
        result = self.run_model(
            api_key=credentials.api_key,
            model_name=input_data.replicate_model_name.api_name,
            prompt=input_data.prompt,
            seed=seed,
            steps=input_data.steps,
            guidance=input_data.guidance,
            interval=input_data.interval,
            aspect_ratio=input_data.aspect_ratio,
            output_format=input_data.output_format,
            output_quality=input_data.output_quality,
            safety_tolerance=input_data.safety_tolerance,
        )
        yield ""result"", result","Point(row=158, column=4)","Point(row=180, column=30)",ReplicateFluxAdvancedModelBlock,autogpt_platform/backend/backend/blocks/replicate_flux_advanced.py
ReplicateFluxAdvancedModelBlock.run_model,function,,"def run_model(
        self,
        api_key: SecretStr,
        model_name,
        prompt,
        seed,
        steps,
        guidance,
        interval,
        aspect_ratio,
        output_format,
        output_quality,
        safety_tolerance,
    ):
        # Initialize Replicate client with the API key
        client = replicate.Client(api_token=api_key.get_secret_value())

        # Run the model with additional parameters
        output: FileOutput | list[FileOutput] = client.run(  # type: ignore This is because they changed the return type, and didn't update the type hint! It should be overloaded depending on the value of `use_file_output` to `FileOutput | list[FileOutput]` but it's `Any | Iterator[Any]`
            f""{model_name}"",
            input={
                ""prompt"": prompt,
                ""seed"": seed,
                ""steps"": steps,
                ""guidance"": guidance,
                ""interval"": interval,
                ""aspect_ratio"": aspect_ratio,
                ""output_format"": output_format,
                ""output_quality"": output_quality,
                ""safety_tolerance"": safety_tolerance,
            },
            wait=False,  # don't arbitrarily return data:octect/stream or sometimes url depending on the model???? what is this api
        )

        # Check if output is a list or a string and extract accordingly; otherwise, assign a default message
        if isinstance(output, list) and len(output) > 0:
            if isinstance(output[0], FileOutput):
                result_url = output[0].url  # If output is a list, get the first element
            else:
                result_url = output[
                    0
                ]  # If output is a list and not a FileOutput, get the first element. Should never happen, but just in case.
        elif isinstance(output, FileOutput):
            result_url = output.url  # If output is a FileOutput, use the url
        elif isinstance(output, str):
            result_url = output  # If output is a string (for some reason due to their janky type hinting), use it directly
        else:
            result_url = (
                ""No output received""  # Fallback message if output is not as expected
            )

        return result_url","Point(row=182, column=4)","Point(row=233, column=25)",ReplicateFluxAdvancedModelBlock,autogpt_platform/backend/backend/blocks/replicate_flux_advanced.py
StoreValueBlock,class,"
    This block allows you to provide a constant value as a block, in a stateless manner.
    The common use-case is simply pass the `input` data, it will `output` the same data.
    The block output will be static, the output can be consumed multiple times.
","class StoreValueBlock(Block):
    """"""
    This block allows you to provide a constant value as a block, in a stateless manner.
    The common use-case is simply pass the `input` data, it will `output` the same data.
    The block output will be static, the output can be consumed multiple times.
    """"""

    class Input(BlockSchema):
        input: Any = SchemaField(
            description=""Trigger the block to produce the output. ""
            ""The value is only used when `data` is None.""
        )
        data: Any = SchemaField(
            description=""The constant data to be retained in the block. ""
            ""This value is passed as `output`."",
            default=None,
        )

    class Output(BlockSchema):
        output: Any = SchemaField(description=""The stored data retained in the block."")

    def __init__(self):
        super().__init__(
            id=""1ff065e9-88e8-4358-9d82-8dc91f622ba9"",
            description=""This block forwards an input value as output, allowing reuse without change."",
            categories={BlockCategory.BASIC},
            input_schema=StoreValueBlock.Input,
            output_schema=StoreValueBlock.Output,
            test_input=[
                {""input"": ""Hello, World!""},
                {""input"": ""Hello, World!"", ""data"": ""Existing Data""},
            ],
            test_output=[
                (""output"", ""Hello, World!""),  # No data provided, so trigger is returned
                (""output"", ""Existing Data""),  # Data is provided, so data is returned.
            ],
            static_output=True,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""output"", input_data.data or input_data.input","Point(row=12, column=0)","Point(row=52, column=59)",,autogpt_platform/backend/backend/blocks/basic.py
Input,class,,"class Input(BlockSchema):
        input: Any = SchemaField(
            description=""Trigger the block to produce the output. ""
            ""The value is only used when `data` is None.""
        )
        data: Any = SchemaField(
            description=""The constant data to be retained in the block. ""
            ""This value is passed as `output`."",
            default=None,
        )","Point(row=19, column=4)","Point(row=28, column=9)",,autogpt_platform/backend/backend/blocks/basic.py
Output,class,,"class Output(BlockSchema):
        output: Any = SchemaField(description=""The stored data retained in the block."")","Point(row=30, column=4)","Point(row=31, column=87)",,autogpt_platform/backend/backend/blocks/basic.py
StoreValueBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""1ff065e9-88e8-4358-9d82-8dc91f622ba9"",
            description=""This block forwards an input value as output, allowing reuse without change."",
            categories={BlockCategory.BASIC},
            input_schema=StoreValueBlock.Input,
            output_schema=StoreValueBlock.Output,
            test_input=[
                {""input"": ""Hello, World!""},
                {""input"": ""Hello, World!"", ""data"": ""Existing Data""},
            ],
            test_output=[
                (""output"", ""Hello, World!""),  # No data provided, so trigger is returned
                (""output"", ""Existing Data""),  # Data is provided, so data is returned.
            ],
            static_output=True,
        )","Point(row=33, column=4)","Point(row=49, column=9)",StoreValueBlock,autogpt_platform/backend/backend/blocks/basic.py
StoreValueBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""output"", input_data.data or input_data.input","Point(row=51, column=4)","Point(row=52, column=59)",StoreValueBlock,autogpt_platform/backend/backend/blocks/basic.py
PrintToConsoleBlock,class,,"class PrintToConsoleBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(description=""The text to print to the console."")

    class Output(BlockSchema):
        status: str = SchemaField(description=""The status of the print operation."")

    def __init__(self):
        super().__init__(
            id=""f3b1c1b2-4c4f-4f0d-8d2f-4c4f0d8d2f4c"",
            description=""Print the given text to the console, this is used for a debugging purpose."",
            categories={BlockCategory.BASIC},
            input_schema=PrintToConsoleBlock.Input,
            output_schema=PrintToConsoleBlock.Output,
            test_input={""text"": ""Hello, World!""},
            test_output=(""status"", ""printed""),
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        print("">>>>> Print: "", input_data.text)
        yield ""status"", ""printed""","Point(row=55, column=0)","Point(row=75, column=33)",,autogpt_platform/backend/backend/blocks/basic.py
Input,class,,"class Input(BlockSchema):
        text: str = SchemaField(description=""The text to print to the console."")","Point(row=56, column=4)","Point(row=57, column=80)",,autogpt_platform/backend/backend/blocks/basic.py
Output,class,,"class Output(BlockSchema):
        status: str = SchemaField(description=""The status of the print operation."")","Point(row=59, column=4)","Point(row=60, column=83)",,autogpt_platform/backend/backend/blocks/basic.py
PrintToConsoleBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""f3b1c1b2-4c4f-4f0d-8d2f-4c4f0d8d2f4c"",
            description=""Print the given text to the console, this is used for a debugging purpose."",
            categories={BlockCategory.BASIC},
            input_schema=PrintToConsoleBlock.Input,
            output_schema=PrintToConsoleBlock.Output,
            test_input={""text"": ""Hello, World!""},
            test_output=(""status"", ""printed""),
        )","Point(row=62, column=4)","Point(row=71, column=9)",PrintToConsoleBlock,autogpt_platform/backend/backend/blocks/basic.py
PrintToConsoleBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        print("">>>>> Print: "", input_data.text)
        yield ""status"", ""printed""","Point(row=73, column=4)","Point(row=75, column=33)",PrintToConsoleBlock,autogpt_platform/backend/backend/blocks/basic.py
FindInDictionaryBlock,class,,"class FindInDictionaryBlock(Block):
    class Input(BlockSchema):
        input: Any = SchemaField(description=""Dictionary to lookup from"")
        key: str | int = SchemaField(description=""Key to lookup in the dictionary"")

    class Output(BlockSchema):
        output: Any = SchemaField(description=""Value found for the given key"")
        missing: Any = SchemaField(
            description=""Value of the input that missing the key""
        )

    def __init__(self):
        super().__init__(
            id=""0e50422c-6dee-4145-83d6-3a5a392f65de"",
            description=""Lookup the given key in the input dictionary/object/list and return the value."",
            input_schema=FindInDictionaryBlock.Input,
            output_schema=FindInDictionaryBlock.Output,
            test_input=[
                {""input"": {""apple"": 1, ""banana"": 2, ""cherry"": 3}, ""key"": ""banana""},
                {""input"": {""x"": 10, ""y"": 20, ""z"": 30}, ""key"": ""w""},
                {""input"": [1, 2, 3], ""key"": 1},
                {""input"": [1, 2, 3], ""key"": 3},
                {""input"": MockObject(value=""!!"", key=""key""), ""key"": ""key""},
                {""input"": [{""k1"": ""v1""}, {""k2"": ""v2""}, {""k1"": ""v3""}], ""key"": ""k1""},
            ],
            test_output=[
                (""output"", 2),
                (""missing"", {""x"": 10, ""y"": 20, ""z"": 30}),
                (""output"", 2),
                (""missing"", [1, 2, 3]),
                (""output"", ""key""),
                (""output"", [""v1"", ""v3""]),
            ],
            categories={BlockCategory.BASIC},
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        obj = input_data.input
        key = input_data.key

        if isinstance(obj, dict) and key in obj:
            yield ""output"", obj[key]
        elif isinstance(obj, list) and isinstance(key, int) and 0 <= key < len(obj):
            yield ""output"", obj[key]
        elif isinstance(obj, list) and isinstance(key, str):
            if len(obj) == 0:
                yield ""output"", []
            elif isinstance(obj[0], dict) and key in obj[0]:
                yield ""output"", [item[key] for item in obj if key in item]
            else:
                yield ""output"", [getattr(val, key) for val in obj if hasattr(val, key)]
        elif isinstance(obj, object) and isinstance(key, str) and hasattr(obj, key):
            yield ""output"", getattr(obj, key)
        else:
            yield ""missing"", input_data.input","Point(row=78, column=0)","Point(row=132, column=45)",,autogpt_platform/backend/backend/blocks/basic.py
Input,class,,"class Input(BlockSchema):
        input: Any = SchemaField(description=""Dictionary to lookup from"")
        key: str | int = SchemaField(description=""Key to lookup in the dictionary"")","Point(row=79, column=4)","Point(row=81, column=83)",,autogpt_platform/backend/backend/blocks/basic.py
Output,class,,"class Output(BlockSchema):
        output: Any = SchemaField(description=""Value found for the given key"")
        missing: Any = SchemaField(
            description=""Value of the input that missing the key""
        )","Point(row=83, column=4)","Point(row=87, column=9)",,autogpt_platform/backend/backend/blocks/basic.py
FindInDictionaryBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""0e50422c-6dee-4145-83d6-3a5a392f65de"",
            description=""Lookup the given key in the input dictionary/object/list and return the value."",
            input_schema=FindInDictionaryBlock.Input,
            output_schema=FindInDictionaryBlock.Output,
            test_input=[
                {""input"": {""apple"": 1, ""banana"": 2, ""cherry"": 3}, ""key"": ""banana""},
                {""input"": {""x"": 10, ""y"": 20, ""z"": 30}, ""key"": ""w""},
                {""input"": [1, 2, 3], ""key"": 1},
                {""input"": [1, 2, 3], ""key"": 3},
                {""input"": MockObject(value=""!!"", key=""key""), ""key"": ""key""},
                {""input"": [{""k1"": ""v1""}, {""k2"": ""v2""}, {""k1"": ""v3""}], ""key"": ""k1""},
            ],
            test_output=[
                (""output"", 2),
                (""missing"", {""x"": 10, ""y"": 20, ""z"": 30}),
                (""output"", 2),
                (""missing"", [1, 2, 3]),
                (""output"", ""key""),
                (""output"", [""v1"", ""v3""]),
            ],
            categories={BlockCategory.BASIC},
        )","Point(row=89, column=4)","Point(row=112, column=9)",FindInDictionaryBlock,autogpt_platform/backend/backend/blocks/basic.py
FindInDictionaryBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        obj = input_data.input
        key = input_data.key

        if isinstance(obj, dict) and key in obj:
            yield ""output"", obj[key]
        elif isinstance(obj, list) and isinstance(key, int) and 0 <= key < len(obj):
            yield ""output"", obj[key]
        elif isinstance(obj, list) and isinstance(key, str):
            if len(obj) == 0:
                yield ""output"", []
            elif isinstance(obj[0], dict) and key in obj[0]:
                yield ""output"", [item[key] for item in obj if key in item]
            else:
                yield ""output"", [getattr(val, key) for val in obj if hasattr(val, key)]
        elif isinstance(obj, object) and isinstance(key, str) and hasattr(obj, key):
            yield ""output"", getattr(obj, key)
        else:
            yield ""missing"", input_data.input","Point(row=114, column=4)","Point(row=132, column=45)",FindInDictionaryBlock,autogpt_platform/backend/backend/blocks/basic.py
AgentInputBlock,class,"
    This block is used to provide input to the graph.

    It takes in a value, name, description, default values list and bool to limit selection to default values.

    It Outputs the value passed as input.
","class AgentInputBlock(Block):
    """"""
    This block is used to provide input to the graph.

    It takes in a value, name, description, default values list and bool to limit selection to default values.

    It Outputs the value passed as input.
    """"""

    class Input(BlockSchema):
        name: str = SchemaField(description=""The name of the input."")
        value: Any = SchemaField(
            description=""The value to be passed as input."",
            default=None,
        )
        title: str | None = SchemaField(
            description=""The title of the input."", default=None, advanced=True
        )
        description: str | None = SchemaField(
            description=""The description of the input."",
            default=None,
            advanced=True,
        )
        placeholder_values: List[Any] = SchemaField(
            description=""The placeholder values to be passed as input."",
            default=[],
            advanced=True,
        )
        limit_to_placeholder_values: bool = SchemaField(
            description=""Whether to limit the selection to placeholder values."",
            default=False,
            advanced=True,
        )
        advanced: bool = SchemaField(
            description=""Whether to show the input in the advanced section, if the field is not required."",
            default=False,
            advanced=True,
        )
        secret: bool = SchemaField(
            description=""Whether the input should be treated as a secret."",
            default=False,
            advanced=True,
        )

    class Output(BlockSchema):
        result: Any = SchemaField(description=""The value passed as input."")

    def __init__(self):
        super().__init__(
            id=""c0a8e994-ebf1-4a9c-a4d8-89d09c86741b"",
            description=""This block is used to provide input to the graph."",
            input_schema=AgentInputBlock.Input,
            output_schema=AgentInputBlock.Output,
            test_input=[
                {
                    ""value"": ""Hello, World!"",
                    ""name"": ""input_1"",
                    ""description"": ""This is a test input."",
                    ""placeholder_values"": [],
                    ""limit_to_placeholder_values"": False,
                },
                {
                    ""value"": ""Hello, World!"",
                    ""name"": ""input_2"",
                    ""description"": ""This is a test input."",
                    ""placeholder_values"": [""Hello, World!""],
                    ""limit_to_placeholder_values"": True,
                },
            ],
            test_output=[
                (""result"", ""Hello, World!""),
                (""result"", ""Hello, World!""),
            ],
            categories={BlockCategory.INPUT, BlockCategory.BASIC},
            block_type=BlockType.INPUT,
            static_output=True,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""result"", input_data.value","Point(row=135, column=0)","Point(row=214, column=40)",,autogpt_platform/backend/backend/blocks/basic.py
Input,class,,"class Input(BlockSchema):
        name: str = SchemaField(description=""The name of the input."")
        value: Any = SchemaField(
            description=""The value to be passed as input."",
            default=None,
        )
        title: str | None = SchemaField(
            description=""The title of the input."", default=None, advanced=True
        )
        description: str | None = SchemaField(
            description=""The description of the input."",
            default=None,
            advanced=True,
        )
        placeholder_values: List[Any] = SchemaField(
            description=""The placeholder values to be passed as input."",
            default=[],
            advanced=True,
        )
        limit_to_placeholder_values: bool = SchemaField(
            description=""Whether to limit the selection to placeholder values."",
            default=False,
            advanced=True,
        )
        advanced: bool = SchemaField(
            description=""Whether to show the input in the advanced section, if the field is not required."",
            default=False,
            advanced=True,
        )
        secret: bool = SchemaField(
            description=""Whether the input should be treated as a secret."",
            default=False,
            advanced=True,
        )","Point(row=144, column=4)","Point(row=177, column=9)",,autogpt_platform/backend/backend/blocks/basic.py
Output,class,,"class Output(BlockSchema):
        result: Any = SchemaField(description=""The value passed as input."")","Point(row=179, column=4)","Point(row=180, column=75)",,autogpt_platform/backend/backend/blocks/basic.py
AgentInputBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""c0a8e994-ebf1-4a9c-a4d8-89d09c86741b"",
            description=""This block is used to provide input to the graph."",
            input_schema=AgentInputBlock.Input,
            output_schema=AgentInputBlock.Output,
            test_input=[
                {
                    ""value"": ""Hello, World!"",
                    ""name"": ""input_1"",
                    ""description"": ""This is a test input."",
                    ""placeholder_values"": [],
                    ""limit_to_placeholder_values"": False,
                },
                {
                    ""value"": ""Hello, World!"",
                    ""name"": ""input_2"",
                    ""description"": ""This is a test input."",
                    ""placeholder_values"": [""Hello, World!""],
                    ""limit_to_placeholder_values"": True,
                },
            ],
            test_output=[
                (""result"", ""Hello, World!""),
                (""result"", ""Hello, World!""),
            ],
            categories={BlockCategory.INPUT, BlockCategory.BASIC},
            block_type=BlockType.INPUT,
            static_output=True,
        )","Point(row=182, column=4)","Point(row=211, column=9)",AgentInputBlock,autogpt_platform/backend/backend/blocks/basic.py
AgentInputBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""result"", input_data.value","Point(row=213, column=4)","Point(row=214, column=40)",AgentInputBlock,autogpt_platform/backend/backend/blocks/basic.py
AgentOutputBlock,class,"
    Records the output of the graph for users to see.

    Behavior:
        If `format` is provided and the `value` is of a type that can be formatted,
        the block attempts to format the recorded_value using the `format`.
        If formatting fails or no `format` is provided, the raw `value` is output.
","class AgentOutputBlock(Block):
    """"""
    Records the output of the graph for users to see.

    Behavior:
        If `format` is provided and the `value` is of a type that can be formatted,
        the block attempts to format the recorded_value using the `format`.
        If formatting fails or no `format` is provided, the raw `value` is output.
    """"""

    class Input(BlockSchema):
        value: Any = SchemaField(
            description=""The value to be recorded as output."",
            default=None,
            advanced=False,
        )
        name: str = SchemaField(description=""The name of the output."")
        title: str | None = SchemaField(
            description=""The title of the input."", default=None, advanced=True
        )
        description: str | None = SchemaField(
            description=""The description of the output."",
            default=None,
            advanced=True,
        )
        format: str = SchemaField(
            description=""The format string to be used to format the recorded_value."",
            default="""",
            advanced=True,
        )
        advanced: bool = SchemaField(
            description=""Whether to treat the output as advanced."",
            default=False,
            advanced=True,
        )
        secret: bool = SchemaField(
            description=""Whether the output should be treated as a secret."",
            default=False,
            advanced=True,
        )

    class Output(BlockSchema):
        output: Any = SchemaField(description=""The value recorded as output."")

    def __init__(self):
        super().__init__(
            id=""363ae599-353e-4804-937e-b2ee3cef3da4"",
            description=(""Stores the output of the graph for users to see.""),
            input_schema=AgentOutputBlock.Input,
            output_schema=AgentOutputBlock.Output,
            test_input=[
                {
                    ""value"": ""Hello, World!"",
                    ""name"": ""output_1"",
                    ""description"": ""This is a test output."",
                    ""format"": ""{{ output_1 }}!!"",
                },
                {
                    ""value"": ""42"",
                    ""name"": ""output_2"",
                    ""description"": ""This is another test output."",
                    ""format"": ""{{ output_2 }}"",
                },
                {
                    ""value"": MockObject(value=""!!"", key=""key""),
                    ""name"": ""output_3"",
                    ""description"": ""This is a test output with a mock object."",
                    ""format"": ""{{ output_3 }}"",
                },
            ],
            test_output=[
                (""output"", ""Hello, World!!!""),
                (""output"", ""42""),
                (""output"", MockObject(value=""!!"", key=""key"")),
            ],
            categories={BlockCategory.OUTPUT, BlockCategory.BASIC},
            block_type=BlockType.OUTPUT,
            static_output=True,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        """"""
        Attempts to format the recorded_value using the fmt_string if provided.
        If formatting fails or no fmt_string is given, returns the original recorded_value.
        """"""
        if input_data.format:
            try:
                fmt = re.sub(r""(?<!{){[ a-zA-Z0-9_]+}"", r""{\g<0>}"", input_data.format)
                template = jinja.from_string(fmt)
                yield ""output"", template.render({input_data.name: input_data.value})
            except Exception as e:
                yield ""output"", f""Error: {e}, {input_data.value}""
        else:
            yield ""output"", input_data.value","Point(row=217, column=0)","Point(row=310, column=44)",,autogpt_platform/backend/backend/blocks/basic.py
Input,class,,"class Input(BlockSchema):
        value: Any = SchemaField(
            description=""The value to be recorded as output."",
            default=None,
            advanced=False,
        )
        name: str = SchemaField(description=""The name of the output."")
        title: str | None = SchemaField(
            description=""The title of the input."", default=None, advanced=True
        )
        description: str | None = SchemaField(
            description=""The description of the output."",
            default=None,
            advanced=True,
        )
        format: str = SchemaField(
            description=""The format string to be used to format the recorded_value."",
            default="""",
            advanced=True,
        )
        advanced: bool = SchemaField(
            description=""Whether to treat the output as advanced."",
            default=False,
            advanced=True,
        )
        secret: bool = SchemaField(
            description=""Whether the output should be treated as a secret."",
            default=False,
            advanced=True,
        )","Point(row=227, column=4)","Point(row=256, column=9)",,autogpt_platform/backend/backend/blocks/basic.py
Output,class,,"class Output(BlockSchema):
        output: Any = SchemaField(description=""The value recorded as output."")","Point(row=258, column=4)","Point(row=259, column=78)",,autogpt_platform/backend/backend/blocks/basic.py
AgentOutputBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""363ae599-353e-4804-937e-b2ee3cef3da4"",
            description=(""Stores the output of the graph for users to see.""),
            input_schema=AgentOutputBlock.Input,
            output_schema=AgentOutputBlock.Output,
            test_input=[
                {
                    ""value"": ""Hello, World!"",
                    ""name"": ""output_1"",
                    ""description"": ""This is a test output."",
                    ""format"": ""{{ output_1 }}!!"",
                },
                {
                    ""value"": ""42"",
                    ""name"": ""output_2"",
                    ""description"": ""This is another test output."",
                    ""format"": ""{{ output_2 }}"",
                },
                {
                    ""value"": MockObject(value=""!!"", key=""key""),
                    ""name"": ""output_3"",
                    ""description"": ""This is a test output with a mock object."",
                    ""format"": ""{{ output_3 }}"",
                },
            ],
            test_output=[
                (""output"", ""Hello, World!!!""),
                (""output"", ""42""),
                (""output"", MockObject(value=""!!"", key=""key"")),
            ],
            categories={BlockCategory.OUTPUT, BlockCategory.BASIC},
            block_type=BlockType.OUTPUT,
            static_output=True,
        )","Point(row=261, column=4)","Point(row=295, column=9)",AgentOutputBlock,autogpt_platform/backend/backend/blocks/basic.py
AgentOutputBlock.run,function,"
        Attempts to format the recorded_value using the fmt_string if provided.
        If formatting fails or no fmt_string is given, returns the original recorded_value.
","def run(self, input_data: Input, **kwargs) -> BlockOutput:
        """"""
        Attempts to format the recorded_value using the fmt_string if provided.
        If formatting fails or no fmt_string is given, returns the original recorded_value.
        """"""
        if input_data.format:
            try:
                fmt = re.sub(r""(?<!{){[ a-zA-Z0-9_]+}"", r""{\g<0>}"", input_data.format)
                template = jinja.from_string(fmt)
                yield ""output"", template.render({input_data.name: input_data.value})
            except Exception as e:
                yield ""output"", f""Error: {e}, {input_data.value}""
        else:
            yield ""output"", input_data.value","Point(row=297, column=4)","Point(row=310, column=44)",AgentOutputBlock,autogpt_platform/backend/backend/blocks/basic.py
AddToDictionaryBlock,class,,"class AddToDictionaryBlock(Block):
    class Input(BlockSchema):
        dictionary: dict | None = SchemaField(
            default=None,
            description=""The dictionary to add the entry to. If not provided, a new dictionary will be created."",
            placeholder='{""key1"": ""value1"", ""key2"": ""value2""}',
        )
        key: str = SchemaField(
            description=""The key for the new entry."", placeholder=""new_key""
        )
        value: Any = SchemaField(
            description=""The value for the new entry."", placeholder=""new_value""
        )

    class Output(BlockSchema):
        updated_dictionary: dict = SchemaField(
            description=""The dictionary with the new entry added.""
        )
        error: str = SchemaField(description=""Error message if the operation failed."")

    def __init__(self):
        super().__init__(
            id=""31d1064e-7446-4693-a7d4-65e5ca1180d1"",
            description=""Adds a new key-value pair to a dictionary. If no dictionary is provided, a new one is created."",
            categories={BlockCategory.BASIC},
            input_schema=AddToDictionaryBlock.Input,
            output_schema=AddToDictionaryBlock.Output,
            test_input=[
                {
                    ""dictionary"": {""existing_key"": ""existing_value""},
                    ""key"": ""new_key"",
                    ""value"": ""new_value"",
                },
                {""key"": ""first_key"", ""value"": ""first_value""},
            ],
            test_output=[
                (
                    ""updated_dictionary"",
                    {""existing_key"": ""existing_value"", ""new_key"": ""new_value""},
                ),
                (""updated_dictionary"", {""first_key"": ""first_value""}),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        # If no dictionary is provided, create a new one
        if input_data.dictionary is None:
            updated_dict = {}
        else:
            # Create a copy of the input dictionary to avoid modifying the original
            updated_dict = input_data.dictionary.copy()

        # Add the new key-value pair
        updated_dict[input_data.key] = input_data.value

        yield ""updated_dictionary"", updated_dict","Point(row=313, column=0)","Point(row=368, column=48)",,autogpt_platform/backend/backend/blocks/basic.py
Input,class,,"class Input(BlockSchema):
        dictionary: dict | None = SchemaField(
            default=None,
            description=""The dictionary to add the entry to. If not provided, a new dictionary will be created."",
            placeholder='{""key1"": ""value1"", ""key2"": ""value2""}',
        )
        key: str = SchemaField(
            description=""The key for the new entry."", placeholder=""new_key""
        )
        value: Any = SchemaField(
            description=""The value for the new entry."", placeholder=""new_value""
        )","Point(row=314, column=4)","Point(row=325, column=9)",,autogpt_platform/backend/backend/blocks/basic.py
Output,class,,"class Output(BlockSchema):
        updated_dictionary: dict = SchemaField(
            description=""The dictionary with the new entry added.""
        )
        error: str = SchemaField(description=""Error message if the operation failed."")","Point(row=327, column=4)","Point(row=331, column=86)",,autogpt_platform/backend/backend/blocks/basic.py
AddToDictionaryBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""31d1064e-7446-4693-a7d4-65e5ca1180d1"",
            description=""Adds a new key-value pair to a dictionary. If no dictionary is provided, a new one is created."",
            categories={BlockCategory.BASIC},
            input_schema=AddToDictionaryBlock.Input,
            output_schema=AddToDictionaryBlock.Output,
            test_input=[
                {
                    ""dictionary"": {""existing_key"": ""existing_value""},
                    ""key"": ""new_key"",
                    ""value"": ""new_value"",
                },
                {""key"": ""first_key"", ""value"": ""first_value""},
            ],
            test_output=[
                (
                    ""updated_dictionary"",
                    {""existing_key"": ""existing_value"", ""new_key"": ""new_value""},
                ),
                (""updated_dictionary"", {""first_key"": ""first_value""}),
            ],
        )","Point(row=333, column=4)","Point(row=355, column=9)",AddToDictionaryBlock,autogpt_platform/backend/backend/blocks/basic.py
AddToDictionaryBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        # If no dictionary is provided, create a new one
        if input_data.dictionary is None:
            updated_dict = {}
        else:
            # Create a copy of the input dictionary to avoid modifying the original
            updated_dict = input_data.dictionary.copy()

        # Add the new key-value pair
        updated_dict[input_data.key] = input_data.value

        yield ""updated_dictionary"", updated_dict","Point(row=357, column=4)","Point(row=368, column=48)",AddToDictionaryBlock,autogpt_platform/backend/backend/blocks/basic.py
AddToListBlock,class,,"class AddToListBlock(Block):
    class Input(BlockSchema):
        list: List[Any] | None = SchemaField(
            default=None,
            description=""The list to add the entry to. If not provided, a new list will be created."",
            placeholder='[1, ""string"", {""key"": ""value""}]',
        )
        entry: Any = SchemaField(
            description=""The entry to add to the list. Can be of any type (string, int, dict, etc.)."",
            placeholder='{""new_key"": ""new_value""}',
        )
        position: int | None = SchemaField(
            default=None,
            description=""The position to insert the new entry. If not provided, the entry will be appended to the end of the list."",
            placeholder=""0"",
        )

    class Output(BlockSchema):
        updated_list: List[Any] = SchemaField(
            description=""The list with the new entry added.""
        )
        error: str = SchemaField(description=""Error message if the operation failed."")

    def __init__(self):
        super().__init__(
            id=""aeb08fc1-2fc1-4141-bc8e-f758f183a822"",
            description=""Adds a new entry to a list. The entry can be of any type. If no list is provided, a new one is created."",
            categories={BlockCategory.BASIC},
            input_schema=AddToListBlock.Input,
            output_schema=AddToListBlock.Output,
            test_input=[
                {
                    ""list"": [1, ""string"", {""existing_key"": ""existing_value""}],
                    ""entry"": {""new_key"": ""new_value""},
                    ""position"": 1,
                },
                {""entry"": ""first_entry""},
                {""list"": [""a"", ""b"", ""c""], ""entry"": ""d""},
            ],
            test_output=[
                (
                    ""updated_list"",
                    [
                        1,
                        {""new_key"": ""new_value""},
                        ""string"",
                        {""existing_key"": ""existing_value""},
                    ],
                ),
                (""updated_list"", [""first_entry""]),
                (""updated_list"", [""a"", ""b"", ""c"", ""d""]),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        # If no list is provided, create a new one
        if input_data.list is None:
            updated_list = []
        else:
            # Create a copy of the input list to avoid modifying the original
            updated_list = input_data.list.copy()

        # Add the new entry
        if input_data.position is None:
            updated_list.append(input_data.entry)
        else:
            updated_list.insert(input_data.position, input_data.entry)

        yield ""updated_list"", updated_list","Point(row=371, column=0)","Point(row=439, column=42)",,autogpt_platform/backend/backend/blocks/basic.py
Input,class,,"class Input(BlockSchema):
        list: List[Any] | None = SchemaField(
            default=None,
            description=""The list to add the entry to. If not provided, a new list will be created."",
            placeholder='[1, ""string"", {""key"": ""value""}]',
        )
        entry: Any = SchemaField(
            description=""The entry to add to the list. Can be of any type (string, int, dict, etc.)."",
            placeholder='{""new_key"": ""new_value""}',
        )
        position: int | None = SchemaField(
            default=None,
            description=""The position to insert the new entry. If not provided, the entry will be appended to the end of the list."",
            placeholder=""0"",
        )","Point(row=372, column=4)","Point(row=386, column=9)",,autogpt_platform/backend/backend/blocks/basic.py
Output,class,,"class Output(BlockSchema):
        updated_list: List[Any] = SchemaField(
            description=""The list with the new entry added.""
        )
        error: str = SchemaField(description=""Error message if the operation failed."")","Point(row=388, column=4)","Point(row=392, column=86)",,autogpt_platform/backend/backend/blocks/basic.py
AddToListBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""aeb08fc1-2fc1-4141-bc8e-f758f183a822"",
            description=""Adds a new entry to a list. The entry can be of any type. If no list is provided, a new one is created."",
            categories={BlockCategory.BASIC},
            input_schema=AddToListBlock.Input,
            output_schema=AddToListBlock.Output,
            test_input=[
                {
                    ""list"": [1, ""string"", {""existing_key"": ""existing_value""}],
                    ""entry"": {""new_key"": ""new_value""},
                    ""position"": 1,
                },
                {""entry"": ""first_entry""},
                {""list"": [""a"", ""b"", ""c""], ""entry"": ""d""},
            ],
            test_output=[
                (
                    ""updated_list"",
                    [
                        1,
                        {""new_key"": ""new_value""},
                        ""string"",
                        {""existing_key"": ""existing_value""},
                    ],
                ),
                (""updated_list"", [""first_entry""]),
                (""updated_list"", [""a"", ""b"", ""c"", ""d""]),
            ],
        )","Point(row=394, column=4)","Point(row=423, column=9)",AddToListBlock,autogpt_platform/backend/backend/blocks/basic.py
AddToListBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        # If no list is provided, create a new one
        if input_data.list is None:
            updated_list = []
        else:
            # Create a copy of the input list to avoid modifying the original
            updated_list = input_data.list.copy()

        # Add the new entry
        if input_data.position is None:
            updated_list.append(input_data.entry)
        else:
            updated_list.insert(input_data.position, input_data.entry)

        yield ""updated_list"", updated_list","Point(row=425, column=4)","Point(row=439, column=42)",AddToListBlock,autogpt_platform/backend/backend/blocks/basic.py
NoteBlock,class,,"class NoteBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(description=""The text to display in the sticky note."")

    class Output(BlockSchema):
        output: str = SchemaField(description=""The text to display in the sticky note."")

    def __init__(self):
        super().__init__(
            id=""cc10ff7b-7753-4ff2-9af6-9399b1a7eddc"",
            description=""This block is used to display a sticky note with the given text."",
            categories={BlockCategory.BASIC},
            input_schema=NoteBlock.Input,
            output_schema=NoteBlock.Output,
            test_input={""text"": ""Hello, World!""},
            test_output=[
                (""output"", ""Hello, World!""),
            ],
            block_type=BlockType.NOTE,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""output"", input_data.text","Point(row=442, column=0)","Point(row=464, column=39)",,autogpt_platform/backend/backend/blocks/basic.py
Input,class,,"class Input(BlockSchema):
        text: str = SchemaField(description=""The text to display in the sticky note."")","Point(row=443, column=4)","Point(row=444, column=86)",,autogpt_platform/backend/backend/blocks/basic.py
Output,class,,"class Output(BlockSchema):
        output: str = SchemaField(description=""The text to display in the sticky note."")","Point(row=446, column=4)","Point(row=447, column=88)",,autogpt_platform/backend/backend/blocks/basic.py
NoteBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""cc10ff7b-7753-4ff2-9af6-9399b1a7eddc"",
            description=""This block is used to display a sticky note with the given text."",
            categories={BlockCategory.BASIC},
            input_schema=NoteBlock.Input,
            output_schema=NoteBlock.Output,
            test_input={""text"": ""Hello, World!""},
            test_output=[
                (""output"", ""Hello, World!""),
            ],
            block_type=BlockType.NOTE,
        )","Point(row=449, column=4)","Point(row=461, column=9)",NoteBlock,autogpt_platform/backend/backend/blocks/basic.py
NoteBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""output"", input_data.text","Point(row=463, column=4)","Point(row=464, column=39)",NoteBlock,autogpt_platform/backend/backend/blocks/basic.py
HttpMethod,class,,"class HttpMethod(Enum):
    GET = ""GET""
    POST = ""POST""
    PUT = ""PUT""
    DELETE = ""DELETE""
    PATCH = ""PATCH""
    OPTIONS = ""OPTIONS""
    HEAD = ""HEAD""","Point(row=9, column=0)","Point(row=16, column=17)",,autogpt_platform/backend/backend/blocks/http.py
SendWebRequestBlock,class,,"class SendWebRequestBlock(Block):
    class Input(BlockSchema):
        url: str = SchemaField(
            description=""The URL to send the request to"",
            placeholder=""https://api.example.com"",
        )
        method: HttpMethod = SchemaField(
            description=""The HTTP method to use for the request"",
            default=HttpMethod.POST,
        )
        headers: dict[str, str] = SchemaField(
            description=""The headers to include in the request"",
            default={},
        )
        json_format: bool = SchemaField(
            title=""JSON format"",
            description=""Whether to send and receive body as JSON"",
            default=True,
        )
        body: Any = SchemaField(
            description=""The body of the request"",
            default=None,
        )

    class Output(BlockSchema):
        response: object = SchemaField(description=""The response from the server"")
        client_error: object = SchemaField(description=""The error on 4xx status codes"")
        server_error: object = SchemaField(description=""The error on 5xx status codes"")

    def __init__(self):
        super().__init__(
            id=""6595ae1f-b924-42cb-9a41-551a0611c4b4"",
            description=""This block makes an HTTP request to the given URL."",
            categories={BlockCategory.OUTPUT},
            input_schema=SendWebRequestBlock.Input,
            output_schema=SendWebRequestBlock.Output,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        if isinstance(input_data.body, str):
            input_data.body = json.loads(input_data.body)

        response = requests.request(
            input_data.method.value,
            input_data.url,
            headers=input_data.headers,
            json=input_data.body if input_data.json_format else None,
            data=input_data.body if not input_data.json_format else None,
        )
        result = response.json() if input_data.json_format else response.text

        if response.status_code // 100 == 2:
            yield ""response"", result
        elif response.status_code // 100 == 4:
            yield ""client_error"", result
        elif response.status_code // 100 == 5:
            yield ""server_error"", result
        else:
            raise ValueError(f""Unexpected status code: {response.status_code}"")","Point(row=19, column=0)","Point(row=77, column=79)",,autogpt_platform/backend/backend/blocks/http.py
Input,class,,"class Input(BlockSchema):
        url: str = SchemaField(
            description=""The URL to send the request to"",
            placeholder=""https://api.example.com"",
        )
        method: HttpMethod = SchemaField(
            description=""The HTTP method to use for the request"",
            default=HttpMethod.POST,
        )
        headers: dict[str, str] = SchemaField(
            description=""The headers to include in the request"",
            default={},
        )
        json_format: bool = SchemaField(
            title=""JSON format"",
            description=""Whether to send and receive body as JSON"",
            default=True,
        )
        body: Any = SchemaField(
            description=""The body of the request"",
            default=None,
        )","Point(row=20, column=4)","Point(row=41, column=9)",,autogpt_platform/backend/backend/blocks/http.py
Output,class,,"class Output(BlockSchema):
        response: object = SchemaField(description=""The response from the server"")
        client_error: object = SchemaField(description=""The error on 4xx status codes"")
        server_error: object = SchemaField(description=""The error on 5xx status codes"")","Point(row=43, column=4)","Point(row=46, column=87)",,autogpt_platform/backend/backend/blocks/http.py
SendWebRequestBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""6595ae1f-b924-42cb-9a41-551a0611c4b4"",
            description=""This block makes an HTTP request to the given URL."",
            categories={BlockCategory.OUTPUT},
            input_schema=SendWebRequestBlock.Input,
            output_schema=SendWebRequestBlock.Output,
        )","Point(row=48, column=4)","Point(row=55, column=9)",SendWebRequestBlock,autogpt_platform/backend/backend/blocks/http.py
SendWebRequestBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        if isinstance(input_data.body, str):
            input_data.body = json.loads(input_data.body)

        response = requests.request(
            input_data.method.value,
            input_data.url,
            headers=input_data.headers,
            json=input_data.body if input_data.json_format else None,
            data=input_data.body if not input_data.json_format else None,
        )
        result = response.json() if input_data.json_format else response.text

        if response.status_code // 100 == 2:
            yield ""response"", result
        elif response.status_code // 100 == 4:
            yield ""client_error"", result
        elif response.status_code // 100 == 5:
            yield ""server_error"", result
        else:
            raise ValueError(f""Unexpected status code: {response.status_code}"")","Point(row=57, column=4)","Point(row=77, column=79)",SendWebRequestBlock,autogpt_platform/backend/backend/blocks/http.py
ReadCsvBlock,class,,"class ReadCsvBlock(Block):
    class Input(BlockSchema):
        contents: str = SchemaField(
            description=""The contents of the CSV file to read"",
            placeholder=""a, b, c\n1,2,3\n4,5,6"",
        )
        delimiter: str = SchemaField(
            description=""The delimiter used in the CSV file"",
            default="","",
        )
        quotechar: str = SchemaField(
            description=""The character used to quote fields"",
            default='""',
        )
        escapechar: str = SchemaField(
            description=""The character used to escape the delimiter"",
            default=""\\"",
        )
        has_header: bool = SchemaField(
            description=""Whether the CSV file has a header row"",
            default=True,
        )
        skip_rows: int = SchemaField(
            description=""The number of rows to skip from the start of the file"",
            default=0,
        )
        strip: bool = SchemaField(
            description=""Whether to strip whitespace from the values"",
            default=True,
        )
        skip_columns: list[str] = SchemaField(
            description=""The columns to skip from the start of the row"",
            default=[],
        )

    class Output(BlockSchema):
        row: dict[str, str] = SchemaField(
            description=""The data produced from each row in the CSV file""
        )
        all_data: list[dict[str, str]] = SchemaField(
            description=""All the data in the CSV file as a list of rows""
        )

    def __init__(self):
        super().__init__(
            id=""acf7625e-d2cb-4941-bfeb-2819fc6fc015"",
            input_schema=ReadCsvBlock.Input,
            output_schema=ReadCsvBlock.Output,
            description=""Reads a CSV file and outputs the data as a list of dictionaries and individual rows via rows."",
            contributors=[ContributorDetails(name=""Nicholas Tindle"")],
            categories={BlockCategory.TEXT, BlockCategory.DATA},
            test_input={
                ""contents"": ""a, b, c\n1,2,3\n4,5,6"",
            },
            test_output=[
                (""row"", {""a"": ""1"", ""b"": ""2"", ""c"": ""3""}),
                (""row"", {""a"": ""4"", ""b"": ""5"", ""c"": ""6""}),
                (
                    ""all_data"",
                    [
                        {""a"": ""1"", ""b"": ""2"", ""c"": ""3""},
                        {""a"": ""4"", ""b"": ""5"", ""c"": ""6""},
                    ],
                ),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        import csv
        from io import StringIO

        csv_file = StringIO(input_data.contents)
        reader = csv.reader(
            csv_file,
            delimiter=input_data.delimiter,
            quotechar=input_data.quotechar,
            escapechar=input_data.escapechar,
        )

        header = None
        if input_data.has_header:
            header = next(reader)
            if input_data.strip:
                header = [h.strip() for h in header]

        for _ in range(input_data.skip_rows):
            next(reader)

        def process_row(row):
            data = {}
            for i, value in enumerate(row):
                if i not in input_data.skip_columns:
                    if input_data.has_header and header:
                        data[header[i]] = value.strip() if input_data.strip else value
                    else:
                        data[str(i)] = value.strip() if input_data.strip else value
            return data

        all_data = []
        for row in reader:
            processed_row = process_row(row)
            all_data.append(processed_row)
            yield ""row"", processed_row

        yield ""all_data"", all_data","Point(row=4, column=0)","Point(row=108, column=34)",,autogpt_platform/backend/backend/blocks/csv.py
Input,class,,"class Input(BlockSchema):
        contents: str = SchemaField(
            description=""The contents of the CSV file to read"",
            placeholder=""a, b, c\n1,2,3\n4,5,6"",
        )
        delimiter: str = SchemaField(
            description=""The delimiter used in the CSV file"",
            default="","",
        )
        quotechar: str = SchemaField(
            description=""The character used to quote fields"",
            default='""',
        )
        escapechar: str = SchemaField(
            description=""The character used to escape the delimiter"",
            default=""\\"",
        )
        has_header: bool = SchemaField(
            description=""Whether the CSV file has a header row"",
            default=True,
        )
        skip_rows: int = SchemaField(
            description=""The number of rows to skip from the start of the file"",
            default=0,
        )
        strip: bool = SchemaField(
            description=""Whether to strip whitespace from the values"",
            default=True,
        )
        skip_columns: list[str] = SchemaField(
            description=""The columns to skip from the start of the row"",
            default=[],
        )","Point(row=5, column=4)","Point(row=37, column=9)",,autogpt_platform/backend/backend/blocks/csv.py
Output,class,,"class Output(BlockSchema):
        row: dict[str, str] = SchemaField(
            description=""The data produced from each row in the CSV file""
        )
        all_data: list[dict[str, str]] = SchemaField(
            description=""All the data in the CSV file as a list of rows""
        )","Point(row=39, column=4)","Point(row=45, column=9)",,autogpt_platform/backend/backend/blocks/csv.py
ReadCsvBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""acf7625e-d2cb-4941-bfeb-2819fc6fc015"",
            input_schema=ReadCsvBlock.Input,
            output_schema=ReadCsvBlock.Output,
            description=""Reads a CSV file and outputs the data as a list of dictionaries and individual rows via rows."",
            contributors=[ContributorDetails(name=""Nicholas Tindle"")],
            categories={BlockCategory.TEXT, BlockCategory.DATA},
            test_input={
                ""contents"": ""a, b, c\n1,2,3\n4,5,6"",
            },
            test_output=[
                (""row"", {""a"": ""1"", ""b"": ""2"", ""c"": ""3""}),
                (""row"", {""a"": ""4"", ""b"": ""5"", ""c"": ""6""}),
                (
                    ""all_data"",
                    [
                        {""a"": ""1"", ""b"": ""2"", ""c"": ""3""},
                        {""a"": ""4"", ""b"": ""5"", ""c"": ""6""},
                    ],
                ),
            ],
        )","Point(row=47, column=4)","Point(row=69, column=9)",ReadCsvBlock,autogpt_platform/backend/backend/blocks/csv.py
ReadCsvBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        import csv
        from io import StringIO

        csv_file = StringIO(input_data.contents)
        reader = csv.reader(
            csv_file,
            delimiter=input_data.delimiter,
            quotechar=input_data.quotechar,
            escapechar=input_data.escapechar,
        )

        header = None
        if input_data.has_header:
            header = next(reader)
            if input_data.strip:
                header = [h.strip() for h in header]

        for _ in range(input_data.skip_rows):
            next(reader)

        def process_row(row):
            data = {}
            for i, value in enumerate(row):
                if i not in input_data.skip_columns:
                    if input_data.has_header and header:
                        data[header[i]] = value.strip() if input_data.strip else value
                    else:
                        data[str(i)] = value.strip() if input_data.strip else value
            return data

        all_data = []
        for row in reader:
            processed_row = process_row(row)
            all_data.append(processed_row)
            yield ""row"", processed_row

        yield ""all_data"", all_data","Point(row=71, column=4)","Point(row=108, column=34)",ReadCsvBlock,autogpt_platform/backend/backend/blocks/csv.py
ReadCsvBlock.run.process_row,function,,"def process_row(row):
            data = {}
            for i, value in enumerate(row):
                if i not in input_data.skip_columns:
                    if input_data.has_header and header:
                        data[header[i]] = value.strip() if input_data.strip else value
                    else:
                        data[str(i)] = value.strip() if input_data.strip else value
            return data","Point(row=92, column=8)","Point(row=100, column=23)",ReadCsvBlock,autogpt_platform/backend/backend/blocks/csv.py
MatchTextPatternBlock,class,,"class MatchTextPatternBlock(Block):
    class Input(BlockSchema):
        text: Any = SchemaField(description=""Text to match"")
        match: str = SchemaField(description=""Pattern (Regex) to match"")
        data: Any = SchemaField(description=""Data to be forwarded to output"")
        case_sensitive: bool = SchemaField(
            description=""Case sensitive match"", default=True
        )
        dot_all: bool = SchemaField(description=""Dot matches all"", default=True)

    class Output(BlockSchema):
        positive: Any = SchemaField(description=""Output data if match is found"")
        negative: Any = SchemaField(description=""Output data if match is not found"")

    def __init__(self):
        super().__init__(
            id=""3060088f-6ed9-4928-9ba7-9c92823a7ccd"",
            description=""Matches text against a regex pattern and forwards data to positive or negative output based on the match."",
            categories={BlockCategory.TEXT},
            input_schema=MatchTextPatternBlock.Input,
            output_schema=MatchTextPatternBlock.Output,
            test_input=[
                {""text"": ""ABC"", ""match"": ""ab"", ""data"": ""X"", ""case_sensitive"": False},
                {""text"": ""ABC"", ""match"": ""ab"", ""data"": ""Y"", ""case_sensitive"": True},
                {""text"": ""Hello World!"", ""match"": "".orld.+"", ""data"": ""Z""},
                {""text"": ""Hello World!"", ""match"": ""World![a-z]+"", ""data"": ""Z""},
            ],
            test_output=[
                (""positive"", ""X""),
                (""negative"", ""Y""),
                (""positive"", ""Z""),
                (""negative"", ""Z""),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        output = input_data.data or input_data.text
        flags = 0
        if not input_data.case_sensitive:
            flags = flags | re.IGNORECASE
        if input_data.dot_all:
            flags = flags | re.DOTALL

        if isinstance(input_data.text, str):
            text = input_data.text
        else:
            text = json.dumps(input_data.text)

        if re.search(input_data.match, text, flags=flags):
            yield ""positive"", output
        else:
            yield ""negative"", output","Point(row=12, column=0)","Point(row=63, column=36)",,autogpt_platform/backend/backend/blocks/text.py
Input,class,,"class Input(BlockSchema):
        text: Any = SchemaField(description=""Text to match"")
        match: str = SchemaField(description=""Pattern (Regex) to match"")
        data: Any = SchemaField(description=""Data to be forwarded to output"")
        case_sensitive: bool = SchemaField(
            description=""Case sensitive match"", default=True
        )
        dot_all: bool = SchemaField(description=""Dot matches all"", default=True)","Point(row=13, column=4)","Point(row=20, column=80)",,autogpt_platform/backend/backend/blocks/text.py
Output,class,,"class Output(BlockSchema):
        positive: Any = SchemaField(description=""Output data if match is found"")
        negative: Any = SchemaField(description=""Output data if match is not found"")","Point(row=22, column=4)","Point(row=24, column=84)",,autogpt_platform/backend/backend/blocks/text.py
MatchTextPatternBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""3060088f-6ed9-4928-9ba7-9c92823a7ccd"",
            description=""Matches text against a regex pattern and forwards data to positive or negative output based on the match."",
            categories={BlockCategory.TEXT},
            input_schema=MatchTextPatternBlock.Input,
            output_schema=MatchTextPatternBlock.Output,
            test_input=[
                {""text"": ""ABC"", ""match"": ""ab"", ""data"": ""X"", ""case_sensitive"": False},
                {""text"": ""ABC"", ""match"": ""ab"", ""data"": ""Y"", ""case_sensitive"": True},
                {""text"": ""Hello World!"", ""match"": "".orld.+"", ""data"": ""Z""},
                {""text"": ""Hello World!"", ""match"": ""World![a-z]+"", ""data"": ""Z""},
            ],
            test_output=[
                (""positive"", ""X""),
                (""negative"", ""Y""),
                (""positive"", ""Z""),
                (""negative"", ""Z""),
            ],
        )","Point(row=26, column=4)","Point(row=45, column=9)",MatchTextPatternBlock,autogpt_platform/backend/backend/blocks/text.py
MatchTextPatternBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        output = input_data.data or input_data.text
        flags = 0
        if not input_data.case_sensitive:
            flags = flags | re.IGNORECASE
        if input_data.dot_all:
            flags = flags | re.DOTALL

        if isinstance(input_data.text, str):
            text = input_data.text
        else:
            text = json.dumps(input_data.text)

        if re.search(input_data.match, text, flags=flags):
            yield ""positive"", output
        else:
            yield ""negative"", output","Point(row=47, column=4)","Point(row=63, column=36)",MatchTextPatternBlock,autogpt_platform/backend/backend/blocks/text.py
ExtractTextInformationBlock,class,,"class ExtractTextInformationBlock(Block):
    class Input(BlockSchema):
        text: Any = SchemaField(description=""Text to parse"")
        pattern: str = SchemaField(description=""Pattern (Regex) to parse"")
        group: int = SchemaField(description=""Group number to extract"", default=0)
        case_sensitive: bool = SchemaField(
            description=""Case sensitive match"", default=True
        )
        dot_all: bool = SchemaField(description=""Dot matches all"", default=True)

    class Output(BlockSchema):
        positive: str = SchemaField(description=""Extracted text"")
        negative: str = SchemaField(description=""Original text"")

    def __init__(self):
        super().__init__(
            id=""3146e4fe-2cdd-4f29-bd12-0c9d5bb4deb0"",
            description=""This block extracts the text from the given text using the pattern (regex)."",
            categories={BlockCategory.TEXT},
            input_schema=ExtractTextInformationBlock.Input,
            output_schema=ExtractTextInformationBlock.Output,
            test_input=[
                {""text"": ""Hello, World!"", ""pattern"": ""Hello, (.+)"", ""group"": 1},
                {""text"": ""Hello, World!"", ""pattern"": ""Hello, (.+)"", ""group"": 0},
                {""text"": ""Hello, World!"", ""pattern"": ""Hello, (.+)"", ""group"": 2},
                {""text"": ""Hello, World!"", ""pattern"": ""hello,"", ""case_sensitive"": False},
            ],
            test_output=[
                (""positive"", ""World!""),
                (""positive"", ""Hello, World!""),
                (""negative"", ""Hello, World!""),
                (""positive"", ""Hello,""),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        flags = 0
        if not input_data.case_sensitive:
            flags = flags | re.IGNORECASE
        if input_data.dot_all:
            flags = flags | re.DOTALL

        if isinstance(input_data.text, str):
            text = input_data.text
        else:
            text = json.dumps(input_data.text)

        match = re.search(input_data.pattern, text, flags)
        if match and input_data.group <= len(match.groups()):
            yield ""positive"", match.group(input_data.group)
        else:
            yield ""negative"", text","Point(row=66, column=0)","Point(row=117, column=34)",,autogpt_platform/backend/backend/blocks/text.py
Input,class,,"class Input(BlockSchema):
        text: Any = SchemaField(description=""Text to parse"")
        pattern: str = SchemaField(description=""Pattern (Regex) to parse"")
        group: int = SchemaField(description=""Group number to extract"", default=0)
        case_sensitive: bool = SchemaField(
            description=""Case sensitive match"", default=True
        )
        dot_all: bool = SchemaField(description=""Dot matches all"", default=True)","Point(row=67, column=4)","Point(row=74, column=80)",,autogpt_platform/backend/backend/blocks/text.py
Output,class,,"class Output(BlockSchema):
        positive: str = SchemaField(description=""Extracted text"")
        negative: str = SchemaField(description=""Original text"")","Point(row=76, column=4)","Point(row=78, column=64)",,autogpt_platform/backend/backend/blocks/text.py
ExtractTextInformationBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""3146e4fe-2cdd-4f29-bd12-0c9d5bb4deb0"",
            description=""This block extracts the text from the given text using the pattern (regex)."",
            categories={BlockCategory.TEXT},
            input_schema=ExtractTextInformationBlock.Input,
            output_schema=ExtractTextInformationBlock.Output,
            test_input=[
                {""text"": ""Hello, World!"", ""pattern"": ""Hello, (.+)"", ""group"": 1},
                {""text"": ""Hello, World!"", ""pattern"": ""Hello, (.+)"", ""group"": 0},
                {""text"": ""Hello, World!"", ""pattern"": ""Hello, (.+)"", ""group"": 2},
                {""text"": ""Hello, World!"", ""pattern"": ""hello,"", ""case_sensitive"": False},
            ],
            test_output=[
                (""positive"", ""World!""),
                (""positive"", ""Hello, World!""),
                (""negative"", ""Hello, World!""),
                (""positive"", ""Hello,""),
            ],
        )","Point(row=80, column=4)","Point(row=99, column=9)",ExtractTextInformationBlock,autogpt_platform/backend/backend/blocks/text.py
ExtractTextInformationBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        flags = 0
        if not input_data.case_sensitive:
            flags = flags | re.IGNORECASE
        if input_data.dot_all:
            flags = flags | re.DOTALL

        if isinstance(input_data.text, str):
            text = input_data.text
        else:
            text = json.dumps(input_data.text)

        match = re.search(input_data.pattern, text, flags)
        if match and input_data.group <= len(match.groups()):
            yield ""positive"", match.group(input_data.group)
        else:
            yield ""negative"", text","Point(row=101, column=4)","Point(row=117, column=34)",ExtractTextInformationBlock,autogpt_platform/backend/backend/blocks/text.py
FillTextTemplateBlock,class,,"class FillTextTemplateBlock(Block):
    class Input(BlockSchema):
        values: dict[str, Any] = SchemaField(
            description=""Values (dict) to be used in format""
        )
        format: str = SchemaField(
            description=""Template to format the text using `values`""
        )

    class Output(BlockSchema):
        output: str = SchemaField(description=""Formatted text"")

    def __init__(self):
        super().__init__(
            id=""db7d8f02-2f44-4c55-ab7a-eae0941f0c30"",
            description=""This block formats the given texts using the format template."",
            categories={BlockCategory.TEXT},
            input_schema=FillTextTemplateBlock.Input,
            output_schema=FillTextTemplateBlock.Output,
            test_input=[
                {
                    ""values"": {""name"": ""Alice"", ""hello"": ""Hello"", ""world"": ""World!""},
                    ""format"": ""{hello}, {world} {{name}}"",
                },
                {
                    ""values"": {""list"": [""Hello"", "" World!""]},
                    ""format"": ""{% for item in list %}{{ item }}{% endfor %}"",
                },
            ],
            test_output=[
                (""output"", ""Hello, World! Alice""),
                (""output"", ""Hello World!""),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        # For python.format compatibility: replace all {...} with {{..}}.
        # But avoid replacing {{...}} to {{{...}}}.
        fmt = re.sub(r""(?<!{){[ a-zA-Z0-9_]+}"", r""{\g<0>}"", input_data.format)
        template = jinja.from_string(fmt)
        yield ""output"", template.render(**input_data.values)","Point(row=120, column=0)","Point(row=160, column=60)",,autogpt_platform/backend/backend/blocks/text.py
Input,class,,"class Input(BlockSchema):
        values: dict[str, Any] = SchemaField(
            description=""Values (dict) to be used in format""
        )
        format: str = SchemaField(
            description=""Template to format the text using `values`""
        )","Point(row=121, column=4)","Point(row=127, column=9)",,autogpt_platform/backend/backend/blocks/text.py
Output,class,,"class Output(BlockSchema):
        output: str = SchemaField(description=""Formatted text"")","Point(row=129, column=4)","Point(row=130, column=63)",,autogpt_platform/backend/backend/blocks/text.py
FillTextTemplateBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""db7d8f02-2f44-4c55-ab7a-eae0941f0c30"",
            description=""This block formats the given texts using the format template."",
            categories={BlockCategory.TEXT},
            input_schema=FillTextTemplateBlock.Input,
            output_schema=FillTextTemplateBlock.Output,
            test_input=[
                {
                    ""values"": {""name"": ""Alice"", ""hello"": ""Hello"", ""world"": ""World!""},
                    ""format"": ""{hello}, {world} {{name}}"",
                },
                {
                    ""values"": {""list"": [""Hello"", "" World!""]},
                    ""format"": ""{% for item in list %}{{ item }}{% endfor %}"",
                },
            ],
            test_output=[
                (""output"", ""Hello, World! Alice""),
                (""output"", ""Hello World!""),
            ],
        )","Point(row=132, column=4)","Point(row=153, column=9)",FillTextTemplateBlock,autogpt_platform/backend/backend/blocks/text.py
FillTextTemplateBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        # For python.format compatibility: replace all {...} with {{..}}.
        # But avoid replacing {{...}} to {{{...}}}.
        fmt = re.sub(r""(?<!{){[ a-zA-Z0-9_]+}"", r""{\g<0>}"", input_data.format)
        template = jinja.from_string(fmt)
        yield ""output"", template.render(**input_data.values)","Point(row=155, column=4)","Point(row=160, column=60)",FillTextTemplateBlock,autogpt_platform/backend/backend/blocks/text.py
CombineTextsBlock,class,,"class CombineTextsBlock(Block):
    class Input(BlockSchema):
        input: list[str] = SchemaField(description=""text input to combine"")
        delimiter: str = SchemaField(
            description=""Delimiter to combine texts"", default=""""
        )

    class Output(BlockSchema):
        output: str = SchemaField(description=""Combined text"")

    def __init__(self):
        super().__init__(
            id=""e30a4d42-7b7d-4e6a-b36e-1f9b8e3b7d85"",
            description=""This block combines multiple input texts into a single output text."",
            categories={BlockCategory.TEXT},
            input_schema=CombineTextsBlock.Input,
            output_schema=CombineTextsBlock.Output,
            test_input=[
                {""input"": [""Hello world I like "", ""cake and to go for walks""]},
                {""input"": [""This is a test"", ""Hi!""], ""delimiter"": ""! ""},
            ],
            test_output=[
                (""output"", ""Hello world I like cake and to go for walks""),
                (""output"", ""This is a test! Hi!""),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        combined_text = input_data.delimiter.join(input_data.input)
        yield ""output"", combined_text","Point(row=163, column=0)","Point(row=192, column=37)",,autogpt_platform/backend/backend/blocks/text.py
Input,class,,"class Input(BlockSchema):
        input: list[str] = SchemaField(description=""text input to combine"")
        delimiter: str = SchemaField(
            description=""Delimiter to combine texts"", default=""""
        )","Point(row=164, column=4)","Point(row=168, column=9)",,autogpt_platform/backend/backend/blocks/text.py
Output,class,,"class Output(BlockSchema):
        output: str = SchemaField(description=""Combined text"")","Point(row=170, column=4)","Point(row=171, column=62)",,autogpt_platform/backend/backend/blocks/text.py
CombineTextsBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""e30a4d42-7b7d-4e6a-b36e-1f9b8e3b7d85"",
            description=""This block combines multiple input texts into a single output text."",
            categories={BlockCategory.TEXT},
            input_schema=CombineTextsBlock.Input,
            output_schema=CombineTextsBlock.Output,
            test_input=[
                {""input"": [""Hello world I like "", ""cake and to go for walks""]},
                {""input"": [""This is a test"", ""Hi!""], ""delimiter"": ""! ""},
            ],
            test_output=[
                (""output"", ""Hello world I like cake and to go for walks""),
                (""output"", ""This is a test! Hi!""),
            ],
        )","Point(row=173, column=4)","Point(row=188, column=9)",CombineTextsBlock,autogpt_platform/backend/backend/blocks/text.py
CombineTextsBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        combined_text = input_data.delimiter.join(input_data.input)
        yield ""output"", combined_text","Point(row=190, column=4)","Point(row=192, column=37)",CombineTextsBlock,autogpt_platform/backend/backend/blocks/text.py
SamplingMethod,class,,"class SamplingMethod(str, Enum):
    RANDOM = ""random""
    SYSTEMATIC = ""systematic""
    TOP = ""top""
    BOTTOM = ""bottom""
    STRATIFIED = ""stratified""
    WEIGHTED = ""weighted""
    RESERVOIR = ""reservoir""
    CLUSTER = ""cluster""","Point(row=9, column=0)","Point(row=17, column=23)",,autogpt_platform/backend/backend/blocks/sampling.py
DataSamplingBlock,class,,"class DataSamplingBlock(Block):
    class Input(BlockSchema):
        data: Union[Dict[str, Any], List[Union[dict, List[Any]]]] = SchemaField(
            description=""The dataset to sample from. Can be a single dictionary, a list of dictionaries, or a list of lists."",
            placeholder=""{'id': 1, 'value': 'a'} or [{'id': 1, 'value': 'a'}, {'id': 2, 'value': 'b'}, ...]"",
        )
        sample_size: int = SchemaField(
            description=""The number of samples to take from the dataset."",
            placeholder=""10"",
            default=10,
        )
        sampling_method: SamplingMethod = SchemaField(
            description=""The method to use for sampling."",
            default=SamplingMethod.RANDOM,
        )
        accumulate: bool = SchemaField(
            description=""Whether to accumulate data before sampling."",
            default=False,
        )
        random_seed: Optional[int] = SchemaField(
            description=""Seed for random number generator (optional)."",
            default=None,
        )
        stratify_key: Optional[str] = SchemaField(
            description=""Key to use for stratified sampling (required for stratified sampling)."",
            default=None,
        )
        weight_key: Optional[str] = SchemaField(
            description=""Key to use for weighted sampling (required for weighted sampling)."",
            default=None,
        )
        cluster_key: Optional[str] = SchemaField(
            description=""Key to use for cluster sampling (required for cluster sampling)."",
            default=None,
        )

    class Output(BlockSchema):
        sampled_data: List[Union[dict, List[Any]]] = SchemaField(
            description=""The sampled subset of the input data.""
        )
        sample_indices: List[int] = SchemaField(
            description=""The indices of the sampled data in the original dataset.""
        )

    def __init__(self):
        super().__init__(
            id=""4a448883-71fa-49cf-91cf-70d793bd7d87"",
            description=""This block samples data from a given dataset using various sampling methods."",
            categories={BlockCategory.LOGIC},
            input_schema=DataSamplingBlock.Input,
            output_schema=DataSamplingBlock.Output,
            test_input={
                ""data"": [
                    {""id"": i, ""value"": chr(97 + i), ""group"": i % 3} for i in range(10)
                ],
                ""sample_size"": 3,
                ""sampling_method"": SamplingMethod.STRATIFIED,
                ""accumulate"": False,
                ""random_seed"": 42,
                ""stratify_key"": ""group"",
            },
            test_output=[
                (
                    ""sampled_data"",
                    [
                        {""id"": 0, ""value"": ""a"", ""group"": 0},
                        {""id"": 1, ""value"": ""b"", ""group"": 1},
                        {""id"": 8, ""value"": ""i"", ""group"": 2},
                    ],
                ),
                (""sample_indices"", [0, 1, 8]),
            ],
        )
        self.accumulated_data = []

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        if input_data.accumulate:
            if isinstance(input_data.data, dict):
                self.accumulated_data.append(input_data.data)
            elif isinstance(input_data.data, list):
                self.accumulated_data.extend(input_data.data)
            else:
                raise ValueError(f""Unsupported data type: {type(input_data.data)}"")

            # If we don't have enough data yet, return without sampling
            if len(self.accumulated_data) < input_data.sample_size:
                return

            data_to_sample = self.accumulated_data
        else:
            # If not accumulating, use the input data directly
            data_to_sample = (
                input_data.data
                if isinstance(input_data.data, list)
                else [input_data.data]
            )

        if input_data.random_seed is not None:
            random.seed(input_data.random_seed)

        data_size = len(data_to_sample)

        if input_data.sample_size > data_size:
            raise ValueError(
                f""Sample size ({input_data.sample_size}) cannot be larger than the dataset size ({data_size}).""
            )

        indices = []

        if input_data.sampling_method == SamplingMethod.RANDOM:
            indices = random.sample(range(data_size), input_data.sample_size)
        elif input_data.sampling_method == SamplingMethod.SYSTEMATIC:
            step = data_size // input_data.sample_size
            start = random.randint(0, step - 1)
            indices = list(range(start, data_size, step))[: input_data.sample_size]
        elif input_data.sampling_method == SamplingMethod.TOP:
            indices = list(range(input_data.sample_size))
        elif input_data.sampling_method == SamplingMethod.BOTTOM:
            indices = list(range(data_size - input_data.sample_size, data_size))
        elif input_data.sampling_method == SamplingMethod.STRATIFIED:
            if not input_data.stratify_key:
                raise ValueError(
                    ""Stratify key must be provided for stratified sampling.""
                )
            strata = defaultdict(list)
            for i, item in enumerate(data_to_sample):
                if isinstance(item, dict):
                    strata_value = item.get(input_data.stratify_key)
                elif hasattr(item, input_data.stratify_key):
                    strata_value = getattr(item, input_data.stratify_key)
                else:
                    raise ValueError(
                        f""Stratify key '{input_data.stratify_key}' not found in item {item}""
                    )

                if strata_value is None:
                    raise ValueError(
                        f""Stratify value for key '{input_data.stratify_key}' is None""
                    )

                strata[str(strata_value)].append(i)

            # Calculate the number of samples to take from each stratum
            stratum_sizes = {
                k: max(1, int(len(v) / data_size * input_data.sample_size))
                for k, v in strata.items()
            }

            # Adjust sizes to ensure we get exactly sample_size samples
            while sum(stratum_sizes.values()) != input_data.sample_size:
                if sum(stratum_sizes.values()) < input_data.sample_size:
                    stratum_sizes[
                        max(stratum_sizes, key=lambda k: stratum_sizes[k])
                    ] += 1
                else:
                    stratum_sizes[
                        max(stratum_sizes, key=lambda k: stratum_sizes[k])
                    ] -= 1

            for stratum, size in stratum_sizes.items():
                indices.extend(random.sample(strata[stratum], size))
        elif input_data.sampling_method == SamplingMethod.WEIGHTED:
            if not input_data.weight_key:
                raise ValueError(""Weight key must be provided for weighted sampling."")
            weights = []
            for item in data_to_sample:
                if isinstance(item, dict):
                    weight = item.get(input_data.weight_key)
                elif hasattr(item, input_data.weight_key):
                    weight = getattr(item, input_data.weight_key)
                else:
                    raise ValueError(
                        f""Weight key '{input_data.weight_key}' not found in item {item}""
                    )

                if weight is None:
                    raise ValueError(
                        f""Weight value for key '{input_data.weight_key}' is None""
                    )
                try:
                    weights.append(float(weight))
                except ValueError:
                    raise ValueError(
                        f""Weight value '{weight}' cannot be converted to a number""
                    )

            if not weights:
                raise ValueError(
                    f""No valid weights found using key '{input_data.weight_key}'""
                )

            indices = random.choices(
                range(data_size), weights=weights, k=input_data.sample_size
            )
        elif input_data.sampling_method == SamplingMethod.RESERVOIR:
            indices = list(range(input_data.sample_size))
            for i in range(input_data.sample_size, data_size):
                j = random.randint(0, i)
                if j < input_data.sample_size:
                    indices[j] = i
        elif input_data.sampling_method == SamplingMethod.CLUSTER:
            if not input_data.cluster_key:
                raise ValueError(""Cluster key must be provided for cluster sampling."")
            clusters = defaultdict(list)
            for i, item in enumerate(data_to_sample):
                if isinstance(item, dict):
                    cluster_value = item.get(input_data.cluster_key)
                elif hasattr(item, input_data.cluster_key):
                    cluster_value = getattr(item, input_data.cluster_key)
                else:
                    raise TypeError(
                        f""Item {item} does not have the cluster key '{input_data.cluster_key}'""
                    )

                clusters[str(cluster_value)].append(i)

            # Randomly select clusters until we have enough samples
            selected_clusters = []
            while (
                sum(len(clusters[c]) for c in selected_clusters)
                < input_data.sample_size
            ):
                available_clusters = [c for c in clusters if c not in selected_clusters]
                if not available_clusters:
                    break
                selected_clusters.append(random.choice(available_clusters))

            for cluster in selected_clusters:
                indices.extend(clusters[cluster])

            # If we have more samples than needed, randomly remove some
            if len(indices) > input_data.sample_size:
                indices = random.sample(indices, input_data.sample_size)
        else:
            raise ValueError(f""Unknown sampling method: {input_data.sampling_method}"")

        sampled_data = [data_to_sample[i] for i in indices]

        # Clear accumulated data after sampling if accumulation is enabled
        if input_data.accumulate:
            self.accumulated_data = []

        yield ""sampled_data"", sampled_data
        yield ""sample_indices"", indices","Point(row=20, column=0)","Point(row=263, column=39)",,autogpt_platform/backend/backend/blocks/sampling.py
Input,class,,"class Input(BlockSchema):
        data: Union[Dict[str, Any], List[Union[dict, List[Any]]]] = SchemaField(
            description=""The dataset to sample from. Can be a single dictionary, a list of dictionaries, or a list of lists."",
            placeholder=""{'id': 1, 'value': 'a'} or [{'id': 1, 'value': 'a'}, {'id': 2, 'value': 'b'}, ...]"",
        )
        sample_size: int = SchemaField(
            description=""The number of samples to take from the dataset."",
            placeholder=""10"",
            default=10,
        )
        sampling_method: SamplingMethod = SchemaField(
            description=""The method to use for sampling."",
            default=SamplingMethod.RANDOM,
        )
        accumulate: bool = SchemaField(
            description=""Whether to accumulate data before sampling."",
            default=False,
        )
        random_seed: Optional[int] = SchemaField(
            description=""Seed for random number generator (optional)."",
            default=None,
        )
        stratify_key: Optional[str] = SchemaField(
            description=""Key to use for stratified sampling (required for stratified sampling)."",
            default=None,
        )
        weight_key: Optional[str] = SchemaField(
            description=""Key to use for weighted sampling (required for weighted sampling)."",
            default=None,
        )
        cluster_key: Optional[str] = SchemaField(
            description=""Key to use for cluster sampling (required for cluster sampling)."",
            default=None,
        )","Point(row=21, column=4)","Point(row=54, column=9)",,autogpt_platform/backend/backend/blocks/sampling.py
Output,class,,"class Output(BlockSchema):
        sampled_data: List[Union[dict, List[Any]]] = SchemaField(
            description=""The sampled subset of the input data.""
        )
        sample_indices: List[int] = SchemaField(
            description=""The indices of the sampled data in the original dataset.""
        )","Point(row=56, column=4)","Point(row=62, column=9)",,autogpt_platform/backend/backend/blocks/sampling.py
DataSamplingBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""4a448883-71fa-49cf-91cf-70d793bd7d87"",
            description=""This block samples data from a given dataset using various sampling methods."",
            categories={BlockCategory.LOGIC},
            input_schema=DataSamplingBlock.Input,
            output_schema=DataSamplingBlock.Output,
            test_input={
                ""data"": [
                    {""id"": i, ""value"": chr(97 + i), ""group"": i % 3} for i in range(10)
                ],
                ""sample_size"": 3,
                ""sampling_method"": SamplingMethod.STRATIFIED,
                ""accumulate"": False,
                ""random_seed"": 42,
                ""stratify_key"": ""group"",
            },
            test_output=[
                (
                    ""sampled_data"",
                    [
                        {""id"": 0, ""value"": ""a"", ""group"": 0},
                        {""id"": 1, ""value"": ""b"", ""group"": 1},
                        {""id"": 8, ""value"": ""i"", ""group"": 2},
                    ],
                ),
                (""sample_indices"", [0, 1, 8]),
            ],
        )
        self.accumulated_data = []","Point(row=64, column=4)","Point(row=93, column=34)",DataSamplingBlock,autogpt_platform/backend/backend/blocks/sampling.py
DataSamplingBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        if input_data.accumulate:
            if isinstance(input_data.data, dict):
                self.accumulated_data.append(input_data.data)
            elif isinstance(input_data.data, list):
                self.accumulated_data.extend(input_data.data)
            else:
                raise ValueError(f""Unsupported data type: {type(input_data.data)}"")

            # If we don't have enough data yet, return without sampling
            if len(self.accumulated_data) < input_data.sample_size:
                return

            data_to_sample = self.accumulated_data
        else:
            # If not accumulating, use the input data directly
            data_to_sample = (
                input_data.data
                if isinstance(input_data.data, list)
                else [input_data.data]
            )

        if input_data.random_seed is not None:
            random.seed(input_data.random_seed)

        data_size = len(data_to_sample)

        if input_data.sample_size > data_size:
            raise ValueError(
                f""Sample size ({input_data.sample_size}) cannot be larger than the dataset size ({data_size}).""
            )

        indices = []

        if input_data.sampling_method == SamplingMethod.RANDOM:
            indices = random.sample(range(data_size), input_data.sample_size)
        elif input_data.sampling_method == SamplingMethod.SYSTEMATIC:
            step = data_size // input_data.sample_size
            start = random.randint(0, step - 1)
            indices = list(range(start, data_size, step))[: input_data.sample_size]
        elif input_data.sampling_method == SamplingMethod.TOP:
            indices = list(range(input_data.sample_size))
        elif input_data.sampling_method == SamplingMethod.BOTTOM:
            indices = list(range(data_size - input_data.sample_size, data_size))
        elif input_data.sampling_method == SamplingMethod.STRATIFIED:
            if not input_data.stratify_key:
                raise ValueError(
                    ""Stratify key must be provided for stratified sampling.""
                )
            strata = defaultdict(list)
            for i, item in enumerate(data_to_sample):
                if isinstance(item, dict):
                    strata_value = item.get(input_data.stratify_key)
                elif hasattr(item, input_data.stratify_key):
                    strata_value = getattr(item, input_data.stratify_key)
                else:
                    raise ValueError(
                        f""Stratify key '{input_data.stratify_key}' not found in item {item}""
                    )

                if strata_value is None:
                    raise ValueError(
                        f""Stratify value for key '{input_data.stratify_key}' is None""
                    )

                strata[str(strata_value)].append(i)

            # Calculate the number of samples to take from each stratum
            stratum_sizes = {
                k: max(1, int(len(v) / data_size * input_data.sample_size))
                for k, v in strata.items()
            }

            # Adjust sizes to ensure we get exactly sample_size samples
            while sum(stratum_sizes.values()) != input_data.sample_size:
                if sum(stratum_sizes.values()) < input_data.sample_size:
                    stratum_sizes[
                        max(stratum_sizes, key=lambda k: stratum_sizes[k])
                    ] += 1
                else:
                    stratum_sizes[
                        max(stratum_sizes, key=lambda k: stratum_sizes[k])
                    ] -= 1

            for stratum, size in stratum_sizes.items():
                indices.extend(random.sample(strata[stratum], size))
        elif input_data.sampling_method == SamplingMethod.WEIGHTED:
            if not input_data.weight_key:
                raise ValueError(""Weight key must be provided for weighted sampling."")
            weights = []
            for item in data_to_sample:
                if isinstance(item, dict):
                    weight = item.get(input_data.weight_key)
                elif hasattr(item, input_data.weight_key):
                    weight = getattr(item, input_data.weight_key)
                else:
                    raise ValueError(
                        f""Weight key '{input_data.weight_key}' not found in item {item}""
                    )

                if weight is None:
                    raise ValueError(
                        f""Weight value for key '{input_data.weight_key}' is None""
                    )
                try:
                    weights.append(float(weight))
                except ValueError:
                    raise ValueError(
                        f""Weight value '{weight}' cannot be converted to a number""
                    )

            if not weights:
                raise ValueError(
                    f""No valid weights found using key '{input_data.weight_key}'""
                )

            indices = random.choices(
                range(data_size), weights=weights, k=input_data.sample_size
            )
        elif input_data.sampling_method == SamplingMethod.RESERVOIR:
            indices = list(range(input_data.sample_size))
            for i in range(input_data.sample_size, data_size):
                j = random.randint(0, i)
                if j < input_data.sample_size:
                    indices[j] = i
        elif input_data.sampling_method == SamplingMethod.CLUSTER:
            if not input_data.cluster_key:
                raise ValueError(""Cluster key must be provided for cluster sampling."")
            clusters = defaultdict(list)
            for i, item in enumerate(data_to_sample):
                if isinstance(item, dict):
                    cluster_value = item.get(input_data.cluster_key)
                elif hasattr(item, input_data.cluster_key):
                    cluster_value = getattr(item, input_data.cluster_key)
                else:
                    raise TypeError(
                        f""Item {item} does not have the cluster key '{input_data.cluster_key}'""
                    )

                clusters[str(cluster_value)].append(i)

            # Randomly select clusters until we have enough samples
            selected_clusters = []
            while (
                sum(len(clusters[c]) for c in selected_clusters)
                < input_data.sample_size
            ):
                available_clusters = [c for c in clusters if c not in selected_clusters]
                if not available_clusters:
                    break
                selected_clusters.append(random.choice(available_clusters))

            for cluster in selected_clusters:
                indices.extend(clusters[cluster])

            # If we have more samples than needed, randomly remove some
            if len(indices) > input_data.sample_size:
                indices = random.sample(indices, input_data.sample_size)
        else:
            raise ValueError(f""Unknown sampling method: {input_data.sampling_method}"")

        sampled_data = [data_to_sample[i] for i in indices]

        # Clear accumulated data after sampling if accumulation is enabled
        if input_data.accumulate:
            self.accumulated_data = []

        yield ""sampled_data"", sampled_data
        yield ""sample_indices"", indices","Point(row=95, column=4)","Point(row=263, column=39)",DataSamplingBlock,autogpt_platform/backend/backend/blocks/sampling.py
TextDecoderBlock,class,,"class TextDecoderBlock(Block):
    class Input(BlockSchema):
        text: str = SchemaField(
            description=""A string containing escaped characters to be decoded"",
            placeholder='Your entire text block with \\n and \\"" escaped characters',
        )

    class Output(BlockSchema):
        decoded_text: str = SchemaField(
            description=""The decoded text with escape sequences processed""
        )

    def __init__(self):
        super().__init__(
            id=""2570e8fe-8447-43ed-84c7-70d657923231"",
            description=""Decodes a string containing escape sequences into actual text"",
            categories={BlockCategory.TEXT},
            input_schema=TextDecoderBlock.Input,
            output_schema=TextDecoderBlock.Output,
            test_input={""text"": """"""Hello\nWorld!\nThis is a \""quoted\"" string.""""""},
            test_output=[
                (
                    ""decoded_text"",
                    """"""Hello
World!
This is a ""quoted"" string."""""",
                )
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        decoded_text = codecs.decode(input_data.text, ""unicode_escape"")
        yield ""decoded_text"", decoded_text","Point(row=6, column=0)","Point(row=38, column=42)",,autogpt_platform/backend/backend/blocks/decoder_block.py
Input,class,,"class Input(BlockSchema):
        text: str = SchemaField(
            description=""A string containing escaped characters to be decoded"",
            placeholder='Your entire text block with \\n and \\"" escaped characters',
        )","Point(row=7, column=4)","Point(row=11, column=9)",,autogpt_platform/backend/backend/blocks/decoder_block.py
Output,class,,"class Output(BlockSchema):
        decoded_text: str = SchemaField(
            description=""The decoded text with escape sequences processed""
        )","Point(row=13, column=4)","Point(row=16, column=9)",,autogpt_platform/backend/backend/blocks/decoder_block.py
TextDecoderBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""2570e8fe-8447-43ed-84c7-70d657923231"",
            description=""Decodes a string containing escape sequences into actual text"",
            categories={BlockCategory.TEXT},
            input_schema=TextDecoderBlock.Input,
            output_schema=TextDecoderBlock.Output,
            test_input={""text"": """"""Hello\nWorld!\nThis is a \""quoted\"" string.""""""},
            test_output=[
                (
                    ""decoded_text"",
                    """"""Hello
World!
This is a ""quoted"" string."""""",
                )
            ],
        )","Point(row=18, column=4)","Point(row=34, column=9)",TextDecoderBlock,autogpt_platform/backend/backend/blocks/decoder_block.py
TextDecoderBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        decoded_text = codecs.decode(input_data.text, ""unicode_escape"")
        yield ""decoded_text"", decoded_text","Point(row=36, column=4)","Point(row=38, column=42)",TextDecoderBlock,autogpt_platform/backend/backend/blocks/decoder_block.py
GetWikipediaSummaryBlock,class,,"class GetWikipediaSummaryBlock(Block, GetRequest):
    class Input(BlockSchema):
        topic: str = SchemaField(description=""The topic to fetch the summary for"")

    class Output(BlockSchema):
        summary: str = SchemaField(description=""The summary of the given topic"")
        error: str = SchemaField(
            description=""Error message if the summary cannot be retrieved""
        )

    def __init__(self):
        super().__init__(
            id=""f5b0f5d0-1862-4d61-94be-3ad0fa772760"",
            description=""This block fetches the summary of a given topic from Wikipedia."",
            categories={BlockCategory.SEARCH},
            input_schema=GetWikipediaSummaryBlock.Input,
            output_schema=GetWikipediaSummaryBlock.Output,
            test_input={""topic"": ""Artificial Intelligence""},
            test_output=(""summary"", ""summary content""),
            test_mock={""get_request"": lambda url, json: {""extract"": ""summary content""}},
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        topic = input_data.topic
        url = f""https://en.wikipedia.org/api/rest_v1/page/summary/{topic}""
        response = self.get_request(url, json=True)
        if ""extract"" not in response:
            raise RuntimeError(f""Unable to parse Wikipedia response: {response}"")
        yield ""summary"", response[""extract""]","Point(row=11, column=0)","Point(row=39, column=44)",,autogpt_platform/backend/backend/blocks/search.py
Input,class,,"class Input(BlockSchema):
        topic: str = SchemaField(description=""The topic to fetch the summary for"")","Point(row=12, column=4)","Point(row=13, column=82)",,autogpt_platform/backend/backend/blocks/search.py
Output,class,,"class Output(BlockSchema):
        summary: str = SchemaField(description=""The summary of the given topic"")
        error: str = SchemaField(
            description=""Error message if the summary cannot be retrieved""
        )","Point(row=15, column=4)","Point(row=19, column=9)",,autogpt_platform/backend/backend/blocks/search.py
GetWikipediaSummaryBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""f5b0f5d0-1862-4d61-94be-3ad0fa772760"",
            description=""This block fetches the summary of a given topic from Wikipedia."",
            categories={BlockCategory.SEARCH},
            input_schema=GetWikipediaSummaryBlock.Input,
            output_schema=GetWikipediaSummaryBlock.Output,
            test_input={""topic"": ""Artificial Intelligence""},
            test_output=(""summary"", ""summary content""),
            test_mock={""get_request"": lambda url, json: {""extract"": ""summary content""}},
        )","Point(row=21, column=4)","Point(row=31, column=9)",GetWikipediaSummaryBlock,autogpt_platform/backend/backend/blocks/search.py
GetWikipediaSummaryBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        topic = input_data.topic
        url = f""https://en.wikipedia.org/api/rest_v1/page/summary/{topic}""
        response = self.get_request(url, json=True)
        if ""extract"" not in response:
            raise RuntimeError(f""Unable to parse Wikipedia response: {response}"")
        yield ""summary"", response[""extract""]","Point(row=33, column=4)","Point(row=39, column=44)",GetWikipediaSummaryBlock,autogpt_platform/backend/backend/blocks/search.py
ExtractWebsiteContentBlock,class,,"class ExtractWebsiteContentBlock(Block, GetRequest):
    class Input(BlockSchema):
        url: str = SchemaField(description=""The URL to scrape the content from"")
        raw_content: bool = SchemaField(
            default=False,
            title=""Raw Content"",
            description=""Whether to do a raw scrape of the content or use Jina-ai Reader to scrape the content"",
            advanced=True,
        )

    class Output(BlockSchema):
        content: str = SchemaField(description=""The scraped content from the given URL"")
        error: str = SchemaField(
            description=""Error message if the content cannot be retrieved""
        )

    def __init__(self):
        super().__init__(
            id=""436c3984-57fd-4b85-8e9a-459b356883bd"",
            description=""This block scrapes the content from the given web URL."",
            categories={BlockCategory.SEARCH},
            input_schema=ExtractWebsiteContentBlock.Input,
            output_schema=ExtractWebsiteContentBlock.Output,
            test_input={""url"": ""https://en.wikipedia.org/wiki/Artificial_intelligence""},
            test_output=(""content"", ""scraped content""),
            test_mock={""get_request"": lambda url, json: ""scraped content""},
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        if input_data.raw_content:
            url = input_data.url
        else:
            url = f""https://r.jina.ai/{input_data.url}""

        content = self.get_request(url, json=False)
        yield ""content"", content","Point(row=42, column=0)","Point(row=77, column=32)",,autogpt_platform/backend/backend/blocks/search.py
Input,class,,"class Input(BlockSchema):
        url: str = SchemaField(description=""The URL to scrape the content from"")
        raw_content: bool = SchemaField(
            default=False,
            title=""Raw Content"",
            description=""Whether to do a raw scrape of the content or use Jina-ai Reader to scrape the content"",
            advanced=True,
        )","Point(row=43, column=4)","Point(row=50, column=9)",,autogpt_platform/backend/backend/blocks/search.py
Output,class,,"class Output(BlockSchema):
        content: str = SchemaField(description=""The scraped content from the given URL"")
        error: str = SchemaField(
            description=""Error message if the content cannot be retrieved""
        )","Point(row=52, column=4)","Point(row=56, column=9)",,autogpt_platform/backend/backend/blocks/search.py
ExtractWebsiteContentBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""436c3984-57fd-4b85-8e9a-459b356883bd"",
            description=""This block scrapes the content from the given web URL."",
            categories={BlockCategory.SEARCH},
            input_schema=ExtractWebsiteContentBlock.Input,
            output_schema=ExtractWebsiteContentBlock.Output,
            test_input={""url"": ""https://en.wikipedia.org/wiki/Artificial_intelligence""},
            test_output=(""content"", ""scraped content""),
            test_mock={""get_request"": lambda url, json: ""scraped content""},
        )","Point(row=58, column=4)","Point(row=68, column=9)",ExtractWebsiteContentBlock,autogpt_platform/backend/backend/blocks/search.py
ExtractWebsiteContentBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        if input_data.raw_content:
            url = input_data.url
        else:
            url = f""https://r.jina.ai/{input_data.url}""

        content = self.get_request(url, json=False)
        yield ""content"", content","Point(row=70, column=4)","Point(row=77, column=32)",ExtractWebsiteContentBlock,autogpt_platform/backend/backend/blocks/search.py
GetWeatherInformationBlock,class,,"class GetWeatherInformationBlock(Block, GetRequest):
    class Input(BlockSchema):
        location: str = SchemaField(
            description=""Location to get weather information for""
        )
        credentials: CredentialsMetaInput[
            Literal[""openweathermap""], Literal[""api_key""]
        ] = CredentialsField(
            provider=""openweathermap"",
            supported_credential_types={""api_key""},
            description=""The OpenWeatherMap integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )
        use_celsius: bool = SchemaField(
            default=True,
            description=""Whether to use Celsius or Fahrenheit for temperature"",
        )

    class Output(BlockSchema):
        temperature: str = SchemaField(
            description=""Temperature in the specified location""
        )
        humidity: str = SchemaField(description=""Humidity in the specified location"")
        condition: str = SchemaField(
            description=""Weather condition in the specified location""
        )
        error: str = SchemaField(
            description=""Error message if the weather information cannot be retrieved""
        )

    def __init__(self):
        super().__init__(
            id=""f7a8b2c3-6d4e-5f8b-9e7f-6d4e5f8b9e7f"",
            input_schema=GetWeatherInformationBlock.Input,
            output_schema=GetWeatherInformationBlock.Output,
            description=""Retrieves weather information for a specified location using OpenWeatherMap API."",
            test_input={
                ""location"": ""New York"",
                ""use_celsius"": True,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[
                (""temperature"", ""21.66""),
                (""humidity"", ""32""),
                (""condition"", ""overcast clouds""),
            ],
            test_mock={
                ""get_request"": lambda url, json: {
                    ""main"": {""temp"": 21.66, ""humidity"": 32},
                    ""weather"": [{""description"": ""overcast clouds""}],
                }
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        units = ""metric"" if input_data.use_celsius else ""imperial""
        api_key = credentials.api_key
        location = input_data.location
        url = f""http://api.openweathermap.org/data/2.5/weather?q={quote(location)}&appid={api_key}&units={units}""
        weather_data = self.get_request(url, json=True)

        if ""main"" in weather_data and ""weather"" in weather_data:
            yield ""temperature"", str(weather_data[""main""][""temp""])
            yield ""humidity"", str(weather_data[""main""][""humidity""])
            yield ""condition"", weather_data[""weather""][0][""description""]
        else:
            raise RuntimeError(f""Expected keys not found in response: {weather_data}"")","Point(row=95, column=0)","Point(row=164, column=86)",,autogpt_platform/backend/backend/blocks/search.py
Input,class,,"class Input(BlockSchema):
        location: str = SchemaField(
            description=""Location to get weather information for""
        )
        credentials: CredentialsMetaInput[
            Literal[""openweathermap""], Literal[""api_key""]
        ] = CredentialsField(
            provider=""openweathermap"",
            supported_credential_types={""api_key""},
            description=""The OpenWeatherMap integration can be used with ""
            ""any API key with sufficient permissions for the blocks it is used on."",
        )
        use_celsius: bool = SchemaField(
            default=True,
            description=""Whether to use Celsius or Fahrenheit for temperature"",
        )","Point(row=96, column=4)","Point(row=111, column=9)",,autogpt_platform/backend/backend/blocks/search.py
Output,class,,"class Output(BlockSchema):
        temperature: str = SchemaField(
            description=""Temperature in the specified location""
        )
        humidity: str = SchemaField(description=""Humidity in the specified location"")
        condition: str = SchemaField(
            description=""Weather condition in the specified location""
        )
        error: str = SchemaField(
            description=""Error message if the weather information cannot be retrieved""
        )","Point(row=113, column=4)","Point(row=123, column=9)",,autogpt_platform/backend/backend/blocks/search.py
GetWeatherInformationBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""f7a8b2c3-6d4e-5f8b-9e7f-6d4e5f8b9e7f"",
            input_schema=GetWeatherInformationBlock.Input,
            output_schema=GetWeatherInformationBlock.Output,
            description=""Retrieves weather information for a specified location using OpenWeatherMap API."",
            test_input={
                ""location"": ""New York"",
                ""use_celsius"": True,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[
                (""temperature"", ""21.66""),
                (""humidity"", ""32""),
                (""condition"", ""overcast clouds""),
            ],
            test_mock={
                ""get_request"": lambda url, json: {
                    ""main"": {""temp"": 21.66, ""humidity"": 32},
                    ""weather"": [{""description"": ""overcast clouds""}],
                }
            },
            test_credentials=TEST_CREDENTIALS,
        )","Point(row=125, column=4)","Point(row=148, column=9)",GetWeatherInformationBlock,autogpt_platform/backend/backend/blocks/search.py
GetWeatherInformationBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        units = ""metric"" if input_data.use_celsius else ""imperial""
        api_key = credentials.api_key
        location = input_data.location
        url = f""http://api.openweathermap.org/data/2.5/weather?q={quote(location)}&appid={api_key}&units={units}""
        weather_data = self.get_request(url, json=True)

        if ""main"" in weather_data and ""weather"" in weather_data:
            yield ""temperature"", str(weather_data[""main""][""temp""])
            yield ""humidity"", str(weather_data[""main""][""humidity""])
            yield ""condition"", weather_data[""weather""][0][""description""]
        else:
            raise RuntimeError(f""Expected keys not found in response: {weather_data}"")","Point(row=150, column=4)","Point(row=164, column=86)",GetWeatherInformationBlock,autogpt_platform/backend/backend/blocks/search.py
ComparisonOperator,class,,"class ComparisonOperator(Enum):
    EQUAL = ""==""
    NOT_EQUAL = ""!=""
    GREATER_THAN = "">""
    LESS_THAN = ""<""
    GREATER_THAN_OR_EQUAL = "">=""
    LESS_THAN_OR_EQUAL = ""<=""","Point(row=7, column=0)","Point(row=13, column=29)",,autogpt_platform/backend/backend/blocks/branching.py
ConditionBlock,class,,"class ConditionBlock(Block):
    class Input(BlockSchema):
        value1: Any = SchemaField(
            description=""Enter the first value for comparison"",
            placeholder=""For example: 10 or 'hello' or True"",
        )
        operator: ComparisonOperator = SchemaField(
            description=""Choose the comparison operator"",
            placeholder=""Select an operator"",
        )
        value2: Any = SchemaField(
            description=""Enter the second value for comparison"",
            placeholder=""For example: 20 or 'world' or False"",
        )
        yes_value: Any = SchemaField(
            description=""(Optional) Value to output if the condition is true. If not provided, value1 will be used."",
            placeholder=""Leave empty to use value1, or enter a specific value"",
            default=None,
        )
        no_value: Any = SchemaField(
            description=""(Optional) Value to output if the condition is false. If not provided, value1 will be used."",
            placeholder=""Leave empty to use value1, or enter a specific value"",
            default=None,
        )

    class Output(BlockSchema):
        result: bool = SchemaField(
            description=""The result of the condition evaluation (True or False)""
        )
        yes_output: Any = SchemaField(
            description=""The output value if the condition is true""
        )
        no_output: Any = SchemaField(
            description=""The output value if the condition is false""
        )

    def __init__(self):
        super().__init__(
            id=""715696a0-e1da-45c8-b209-c2fa9c3b0be6"",
            input_schema=ConditionBlock.Input,
            output_schema=ConditionBlock.Output,
            description=""Handles conditional logic based on comparison operators"",
            categories={BlockCategory.LOGIC},
            test_input={
                ""value1"": 10,
                ""operator"": ComparisonOperator.GREATER_THAN.value,
                ""value2"": 5,
                ""yes_value"": ""Greater"",
                ""no_value"": ""Not greater"",
            },
            test_output=[
                (""result"", True),
                (""yes_output"", ""Greater""),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        operator = input_data.operator

        value1 = input_data.value1
        if isinstance(value1, str):
            value1 = float(value1.strip())

        value2 = input_data.value2
        if isinstance(value2, str):
            value2 = float(value2.strip())

        yes_value = input_data.yes_value if input_data.yes_value is not None else value1
        no_value = input_data.no_value if input_data.no_value is not None else value2

        comparison_funcs = {
            ComparisonOperator.EQUAL: lambda a, b: a == b,
            ComparisonOperator.NOT_EQUAL: lambda a, b: a != b,
            ComparisonOperator.GREATER_THAN: lambda a, b: a > b,
            ComparisonOperator.LESS_THAN: lambda a, b: a < b,
            ComparisonOperator.GREATER_THAN_OR_EQUAL: lambda a, b: a >= b,
            ComparisonOperator.LESS_THAN_OR_EQUAL: lambda a, b: a <= b,
        }

        result = comparison_funcs[operator](value1, value2)

        yield ""result"", result

        if result:
            yield ""yes_output"", yes_value
        else:
            yield ""no_output"", no_value","Point(row=16, column=0)","Point(row=102, column=39)",,autogpt_platform/backend/backend/blocks/branching.py
Input,class,,"class Input(BlockSchema):
        value1: Any = SchemaField(
            description=""Enter the first value for comparison"",
            placeholder=""For example: 10 or 'hello' or True"",
        )
        operator: ComparisonOperator = SchemaField(
            description=""Choose the comparison operator"",
            placeholder=""Select an operator"",
        )
        value2: Any = SchemaField(
            description=""Enter the second value for comparison"",
            placeholder=""For example: 20 or 'world' or False"",
        )
        yes_value: Any = SchemaField(
            description=""(Optional) Value to output if the condition is true. If not provided, value1 will be used."",
            placeholder=""Leave empty to use value1, or enter a specific value"",
            default=None,
        )
        no_value: Any = SchemaField(
            description=""(Optional) Value to output if the condition is false. If not provided, value1 will be used."",
            placeholder=""Leave empty to use value1, or enter a specific value"",
            default=None,
        )","Point(row=17, column=4)","Point(row=39, column=9)",,autogpt_platform/backend/backend/blocks/branching.py
Output,class,,"class Output(BlockSchema):
        result: bool = SchemaField(
            description=""The result of the condition evaluation (True or False)""
        )
        yes_output: Any = SchemaField(
            description=""The output value if the condition is true""
        )
        no_output: Any = SchemaField(
            description=""The output value if the condition is false""
        )","Point(row=41, column=4)","Point(row=50, column=9)",,autogpt_platform/backend/backend/blocks/branching.py
ConditionBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""715696a0-e1da-45c8-b209-c2fa9c3b0be6"",
            input_schema=ConditionBlock.Input,
            output_schema=ConditionBlock.Output,
            description=""Handles conditional logic based on comparison operators"",
            categories={BlockCategory.LOGIC},
            test_input={
                ""value1"": 10,
                ""operator"": ComparisonOperator.GREATER_THAN.value,
                ""value2"": 5,
                ""yes_value"": ""Greater"",
                ""no_value"": ""Not greater"",
            },
            test_output=[
                (""result"", True),
                (""yes_output"", ""Greater""),
            ],
        )","Point(row=52, column=4)","Point(row=70, column=9)",ConditionBlock,autogpt_platform/backend/backend/blocks/branching.py
ConditionBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        operator = input_data.operator

        value1 = input_data.value1
        if isinstance(value1, str):
            value1 = float(value1.strip())

        value2 = input_data.value2
        if isinstance(value2, str):
            value2 = float(value2.strip())

        yes_value = input_data.yes_value if input_data.yes_value is not None else value1
        no_value = input_data.no_value if input_data.no_value is not None else value2

        comparison_funcs = {
            ComparisonOperator.EQUAL: lambda a, b: a == b,
            ComparisonOperator.NOT_EQUAL: lambda a, b: a != b,
            ComparisonOperator.GREATER_THAN: lambda a, b: a > b,
            ComparisonOperator.LESS_THAN: lambda a, b: a < b,
            ComparisonOperator.GREATER_THAN_OR_EQUAL: lambda a, b: a >= b,
            ComparisonOperator.LESS_THAN_OR_EQUAL: lambda a, b: a <= b,
        }

        result = comparison_funcs[operator](value1, value2)

        yield ""result"", result

        if result:
            yield ""yes_output"", yes_value
        else:
            yield ""no_output"", no_value","Point(row=72, column=4)","Point(row=102, column=39)",ConditionBlock,autogpt_platform/backend/backend/blocks/branching.py
BlockInstallationBlock,class,"
    This block allows the verification and installation of other blocks in the system.

    NOTE:
        This block allows remote code execution on the server, and it should be used
        for development purposes only.
","class BlockInstallationBlock(Block):
    """"""
    This block allows the verification and installation of other blocks in the system.

    NOTE:
        This block allows remote code execution on the server, and it should be used
        for development purposes only.
    """"""

    class Input(BlockSchema):
        code: str = SchemaField(
            description=""Python code of the block to be installed"",
        )

    class Output(BlockSchema):
        success: str = SchemaField(
            description=""Success message if the block is installed successfully"",
        )
        error: str = SchemaField(
            description=""Error message if the block installation fails"",
        )

    def __init__(self):
        super().__init__(
            id=""45e78db5-03e9-447f-9395-308d712f5f08"",
            description=""Given a code string, this block allows the verification and installation of a block code into the system."",
            categories={BlockCategory.BASIC},
            input_schema=BlockInstallationBlock.Input,
            output_schema=BlockInstallationBlock.Output,
            disabled=True,
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        code = input_data.code

        if search := re.search(r""class (\w+)\(Block\):"", code):
            class_name = search.group(1)
        else:
            raise RuntimeError(""No class found in the code."")

        if search := re.search(r""id=\""(\w+-\w+-\w+-\w+-\w+)\"""", code):
            file_name = search.group(1)
        else:
            raise RuntimeError(""No UUID found in the code."")

        block_dir = os.path.dirname(__file__)
        file_path = f""{block_dir}/{file_name}.py""
        module_name = f""backend.blocks.{file_name}""
        with open(file_path, ""w"") as f:
            f.write(code)

        try:
            module = __import__(module_name, fromlist=[class_name])
            block_class: Type[Block] = getattr(module, class_name)
            block = block_class()

            from backend.util.test import execute_block_test

            execute_block_test(block)
            yield ""success"", ""Block installed successfully.""
        except Exception as e:
            os.remove(file_path)
            raise RuntimeError(f""[Code]\n{code}\n\n[Error]\n{str(e)}"")","Point(row=8, column=0)","Point(row=70, column=70)",,autogpt_platform/backend/backend/blocks/block.py
Input,class,,"class Input(BlockSchema):
        code: str = SchemaField(
            description=""Python code of the block to be installed"",
        )","Point(row=17, column=4)","Point(row=20, column=9)",,autogpt_platform/backend/backend/blocks/block.py
Output,class,,"class Output(BlockSchema):
        success: str = SchemaField(
            description=""Success message if the block is installed successfully"",
        )
        error: str = SchemaField(
            description=""Error message if the block installation fails"",
        )","Point(row=22, column=4)","Point(row=28, column=9)",,autogpt_platform/backend/backend/blocks/block.py
BlockInstallationBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""45e78db5-03e9-447f-9395-308d712f5f08"",
            description=""Given a code string, this block allows the verification and installation of a block code into the system."",
            categories={BlockCategory.BASIC},
            input_schema=BlockInstallationBlock.Input,
            output_schema=BlockInstallationBlock.Output,
            disabled=True,
        )","Point(row=30, column=4)","Point(row=38, column=9)",BlockInstallationBlock,autogpt_platform/backend/backend/blocks/block.py
BlockInstallationBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        code = input_data.code

        if search := re.search(r""class (\w+)\(Block\):"", code):
            class_name = search.group(1)
        else:
            raise RuntimeError(""No class found in the code."")

        if search := re.search(r""id=\""(\w+-\w+-\w+-\w+-\w+)\"""", code):
            file_name = search.group(1)
        else:
            raise RuntimeError(""No UUID found in the code."")

        block_dir = os.path.dirname(__file__)
        file_path = f""{block_dir}/{file_name}.py""
        module_name = f""backend.blocks.{file_name}""
        with open(file_path, ""w"") as f:
            f.write(code)

        try:
            module = __import__(module_name, fromlist=[class_name])
            block_class: Type[Block] = getattr(module, class_name)
            block = block_class()

            from backend.util.test import execute_block_test

            execute_block_test(block)
            yield ""success"", ""Block installed successfully.""
        except Exception as e:
            os.remove(file_path)
            raise RuntimeError(f""[Code]\n{code}\n\n[Error]\n{str(e)}"")","Point(row=40, column=4)","Point(row=70, column=70)",BlockInstallationBlock,autogpt_platform/backend/backend/blocks/block.py
Operation,class,,"class Operation(Enum):
    ADD = ""Add""
    SUBTRACT = ""Subtract""
    MULTIPLY = ""Multiply""
    DIVIDE = ""Divide""
    POWER = ""Power""","Point(row=8, column=0)","Point(row=13, column=19)",,autogpt_platform/backend/backend/blocks/maths.py
CalculatorBlock,class,,"class CalculatorBlock(Block):
    class Input(BlockSchema):
        operation: Operation = SchemaField(
            description=""Choose the math operation you want to perform"",
            placeholder=""Select an operation"",
        )
        a: float = SchemaField(
            description=""Enter the first number (A)"", placeholder=""For example: 10""
        )
        b: float = SchemaField(
            description=""Enter the second number (B)"", placeholder=""For example: 5""
        )
        round_result: bool = SchemaField(
            description=""Do you want to round the result to a whole number?"",
            default=False,
        )

    class Output(BlockSchema):
        result: float = SchemaField(description=""The result of your calculation"")

    def __init__(self):
        super().__init__(
            id=""b1ab9b19-67a6-406d-abf5-2dba76d00c79"",
            input_schema=CalculatorBlock.Input,
            output_schema=CalculatorBlock.Output,
            description=""Performs a mathematical operation on two numbers."",
            categories={BlockCategory.LOGIC},
            test_input={
                ""operation"": Operation.ADD.value,
                ""a"": 10.0,
                ""b"": 5.0,
                ""round_result"": False,
            },
            test_output=[
                (""result"", 15.0),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        operation = input_data.operation
        a = input_data.a
        b = input_data.b

        operations = {
            Operation.ADD: operator.add,
            Operation.SUBTRACT: operator.sub,
            Operation.MULTIPLY: operator.mul,
            Operation.DIVIDE: operator.truediv,
            Operation.POWER: operator.pow,
        }

        op_func = operations[operation]

        try:
            if operation == Operation.DIVIDE and b == 0:
                raise ZeroDivisionError(""Cannot divide by zero"")

            result = op_func(a, b)

            if input_data.round_result:
                result = round(result)

            yield ""result"", result

        except ZeroDivisionError:
            yield ""result"", float(""inf"")  # Return infinity for division by zero
        except Exception:
            yield ""result"", float(""nan"")  # Return NaN for other errors","Point(row=16, column=0)","Point(row=83, column=71)",,autogpt_platform/backend/backend/blocks/maths.py
Input,class,,"class Input(BlockSchema):
        operation: Operation = SchemaField(
            description=""Choose the math operation you want to perform"",
            placeholder=""Select an operation"",
        )
        a: float = SchemaField(
            description=""Enter the first number (A)"", placeholder=""For example: 10""
        )
        b: float = SchemaField(
            description=""Enter the second number (B)"", placeholder=""For example: 5""
        )
        round_result: bool = SchemaField(
            description=""Do you want to round the result to a whole number?"",
            default=False,
        )","Point(row=17, column=4)","Point(row=31, column=9)",,autogpt_platform/backend/backend/blocks/maths.py
Output,class,,"class Output(BlockSchema):
        result: float = SchemaField(description=""The result of your calculation"")","Point(row=33, column=4)","Point(row=34, column=81)",,autogpt_platform/backend/backend/blocks/maths.py
CalculatorBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""b1ab9b19-67a6-406d-abf5-2dba76d00c79"",
            input_schema=CalculatorBlock.Input,
            output_schema=CalculatorBlock.Output,
            description=""Performs a mathematical operation on two numbers."",
            categories={BlockCategory.LOGIC},
            test_input={
                ""operation"": Operation.ADD.value,
                ""a"": 10.0,
                ""b"": 5.0,
                ""round_result"": False,
            },
            test_output=[
                (""result"", 15.0),
            ],
        )","Point(row=36, column=4)","Point(row=52, column=9)",CalculatorBlock,autogpt_platform/backend/backend/blocks/maths.py
CalculatorBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        operation = input_data.operation
        a = input_data.a
        b = input_data.b

        operations = {
            Operation.ADD: operator.add,
            Operation.SUBTRACT: operator.sub,
            Operation.MULTIPLY: operator.mul,
            Operation.DIVIDE: operator.truediv,
            Operation.POWER: operator.pow,
        }

        op_func = operations[operation]

        try:
            if operation == Operation.DIVIDE and b == 0:
                raise ZeroDivisionError(""Cannot divide by zero"")

            result = op_func(a, b)

            if input_data.round_result:
                result = round(result)

            yield ""result"", result

        except ZeroDivisionError:
            yield ""result"", float(""inf"")  # Return infinity for division by zero
        except Exception:
            yield ""result"", float(""nan"")  # Return NaN for other errors","Point(row=54, column=4)","Point(row=83, column=71)",CalculatorBlock,autogpt_platform/backend/backend/blocks/maths.py
CountItemsBlock,class,,"class CountItemsBlock(Block):
    class Input(BlockSchema):
        collection: Any = SchemaField(
            description=""Enter the collection you want to count. This can be a list, dictionary, string, or any other iterable."",
            placeholder=""For example: [1, 2, 3] or {'a': 1, 'b': 2} or 'hello'"",
        )

    class Output(BlockSchema):
        count: int = SchemaField(description=""The number of items in the collection"")

    def __init__(self):
        super().__init__(
            id=""3c9c2f42-b0c3-435f-ba35-05f7a25c772a"",
            input_schema=CountItemsBlock.Input,
            output_schema=CountItemsBlock.Output,
            description=""Counts the number of items in a collection."",
            categories={BlockCategory.LOGIC},
            test_input={""collection"": [1, 2, 3, 4, 5]},
            test_output=[
                (""count"", 5),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        collection = input_data.collection

        try:
            if isinstance(collection, (str, list, tuple, set, dict)):
                count = len(collection)
            elif hasattr(collection, ""__iter__""):
                count = sum(1 for _ in collection)
            else:
                raise ValueError(""Input is not a countable collection"")

            yield ""count"", count

        except Exception:
            yield ""count"", -1  # Return -1 to indicate an error","Point(row=86, column=0)","Point(row=123, column=63)",,autogpt_platform/backend/backend/blocks/maths.py
Input,class,,"class Input(BlockSchema):
        collection: Any = SchemaField(
            description=""Enter the collection you want to count. This can be a list, dictionary, string, or any other iterable."",
            placeholder=""For example: [1, 2, 3] or {'a': 1, 'b': 2} or 'hello'"",
        )","Point(row=87, column=4)","Point(row=91, column=9)",,autogpt_platform/backend/backend/blocks/maths.py
Output,class,,"class Output(BlockSchema):
        count: int = SchemaField(description=""The number of items in the collection"")","Point(row=93, column=4)","Point(row=94, column=85)",,autogpt_platform/backend/backend/blocks/maths.py
CountItemsBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""3c9c2f42-b0c3-435f-ba35-05f7a25c772a"",
            input_schema=CountItemsBlock.Input,
            output_schema=CountItemsBlock.Output,
            description=""Counts the number of items in a collection."",
            categories={BlockCategory.LOGIC},
            test_input={""collection"": [1, 2, 3, 4, 5]},
            test_output=[
                (""count"", 5),
            ],
        )","Point(row=96, column=4)","Point(row=107, column=9)",CountItemsBlock,autogpt_platform/backend/backend/blocks/maths.py
CountItemsBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        collection = input_data.collection

        try:
            if isinstance(collection, (str, list, tuple, set, dict)):
                count = len(collection)
            elif hasattr(collection, ""__iter__""):
                count = sum(1 for _ in collection)
            else:
                raise ValueError(""Input is not a countable collection"")

            yield ""count"", count

        except Exception:
            yield ""count"", -1  # Return -1 to indicate an error","Point(row=109, column=4)","Point(row=123, column=63)",CountItemsBlock,autogpt_platform/backend/backend/blocks/maths.py
GetCurrentTimeBlock,class,,"class GetCurrentTimeBlock(Block):
    class Input(BlockSchema):
        trigger: str = SchemaField(
            description=""Trigger any data to output the current time""
        )
        format: str = SchemaField(
            description=""Format of the time to output"", default=""%H:%M:%S""
        )

    class Output(BlockSchema):
        time: str = SchemaField(
            description=""Current time in the specified format (default: %H:%M:%S)""
        )

    def __init__(self):
        super().__init__(
            id=""a892b8d9-3e4e-4e9c-9c1e-75f8efcf1bfa"",
            description=""This block outputs the current time."",
            categories={BlockCategory.TEXT},
            input_schema=GetCurrentTimeBlock.Input,
            output_schema=GetCurrentTimeBlock.Output,
            test_input=[
                {""trigger"": ""Hello""},
                {""trigger"": ""Hello"", ""format"": ""%H:%M""},
            ],
            test_output=[
                (""time"", lambda _: time.strftime(""%H:%M:%S"")),
                (""time"", lambda _: time.strftime(""%H:%M"")),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        current_time = time.strftime(input_data.format)
        yield ""time"", current_time","Point(row=8, column=0)","Point(row=41, column=34)",,autogpt_platform/backend/backend/blocks/time_blocks.py
Input,class,,"class Input(BlockSchema):
        trigger: str = SchemaField(
            description=""Trigger any data to output the current time""
        )
        format: str = SchemaField(
            description=""Format of the time to output"", default=""%H:%M:%S""
        )","Point(row=9, column=4)","Point(row=15, column=9)",,autogpt_platform/backend/backend/blocks/time_blocks.py
Output,class,,"class Output(BlockSchema):
        time: str = SchemaField(
            description=""Current time in the specified format (default: %H:%M:%S)""
        )","Point(row=17, column=4)","Point(row=20, column=9)",,autogpt_platform/backend/backend/blocks/time_blocks.py
GetCurrentTimeBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""a892b8d9-3e4e-4e9c-9c1e-75f8efcf1bfa"",
            description=""This block outputs the current time."",
            categories={BlockCategory.TEXT},
            input_schema=GetCurrentTimeBlock.Input,
            output_schema=GetCurrentTimeBlock.Output,
            test_input=[
                {""trigger"": ""Hello""},
                {""trigger"": ""Hello"", ""format"": ""%H:%M""},
            ],
            test_output=[
                (""time"", lambda _: time.strftime(""%H:%M:%S"")),
                (""time"", lambda _: time.strftime(""%H:%M"")),
            ],
        )","Point(row=22, column=4)","Point(row=37, column=9)",GetCurrentTimeBlock,autogpt_platform/backend/backend/blocks/time_blocks.py
GetCurrentTimeBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        current_time = time.strftime(input_data.format)
        yield ""time"", current_time","Point(row=39, column=4)","Point(row=41, column=34)",GetCurrentTimeBlock,autogpt_platform/backend/backend/blocks/time_blocks.py
GetCurrentDateBlock,class,,"class GetCurrentDateBlock(Block):
    class Input(BlockSchema):
        trigger: str = SchemaField(
            description=""Trigger any data to output the current date""
        )
        offset: Union[int, str] = SchemaField(
            title=""Days Offset"",
            description=""Offset in days from the current date"",
            default=0,
        )
        format: str = SchemaField(
            description=""Format of the date to output"", default=""%Y-%m-%d""
        )

    class Output(BlockSchema):
        date: str = SchemaField(
            description=""Current date in the specified format (default: YYYY-MM-DD)""
        )

    def __init__(self):
        super().__init__(
            id=""b29c1b50-5d0e-4d9f-8f9d-1b0e6fcbf0b1"",
            description=""This block outputs the current date with an optional offset."",
            categories={BlockCategory.TEXT},
            input_schema=GetCurrentDateBlock.Input,
            output_schema=GetCurrentDateBlock.Output,
            test_input=[
                {""trigger"": ""Hello"", ""offset"": ""7""},
                {""trigger"": ""Hello"", ""offset"": ""7"", ""format"": ""%m/%d/%Y""},
            ],
            test_output=[
                (
                    ""date"",
                    lambda t: abs(datetime.now() - datetime.strptime(t, ""%Y-%m-%d""))
                    < timedelta(days=8),  # 7 days difference + 1 day error margin.
                ),
                (
                    ""date"",
                    lambda t: abs(datetime.now() - datetime.strptime(t, ""%m/%d/%Y""))
                    < timedelta(days=8),
                    # 7 days difference + 1 day error margin.
                ),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        try:
            offset = int(input_data.offset)
        except ValueError:
            offset = 0
        current_date = datetime.now() - timedelta(days=offset)
        yield ""date"", current_date.strftime(input_data.format)","Point(row=44, column=0)","Point(row=95, column=62)",,autogpt_platform/backend/backend/blocks/time_blocks.py
Input,class,,"class Input(BlockSchema):
        trigger: str = SchemaField(
            description=""Trigger any data to output the current date""
        )
        offset: Union[int, str] = SchemaField(
            title=""Days Offset"",
            description=""Offset in days from the current date"",
            default=0,
        )
        format: str = SchemaField(
            description=""Format of the date to output"", default=""%Y-%m-%d""
        )","Point(row=45, column=4)","Point(row=56, column=9)",,autogpt_platform/backend/backend/blocks/time_blocks.py
Output,class,,"class Output(BlockSchema):
        date: str = SchemaField(
            description=""Current date in the specified format (default: YYYY-MM-DD)""
        )","Point(row=58, column=4)","Point(row=61, column=9)",,autogpt_platform/backend/backend/blocks/time_blocks.py
GetCurrentDateBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""b29c1b50-5d0e-4d9f-8f9d-1b0e6fcbf0b1"",
            description=""This block outputs the current date with an optional offset."",
            categories={BlockCategory.TEXT},
            input_schema=GetCurrentDateBlock.Input,
            output_schema=GetCurrentDateBlock.Output,
            test_input=[
                {""trigger"": ""Hello"", ""offset"": ""7""},
                {""trigger"": ""Hello"", ""offset"": ""7"", ""format"": ""%m/%d/%Y""},
            ],
            test_output=[
                (
                    ""date"",
                    lambda t: abs(datetime.now() - datetime.strptime(t, ""%Y-%m-%d""))
                    < timedelta(days=8),  # 7 days difference + 1 day error margin.
                ),
                (
                    ""date"",
                    lambda t: abs(datetime.now() - datetime.strptime(t, ""%m/%d/%Y""))
                    < timedelta(days=8),
                    # 7 days difference + 1 day error margin.
                ),
            ],
        )","Point(row=63, column=4)","Point(row=87, column=9)",GetCurrentDateBlock,autogpt_platform/backend/backend/blocks/time_blocks.py
GetCurrentDateBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        try:
            offset = int(input_data.offset)
        except ValueError:
            offset = 0
        current_date = datetime.now() - timedelta(days=offset)
        yield ""date"", current_date.strftime(input_data.format)","Point(row=89, column=4)","Point(row=95, column=62)",GetCurrentDateBlock,autogpt_platform/backend/backend/blocks/time_blocks.py
GetCurrentDateAndTimeBlock,class,,"class GetCurrentDateAndTimeBlock(Block):
    class Input(BlockSchema):
        trigger: str = SchemaField(
            description=""Trigger any data to output the current date and time""
        )
        format: str = SchemaField(
            description=""Format of the date and time to output"",
            default=""%Y-%m-%d %H:%M:%S"",
        )

    class Output(BlockSchema):
        date_time: str = SchemaField(
            description=""Current date and time in the specified format (default: YYYY-MM-DD HH:MM:SS)""
        )

    def __init__(self):
        super().__init__(
            id=""716a67b3-6760-42e7-86dc-18645c6e00fc"",
            description=""This block outputs the current date and time."",
            categories={BlockCategory.TEXT},
            input_schema=GetCurrentDateAndTimeBlock.Input,
            output_schema=GetCurrentDateAndTimeBlock.Output,
            test_input=[
                {""trigger"": ""Hello""},
            ],
            test_output=[
                (
                    ""date_time"",
                    lambda t: abs(
                        datetime.now() - datetime.strptime(t, ""%Y-%m-%d %H:%M:%S"")
                    )
                    < timedelta(seconds=10),  # 10 seconds error margin.
                ),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        current_date_time = time.strftime(input_data.format)
        yield ""date_time"", current_date_time","Point(row=98, column=0)","Point(row=136, column=44)",,autogpt_platform/backend/backend/blocks/time_blocks.py
Input,class,,"class Input(BlockSchema):
        trigger: str = SchemaField(
            description=""Trigger any data to output the current date and time""
        )
        format: str = SchemaField(
            description=""Format of the date and time to output"",
            default=""%Y-%m-%d %H:%M:%S"",
        )","Point(row=99, column=4)","Point(row=106, column=9)",,autogpt_platform/backend/backend/blocks/time_blocks.py
Output,class,,"class Output(BlockSchema):
        date_time: str = SchemaField(
            description=""Current date and time in the specified format (default: YYYY-MM-DD HH:MM:SS)""
        )","Point(row=108, column=4)","Point(row=111, column=9)",,autogpt_platform/backend/backend/blocks/time_blocks.py
GetCurrentDateAndTimeBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""716a67b3-6760-42e7-86dc-18645c6e00fc"",
            description=""This block outputs the current date and time."",
            categories={BlockCategory.TEXT},
            input_schema=GetCurrentDateAndTimeBlock.Input,
            output_schema=GetCurrentDateAndTimeBlock.Output,
            test_input=[
                {""trigger"": ""Hello""},
            ],
            test_output=[
                (
                    ""date_time"",
                    lambda t: abs(
                        datetime.now() - datetime.strptime(t, ""%Y-%m-%d %H:%M:%S"")
                    )
                    < timedelta(seconds=10),  # 10 seconds error margin.
                ),
            ],
        )","Point(row=113, column=4)","Point(row=132, column=9)",GetCurrentDateAndTimeBlock,autogpt_platform/backend/backend/blocks/time_blocks.py
GetCurrentDateAndTimeBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        current_date_time = time.strftime(input_data.format)
        yield ""date_time"", current_date_time","Point(row=134, column=4)","Point(row=136, column=44)",GetCurrentDateAndTimeBlock,autogpt_platform/backend/backend/blocks/time_blocks.py
CountdownTimerBlock,class,,"class CountdownTimerBlock(Block):
    class Input(BlockSchema):
        input_message: Any = SchemaField(
            description=""Message to output after the timer finishes"",
            default=""timer finished"",
        )
        seconds: Union[int, str] = SchemaField(
            description=""Duration in seconds"", default=0
        )
        minutes: Union[int, str] = SchemaField(
            description=""Duration in minutes"", default=0
        )
        hours: Union[int, str] = SchemaField(description=""Duration in hours"", default=0)
        days: Union[int, str] = SchemaField(description=""Duration in days"", default=0)

    class Output(BlockSchema):
        output_message: str = SchemaField(
            description=""Message after the timer finishes""
        )

    def __init__(self):
        super().__init__(
            id=""d67a9c52-5e4e-11e2-bcfd-0800200c9a71"",
            description=""This block triggers after a specified duration."",
            categories={BlockCategory.TEXT},
            input_schema=CountdownTimerBlock.Input,
            output_schema=CountdownTimerBlock.Output,
            test_input=[
                {""seconds"": 1},
                {""input_message"": ""Custom message""},
            ],
            test_output=[
                (""output_message"", ""timer finished""),
                (""output_message"", ""Custom message""),
            ],
        )

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        seconds = int(input_data.seconds)
        minutes = int(input_data.minutes)
        hours = int(input_data.hours)
        days = int(input_data.days)

        total_seconds = seconds + minutes * 60 + hours * 3600 + days * 86400

        time.sleep(total_seconds)
        yield ""output_message"", input_data.input_message","Point(row=139, column=0)","Point(row=185, column=56)",,autogpt_platform/backend/backend/blocks/time_blocks.py
Input,class,,"class Input(BlockSchema):
        input_message: Any = SchemaField(
            description=""Message to output after the timer finishes"",
            default=""timer finished"",
        )
        seconds: Union[int, str] = SchemaField(
            description=""Duration in seconds"", default=0
        )
        minutes: Union[int, str] = SchemaField(
            description=""Duration in minutes"", default=0
        )
        hours: Union[int, str] = SchemaField(description=""Duration in hours"", default=0)
        days: Union[int, str] = SchemaField(description=""Duration in days"", default=0)","Point(row=140, column=4)","Point(row=152, column=86)",,autogpt_platform/backend/backend/blocks/time_blocks.py
Output,class,,"class Output(BlockSchema):
        output_message: str = SchemaField(
            description=""Message after the timer finishes""
        )","Point(row=154, column=4)","Point(row=157, column=9)",,autogpt_platform/backend/backend/blocks/time_blocks.py
CountdownTimerBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""d67a9c52-5e4e-11e2-bcfd-0800200c9a71"",
            description=""This block triggers after a specified duration."",
            categories={BlockCategory.TEXT},
            input_schema=CountdownTimerBlock.Input,
            output_schema=CountdownTimerBlock.Output,
            test_input=[
                {""seconds"": 1},
                {""input_message"": ""Custom message""},
            ],
            test_output=[
                (""output_message"", ""timer finished""),
                (""output_message"", ""Custom message""),
            ],
        )","Point(row=159, column=4)","Point(row=174, column=9)",CountdownTimerBlock,autogpt_platform/backend/backend/blocks/time_blocks.py
CountdownTimerBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        seconds = int(input_data.seconds)
        minutes = int(input_data.minutes)
        hours = int(input_data.hours)
        days = int(input_data.days)

        total_seconds = seconds + minutes * 60 + hours * 3600 + days * 86400

        time.sleep(total_seconds)
        yield ""output_message"", input_data.input_message","Point(row=176, column=4)","Point(row=185, column=56)",CountdownTimerBlock,autogpt_platform/backend/backend/blocks/time_blocks.py
RedditCredentials,class,,"class RedditCredentials(BaseModel):
    client_id: BlockSecret = SecretField(key=""reddit_client_id"")
    client_secret: BlockSecret = SecretField(key=""reddit_client_secret"")
    username: BlockSecret = SecretField(key=""reddit_username"")
    password: BlockSecret = SecretField(key=""reddit_password"")
    user_agent: str = ""AutoGPT:1.0 (by /u/autogpt)""

    model_config = ConfigDict(title=""Reddit Credentials"")","Point(row=11, column=0)","Point(row=18, column=57)",,autogpt_platform/backend/backend/blocks/reddit.py
RedditPost,class,,"class RedditPost(BaseModel):
    id: str
    subreddit: str
    title: str
    body: str","Point(row=21, column=0)","Point(row=25, column=13)",,autogpt_platform/backend/backend/blocks/reddit.py
RedditComment,class,,"class RedditComment(BaseModel):
    post_id: str
    comment: str","Point(row=28, column=0)","Point(row=30, column=16)",,autogpt_platform/backend/backend/blocks/reddit.py
get_praw,function,,"def get_praw(creds: RedditCredentials) -> praw.Reddit:
    client = praw.Reddit(
        client_id=creds.client_id.get_secret_value(),
        client_secret=creds.client_secret.get_secret_value(),
        username=creds.username.get_secret_value(),
        password=creds.password.get_secret_value(),
        user_agent=creds.user_agent,
    )
    me = client.user.me()
    if not me:
        raise ValueError(""Invalid Reddit credentials."")
    print(f""Logged in as Reddit user: {me.name}"")
    return client","Point(row=33, column=0)","Point(row=45, column=17)",,autogpt_platform/backend/backend/blocks/reddit.py
GetRedditPostsBlock,class,,"class GetRedditPostsBlock(Block):
    class Input(BlockSchema):
        subreddit: str = SchemaField(description=""Subreddit name"")
        creds: RedditCredentials = SchemaField(
            description=""Reddit credentials"",
            default=RedditCredentials(),
        )
        last_minutes: int | None = SchemaField(
            description=""Post time to stop minutes ago while fetching posts"",
            default=None,
        )
        last_post: str | None = SchemaField(
            description=""Post ID to stop when reached while fetching posts"",
            default=None,
        )
        post_limit: int | None = SchemaField(
            description=""Number of posts to fetch"", default=10
        )

    class Output(BlockSchema):
        post: RedditPost = SchemaField(description=""Reddit post"")

    def __init__(self):
        super().__init__(
            disabled=True,
            id=""c6731acb-4285-4ee1-bc9b-03d0766c370f"",
            description=""This block fetches Reddit posts from a defined subreddit name."",
            categories={BlockCategory.SOCIAL},
            input_schema=GetRedditPostsBlock.Input,
            output_schema=GetRedditPostsBlock.Output,
            test_input={
                ""creds"": {
                    ""client_id"": ""client_id"",
                    ""client_secret"": ""client_secret"",
                    ""username"": ""username"",
                    ""password"": ""password"",
                    ""user_agent"": ""user_agent"",
                },
                ""subreddit"": ""subreddit"",
                ""last_post"": ""id3"",
                ""post_limit"": 2,
            },
            test_output=[
                (
                    ""post"",
                    RedditPost(
                        id=""id1"", subreddit=""subreddit"", title=""title1"", body=""body1""
                    ),
                ),
                (
                    ""post"",
                    RedditPost(
                        id=""id2"", subreddit=""subreddit"", title=""title2"", body=""body2""
                    ),
                ),
            ],
            test_mock={
                ""get_posts"": lambda _: [
                    MockObject(id=""id1"", title=""title1"", selftext=""body1""),
                    MockObject(id=""id2"", title=""title2"", selftext=""body2""),
                    MockObject(id=""id3"", title=""title2"", selftext=""body2""),
                ]
            },
        )

    @staticmethod
    def get_posts(input_data: Input) -> Iterator[praw.reddit.Submission]:
        client = get_praw(input_data.creds)
        subreddit = client.subreddit(input_data.subreddit)
        return subreddit.new(limit=input_data.post_limit or 10)

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        current_time = datetime.now(tz=timezone.utc)
        for post in self.get_posts(input_data):
            if input_data.last_minutes:
                post_datetime = datetime.fromtimestamp(
                    post.created_utc, tz=timezone.utc
                )
                time_difference = current_time - post_datetime
                if time_difference.total_seconds() / 60 > input_data.last_minutes:
                    continue

            if input_data.last_post and post.id == input_data.last_post:
                break

            yield ""post"", RedditPost(
                id=post.id,
                subreddit=input_data.subreddit,
                title=post.title,
                body=post.selftext,
            )","Point(row=48, column=0)","Point(row=138, column=13)",,autogpt_platform/backend/backend/blocks/reddit.py
Input,class,,"class Input(BlockSchema):
        subreddit: str = SchemaField(description=""Subreddit name"")
        creds: RedditCredentials = SchemaField(
            description=""Reddit credentials"",
            default=RedditCredentials(),
        )
        last_minutes: int | None = SchemaField(
            description=""Post time to stop minutes ago while fetching posts"",
            default=None,
        )
        last_post: str | None = SchemaField(
            description=""Post ID to stop when reached while fetching posts"",
            default=None,
        )
        post_limit: int | None = SchemaField(
            description=""Number of posts to fetch"", default=10
        )","Point(row=49, column=4)","Point(row=65, column=9)",,autogpt_platform/backend/backend/blocks/reddit.py
Output,class,,"class Output(BlockSchema):
        post: RedditPost = SchemaField(description=""Reddit post"")","Point(row=67, column=4)","Point(row=68, column=65)",,autogpt_platform/backend/backend/blocks/reddit.py
GetRedditPostsBlock.__init__,function,,"def __init__(self):
        super().__init__(
            disabled=True,
            id=""c6731acb-4285-4ee1-bc9b-03d0766c370f"",
            description=""This block fetches Reddit posts from a defined subreddit name."",
            categories={BlockCategory.SOCIAL},
            input_schema=GetRedditPostsBlock.Input,
            output_schema=GetRedditPostsBlock.Output,
            test_input={
                ""creds"": {
                    ""client_id"": ""client_id"",
                    ""client_secret"": ""client_secret"",
                    ""username"": ""username"",
                    ""password"": ""password"",
                    ""user_agent"": ""user_agent"",
                },
                ""subreddit"": ""subreddit"",
                ""last_post"": ""id3"",
                ""post_limit"": 2,
            },
            test_output=[
                (
                    ""post"",
                    RedditPost(
                        id=""id1"", subreddit=""subreddit"", title=""title1"", body=""body1""
                    ),
                ),
                (
                    ""post"",
                    RedditPost(
                        id=""id2"", subreddit=""subreddit"", title=""title2"", body=""body2""
                    ),
                ),
            ],
            test_mock={
                ""get_posts"": lambda _: [
                    MockObject(id=""id1"", title=""title1"", selftext=""body1""),
                    MockObject(id=""id2"", title=""title2"", selftext=""body2""),
                    MockObject(id=""id3"", title=""title2"", selftext=""body2""),
                ]
            },
        )","Point(row=70, column=4)","Point(row=111, column=9)",GetRedditPostsBlock,autogpt_platform/backend/backend/blocks/reddit.py
GetRedditPostsBlock.get_posts,function,,"def get_posts(input_data: Input) -> Iterator[praw.reddit.Submission]:
        client = get_praw(input_data.creds)
        subreddit = client.subreddit(input_data.subreddit)
        return subreddit.new(limit=input_data.post_limit or 10)","Point(row=114, column=4)","Point(row=117, column=63)",GetRedditPostsBlock,autogpt_platform/backend/backend/blocks/reddit.py
GetRedditPostsBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        current_time = datetime.now(tz=timezone.utc)
        for post in self.get_posts(input_data):
            if input_data.last_minutes:
                post_datetime = datetime.fromtimestamp(
                    post.created_utc, tz=timezone.utc
                )
                time_difference = current_time - post_datetime
                if time_difference.total_seconds() / 60 > input_data.last_minutes:
                    continue

            if input_data.last_post and post.id == input_data.last_post:
                break

            yield ""post"", RedditPost(
                id=post.id,
                subreddit=input_data.subreddit,
                title=post.title,
                body=post.selftext,
            )","Point(row=119, column=4)","Point(row=138, column=13)",GetRedditPostsBlock,autogpt_platform/backend/backend/blocks/reddit.py
PostRedditCommentBlock,class,,"class PostRedditCommentBlock(Block):
    class Input(BlockSchema):
        creds: RedditCredentials = SchemaField(
            description=""Reddit credentials"", default=RedditCredentials()
        )
        data: RedditComment = SchemaField(description=""Reddit comment"")

    class Output(BlockSchema):
        comment_id: str = SchemaField(description=""Posted comment ID"")

    def __init__(self):
        super().__init__(
            id=""4a92261b-701e-4ffb-8970-675fd28e261f"",
            description=""This block posts a Reddit comment on a specified Reddit post."",
            categories={BlockCategory.SOCIAL},
            input_schema=PostRedditCommentBlock.Input,
            output_schema=PostRedditCommentBlock.Output,
            test_input={""data"": {""post_id"": ""id"", ""comment"": ""comment""}},
            test_output=[(""comment_id"", ""dummy_comment_id"")],
            test_mock={""reply_post"": lambda creds, comment: ""dummy_comment_id""},
        )

    @staticmethod
    def reply_post(creds: RedditCredentials, comment: RedditComment) -> str:
        client = get_praw(creds)
        submission = client.submission(id=comment.post_id)
        new_comment = submission.reply(comment.comment)
        if not new_comment:
            raise ValueError(""Failed to post comment."")
        return new_comment.id

    def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""comment_id"", self.reply_post(input_data.creds, input_data.data)","Point(row=141, column=0)","Point(row=173, column=78)",,autogpt_platform/backend/backend/blocks/reddit.py
Input,class,,"class Input(BlockSchema):
        creds: RedditCredentials = SchemaField(
            description=""Reddit credentials"", default=RedditCredentials()
        )
        data: RedditComment = SchemaField(description=""Reddit comment"")","Point(row=142, column=4)","Point(row=146, column=71)",,autogpt_platform/backend/backend/blocks/reddit.py
Output,class,,"class Output(BlockSchema):
        comment_id: str = SchemaField(description=""Posted comment ID"")","Point(row=148, column=4)","Point(row=149, column=70)",,autogpt_platform/backend/backend/blocks/reddit.py
PostRedditCommentBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""4a92261b-701e-4ffb-8970-675fd28e261f"",
            description=""This block posts a Reddit comment on a specified Reddit post."",
            categories={BlockCategory.SOCIAL},
            input_schema=PostRedditCommentBlock.Input,
            output_schema=PostRedditCommentBlock.Output,
            test_input={""data"": {""post_id"": ""id"", ""comment"": ""comment""}},
            test_output=[(""comment_id"", ""dummy_comment_id"")],
            test_mock={""reply_post"": lambda creds, comment: ""dummy_comment_id""},
        )","Point(row=151, column=4)","Point(row=161, column=9)",PostRedditCommentBlock,autogpt_platform/backend/backend/blocks/reddit.py
PostRedditCommentBlock.reply_post,function,,"def reply_post(creds: RedditCredentials, comment: RedditComment) -> str:
        client = get_praw(creds)
        submission = client.submission(id=comment.post_id)
        new_comment = submission.reply(comment.comment)
        if not new_comment:
            raise ValueError(""Failed to post comment."")
        return new_comment.id","Point(row=164, column=4)","Point(row=170, column=29)",PostRedditCommentBlock,autogpt_platform/backend/backend/blocks/reddit.py
PostRedditCommentBlock.run,function,,"def run(self, input_data: Input, **kwargs) -> BlockOutput:
        yield ""comment_id"", self.reply_post(input_data.creds, input_data.data)","Point(row=172, column=4)","Point(row=173, column=78)",PostRedditCommentBlock,autogpt_platform/backend/backend/blocks/reddit.py
IdeogramModelName,class,,"class IdeogramModelName(str, Enum):
    V2 = ""V_2""
    V1 = ""V_1""
    V1_TURBO = ""V_1_TURBO""
    V2_TURBO = ""V_2_TURBO""","Point(row=26, column=0)","Point(row=30, column=26)",,autogpt_platform/backend/backend/blocks/ideogram.py
MagicPromptOption,class,,"class MagicPromptOption(str, Enum):
    AUTO = ""AUTO""
    ON = ""ON""
    OFF = ""OFF""","Point(row=33, column=0)","Point(row=36, column=15)",,autogpt_platform/backend/backend/blocks/ideogram.py
StyleType,class,,"class StyleType(str, Enum):
    AUTO = ""AUTO""
    GENERAL = ""GENERAL""
    REALISTIC = ""REALISTIC""
    DESIGN = ""DESIGN""
    RENDER_3D = ""RENDER_3D""
    ANIME = ""ANIME""","Point(row=39, column=0)","Point(row=45, column=19)",,autogpt_platform/backend/backend/blocks/ideogram.py
ColorPalettePreset,class,,"class ColorPalettePreset(str, Enum):
    NONE = ""NONE""
    EMBER = ""EMBER""
    FRESH = ""FRESH""
    JUNGLE = ""JUNGLE""
    MAGIC = ""MAGIC""
    MELON = ""MELON""
    MOSAIC = ""MOSAIC""
    PASTEL = ""PASTEL""
    ULTRAMARINE = ""ULTRAMARINE""","Point(row=48, column=0)","Point(row=57, column=31)",,autogpt_platform/backend/backend/blocks/ideogram.py
AspectRatio,class,,"class AspectRatio(str, Enum):
    ASPECT_10_16 = ""ASPECT_10_16""
    ASPECT_16_10 = ""ASPECT_16_10""
    ASPECT_9_16 = ""ASPECT_9_16""
    ASPECT_16_9 = ""ASPECT_16_9""
    ASPECT_3_2 = ""ASPECT_3_2""
    ASPECT_2_3 = ""ASPECT_2_3""
    ASPECT_4_3 = ""ASPECT_4_3""
    ASPECT_3_4 = ""ASPECT_3_4""
    ASPECT_1_1 = ""ASPECT_1_1""
    ASPECT_1_3 = ""ASPECT_1_3""
    ASPECT_3_1 = ""ASPECT_3_1""","Point(row=60, column=0)","Point(row=71, column=29)",,autogpt_platform/backend/backend/blocks/ideogram.py
UpscaleOption,class,,"class UpscaleOption(str, Enum):
    AI_UPSCALE = ""AI Upscale""
    NO_UPSCALE = ""No Upscale""","Point(row=74, column=0)","Point(row=76, column=29)",,autogpt_platform/backend/backend/blocks/ideogram.py
IdeogramModelBlock,class,,"class IdeogramModelBlock(Block):
    class Input(BlockSchema):

        credentials: CredentialsMetaInput[Literal[""ideogram""], Literal[""api_key""]] = (
            CredentialsField(
                provider=""ideogram"",
                supported_credential_types={""api_key""},
                description=""The Ideogram integration can be used with any API key with sufficient permissions for the blocks it is used on."",
            )
        )
        prompt: str = SchemaField(
            description=""Text prompt for image generation"",
            placeholder=""e.g., 'A futuristic cityscape at sunset'"",
            title=""Prompt"",
        )
        ideogram_model_name: IdeogramModelName = SchemaField(
            description=""The name of the Image Generation Model, e.g., V_2"",
            default=IdeogramModelName.V2,
            title=""Image Generation Model"",
            advanced=False,
        )
        aspect_ratio: AspectRatio = SchemaField(
            description=""Aspect ratio for the generated image"",
            default=AspectRatio.ASPECT_1_1,
            title=""Aspect Ratio"",
            advanced=False,
        )
        upscale: UpscaleOption = SchemaField(
            description=""Upscale the generated image"",
            default=UpscaleOption.NO_UPSCALE,
            title=""Upscale Image"",
            advanced=False,
        )
        magic_prompt_option: MagicPromptOption = SchemaField(
            description=""Whether to use MagicPrompt for enhancing the request"",
            default=MagicPromptOption.AUTO,
            title=""Magic Prompt Option"",
            advanced=True,
        )
        seed: Optional[int] = SchemaField(
            description=""Random seed. Set for reproducible generation"",
            default=None,
            title=""Seed"",
            advanced=True,
        )
        style_type: StyleType = SchemaField(
            description=""Style type to apply, applicable for V_2 and above"",
            default=StyleType.AUTO,
            title=""Style Type"",
            advanced=True,
        )
        negative_prompt: Optional[str] = SchemaField(
            description=""Description of what to exclude from the image"",
            default=None,
            title=""Negative Prompt"",
            advanced=True,
        )
        color_palette_name: ColorPalettePreset = SchemaField(
            description=""Color palette preset name, choose 'None' to skip"",
            default=ColorPalettePreset.NONE,
            title=""Color Palette Preset"",
            advanced=True,
        )

    class Output(BlockSchema):
        result: str = SchemaField(description=""Generated image URL"")
        error: str = SchemaField(description=""Error message if the model run failed"")

    def __init__(self):
        super().__init__(
            id=""6ab085e2-20b3-4055-bc3e-08036e01eca6"",
            description=""This block runs Ideogram models with both simple and advanced settings."",
            categories={BlockCategory.AI},
            input_schema=IdeogramModelBlock.Input,
            output_schema=IdeogramModelBlock.Output,
            test_input={
                ""ideogram_model_name"": IdeogramModelName.V2,
                ""prompt"": ""A futuristic cityscape at sunset"",
                ""aspect_ratio"": AspectRatio.ASPECT_1_1,
                ""upscale"": UpscaleOption.NO_UPSCALE,
                ""magic_prompt_option"": MagicPromptOption.AUTO,
                ""seed"": None,
                ""style_type"": StyleType.AUTO,
                ""negative_prompt"": None,
                ""color_palette_name"": ColorPalettePreset.NONE,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[
                (
                    ""result"",
                    ""https://ideogram.ai/api/images/test-generated-image-url.png"",
                ),
            ],
            test_mock={
                ""run_model"": lambda api_key, model_name, prompt, seed, aspect_ratio, magic_prompt_option, style_type, negative_prompt, color_palette_name: ""https://ideogram.ai/api/images/test-generated-image-url.png"",
                ""upscale_image"": lambda api_key, image_url: ""https://ideogram.ai/api/images/test-upscaled-image-url.png"",
            },
            test_credentials=TEST_CREDENTIALS,
        )

    def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        seed = input_data.seed

        # Step 1: Generate the image
        result = self.run_model(
            api_key=credentials.api_key,
            model_name=input_data.ideogram_model_name.value,
            prompt=input_data.prompt,
            seed=seed,
            aspect_ratio=input_data.aspect_ratio.value,
            magic_prompt_option=input_data.magic_prompt_option.value,
            style_type=input_data.style_type.value,
            negative_prompt=input_data.negative_prompt,
            color_palette_name=input_data.color_palette_name.value,
        )

        # Step 2: Upscale the image if requested
        if input_data.upscale == UpscaleOption.AI_UPSCALE:
            result = self.upscale_image(
                api_key=credentials.api_key,
                image_url=result,
            )

        yield ""result"", result

    def run_model(
        self,
        api_key: SecretStr,
        model_name: str,
        prompt: str,
        seed: Optional[int],
        aspect_ratio: str,
        magic_prompt_option: str,
        style_type: str,
        negative_prompt: Optional[str],
        color_palette_name: str,
    ):
        url = ""https://api.ideogram.ai/generate""
        headers = {
            ""Api-Key"": api_key.get_secret_value(),
            ""Content-Type"": ""application/json"",
        }

        data: Dict[str, Any] = {
            ""image_request"": {
                ""prompt"": prompt,
                ""model"": model_name,
                ""aspect_ratio"": aspect_ratio,
                ""magic_prompt_option"": magic_prompt_option,
                ""style_type"": style_type,
            }
        }

        if seed is not None:
            data[""image_request""][""seed""] = seed

        if negative_prompt:
            data[""image_request""][""negative_prompt""] = negative_prompt

        if color_palette_name != ""NONE"":
            data[""image_request""][""color_palette""] = {""name"": color_palette_name}

        try:
            response = requests.post(url, json=data, headers=headers)
            return response.json()[""data""][0][""url""]
        except RequestException as e:
            raise Exception(f""Failed to fetch image: {str(e)}"")

    def upscale_image(self, api_key: SecretStr, image_url: str):
        url = ""https://api.ideogram.ai/upscale""
        headers = {
            ""Api-Key"": api_key.get_secret_value(),
        }

        try:
            # Step 1: Download the image from the provided URL
            image_response = requests.get(image_url)

            # Step 2: Send the downloaded image to the upscale API
            files = {
                ""image_file"": (""image.png"", image_response.content, ""image/png""),
            }

            response = requests.post(
                url,
                headers=headers,
                data={
                    ""image_request"": ""{}"",  # Empty JSON object
                },
                files=files,
            )

            return response.json()[""data""][0][""url""]

        except RequestException as e:
            raise Exception(f""Failed to upscale image: {str(e)}"")","Point(row=79, column=0)","Point(row=276, column=65)",,autogpt_platform/backend/backend/blocks/ideogram.py
Input,class,,"class Input(BlockSchema):

        credentials: CredentialsMetaInput[Literal[""ideogram""], Literal[""api_key""]] = (
            CredentialsField(
                provider=""ideogram"",
                supported_credential_types={""api_key""},
                description=""The Ideogram integration can be used with any API key with sufficient permissions for the blocks it is used on."",
            )
        )
        prompt: str = SchemaField(
            description=""Text prompt for image generation"",
            placeholder=""e.g., 'A futuristic cityscape at sunset'"",
            title=""Prompt"",
        )
        ideogram_model_name: IdeogramModelName = SchemaField(
            description=""The name of the Image Generation Model, e.g., V_2"",
            default=IdeogramModelName.V2,
            title=""Image Generation Model"",
            advanced=False,
        )
        aspect_ratio: AspectRatio = SchemaField(
            description=""Aspect ratio for the generated image"",
            default=AspectRatio.ASPECT_1_1,
            title=""Aspect Ratio"",
            advanced=False,
        )
        upscale: UpscaleOption = SchemaField(
            description=""Upscale the generated image"",
            default=UpscaleOption.NO_UPSCALE,
            title=""Upscale Image"",
            advanced=False,
        )
        magic_prompt_option: MagicPromptOption = SchemaField(
            description=""Whether to use MagicPrompt for enhancing the request"",
            default=MagicPromptOption.AUTO,
            title=""Magic Prompt Option"",
            advanced=True,
        )
        seed: Optional[int] = SchemaField(
            description=""Random seed. Set for reproducible generation"",
            default=None,
            title=""Seed"",
            advanced=True,
        )
        style_type: StyleType = SchemaField(
            description=""Style type to apply, applicable for V_2 and above"",
            default=StyleType.AUTO,
            title=""Style Type"",
            advanced=True,
        )
        negative_prompt: Optional[str] = SchemaField(
            description=""Description of what to exclude from the image"",
            default=None,
            title=""Negative Prompt"",
            advanced=True,
        )
        color_palette_name: ColorPalettePreset = SchemaField(
            description=""Color palette preset name, choose 'None' to skip"",
            default=ColorPalettePreset.NONE,
            title=""Color Palette Preset"",
            advanced=True,
        )","Point(row=80, column=4)","Point(row=141, column=9)",,autogpt_platform/backend/backend/blocks/ideogram.py
Output,class,,"class Output(BlockSchema):
        result: str = SchemaField(description=""Generated image URL"")
        error: str = SchemaField(description=""Error message if the model run failed"")","Point(row=143, column=4)","Point(row=145, column=85)",,autogpt_platform/backend/backend/blocks/ideogram.py
IdeogramModelBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""6ab085e2-20b3-4055-bc3e-08036e01eca6"",
            description=""This block runs Ideogram models with both simple and advanced settings."",
            categories={BlockCategory.AI},
            input_schema=IdeogramModelBlock.Input,
            output_schema=IdeogramModelBlock.Output,
            test_input={
                ""ideogram_model_name"": IdeogramModelName.V2,
                ""prompt"": ""A futuristic cityscape at sunset"",
                ""aspect_ratio"": AspectRatio.ASPECT_1_1,
                ""upscale"": UpscaleOption.NO_UPSCALE,
                ""magic_prompt_option"": MagicPromptOption.AUTO,
                ""seed"": None,
                ""style_type"": StyleType.AUTO,
                ""negative_prompt"": None,
                ""color_palette_name"": ColorPalettePreset.NONE,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_output=[
                (
                    ""result"",
                    ""https://ideogram.ai/api/images/test-generated-image-url.png"",
                ),
            ],
            test_mock={
                ""run_model"": lambda api_key, model_name, prompt, seed, aspect_ratio, magic_prompt_option, style_type, negative_prompt, color_palette_name: ""https://ideogram.ai/api/images/test-generated-image-url.png"",
                ""upscale_image"": lambda api_key, image_url: ""https://ideogram.ai/api/images/test-upscaled-image-url.png"",
            },
            test_credentials=TEST_CREDENTIALS,
        )","Point(row=147, column=4)","Point(row=177, column=9)",IdeogramModelBlock,autogpt_platform/backend/backend/blocks/ideogram.py
IdeogramModelBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: APIKeyCredentials, **kwargs
    ) -> BlockOutput:
        seed = input_data.seed

        # Step 1: Generate the image
        result = self.run_model(
            api_key=credentials.api_key,
            model_name=input_data.ideogram_model_name.value,
            prompt=input_data.prompt,
            seed=seed,
            aspect_ratio=input_data.aspect_ratio.value,
            magic_prompt_option=input_data.magic_prompt_option.value,
            style_type=input_data.style_type.value,
            negative_prompt=input_data.negative_prompt,
            color_palette_name=input_data.color_palette_name.value,
        )

        # Step 2: Upscale the image if requested
        if input_data.upscale == UpscaleOption.AI_UPSCALE:
            result = self.upscale_image(
                api_key=credentials.api_key,
                image_url=result,
            )

        yield ""result"", result","Point(row=179, column=4)","Point(row=204, column=30)",IdeogramModelBlock,autogpt_platform/backend/backend/blocks/ideogram.py
IdeogramModelBlock.run_model,function,,"def run_model(
        self,
        api_key: SecretStr,
        model_name: str,
        prompt: str,
        seed: Optional[int],
        aspect_ratio: str,
        magic_prompt_option: str,
        style_type: str,
        negative_prompt: Optional[str],
        color_palette_name: str,
    ):
        url = ""https://api.ideogram.ai/generate""
        headers = {
            ""Api-Key"": api_key.get_secret_value(),
            ""Content-Type"": ""application/json"",
        }

        data: Dict[str, Any] = {
            ""image_request"": {
                ""prompt"": prompt,
                ""model"": model_name,
                ""aspect_ratio"": aspect_ratio,
                ""magic_prompt_option"": magic_prompt_option,
                ""style_type"": style_type,
            }
        }

        if seed is not None:
            data[""image_request""][""seed""] = seed

        if negative_prompt:
            data[""image_request""][""negative_prompt""] = negative_prompt

        if color_palette_name != ""NONE"":
            data[""image_request""][""color_palette""] = {""name"": color_palette_name}

        try:
            response = requests.post(url, json=data, headers=headers)
            return response.json()[""data""][0][""url""]
        except RequestException as e:
            raise Exception(f""Failed to fetch image: {str(e)}"")","Point(row=206, column=4)","Point(row=247, column=63)",IdeogramModelBlock,autogpt_platform/backend/backend/blocks/ideogram.py
IdeogramModelBlock.upscale_image,function,,"def upscale_image(self, api_key: SecretStr, image_url: str):
        url = ""https://api.ideogram.ai/upscale""
        headers = {
            ""Api-Key"": api_key.get_secret_value(),
        }

        try:
            # Step 1: Download the image from the provided URL
            image_response = requests.get(image_url)

            # Step 2: Send the downloaded image to the upscale API
            files = {
                ""image_file"": (""image.png"", image_response.content, ""image/png""),
            }

            response = requests.post(
                url,
                headers=headers,
                data={
                    ""image_request"": ""{}"",  # Empty JSON object
                },
                files=files,
            )

            return response.json()[""data""][0][""url""]

        except RequestException as e:
            raise Exception(f""Failed to upscale image: {str(e)}"")","Point(row=249, column=4)","Point(row=276, column=65)",IdeogramModelBlock,autogpt_platform/backend/backend/blocks/ideogram.py
GoogleSheetsReadBlock,class,,"class GoogleSheetsReadBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/spreadsheets.readonly""]
        )
        spreadsheet_id: str = SchemaField(
            description=""The ID of the spreadsheet to read from"",
        )
        range: str = SchemaField(
            description=""The A1 notation of the range to read"",
        )

    class Output(BlockSchema):
        result: list[list[str]] = SchemaField(
            description=""The data read from the spreadsheet"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""5724e902-3635-47e9-a108-aaa0263a4988"",
            description=""This block reads data from a Google Sheets spreadsheet."",
            categories={BlockCategory.DATA},
            input_schema=GoogleSheetsReadBlock.Input,
            output_schema=GoogleSheetsReadBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""spreadsheet_id"": ""1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms"",
                ""range"": ""Sheet1!A1:B2"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    [
                        [""Name"", ""Score""],
                        [""Alice"", ""85""],
                    ],
                ),
            ],
            test_mock={
                ""_read_sheet"": lambda *args, **kwargs: [
                    [""Name"", ""Score""],
                    [""Alice"", ""85""],
                ],
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = self._build_service(credentials, **kwargs)
        data = self._read_sheet(service, input_data.spreadsheet_id, input_data.range)
        yield ""result"", data

    @staticmethod
    def _build_service(credentials: GoogleCredentials, **kwargs):
        creds = Credentials(
            token=(
                credentials.access_token.get_secret_value()
                if credentials.access_token
                else None
            ),
            refresh_token=(
                credentials.refresh_token.get_secret_value()
                if credentials.refresh_token
                else None
            ),
            token_uri=""https://oauth2.googleapis.com/token"",
            client_id=kwargs.get(""client_id""),
            client_secret=kwargs.get(""client_secret""),
            scopes=credentials.scopes,
        )
        return build(""sheets"", ""v4"", credentials=creds)

    def _read_sheet(self, service, spreadsheet_id: str, range: str) -> list[list[str]]:
        sheet = service.spreadsheets()
        result = sheet.values().get(spreadsheetId=spreadsheet_id, range=range).execute()
        return result.get(""values"", [])","Point(row=16, column=0)","Point(row=97, column=39)",,autogpt_platform/backend/backend/blocks/google/sheets.py
Input,class,,"class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/spreadsheets.readonly""]
        )
        spreadsheet_id: str = SchemaField(
            description=""The ID of the spreadsheet to read from"",
        )
        range: str = SchemaField(
            description=""The A1 notation of the range to read"",
        )","Point(row=17, column=4)","Point(row=26, column=9)",,autogpt_platform/backend/backend/blocks/google/sheets.py
Output,class,,"class Output(BlockSchema):
        result: list[list[str]] = SchemaField(
            description=""The data read from the spreadsheet"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )","Point(row=28, column=4)","Point(row=34, column=9)",,autogpt_platform/backend/backend/blocks/google/sheets.py
GoogleSheetsReadBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""5724e902-3635-47e9-a108-aaa0263a4988"",
            description=""This block reads data from a Google Sheets spreadsheet."",
            categories={BlockCategory.DATA},
            input_schema=GoogleSheetsReadBlock.Input,
            output_schema=GoogleSheetsReadBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""spreadsheet_id"": ""1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms"",
                ""range"": ""Sheet1!A1:B2"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    [
                        [""Name"", ""Score""],
                        [""Alice"", ""85""],
                    ],
                ),
            ],
            test_mock={
                ""_read_sheet"": lambda *args, **kwargs: [
                    [""Name"", ""Score""],
                    [""Alice"", ""85""],
                ],
            },
        )","Point(row=36, column=4)","Point(row=65, column=9)",GoogleSheetsReadBlock,autogpt_platform/backend/backend/blocks/google/sheets.py
GoogleSheetsReadBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = self._build_service(credentials, **kwargs)
        data = self._read_sheet(service, input_data.spreadsheet_id, input_data.range)
        yield ""result"", data","Point(row=67, column=4)","Point(row=72, column=28)",GoogleSheetsReadBlock,autogpt_platform/backend/backend/blocks/google/sheets.py
GoogleSheetsReadBlock._build_service,function,,"def _build_service(credentials: GoogleCredentials, **kwargs):
        creds = Credentials(
            token=(
                credentials.access_token.get_secret_value()
                if credentials.access_token
                else None
            ),
            refresh_token=(
                credentials.refresh_token.get_secret_value()
                if credentials.refresh_token
                else None
            ),
            token_uri=""https://oauth2.googleapis.com/token"",
            client_id=kwargs.get(""client_id""),
            client_secret=kwargs.get(""client_secret""),
            scopes=credentials.scopes,
        )
        return build(""sheets"", ""v4"", credentials=creds)","Point(row=75, column=4)","Point(row=92, column=55)",GoogleSheetsReadBlock,autogpt_platform/backend/backend/blocks/google/sheets.py
GoogleSheetsReadBlock._read_sheet,function,,"def _read_sheet(self, service, spreadsheet_id: str, range: str) -> list[list[str]]:
        sheet = service.spreadsheets()
        result = sheet.values().get(spreadsheetId=spreadsheet_id, range=range).execute()
        return result.get(""values"", [])","Point(row=94, column=4)","Point(row=97, column=39)",GoogleSheetsReadBlock,autogpt_platform/backend/backend/blocks/google/sheets.py
GoogleSheetsWriteBlock,class,,"class GoogleSheetsWriteBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/spreadsheets""]
        )
        spreadsheet_id: str = SchemaField(
            description=""The ID of the spreadsheet to write to"",
        )
        range: str = SchemaField(
            description=""The A1 notation of the range to write"",
        )
        values: list[list[str]] = SchemaField(
            description=""The data to write to the spreadsheet"",
        )

    class Output(BlockSchema):
        result: dict = SchemaField(
            description=""The result of the write operation"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""d9291e87-301d-47a8-91fe-907fb55460e5"",
            description=""This block writes data to a Google Sheets spreadsheet."",
            categories={BlockCategory.DATA},
            input_schema=GoogleSheetsWriteBlock.Input,
            output_schema=GoogleSheetsWriteBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""spreadsheet_id"": ""1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms"",
                ""range"": ""Sheet1!A1:B2"",
                ""values"": [
                    [""Name"", ""Score""],
                    [""Bob"", ""90""],
                ],
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    {""updatedCells"": 4, ""updatedColumns"": 2, ""updatedRows"": 2},
                ),
            ],
            test_mock={
                ""_write_sheet"": lambda *args, **kwargs: {
                    ""updatedCells"": 4,
                    ""updatedColumns"": 2,
                    ""updatedRows"": 2,
                },
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GoogleSheetsReadBlock._build_service(credentials, **kwargs)
        result = self._write_sheet(
            service,
            input_data.spreadsheet_id,
            input_data.range,
            input_data.values,
        )
        yield ""result"", result

    def _write_sheet(
        self, service, spreadsheet_id: str, range: str, values: list[list[str]]
    ) -> dict:
        body = {""values"": values}
        result = (
            service.spreadsheets()
            .values()
            .update(
                spreadsheetId=spreadsheet_id,
                range=range,
                valueInputOption=""USER_ENTERED"",
                body=body,
            )
            .execute()
        )
        return result","Point(row=100, column=0)","Point(row=183, column=21)",,autogpt_platform/backend/backend/blocks/google/sheets.py
Input,class,,"class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/spreadsheets""]
        )
        spreadsheet_id: str = SchemaField(
            description=""The ID of the spreadsheet to write to"",
        )
        range: str = SchemaField(
            description=""The A1 notation of the range to write"",
        )
        values: list[list[str]] = SchemaField(
            description=""The data to write to the spreadsheet"",
        )","Point(row=101, column=4)","Point(row=113, column=9)",,autogpt_platform/backend/backend/blocks/google/sheets.py
Output,class,,"class Output(BlockSchema):
        result: dict = SchemaField(
            description=""The result of the write operation"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )","Point(row=115, column=4)","Point(row=121, column=9)",,autogpt_platform/backend/backend/blocks/google/sheets.py
GoogleSheetsWriteBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""d9291e87-301d-47a8-91fe-907fb55460e5"",
            description=""This block writes data to a Google Sheets spreadsheet."",
            categories={BlockCategory.DATA},
            input_schema=GoogleSheetsWriteBlock.Input,
            output_schema=GoogleSheetsWriteBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""spreadsheet_id"": ""1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms"",
                ""range"": ""Sheet1!A1:B2"",
                ""values"": [
                    [""Name"", ""Score""],
                    [""Bob"", ""90""],
                ],
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    {""updatedCells"": 4, ""updatedColumns"": 2, ""updatedRows"": 2},
                ),
            ],
            test_mock={
                ""_write_sheet"": lambda *args, **kwargs: {
                    ""updatedCells"": 4,
                    ""updatedColumns"": 2,
                    ""updatedRows"": 2,
                },
            },
        )","Point(row=123, column=4)","Point(row=154, column=9)",GoogleSheetsWriteBlock,autogpt_platform/backend/backend/blocks/google/sheets.py
GoogleSheetsWriteBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GoogleSheetsReadBlock._build_service(credentials, **kwargs)
        result = self._write_sheet(
            service,
            input_data.spreadsheet_id,
            input_data.range,
            input_data.values,
        )
        yield ""result"", result","Point(row=156, column=4)","Point(row=166, column=30)",GoogleSheetsWriteBlock,autogpt_platform/backend/backend/blocks/google/sheets.py
GoogleSheetsWriteBlock._write_sheet,function,,"def _write_sheet(
        self, service, spreadsheet_id: str, range: str, values: list[list[str]]
    ) -> dict:
        body = {""values"": values}
        result = (
            service.spreadsheets()
            .values()
            .update(
                spreadsheetId=spreadsheet_id,
                range=range,
                valueInputOption=""USER_ENTERED"",
                body=body,
            )
            .execute()
        )
        return result","Point(row=168, column=4)","Point(row=183, column=21)",GoogleSheetsWriteBlock,autogpt_platform/backend/backend/blocks/google/sheets.py
Attachment,class,,"class Attachment(BaseModel):
    filename: str
    content_type: str
    size: int
    attachment_id: str","Point(row=21, column=0)","Point(row=25, column=22)",,autogpt_platform/backend/backend/blocks/google/gmail.py
Email,class,,"class Email(BaseModel):
    id: str
    subject: str
    snippet: str
    from_: str
    to: str
    date: str
    body: str = """"  # Default to an empty string
    sizeEstimate: int
    attachments: List[Attachment]","Point(row=28, column=0)","Point(row=37, column=33)",,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailReadBlock,class,,"class GmailReadBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.readonly""]
        )
        query: str = SchemaField(
            description=""Search query for reading emails"",
            default=""is:unread"",
        )
        max_results: int = SchemaField(
            description=""Maximum number of emails to retrieve"",
            default=10,
        )

    class Output(BlockSchema):
        email: Email = SchemaField(
            description=""Email data"",
        )
        emails: list[Email] = SchemaField(
            description=""List of email data"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""25310c70-b89b-43ba-b25c-4dfa7e2a481c"",
            description=""This block reads emails from Gmail."",
            categories={BlockCategory.COMMUNICATION},
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            input_schema=GmailReadBlock.Input,
            output_schema=GmailReadBlock.Output,
            test_input={
                ""query"": ""is:unread"",
                ""max_results"": 5,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""email"",
                    {
                        ""id"": ""1"",
                        ""subject"": ""Test Email"",
                        ""snippet"": ""This is a test email"",
                        ""from_"": ""test@example.com"",
                        ""to"": ""recipient@example.com"",
                        ""date"": ""2024-01-01"",
                        ""body"": ""This is a test email"",
                        ""sizeEstimate"": 100,
                        ""attachments"": [],
                    },
                ),
                (
                    ""emails"",
                    [
                        {
                            ""id"": ""1"",
                            ""subject"": ""Test Email"",
                            ""snippet"": ""This is a test email"",
                            ""from_"": ""test@example.com"",
                            ""to"": ""recipient@example.com"",
                            ""date"": ""2024-01-01"",
                            ""body"": ""This is a test email"",
                            ""sizeEstimate"": 100,
                            ""attachments"": [],
                        }
                    ],
                ),
            ],
            test_mock={
                ""_read_emails"": lambda *args, **kwargs: [
                    {
                        ""id"": ""1"",
                        ""subject"": ""Test Email"",
                        ""snippet"": ""This is a test email"",
                        ""from_"": ""test@example.com"",
                        ""to"": ""recipient@example.com"",
                        ""date"": ""2024-01-01"",
                        ""body"": ""This is a test email"",
                        ""sizeEstimate"": 100,
                        ""attachments"": [],
                    }
                ],
                ""_send_email"": lambda *args, **kwargs: {""id"": ""1"", ""status"": ""sent""},
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = self._build_service(credentials, **kwargs)
        messages = self._read_emails(service, input_data.query, input_data.max_results)
        for email in messages:
            yield ""email"", email
        yield ""emails"", messages

    @staticmethod
    def _build_service(credentials: GoogleCredentials, **kwargs):
        creds = Credentials(
            token=(
                credentials.access_token.get_secret_value()
                if credentials.access_token
                else None
            ),
            refresh_token=(
                credentials.refresh_token.get_secret_value()
                if credentials.refresh_token
                else None
            ),
            token_uri=""https://oauth2.googleapis.com/token"",
            client_id=kwargs.get(""client_id""),
            client_secret=kwargs.get(""client_secret""),
            scopes=credentials.scopes,
        )
        return build(""gmail"", ""v1"", credentials=creds)

    def _read_emails(
        self, service, query: str | None, max_results: int | None
    ) -> list[Email]:
        results = (
            service.users()
            .messages()
            .list(userId=""me"", q=query or """", maxResults=max_results or 10)
            .execute()
        )
        messages = results.get(""messages"", [])

        email_data = []
        for message in messages:
            msg = (
                service.users()
                .messages()
                .get(userId=""me"", id=message[""id""], format=""full"")
                .execute()
            )

            headers = {
                header[""name""].lower(): header[""value""]
                for header in msg[""payload""][""headers""]
            }

            attachments = self._get_attachments(service, msg)

            email = Email(
                id=msg[""id""],
                subject=headers.get(""subject"", ""No Subject""),
                snippet=msg[""snippet""],
                from_=parseaddr(headers.get(""from"", """"))[1],
                to=parseaddr(headers.get(""to"", """"))[1],
                date=headers.get(""date"", """"),
                body=self._get_email_body(msg),
                sizeEstimate=msg[""sizeEstimate""],
                attachments=attachments,
            )
            email_data.append(email)

        return email_data

    def _get_email_body(self, msg):
        if ""parts"" in msg[""payload""]:
            for part in msg[""payload""][""parts""]:
                if part[""mimeType""] == ""text/plain"":
                    return base64.urlsafe_b64decode(part[""body""][""data""]).decode(
                        ""utf-8""
                    )
        elif msg[""payload""][""mimeType""] == ""text/plain"":
            return base64.urlsafe_b64decode(msg[""payload""][""body""][""data""]).decode(
                ""utf-8""
            )

        return ""This email does not contain a text body.""

    def _get_attachments(self, service, message):
        attachments = []
        if ""parts"" in message[""payload""]:
            for part in message[""payload""][""parts""]:
                if part[""filename""]:
                    attachment = Attachment(
                        filename=part[""filename""],
                        content_type=part[""mimeType""],
                        size=int(part[""body""].get(""size"", 0)),
                        attachment_id=part[""body""][""attachmentId""],
                    )
                    attachments.append(attachment)
        return attachments

    # Add a new method to download attachment content
    def download_attachment(self, service, message_id: str, attachment_id: str):
        attachment = (
            service.users()
            .messages()
            .attachments()
            .get(userId=""me"", messageId=message_id, id=attachment_id)
            .execute()
        )
        file_data = base64.urlsafe_b64decode(attachment[""data""].encode(""UTF-8""))
        return file_data","Point(row=40, column=0)","Point(row=238, column=24)",,autogpt_platform/backend/backend/blocks/google/gmail.py
Input,class,,"class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.readonly""]
        )
        query: str = SchemaField(
            description=""Search query for reading emails"",
            default=""is:unread"",
        )
        max_results: int = SchemaField(
            description=""Maximum number of emails to retrieve"",
            default=10,
        )","Point(row=41, column=4)","Point(row=52, column=9)",,autogpt_platform/backend/backend/blocks/google/gmail.py
Output,class,,"class Output(BlockSchema):
        email: Email = SchemaField(
            description=""Email data"",
        )
        emails: list[Email] = SchemaField(
            description=""List of email data"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )","Point(row=54, column=4)","Point(row=63, column=9)",,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailReadBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""25310c70-b89b-43ba-b25c-4dfa7e2a481c"",
            description=""This block reads emails from Gmail."",
            categories={BlockCategory.COMMUNICATION},
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            input_schema=GmailReadBlock.Input,
            output_schema=GmailReadBlock.Output,
            test_input={
                ""query"": ""is:unread"",
                ""max_results"": 5,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""email"",
                    {
                        ""id"": ""1"",
                        ""subject"": ""Test Email"",
                        ""snippet"": ""This is a test email"",
                        ""from_"": ""test@example.com"",
                        ""to"": ""recipient@example.com"",
                        ""date"": ""2024-01-01"",
                        ""body"": ""This is a test email"",
                        ""sizeEstimate"": 100,
                        ""attachments"": [],
                    },
                ),
                (
                    ""emails"",
                    [
                        {
                            ""id"": ""1"",
                            ""subject"": ""Test Email"",
                            ""snippet"": ""This is a test email"",
                            ""from_"": ""test@example.com"",
                            ""to"": ""recipient@example.com"",
                            ""date"": ""2024-01-01"",
                            ""body"": ""This is a test email"",
                            ""sizeEstimate"": 100,
                            ""attachments"": [],
                        }
                    ],
                ),
            ],
            test_mock={
                ""_read_emails"": lambda *args, **kwargs: [
                    {
                        ""id"": ""1"",
                        ""subject"": ""Test Email"",
                        ""snippet"": ""This is a test email"",
                        ""from_"": ""test@example.com"",
                        ""to"": ""recipient@example.com"",
                        ""date"": ""2024-01-01"",
                        ""body"": ""This is a test email"",
                        ""sizeEstimate"": 100,
                        ""attachments"": [],
                    }
                ],
                ""_send_email"": lambda *args, **kwargs: {""id"": ""1"", ""status"": ""sent""},
            },
        )","Point(row=65, column=4)","Point(row=127, column=9)",GmailReadBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailReadBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = self._build_service(credentials, **kwargs)
        messages = self._read_emails(service, input_data.query, input_data.max_results)
        for email in messages:
            yield ""email"", email
        yield ""emails"", messages","Point(row=129, column=4)","Point(row=136, column=32)",GmailReadBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailReadBlock._build_service,function,,"def _build_service(credentials: GoogleCredentials, **kwargs):
        creds = Credentials(
            token=(
                credentials.access_token.get_secret_value()
                if credentials.access_token
                else None
            ),
            refresh_token=(
                credentials.refresh_token.get_secret_value()
                if credentials.refresh_token
                else None
            ),
            token_uri=""https://oauth2.googleapis.com/token"",
            client_id=kwargs.get(""client_id""),
            client_secret=kwargs.get(""client_secret""),
            scopes=credentials.scopes,
        )
        return build(""gmail"", ""v1"", credentials=creds)","Point(row=139, column=4)","Point(row=156, column=54)",GmailReadBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailReadBlock._read_emails,function,,"def _read_emails(
        self, service, query: str | None, max_results: int | None
    ) -> list[Email]:
        results = (
            service.users()
            .messages()
            .list(userId=""me"", q=query or """", maxResults=max_results or 10)
            .execute()
        )
        messages = results.get(""messages"", [])

        email_data = []
        for message in messages:
            msg = (
                service.users()
                .messages()
                .get(userId=""me"", id=message[""id""], format=""full"")
                .execute()
            )

            headers = {
                header[""name""].lower(): header[""value""]
                for header in msg[""payload""][""headers""]
            }

            attachments = self._get_attachments(service, msg)

            email = Email(
                id=msg[""id""],
                subject=headers.get(""subject"", ""No Subject""),
                snippet=msg[""snippet""],
                from_=parseaddr(headers.get(""from"", """"))[1],
                to=parseaddr(headers.get(""to"", """"))[1],
                date=headers.get(""date"", """"),
                body=self._get_email_body(msg),
                sizeEstimate=msg[""sizeEstimate""],
                attachments=attachments,
            )
            email_data.append(email)

        return email_data","Point(row=158, column=4)","Point(row=198, column=25)",GmailReadBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailReadBlock._get_email_body,function,,"def _get_email_body(self, msg):
        if ""parts"" in msg[""payload""]:
            for part in msg[""payload""][""parts""]:
                if part[""mimeType""] == ""text/plain"":
                    return base64.urlsafe_b64decode(part[""body""][""data""]).decode(
                        ""utf-8""
                    )
        elif msg[""payload""][""mimeType""] == ""text/plain"":
            return base64.urlsafe_b64decode(msg[""payload""][""body""][""data""]).decode(
                ""utf-8""
            )

        return ""This email does not contain a text body.""","Point(row=200, column=4)","Point(row=212, column=57)",GmailReadBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailReadBlock._get_attachments,function,,"def _get_attachments(self, service, message):
        attachments = []
        if ""parts"" in message[""payload""]:
            for part in message[""payload""][""parts""]:
                if part[""filename""]:
                    attachment = Attachment(
                        filename=part[""filename""],
                        content_type=part[""mimeType""],
                        size=int(part[""body""].get(""size"", 0)),
                        attachment_id=part[""body""][""attachmentId""],
                    )
                    attachments.append(attachment)
        return attachments","Point(row=214, column=4)","Point(row=226, column=26)",GmailReadBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailReadBlock.download_attachment,function,,"def download_attachment(self, service, message_id: str, attachment_id: str):
        attachment = (
            service.users()
            .messages()
            .attachments()
            .get(userId=""me"", messageId=message_id, id=attachment_id)
            .execute()
        )
        file_data = base64.urlsafe_b64decode(attachment[""data""].encode(""UTF-8""))
        return file_data","Point(row=229, column=4)","Point(row=238, column=24)",GmailReadBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailSendBlock,class,,"class GmailSendBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.send""]
        )
        to: str = SchemaField(
            description=""Recipient email address"",
        )
        subject: str = SchemaField(
            description=""Email subject"",
        )
        body: str = SchemaField(
            description=""Email body"",
        )

    class Output(BlockSchema):
        result: dict = SchemaField(
            description=""Send confirmation"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""6c27abc2-e51d-499e-a85f-5a0041ba94f0"",
            description=""This block sends an email using Gmail."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailSendBlock.Input,
            output_schema=GmailSendBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""to"": ""recipient@example.com"",
                ""subject"": ""Test Email"",
                ""body"": ""This is a test email sent from GmailSendBlock."",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""result"", {""id"": ""1"", ""status"": ""sent""}),
            ],
            test_mock={
                ""_send_email"": lambda *args, **kwargs: {""id"": ""1"", ""status"": ""sent""},
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        send_result = self._send_email(
            service, input_data.to, input_data.subject, input_data.body
        )
        yield ""result"", send_result

    def _send_email(self, service, to: str, subject: str, body: str) -> dict:
        if not to or not subject or not body:
            raise ValueError(""To, subject, and body are required for sending an email"")
        message = self._create_message(to, subject, body)
        sent_message = (
            service.users().messages().send(userId=""me"", body=message).execute()
        )
        return {""id"": sent_message[""id""], ""status"": ""sent""}

    def _create_message(self, to: str, subject: str, body: str) -> dict:
        import base64
        from email.mime.text import MIMEText

        message = MIMEText(body)
        message[""to""] = to
        message[""subject""] = subject
        raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode(""utf-8"")
        return {""raw"": raw_message}","Point(row=241, column=0)","Point(row=313, column=35)",,autogpt_platform/backend/backend/blocks/google/gmail.py
Input,class,,"class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.send""]
        )
        to: str = SchemaField(
            description=""Recipient email address"",
        )
        subject: str = SchemaField(
            description=""Email subject"",
        )
        body: str = SchemaField(
            description=""Email body"",
        )","Point(row=242, column=4)","Point(row=254, column=9)",,autogpt_platform/backend/backend/blocks/google/gmail.py
Output,class,,"class Output(BlockSchema):
        result: dict = SchemaField(
            description=""Send confirmation"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )","Point(row=256, column=4)","Point(row=262, column=9)",,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailSendBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""6c27abc2-e51d-499e-a85f-5a0041ba94f0"",
            description=""This block sends an email using Gmail."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailSendBlock.Input,
            output_schema=GmailSendBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""to"": ""recipient@example.com"",
                ""subject"": ""Test Email"",
                ""body"": ""This is a test email sent from GmailSendBlock."",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""result"", {""id"": ""1"", ""status"": ""sent""}),
            ],
            test_mock={
                ""_send_email"": lambda *args, **kwargs: {""id"": ""1"", ""status"": ""sent""},
            },
        )","Point(row=264, column=4)","Point(row=285, column=9)",GmailSendBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailSendBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        send_result = self._send_email(
            service, input_data.to, input_data.subject, input_data.body
        )
        yield ""result"", send_result","Point(row=287, column=4)","Point(row=294, column=35)",GmailSendBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailSendBlock._send_email,function,,"def _send_email(self, service, to: str, subject: str, body: str) -> dict:
        if not to or not subject or not body:
            raise ValueError(""To, subject, and body are required for sending an email"")
        message = self._create_message(to, subject, body)
        sent_message = (
            service.users().messages().send(userId=""me"", body=message).execute()
        )
        return {""id"": sent_message[""id""], ""status"": ""sent""}","Point(row=296, column=4)","Point(row=303, column=59)",GmailSendBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailSendBlock._create_message,function,,"def _create_message(self, to: str, subject: str, body: str) -> dict:
        import base64
        from email.mime.text import MIMEText

        message = MIMEText(body)
        message[""to""] = to
        message[""subject""] = subject
        raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode(""utf-8"")
        return {""raw"": raw_message}","Point(row=305, column=4)","Point(row=313, column=35)",GmailSendBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailListLabelsBlock,class,,"class GmailListLabelsBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.labels""]
        )

    class Output(BlockSchema):
        result: list[dict] = SchemaField(
            description=""List of labels"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""3e1c2c1c-c689-4520-b956-1f3bf4e02bb7"",
            description=""This block lists all labels in Gmail."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailListLabelsBlock.Input,
            output_schema=GmailListLabelsBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    [
                        {""id"": ""Label_1"", ""name"": ""Important""},
                        {""id"": ""Label_2"", ""name"": ""Work""},
                    ],
                ),
            ],
            test_mock={
                ""_list_labels"": lambda *args, **kwargs: [
                    {""id"": ""Label_1"", ""name"": ""Important""},
                    {""id"": ""Label_2"", ""name"": ""Work""},
                ],
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        labels = self._list_labels(service)
        yield ""result"", labels

    def _list_labels(self, service) -> list[dict]:
        results = service.users().labels().list(userId=""me"").execute()
        labels = results.get(""labels"", [])
        return [{""id"": label[""id""], ""name"": label[""name""]} for label in labels]","Point(row=316, column=0)","Point(row=369, column=79)",,autogpt_platform/backend/backend/blocks/google/gmail.py
Input,class,,"class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.labels""]
        )","Point(row=317, column=4)","Point(row=320, column=9)",,autogpt_platform/backend/backend/blocks/google/gmail.py
Output,class,,"class Output(BlockSchema):
        result: list[dict] = SchemaField(
            description=""List of labels"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )","Point(row=322, column=4)","Point(row=328, column=9)",,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailListLabelsBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""3e1c2c1c-c689-4520-b956-1f3bf4e02bb7"",
            description=""This block lists all labels in Gmail."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailListLabelsBlock.Input,
            output_schema=GmailListLabelsBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    [
                        {""id"": ""Label_1"", ""name"": ""Important""},
                        {""id"": ""Label_2"", ""name"": ""Work""},
                    ],
                ),
            ],
            test_mock={
                ""_list_labels"": lambda *args, **kwargs: [
                    {""id"": ""Label_1"", ""name"": ""Important""},
                    {""id"": ""Label_2"", ""name"": ""Work""},
                ],
            },
        )","Point(row=330, column=4)","Point(row=357, column=9)",GmailListLabelsBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailListLabelsBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        labels = self._list_labels(service)
        yield ""result"", labels","Point(row=359, column=4)","Point(row=364, column=30)",GmailListLabelsBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailListLabelsBlock._list_labels,function,,"def _list_labels(self, service) -> list[dict]:
        results = service.users().labels().list(userId=""me"").execute()
        labels = results.get(""labels"", [])
        return [{""id"": label[""id""], ""name"": label[""name""]} for label in labels]","Point(row=366, column=4)","Point(row=369, column=79)",GmailListLabelsBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailAddLabelBlock,class,,"class GmailAddLabelBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.modify""]
        )
        message_id: str = SchemaField(
            description=""Message ID to add label to"",
        )
        label_name: str = SchemaField(
            description=""Label name to add"",
        )

    class Output(BlockSchema):
        result: dict = SchemaField(
            description=""Label addition result"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""f884b2fb-04f4-4265-9658-14f433926ac9"",
            description=""This block adds a label to a Gmail message."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailAddLabelBlock.Input,
            output_schema=GmailAddLabelBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""message_id"": ""12345"",
                ""label_name"": ""Important"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    {""status"": ""Label added successfully"", ""label_id"": ""Label_1""},
                ),
            ],
            test_mock={
                ""_add_label"": lambda *args, **kwargs: {
                    ""status"": ""Label added successfully"",
                    ""label_id"": ""Label_1"",
                },
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        result = self._add_label(service, input_data.message_id, input_data.label_name)
        yield ""result"", result

    def _add_label(self, service, message_id: str, label_name: str) -> dict:
        label_id = self._get_or_create_label(service, label_name)
        service.users().messages().modify(
            userId=""me"", id=message_id, body={""addLabelIds"": [label_id]}
        ).execute()
        return {""status"": ""Label added successfully"", ""label_id"": label_id}

    def _get_or_create_label(self, service, label_name: str) -> str:
        label_id = self._get_label_id(service, label_name)
        if not label_id:
            label = (
                service.users()
                .labels()
                .create(userId=""me"", body={""name"": label_name})
                .execute()
            )
            label_id = label[""id""]
        return label_id

    def _get_label_id(self, service, label_name: str) -> str | None:
        results = service.users().labels().list(userId=""me"").execute()
        labels = results.get(""labels"", [])
        for label in labels:
            if label[""name""] == label_name:
                return label[""id""]
        return None","Point(row=372, column=0)","Point(row=452, column=19)",,autogpt_platform/backend/backend/blocks/google/gmail.py
Input,class,,"class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.modify""]
        )
        message_id: str = SchemaField(
            description=""Message ID to add label to"",
        )
        label_name: str = SchemaField(
            description=""Label name to add"",
        )","Point(row=373, column=4)","Point(row=382, column=9)",,autogpt_platform/backend/backend/blocks/google/gmail.py
Output,class,,"class Output(BlockSchema):
        result: dict = SchemaField(
            description=""Label addition result"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )","Point(row=384, column=4)","Point(row=390, column=9)",,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailAddLabelBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""f884b2fb-04f4-4265-9658-14f433926ac9"",
            description=""This block adds a label to a Gmail message."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailAddLabelBlock.Input,
            output_schema=GmailAddLabelBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""message_id"": ""12345"",
                ""label_name"": ""Important"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    {""status"": ""Label added successfully"", ""label_id"": ""Label_1""},
                ),
            ],
            test_mock={
                ""_add_label"": lambda *args, **kwargs: {
                    ""status"": ""Label added successfully"",
                    ""label_id"": ""Label_1"",
                },
            },
        )","Point(row=392, column=4)","Point(row=418, column=9)",GmailAddLabelBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailAddLabelBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        result = self._add_label(service, input_data.message_id, input_data.label_name)
        yield ""result"", result","Point(row=420, column=4)","Point(row=425, column=30)",GmailAddLabelBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailAddLabelBlock._add_label,function,,"def _add_label(self, service, message_id: str, label_name: str) -> dict:
        label_id = self._get_or_create_label(service, label_name)
        service.users().messages().modify(
            userId=""me"", id=message_id, body={""addLabelIds"": [label_id]}
        ).execute()
        return {""status"": ""Label added successfully"", ""label_id"": label_id}","Point(row=427, column=4)","Point(row=432, column=75)",GmailAddLabelBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailAddLabelBlock._get_or_create_label,function,,"def _get_or_create_label(self, service, label_name: str) -> str:
        label_id = self._get_label_id(service, label_name)
        if not label_id:
            label = (
                service.users()
                .labels()
                .create(userId=""me"", body={""name"": label_name})
                .execute()
            )
            label_id = label[""id""]
        return label_id","Point(row=434, column=4)","Point(row=444, column=23)",GmailAddLabelBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailAddLabelBlock._get_label_id,function,,"def _get_label_id(self, service, label_name: str) -> str | None:
        results = service.users().labels().list(userId=""me"").execute()
        labels = results.get(""labels"", [])
        for label in labels:
            if label[""name""] == label_name:
                return label[""id""]
        return None","Point(row=446, column=4)","Point(row=452, column=19)",GmailAddLabelBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailRemoveLabelBlock,class,,"class GmailRemoveLabelBlock(Block):
    class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.modify""]
        )
        message_id: str = SchemaField(
            description=""Message ID to remove label from"",
        )
        label_name: str = SchemaField(
            description=""Label name to remove"",
        )

    class Output(BlockSchema):
        result: dict = SchemaField(
            description=""Label removal result"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )

    def __init__(self):
        super().__init__(
            id=""0afc0526-aba1-4b2b-888e-a22b7c3f359d"",
            description=""This block removes a label from a Gmail message."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailRemoveLabelBlock.Input,
            output_schema=GmailRemoveLabelBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""message_id"": ""12345"",
                ""label_name"": ""Important"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    {""status"": ""Label removed successfully"", ""label_id"": ""Label_1""},
                ),
            ],
            test_mock={
                ""_remove_label"": lambda *args, **kwargs: {
                    ""status"": ""Label removed successfully"",
                    ""label_id"": ""Label_1"",
                },
            },
        )

    def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        result = self._remove_label(
            service, input_data.message_id, input_data.label_name
        )
        yield ""result"", result

    def _remove_label(self, service, message_id: str, label_name: str) -> dict:
        label_id = self._get_label_id(service, label_name)
        if label_id:
            service.users().messages().modify(
                userId=""me"", id=message_id, body={""removeLabelIds"": [label_id]}
            ).execute()
            return {""status"": ""Label removed successfully"", ""label_id"": label_id}
        else:
            return {""status"": ""Label not found"", ""label_name"": label_name}

    def _get_label_id(self, service, label_name: str) -> str | None:
        results = service.users().labels().list(userId=""me"").execute()
        labels = results.get(""labels"", [])
        for label in labels:
            if label[""name""] == label_name:
                return label[""id""]
        return None","Point(row=455, column=0)","Point(row=528, column=19)",,autogpt_platform/backend/backend/blocks/google/gmail.py
Input,class,,"class Input(BlockSchema):
        credentials: GoogleCredentialsInput = GoogleCredentialsField(
            [""https://www.googleapis.com/auth/gmail.modify""]
        )
        message_id: str = SchemaField(
            description=""Message ID to remove label from"",
        )
        label_name: str = SchemaField(
            description=""Label name to remove"",
        )","Point(row=456, column=4)","Point(row=465, column=9)",,autogpt_platform/backend/backend/blocks/google/gmail.py
Output,class,,"class Output(BlockSchema):
        result: dict = SchemaField(
            description=""Label removal result"",
        )
        error: str = SchemaField(
            description=""Error message if any"",
        )","Point(row=467, column=4)","Point(row=473, column=9)",,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailRemoveLabelBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""0afc0526-aba1-4b2b-888e-a22b7c3f359d"",
            description=""This block removes a label from a Gmail message."",
            categories={BlockCategory.COMMUNICATION},
            input_schema=GmailRemoveLabelBlock.Input,
            output_schema=GmailRemoveLabelBlock.Output,
            disabled=not GOOGLE_OAUTH_IS_CONFIGURED,
            test_input={
                ""message_id"": ""12345"",
                ""label_name"": ""Important"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""result"",
                    {""status"": ""Label removed successfully"", ""label_id"": ""Label_1""},
                ),
            ],
            test_mock={
                ""_remove_label"": lambda *args, **kwargs: {
                    ""status"": ""Label removed successfully"",
                    ""label_id"": ""Label_1"",
                },
            },
        )","Point(row=475, column=4)","Point(row=501, column=9)",GmailRemoveLabelBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailRemoveLabelBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: GoogleCredentials, **kwargs
    ) -> BlockOutput:
        service = GmailReadBlock._build_service(credentials, **kwargs)
        result = self._remove_label(
            service, input_data.message_id, input_data.label_name
        )
        yield ""result"", result","Point(row=503, column=4)","Point(row=510, column=30)",GmailRemoveLabelBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailRemoveLabelBlock._remove_label,function,,"def _remove_label(self, service, message_id: str, label_name: str) -> dict:
        label_id = self._get_label_id(service, label_name)
        if label_id:
            service.users().messages().modify(
                userId=""me"", id=message_id, body={""removeLabelIds"": [label_id]}
            ).execute()
            return {""status"": ""Label removed successfully"", ""label_id"": label_id}
        else:
            return {""status"": ""Label not found"", ""label_name"": label_name}","Point(row=512, column=4)","Point(row=520, column=74)",GmailRemoveLabelBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GmailRemoveLabelBlock._get_label_id,function,,"def _get_label_id(self, service, label_name: str) -> str | None:
        results = service.users().labels().list(userId=""me"").execute()
        labels = results.get(""labels"", [])
        for label in labels:
            if label[""name""] == label_name:
                return label[""id""]
        return None","Point(row=522, column=4)","Point(row=528, column=19)",GmailRemoveLabelBlock,autogpt_platform/backend/backend/blocks/google/gmail.py
GoogleCredentialsField,function,"
    Creates a Google credentials input on a block.

    Params:
        scopes: The authorization scopes needed for the block to work.
","def GoogleCredentialsField(scopes: list[str]) -> GoogleCredentialsInput:
    """"""
    Creates a Google credentials input on a block.

    Params:
        scopes: The authorization scopes needed for the block to work.
    """"""
    return CredentialsField(
        provider=""google"",
        supported_credential_types={""oauth2""},
        required_scopes=set(scopes),
        description=""The Google integration requires OAuth2 authentication."",
    )","Point(row=18, column=0)","Point(row=30, column=5)",,autogpt_platform/backend/backend/blocks/google/_auth.py
is_github_url,function,,"def is_github_url(url: str) -> bool:
    return urlparse(url).netloc == ""github.com""","Point(row=17, column=0)","Point(row=18, column=47)",,autogpt_platform/backend/backend/blocks/github/issues.py
GithubCommentBlock,class,,"class GithubCommentBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue or pull request"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        comment: str = SchemaField(
            description=""Comment to post on the issue or pull request"",
            placeholder=""Enter your comment"",
        )

    class Output(BlockSchema):
        id: int = SchemaField(description=""ID of the created comment"")
        url: str = SchemaField(description=""URL to the comment on GitHub"")
        error: str = SchemaField(
            description=""Error message if the comment posting failed""
        )

    def __init__(self):
        super().__init__(
            id=""a8db4d8d-db1c-4a25-a1b0-416a8c33602b"",
            description=""This block posts a comment on a specified GitHub issue or pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubCommentBlock.Input,
            output_schema=GithubCommentBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""comment"": ""This is a test comment."",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""id"", 1337),
                (""url"", ""https://github.com/owner/repo/issues/1#issuecomment-1337""),
            ],
            test_mock={
                ""post_comment"": lambda *args, **kwargs: (
                    1337,
                    ""https://github.com/owner/repo/issues/1#issuecomment-1337"",
                )
            },
        )

    @staticmethod
    def post_comment(
        credentials: GithubCredentials, issue_url: str, body_text: str
    ) -> tuple[int, str]:
        api = get_api(credentials)
        data = {""body"": body_text}
        comments_url = issue_url + ""/comments""
        response = api.post(comments_url, json=data)
        comment = response.json()
        return comment[""id""], comment[""html_url""]

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        id, url = self.post_comment(
            credentials,
            input_data.issue_url,
            input_data.comment,
        )
        yield ""id"", id
        yield ""url"", url","Point(row=22, column=0)","Point(row=90, column=24)",,autogpt_platform/backend/backend/blocks/github/issues.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue or pull request"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        comment: str = SchemaField(
            description=""Comment to post on the issue or pull request"",
            placeholder=""Enter your comment"",
        )","Point(row=23, column=4)","Point(row=32, column=9)",,autogpt_platform/backend/backend/blocks/github/issues.py
Output,class,,"class Output(BlockSchema):
        id: int = SchemaField(description=""ID of the created comment"")
        url: str = SchemaField(description=""URL to the comment on GitHub"")
        error: str = SchemaField(
            description=""Error message if the comment posting failed""
        )","Point(row=34, column=4)","Point(row=39, column=9)",,autogpt_platform/backend/backend/blocks/github/issues.py
GithubCommentBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""a8db4d8d-db1c-4a25-a1b0-416a8c33602b"",
            description=""This block posts a comment on a specified GitHub issue or pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubCommentBlock.Input,
            output_schema=GithubCommentBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""comment"": ""This is a test comment."",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""id"", 1337),
                (""url"", ""https://github.com/owner/repo/issues/1#issuecomment-1337""),
            ],
            test_mock={
                ""post_comment"": lambda *args, **kwargs: (
                    1337,
                    ""https://github.com/owner/repo/issues/1#issuecomment-1337"",
                )
            },
        )","Point(row=41, column=4)","Point(row=64, column=9)",GithubCommentBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubCommentBlock.post_comment,function,,"def post_comment(
        credentials: GithubCredentials, issue_url: str, body_text: str
    ) -> tuple[int, str]:
        api = get_api(credentials)
        data = {""body"": body_text}
        comments_url = issue_url + ""/comments""
        response = api.post(comments_url, json=data)
        comment = response.json()
        return comment[""id""], comment[""html_url""]","Point(row=67, column=4)","Point(row=75, column=49)",GithubCommentBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubCommentBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        id, url = self.post_comment(
            credentials,
            input_data.issue_url,
            input_data.comment,
        )
        yield ""id"", id
        yield ""url"", url","Point(row=77, column=4)","Point(row=90, column=24)",GithubCommentBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubMakeIssueBlock,class,,"class GithubMakeIssueBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        title: str = SchemaField(
            description=""Title of the issue"", placeholder=""Enter the issue title""
        )
        body: str = SchemaField(
            description=""Body of the issue"", placeholder=""Enter the issue body""
        )

    class Output(BlockSchema):
        number: int = SchemaField(description=""Number of the created issue"")
        url: str = SchemaField(description=""URL of the created issue"")
        error: str = SchemaField(
            description=""Error message if the issue creation failed""
        )

    def __init__(self):
        super().__init__(
            id=""691dad47-f494-44c3-a1e8-05b7990f2dab"",
            description=""This block creates a new issue on a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubMakeIssueBlock.Input,
            output_schema=GithubMakeIssueBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""title"": ""Test Issue"",
                ""body"": ""This is a test issue."",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""number"", 1),
                (""url"", ""https://github.com/owner/repo/issues/1""),
            ],
            test_mock={
                ""create_issue"": lambda *args, **kwargs: (
                    1,
                    ""https://github.com/owner/repo/issues/1"",
                )
            },
        )

    @staticmethod
    def create_issue(
        credentials: GithubCredentials, repo_url: str, title: str, body: str
    ) -> tuple[int, str]:
        api = get_api(credentials)
        data = {""title"": title, ""body"": body}
        issues_url = repo_url + ""/issues""
        response = api.post(issues_url, json=data)
        issue = response.json()
        return issue[""number""], issue[""html_url""]

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        number, url = self.create_issue(
            credentials,
            input_data.repo_url,
            input_data.title,
            input_data.body,
        )
        yield ""number"", number
        yield ""url"", url","Point(row=96, column=0)","Point(row=168, column=24)",,autogpt_platform/backend/backend/blocks/github/issues.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        title: str = SchemaField(
            description=""Title of the issue"", placeholder=""Enter the issue title""
        )
        body: str = SchemaField(
            description=""Body of the issue"", placeholder=""Enter the issue body""
        )","Point(row=97, column=4)","Point(row=108, column=9)",,autogpt_platform/backend/backend/blocks/github/issues.py
Output,class,,"class Output(BlockSchema):
        number: int = SchemaField(description=""Number of the created issue"")
        url: str = SchemaField(description=""URL of the created issue"")
        error: str = SchemaField(
            description=""Error message if the issue creation failed""
        )","Point(row=110, column=4)","Point(row=115, column=9)",,autogpt_platform/backend/backend/blocks/github/issues.py
GithubMakeIssueBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""691dad47-f494-44c3-a1e8-05b7990f2dab"",
            description=""This block creates a new issue on a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubMakeIssueBlock.Input,
            output_schema=GithubMakeIssueBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""title"": ""Test Issue"",
                ""body"": ""This is a test issue."",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""number"", 1),
                (""url"", ""https://github.com/owner/repo/issues/1""),
            ],
            test_mock={
                ""create_issue"": lambda *args, **kwargs: (
                    1,
                    ""https://github.com/owner/repo/issues/1"",
                )
            },
        )","Point(row=117, column=4)","Point(row=141, column=9)",GithubMakeIssueBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubMakeIssueBlock.create_issue,function,,"def create_issue(
        credentials: GithubCredentials, repo_url: str, title: str, body: str
    ) -> tuple[int, str]:
        api = get_api(credentials)
        data = {""title"": title, ""body"": body}
        issues_url = repo_url + ""/issues""
        response = api.post(issues_url, json=data)
        issue = response.json()
        return issue[""number""], issue[""html_url""]","Point(row=144, column=4)","Point(row=152, column=49)",GithubMakeIssueBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubMakeIssueBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        number, url = self.create_issue(
            credentials,
            input_data.repo_url,
            input_data.title,
            input_data.body,
        )
        yield ""number"", number
        yield ""url"", url","Point(row=154, column=4)","Point(row=168, column=24)",GithubMakeIssueBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubReadIssueBlock,class,,"class GithubReadIssueBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )

    class Output(BlockSchema):
        title: str = SchemaField(description=""Title of the issue"")
        body: str = SchemaField(description=""Body of the issue"")
        user: str = SchemaField(description=""User who created the issue"")
        error: str = SchemaField(
            description=""Error message if reading the issue failed""
        )

    def __init__(self):
        super().__init__(
            id=""6443c75d-032a-4772-9c08-230c707c8acc"",
            description=""This block reads the body, title, and user of a specified GitHub issue."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadIssueBlock.Input,
            output_schema=GithubReadIssueBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""title"", ""Title of the issue""),
                (""body"", ""This is the body of the issue.""),
                (""user"", ""username""),
            ],
            test_mock={
                ""read_issue"": lambda *args, **kwargs: (
                    ""Title of the issue"",
                    ""This is the body of the issue."",
                    ""username"",
                )
            },
        )

    @staticmethod
    def read_issue(
        credentials: GithubCredentials, issue_url: str
    ) -> tuple[str, str, str]:
        api = get_api(credentials)
        response = api.get(issue_url)
        data = response.json()
        title = data.get(""title"", ""No title found"")
        body = data.get(""body"", ""No body content found"")
        user = data.get(""user"", {}).get(""login"", ""No user found"")
        return title, body, user

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        title, body, user = self.read_issue(
            credentials,
            input_data.issue_url,
        )
        yield ""title"", title
        yield ""body"", body
        yield ""user"", user","Point(row=171, column=0)","Point(row=238, column=26)",,autogpt_platform/backend/backend/blocks/github/issues.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )","Point(row=172, column=4)","Point(row=177, column=9)",,autogpt_platform/backend/backend/blocks/github/issues.py
Output,class,,"class Output(BlockSchema):
        title: str = SchemaField(description=""Title of the issue"")
        body: str = SchemaField(description=""Body of the issue"")
        user: str = SchemaField(description=""User who created the issue"")
        error: str = SchemaField(
            description=""Error message if reading the issue failed""
        )","Point(row=179, column=4)","Point(row=185, column=9)",,autogpt_platform/backend/backend/blocks/github/issues.py
GithubReadIssueBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""6443c75d-032a-4772-9c08-230c707c8acc"",
            description=""This block reads the body, title, and user of a specified GitHub issue."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadIssueBlock.Input,
            output_schema=GithubReadIssueBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""title"", ""Title of the issue""),
                (""body"", ""This is the body of the issue.""),
                (""user"", ""username""),
            ],
            test_mock={
                ""read_issue"": lambda *args, **kwargs: (
                    ""Title of the issue"",
                    ""This is the body of the issue."",
                    ""username"",
                )
            },
        )","Point(row=187, column=4)","Point(row=211, column=9)",GithubReadIssueBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubReadIssueBlock.read_issue,function,,"def read_issue(
        credentials: GithubCredentials, issue_url: str
    ) -> tuple[str, str, str]:
        api = get_api(credentials)
        response = api.get(issue_url)
        data = response.json()
        title = data.get(""title"", ""No title found"")
        body = data.get(""body"", ""No body content found"")
        user = data.get(""user"", {}).get(""login"", ""No user found"")
        return title, body, user","Point(row=214, column=4)","Point(row=223, column=32)",GithubReadIssueBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubReadIssueBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        title, body, user = self.read_issue(
            credentials,
            input_data.issue_url,
        )
        yield ""title"", title
        yield ""body"", body
        yield ""user"", user","Point(row=225, column=4)","Point(row=238, column=26)",GithubReadIssueBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubListIssuesBlock,class,,"class GithubListIssuesBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )

    class Output(BlockSchema):
        class IssueItem(TypedDict):
            title: str
            url: str

        issue: IssueItem = SchemaField(
            title=""Issue"", description=""Issues with their title and URL""
        )
        error: str = SchemaField(description=""Error message if listing issues failed"")

    def __init__(self):
        super().__init__(
            id=""c215bfd7-0e57-4573-8f8c-f7d4963dcd74"",
            description=""This block lists all issues for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListIssuesBlock.Input,
            output_schema=GithubListIssuesBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""issue"",
                    {
                        ""title"": ""Issue 1"",
                        ""url"": ""https://github.com/owner/repo/issues/1"",
                    },
                )
            ],
            test_mock={
                ""list_issues"": lambda *args, **kwargs: [
                    {
                        ""title"": ""Issue 1"",
                        ""url"": ""https://github.com/owner/repo/issues/1"",
                    }
                ]
            },
        )

    @staticmethod
    def list_issues(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.IssueItem]:
        api = get_api(credentials)
        issues_url = repo_url + ""/issues""
        response = api.get(issues_url)
        data = response.json()
        issues: list[GithubListIssuesBlock.Output.IssueItem] = [
            {""title"": issue[""title""], ""url"": issue[""html_url""]} for issue in data
        ]
        return issues

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        issues = self.list_issues(
            credentials,
            input_data.repo_url,
        )
        yield from ((""issue"", issue) for issue in issues)","Point(row=241, column=0)","Point(row=314, column=57)",,autogpt_platform/backend/backend/blocks/github/issues.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )","Point(row=242, column=4)","Point(row=247, column=9)",,autogpt_platform/backend/backend/blocks/github/issues.py
Output,class,,"class Output(BlockSchema):
        class IssueItem(TypedDict):
            title: str
            url: str

        issue: IssueItem = SchemaField(
            title=""Issue"", description=""Issues with their title and URL""
        )
        error: str = SchemaField(description=""Error message if listing issues failed"")","Point(row=249, column=4)","Point(row=257, column=86)",,autogpt_platform/backend/backend/blocks/github/issues.py
IssueItem,class,,"class IssueItem(TypedDict):
            title: str
            url: str","Point(row=250, column=8)","Point(row=252, column=20)",,autogpt_platform/backend/backend/blocks/github/issues.py
GithubListIssuesBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""c215bfd7-0e57-4573-8f8c-f7d4963dcd74"",
            description=""This block lists all issues for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListIssuesBlock.Input,
            output_schema=GithubListIssuesBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""issue"",
                    {
                        ""title"": ""Issue 1"",
                        ""url"": ""https://github.com/owner/repo/issues/1"",
                    },
                )
            ],
            test_mock={
                ""list_issues"": lambda *args, **kwargs: [
                    {
                        ""title"": ""Issue 1"",
                        ""url"": ""https://github.com/owner/repo/issues/1"",
                    }
                ]
            },
        )","Point(row=259, column=4)","Point(row=288, column=9)",GithubListIssuesBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubListIssuesBlock.list_issues,function,,"def list_issues(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.IssueItem]:
        api = get_api(credentials)
        issues_url = repo_url + ""/issues""
        response = api.get(issues_url)
        data = response.json()
        issues: list[GithubListIssuesBlock.Output.IssueItem] = [
            {""title"": issue[""title""], ""url"": issue[""html_url""]} for issue in data
        ]
        return issues","Point(row=291, column=4)","Point(row=301, column=21)",GithubListIssuesBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubListIssuesBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        issues = self.list_issues(
            credentials,
            input_data.repo_url,
        )
        yield from ((""issue"", issue) for issue in issues)","Point(row=303, column=4)","Point(row=314, column=57)",GithubListIssuesBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubAddLabelBlock,class,,"class GithubAddLabelBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue or pull request"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        label: str = SchemaField(
            description=""Label to add to the issue or pull request"",
            placeholder=""Enter the label"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the label addition operation"")
        error: str = SchemaField(
            description=""Error message if the label addition failed""
        )

    def __init__(self):
        super().__init__(
            id=""98bd6b77-9506-43d5-b669-6b9733c4b1f1"",
            description=""This block adds a label to a specified GitHub issue or pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubAddLabelBlock.Input,
            output_schema=GithubAddLabelBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""label"": ""bug"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Label added successfully"")],
            test_mock={""add_label"": lambda *args, **kwargs: ""Label added successfully""},
        )

    @staticmethod
    def add_label(credentials: GithubCredentials, issue_url: str, label: str) -> str:
        api = get_api(credentials)
        data = {""labels"": [label]}
        labels_url = issue_url + ""/labels""
        api.post(labels_url, json=data)
        return ""Label added successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.add_label(
            credentials,
            input_data.issue_url,
            input_data.label,
        )
        yield ""status"", status","Point(row=317, column=0)","Point(row=372, column=30)",,autogpt_platform/backend/backend/blocks/github/issues.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue or pull request"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        label: str = SchemaField(
            description=""Label to add to the issue or pull request"",
            placeholder=""Enter the label"",
        )","Point(row=318, column=4)","Point(row=327, column=9)",,autogpt_platform/backend/backend/blocks/github/issues.py
Output,class,,"class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the label addition operation"")
        error: str = SchemaField(
            description=""Error message if the label addition failed""
        )","Point(row=329, column=4)","Point(row=333, column=9)",,autogpt_platform/backend/backend/blocks/github/issues.py
GithubAddLabelBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""98bd6b77-9506-43d5-b669-6b9733c4b1f1"",
            description=""This block adds a label to a specified GitHub issue or pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubAddLabelBlock.Input,
            output_schema=GithubAddLabelBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""label"": ""bug"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Label added successfully"")],
            test_mock={""add_label"": lambda *args, **kwargs: ""Label added successfully""},
        )","Point(row=335, column=4)","Point(row=350, column=9)",GithubAddLabelBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubAddLabelBlock.add_label,function,,"def add_label(credentials: GithubCredentials, issue_url: str, label: str) -> str:
        api = get_api(credentials)
        data = {""labels"": [label]}
        labels_url = issue_url + ""/labels""
        api.post(labels_url, json=data)
        return ""Label added successfully""","Point(row=353, column=4)","Point(row=358, column=41)",GithubAddLabelBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubAddLabelBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.add_label(
            credentials,
            input_data.issue_url,
            input_data.label,
        )
        yield ""status"", status","Point(row=360, column=4)","Point(row=372, column=30)",GithubAddLabelBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubRemoveLabelBlock,class,,"class GithubRemoveLabelBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue or pull request"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        label: str = SchemaField(
            description=""Label to remove from the issue or pull request"",
            placeholder=""Enter the label"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the label removal operation"")
        error: str = SchemaField(
            description=""Error message if the label removal failed""
        )

    def __init__(self):
        super().__init__(
            id=""78f050c5-3e3a-48c0-9e5b-ef1ceca5589c"",
            description=""This block removes a label from a specified GitHub issue or pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubRemoveLabelBlock.Input,
            output_schema=GithubRemoveLabelBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""label"": ""bug"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Label removed successfully"")],
            test_mock={
                ""remove_label"": lambda *args, **kwargs: ""Label removed successfully""
            },
        )

    @staticmethod
    def remove_label(credentials: GithubCredentials, issue_url: str, label: str) -> str:
        api = get_api(credentials)
        label_url = issue_url + f""/labels/{label}""
        api.delete(label_url)
        return ""Label removed successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.remove_label(
            credentials,
            input_data.issue_url,
            input_data.label,
        )
        yield ""status"", status","Point(row=375, column=0)","Point(row=431, column=30)",,autogpt_platform/backend/backend/blocks/github/issues.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue or pull request"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        label: str = SchemaField(
            description=""Label to remove from the issue or pull request"",
            placeholder=""Enter the label"",
        )","Point(row=376, column=4)","Point(row=385, column=9)",,autogpt_platform/backend/backend/blocks/github/issues.py
Output,class,,"class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the label removal operation"")
        error: str = SchemaField(
            description=""Error message if the label removal failed""
        )","Point(row=387, column=4)","Point(row=391, column=9)",,autogpt_platform/backend/backend/blocks/github/issues.py
GithubRemoveLabelBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""78f050c5-3e3a-48c0-9e5b-ef1ceca5589c"",
            description=""This block removes a label from a specified GitHub issue or pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubRemoveLabelBlock.Input,
            output_schema=GithubRemoveLabelBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""label"": ""bug"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Label removed successfully"")],
            test_mock={
                ""remove_label"": lambda *args, **kwargs: ""Label removed successfully""
            },
        )","Point(row=393, column=4)","Point(row=410, column=9)",GithubRemoveLabelBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubRemoveLabelBlock.remove_label,function,,"def remove_label(credentials: GithubCredentials, issue_url: str, label: str) -> str:
        api = get_api(credentials)
        label_url = issue_url + f""/labels/{label}""
        api.delete(label_url)
        return ""Label removed successfully""","Point(row=413, column=4)","Point(row=417, column=43)",GithubRemoveLabelBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubRemoveLabelBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.remove_label(
            credentials,
            input_data.issue_url,
            input_data.label,
        )
        yield ""status"", status","Point(row=419, column=4)","Point(row=431, column=30)",GithubRemoveLabelBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubAssignIssueBlock,class,,"class GithubAssignIssueBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        assignee: str = SchemaField(
            description=""Username to assign to the issue"",
            placeholder=""Enter the username"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the issue assignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the issue assignment failed""
        )

    def __init__(self):
        super().__init__(
            id=""90507c72-b0ff-413a-886a-23bbbd66f542"",
            description=""This block assigns a user to a specified GitHub issue."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubAssignIssueBlock.Input,
            output_schema=GithubAssignIssueBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""assignee"": ""username1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Issue assigned successfully"")],
            test_mock={
                ""assign_issue"": lambda *args, **kwargs: ""Issue assigned successfully""
            },
        )

    @staticmethod
    def assign_issue(
        credentials: GithubCredentials,
        issue_url: str,
        assignee: str,
    ) -> str:
        api = get_api(credentials)
        assignees_url = issue_url + ""/assignees""
        data = {""assignees"": [assignee]}
        api.post(assignees_url, json=data)
        return ""Issue assigned successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.assign_issue(
            credentials,
            input_data.issue_url,
            input_data.assignee,
        )
        yield ""status"", status","Point(row=434, column=0)","Point(row=497, column=30)",,autogpt_platform/backend/backend/blocks/github/issues.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        assignee: str = SchemaField(
            description=""Username to assign to the issue"",
            placeholder=""Enter the username"",
        )","Point(row=435, column=4)","Point(row=444, column=9)",,autogpt_platform/backend/backend/blocks/github/issues.py
Output,class,,"class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the issue assignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the issue assignment failed""
        )","Point(row=446, column=4)","Point(row=452, column=9)",,autogpt_platform/backend/backend/blocks/github/issues.py
GithubAssignIssueBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""90507c72-b0ff-413a-886a-23bbbd66f542"",
            description=""This block assigns a user to a specified GitHub issue."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubAssignIssueBlock.Input,
            output_schema=GithubAssignIssueBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""assignee"": ""username1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Issue assigned successfully"")],
            test_mock={
                ""assign_issue"": lambda *args, **kwargs: ""Issue assigned successfully""
            },
        )","Point(row=454, column=4)","Point(row=471, column=9)",GithubAssignIssueBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubAssignIssueBlock.assign_issue,function,,"def assign_issue(
        credentials: GithubCredentials,
        issue_url: str,
        assignee: str,
    ) -> str:
        api = get_api(credentials)
        assignees_url = issue_url + ""/assignees""
        data = {""assignees"": [assignee]}
        api.post(assignees_url, json=data)
        return ""Issue assigned successfully""","Point(row=474, column=4)","Point(row=483, column=44)",GithubAssignIssueBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubAssignIssueBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.assign_issue(
            credentials,
            input_data.issue_url,
            input_data.assignee,
        )
        yield ""status"", status","Point(row=485, column=4)","Point(row=497, column=30)",GithubAssignIssueBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubUnassignIssueBlock,class,,"class GithubUnassignIssueBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        assignee: str = SchemaField(
            description=""Username to unassign from the issue"",
            placeholder=""Enter the username"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the issue unassignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the issue unassignment failed""
        )

    def __init__(self):
        super().__init__(
            id=""d154002a-38f4-46c2-962d-2488f2b05ece"",
            description=""This block unassigns a user from a specified GitHub issue."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubUnassignIssueBlock.Input,
            output_schema=GithubUnassignIssueBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""assignee"": ""username1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Issue unassigned successfully"")],
            test_mock={
                ""unassign_issue"": lambda *args, **kwargs: ""Issue unassigned successfully""
            },
        )

    @staticmethod
    def unassign_issue(
        credentials: GithubCredentials,
        issue_url: str,
        assignee: str,
    ) -> str:
        api = get_api(credentials)
        assignees_url = issue_url + ""/assignees""
        data = {""assignees"": [assignee]}
        api.delete(assignees_url, json=data)
        return ""Issue unassigned successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.unassign_issue(
            credentials,
            input_data.issue_url,
            input_data.assignee,
        )
        yield ""status"", status","Point(row=500, column=0)","Point(row=563, column=30)",,autogpt_platform/backend/backend/blocks/github/issues.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        issue_url: str = SchemaField(
            description=""URL of the GitHub issue"",
            placeholder=""https://github.com/owner/repo/issues/1"",
        )
        assignee: str = SchemaField(
            description=""Username to unassign from the issue"",
            placeholder=""Enter the username"",
        )","Point(row=501, column=4)","Point(row=510, column=9)",,autogpt_platform/backend/backend/blocks/github/issues.py
Output,class,,"class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the issue unassignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the issue unassignment failed""
        )","Point(row=512, column=4)","Point(row=518, column=9)",,autogpt_platform/backend/backend/blocks/github/issues.py
GithubUnassignIssueBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""d154002a-38f4-46c2-962d-2488f2b05ece"",
            description=""This block unassigns a user from a specified GitHub issue."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubUnassignIssueBlock.Input,
            output_schema=GithubUnassignIssueBlock.Output,
            test_input={
                ""issue_url"": ""https://github.com/owner/repo/issues/1"",
                ""assignee"": ""username1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Issue unassigned successfully"")],
            test_mock={
                ""unassign_issue"": lambda *args, **kwargs: ""Issue unassigned successfully""
            },
        )","Point(row=520, column=4)","Point(row=537, column=9)",GithubUnassignIssueBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubUnassignIssueBlock.unassign_issue,function,,"def unassign_issue(
        credentials: GithubCredentials,
        issue_url: str,
        assignee: str,
    ) -> str:
        api = get_api(credentials)
        assignees_url = issue_url + ""/assignees""
        data = {""assignees"": [assignee]}
        api.delete(assignees_url, json=data)
        return ""Issue unassigned successfully""","Point(row=540, column=4)","Point(row=549, column=46)",GithubUnassignIssueBlock,autogpt_platform/backend/backend/blocks/github/issues.py
GithubUnassignIssueBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.unassign_issue(
            credentials,
            input_data.issue_url,
            input_data.assignee,
        )
        yield ""status"", status","Point(row=551, column=4)","Point(row=563, column=30)",GithubUnassignIssueBlock,autogpt_platform/backend/backend/blocks/github/issues.py
_convert_to_api_url,function,"
    Converts a standard GitHub URL to the corresponding GitHub API URL.
    Handles repository URLs, issue URLs, pull request URLs, and more.
","def _convert_to_api_url(url: str) -> str:
    """"""
    Converts a standard GitHub URL to the corresponding GitHub API URL.
    Handles repository URLs, issue URLs, pull request URLs, and more.
    """"""
    parsed_url = urlparse(url)
    path_parts = parsed_url.path.strip(""/"").split(""/"")

    if len(path_parts) >= 2:
        owner, repo = path_parts[0], path_parts[1]
        api_base = f""https://api.github.com/repos/{owner}/{repo}""

        if len(path_parts) > 2:
            additional_path = ""/"".join(path_parts[2:])
            api_url = f""{api_base}/{additional_path}""
        else:
            # Repository base URL
            api_url = api_base
    else:
        raise ValueError(""Invalid GitHub URL format."")

    return api_url","Point(row=6, column=0)","Point(row=27, column=18)",,autogpt_platform/backend/backend/blocks/github/_api.py
_get_headers,function,,"def _get_headers(credentials: GithubCredentials) -> dict[str, str]:
    return {
        ""Authorization"": credentials.bearer(),
        ""Accept"": ""application/vnd.github.v3+json"",
    }","Point(row=30, column=0)","Point(row=34, column=5)",,autogpt_platform/backend/backend/blocks/github/_api.py
get_api,function,,"def get_api(credentials: GithubCredentials) -> Requests:
    return Requests(
        trusted_origins=[""https://api.github.com"", ""https://github.com""],
        extra_url_validator=_convert_to_api_url,
        extra_headers=_get_headers(credentials),
    )","Point(row=37, column=0)","Point(row=42, column=5)",,autogpt_platform/backend/backend/blocks/github/_api.py
GithubListTagsBlock,class,,"class GithubListTagsBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )

    class Output(BlockSchema):
        class TagItem(TypedDict):
            name: str
            url: str

        tag: TagItem = SchemaField(
            title=""Tag"", description=""Tags with their name and file tree browser URL""
        )
        error: str = SchemaField(description=""Error message if listing tags failed"")

    def __init__(self):
        super().__init__(
            id=""358924e7-9a11-4d1a-a0f2-13c67fe59e2e"",
            description=""This block lists all tags for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListTagsBlock.Input,
            output_schema=GithubListTagsBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""tag"",
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/tree/v1.0.0"",
                    },
                )
            ],
            test_mock={
                ""list_tags"": lambda *args, **kwargs: [
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/tree/v1.0.0"",
                    }
                ]
            },
        )

    @staticmethod
    def list_tags(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.TagItem]:
        api = get_api(credentials)
        tags_url = repo_url + ""/tags""
        response = api.get(tags_url)
        data = response.json()
        repo_path = repo_url.replace(""https://github.com/"", """")
        tags: list[GithubListTagsBlock.Output.TagItem] = [
            {
                ""name"": tag[""name""],
                ""url"": f""https://github.com/{repo_path}/tree/{tag['name']}"",
            }
            for tag in data
        ]
        return tags

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        tags = self.list_tags(
            credentials,
            input_data.repo_url,
        )
        yield from ((""tag"", tag) for tag in tags)","Point(row=17, column=0)","Point(row=95, column=49)",,autogpt_platform/backend/backend/blocks/github/repo.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )","Point(row=18, column=4)","Point(row=23, column=9)",,autogpt_platform/backend/backend/blocks/github/repo.py
Output,class,,"class Output(BlockSchema):
        class TagItem(TypedDict):
            name: str
            url: str

        tag: TagItem = SchemaField(
            title=""Tag"", description=""Tags with their name and file tree browser URL""
        )
        error: str = SchemaField(description=""Error message if listing tags failed"")","Point(row=25, column=4)","Point(row=33, column=84)",,autogpt_platform/backend/backend/blocks/github/repo.py
TagItem,class,,"class TagItem(TypedDict):
            name: str
            url: str","Point(row=26, column=8)","Point(row=28, column=20)",,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListTagsBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""358924e7-9a11-4d1a-a0f2-13c67fe59e2e"",
            description=""This block lists all tags for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListTagsBlock.Input,
            output_schema=GithubListTagsBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""tag"",
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/tree/v1.0.0"",
                    },
                )
            ],
            test_mock={
                ""list_tags"": lambda *args, **kwargs: [
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/tree/v1.0.0"",
                    }
                ]
            },
        )","Point(row=35, column=4)","Point(row=64, column=9)",GithubListTagsBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListTagsBlock.list_tags,function,,"def list_tags(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.TagItem]:
        api = get_api(credentials)
        tags_url = repo_url + ""/tags""
        response = api.get(tags_url)
        data = response.json()
        repo_path = repo_url.replace(""https://github.com/"", """")
        tags: list[GithubListTagsBlock.Output.TagItem] = [
            {
                ""name"": tag[""name""],
                ""url"": f""https://github.com/{repo_path}/tree/{tag['name']}"",
            }
            for tag in data
        ]
        return tags","Point(row=67, column=4)","Point(row=82, column=19)",GithubListTagsBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListTagsBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        tags = self.list_tags(
            credentials,
            input_data.repo_url,
        )
        yield from ((""tag"", tag) for tag in tags)","Point(row=84, column=4)","Point(row=95, column=49)",GithubListTagsBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListBranchesBlock,class,,"class GithubListBranchesBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )

    class Output(BlockSchema):
        class BranchItem(TypedDict):
            name: str
            url: str

        branch: BranchItem = SchemaField(
            title=""Branch"",
            description=""Branches with their name and file tree browser URL"",
        )
        error: str = SchemaField(description=""Error message if listing branches failed"")

    def __init__(self):
        super().__init__(
            id=""74243e49-2bec-4916-8bf4-db43d44aead5"",
            description=""This block lists all branches for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListBranchesBlock.Input,
            output_schema=GithubListBranchesBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""branch"",
                    {
                        ""name"": ""main"",
                        ""url"": ""https://github.com/owner/repo/tree/main"",
                    },
                )
            ],
            test_mock={
                ""list_branches"": lambda *args, **kwargs: [
                    {
                        ""name"": ""main"",
                        ""url"": ""https://github.com/owner/repo/tree/main"",
                    }
                ]
            },
        )

    @staticmethod
    def list_branches(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.BranchItem]:
        api = get_api(credentials)
        branches_url = repo_url + ""/branches""
        response = api.get(branches_url)
        data = response.json()
        repo_path = repo_url.replace(""https://github.com/"", """")
        branches: list[GithubListBranchesBlock.Output.BranchItem] = [
            {
                ""name"": branch[""name""],
                ""url"": f""https://github.com/{repo_path}/tree/{branch['name']}"",
            }
            for branch in data
        ]
        return branches

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        branches = self.list_branches(
            credentials,
            input_data.repo_url,
        )
        yield from ((""branch"", branch) for branch in branches)","Point(row=98, column=0)","Point(row=177, column=62)",,autogpt_platform/backend/backend/blocks/github/repo.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )","Point(row=99, column=4)","Point(row=104, column=9)",,autogpt_platform/backend/backend/blocks/github/repo.py
Output,class,,"class Output(BlockSchema):
        class BranchItem(TypedDict):
            name: str
            url: str

        branch: BranchItem = SchemaField(
            title=""Branch"",
            description=""Branches with their name and file tree browser URL"",
        )
        error: str = SchemaField(description=""Error message if listing branches failed"")","Point(row=106, column=4)","Point(row=115, column=88)",,autogpt_platform/backend/backend/blocks/github/repo.py
BranchItem,class,,"class BranchItem(TypedDict):
            name: str
            url: str","Point(row=107, column=8)","Point(row=109, column=20)",,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListBranchesBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""74243e49-2bec-4916-8bf4-db43d44aead5"",
            description=""This block lists all branches for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListBranchesBlock.Input,
            output_schema=GithubListBranchesBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""branch"",
                    {
                        ""name"": ""main"",
                        ""url"": ""https://github.com/owner/repo/tree/main"",
                    },
                )
            ],
            test_mock={
                ""list_branches"": lambda *args, **kwargs: [
                    {
                        ""name"": ""main"",
                        ""url"": ""https://github.com/owner/repo/tree/main"",
                    }
                ]
            },
        )","Point(row=117, column=4)","Point(row=146, column=9)",GithubListBranchesBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListBranchesBlock.list_branches,function,,"def list_branches(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.BranchItem]:
        api = get_api(credentials)
        branches_url = repo_url + ""/branches""
        response = api.get(branches_url)
        data = response.json()
        repo_path = repo_url.replace(""https://github.com/"", """")
        branches: list[GithubListBranchesBlock.Output.BranchItem] = [
            {
                ""name"": branch[""name""],
                ""url"": f""https://github.com/{repo_path}/tree/{branch['name']}"",
            }
            for branch in data
        ]
        return branches","Point(row=149, column=4)","Point(row=164, column=23)",GithubListBranchesBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListBranchesBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        branches = self.list_branches(
            credentials,
            input_data.repo_url,
        )
        yield from ((""branch"", branch) for branch in branches)","Point(row=166, column=4)","Point(row=177, column=62)",GithubListBranchesBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListDiscussionsBlock,class,,"class GithubListDiscussionsBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        num_discussions: int = SchemaField(
            description=""Number of discussions to fetch"", default=5
        )

    class Output(BlockSchema):
        class DiscussionItem(TypedDict):
            title: str
            url: str

        discussion: DiscussionItem = SchemaField(
            title=""Discussion"", description=""Discussions with their title and URL""
        )
        error: str = SchemaField(
            description=""Error message if listing discussions failed""
        )

    def __init__(self):
        super().__init__(
            id=""3ef1a419-3d76-4e07-b761-de9dad4d51d7"",
            description=""This block lists recent discussions for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListDiscussionsBlock.Input,
            output_schema=GithubListDiscussionsBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""num_discussions"": 3,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""discussion"",
                    {
                        ""title"": ""Discussion 1"",
                        ""url"": ""https://github.com/owner/repo/discussions/1"",
                    },
                )
            ],
            test_mock={
                ""list_discussions"": lambda *args, **kwargs: [
                    {
                        ""title"": ""Discussion 1"",
                        ""url"": ""https://github.com/owner/repo/discussions/1"",
                    }
                ]
            },
        )

    @staticmethod
    def list_discussions(
        credentials: GithubCredentials, repo_url: str, num_discussions: int
    ) -> list[Output.DiscussionItem]:
        api = get_api(credentials)
        # GitHub GraphQL API endpoint is different; we'll use api.post with custom URL
        repo_path = repo_url.replace(""https://github.com/"", """")
        owner, repo = repo_path.split(""/"")
        query = """"""
        query($owner: String!, $repo: String!, $num: Int!) {
            repository(owner: $owner, name: $repo) {
                discussions(first: $num) {
                    nodes {
                        title
                        url
                    }
                }
            }
        }
        """"""
        variables = {""owner"": owner, ""repo"": repo, ""num"": num_discussions}
        response = api.post(
            ""https://api.github.com/graphql"",
            json={""query"": query, ""variables"": variables},
        )
        data = response.json()
        discussions: list[GithubListDiscussionsBlock.Output.DiscussionItem] = [
            {""title"": discussion[""title""], ""url"": discussion[""url""]}
            for discussion in data[""data""][""repository""][""discussions""][""nodes""]
        ]
        return discussions

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        discussions = self.list_discussions(
            credentials, input_data.repo_url, input_data.num_discussions
        )
        yield from ((""discussion"", discussion) for discussion in discussions)","Point(row=180, column=0)","Point(row=277, column=77)",,autogpt_platform/backend/backend/blocks/github/repo.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        num_discussions: int = SchemaField(
            description=""Number of discussions to fetch"", default=5
        )","Point(row=181, column=4)","Point(row=189, column=9)",,autogpt_platform/backend/backend/blocks/github/repo.py
Output,class,,"class Output(BlockSchema):
        class DiscussionItem(TypedDict):
            title: str
            url: str

        discussion: DiscussionItem = SchemaField(
            title=""Discussion"", description=""Discussions with their title and URL""
        )
        error: str = SchemaField(
            description=""Error message if listing discussions failed""
        )","Point(row=191, column=4)","Point(row=201, column=9)",,autogpt_platform/backend/backend/blocks/github/repo.py
DiscussionItem,class,,"class DiscussionItem(TypedDict):
            title: str
            url: str","Point(row=192, column=8)","Point(row=194, column=20)",,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListDiscussionsBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""3ef1a419-3d76-4e07-b761-de9dad4d51d7"",
            description=""This block lists recent discussions for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListDiscussionsBlock.Input,
            output_schema=GithubListDiscussionsBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""num_discussions"": 3,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""discussion"",
                    {
                        ""title"": ""Discussion 1"",
                        ""url"": ""https://github.com/owner/repo/discussions/1"",
                    },
                )
            ],
            test_mock={
                ""list_discussions"": lambda *args, **kwargs: [
                    {
                        ""title"": ""Discussion 1"",
                        ""url"": ""https://github.com/owner/repo/discussions/1"",
                    }
                ]
            },
        )","Point(row=203, column=4)","Point(row=233, column=9)",GithubListDiscussionsBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListDiscussionsBlock.list_discussions,function,,"def list_discussions(
        credentials: GithubCredentials, repo_url: str, num_discussions: int
    ) -> list[Output.DiscussionItem]:
        api = get_api(credentials)
        # GitHub GraphQL API endpoint is different; we'll use api.post with custom URL
        repo_path = repo_url.replace(""https://github.com/"", """")
        owner, repo = repo_path.split(""/"")
        query = """"""
        query($owner: String!, $repo: String!, $num: Int!) {
            repository(owner: $owner, name: $repo) {
                discussions(first: $num) {
                    nodes {
                        title
                        url
                    }
                }
            }
        }
        """"""
        variables = {""owner"": owner, ""repo"": repo, ""num"": num_discussions}
        response = api.post(
            ""https://api.github.com/graphql"",
            json={""query"": query, ""variables"": variables},
        )
        data = response.json()
        discussions: list[GithubListDiscussionsBlock.Output.DiscussionItem] = [
            {""title"": discussion[""title""], ""url"": discussion[""url""]}
            for discussion in data[""data""][""repository""][""discussions""][""nodes""]
        ]
        return discussions","Point(row=236, column=4)","Point(row=265, column=26)",GithubListDiscussionsBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListDiscussionsBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        discussions = self.list_discussions(
            credentials, input_data.repo_url, input_data.num_discussions
        )
        yield from ((""discussion"", discussion) for discussion in discussions)","Point(row=267, column=4)","Point(row=277, column=77)",GithubListDiscussionsBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListReleasesBlock,class,,"class GithubListReleasesBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )

    class Output(BlockSchema):
        class ReleaseItem(TypedDict):
            name: str
            url: str

        release: ReleaseItem = SchemaField(
            title=""Release"",
            description=""Releases with their name and file tree browser URL"",
        )
        error: str = SchemaField(description=""Error message if listing releases failed"")

    def __init__(self):
        super().__init__(
            id=""3460367a-6ba7-4645-8ce6-47b05d040b92"",
            description=""This block lists all releases for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListReleasesBlock.Input,
            output_schema=GithubListReleasesBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""release"",
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/releases/tag/v1.0.0"",
                    },
                )
            ],
            test_mock={
                ""list_releases"": lambda *args, **kwargs: [
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/releases/tag/v1.0.0"",
                    }
                ]
            },
        )

    @staticmethod
    def list_releases(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.ReleaseItem]:
        api = get_api(credentials)
        releases_url = repo_url + ""/releases""
        response = api.get(releases_url)
        data = response.json()
        releases: list[GithubListReleasesBlock.Output.ReleaseItem] = [
            {""name"": release[""name""], ""url"": release[""html_url""]} for release in data
        ]
        return releases

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        releases = self.list_releases(
            credentials,
            input_data.repo_url,
        )
        yield from ((""release"", release) for release in releases)","Point(row=280, column=0)","Point(row=354, column=65)",,autogpt_platform/backend/backend/blocks/github/repo.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )","Point(row=281, column=4)","Point(row=286, column=9)",,autogpt_platform/backend/backend/blocks/github/repo.py
Output,class,,"class Output(BlockSchema):
        class ReleaseItem(TypedDict):
            name: str
            url: str

        release: ReleaseItem = SchemaField(
            title=""Release"",
            description=""Releases with their name and file tree browser URL"",
        )
        error: str = SchemaField(description=""Error message if listing releases failed"")","Point(row=288, column=4)","Point(row=297, column=88)",,autogpt_platform/backend/backend/blocks/github/repo.py
ReleaseItem,class,,"class ReleaseItem(TypedDict):
            name: str
            url: str","Point(row=289, column=8)","Point(row=291, column=20)",,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListReleasesBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""3460367a-6ba7-4645-8ce6-47b05d040b92"",
            description=""This block lists all releases for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListReleasesBlock.Input,
            output_schema=GithubListReleasesBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""release"",
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/releases/tag/v1.0.0"",
                    },
                )
            ],
            test_mock={
                ""list_releases"": lambda *args, **kwargs: [
                    {
                        ""name"": ""v1.0.0"",
                        ""url"": ""https://github.com/owner/repo/releases/tag/v1.0.0"",
                    }
                ]
            },
        )","Point(row=299, column=4)","Point(row=328, column=9)",GithubListReleasesBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListReleasesBlock.list_releases,function,,"def list_releases(
        credentials: GithubCredentials, repo_url: str
    ) -> list[Output.ReleaseItem]:
        api = get_api(credentials)
        releases_url = repo_url + ""/releases""
        response = api.get(releases_url)
        data = response.json()
        releases: list[GithubListReleasesBlock.Output.ReleaseItem] = [
            {""name"": release[""name""], ""url"": release[""html_url""]} for release in data
        ]
        return releases","Point(row=331, column=4)","Point(row=341, column=23)",GithubListReleasesBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListReleasesBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        releases = self.list_releases(
            credentials,
            input_data.repo_url,
        )
        yield from ((""release"", release) for release in releases)","Point(row=343, column=4)","Point(row=354, column=65)",GithubListReleasesBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubReadFileBlock,class,,"class GithubReadFileBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        file_path: str = SchemaField(
            description=""Path to the file in the repository"",
            placeholder=""path/to/file"",
        )
        branch: str = SchemaField(
            description=""Branch to read from"",
            placeholder=""branch_name"",
            default=""master"",
        )

    class Output(BlockSchema):
        text_content: str = SchemaField(
            description=""Content of the file (decoded as UTF-8 text)""
        )
        raw_content: str = SchemaField(
            description=""Raw base64-encoded content of the file""
        )
        size: int = SchemaField(description=""The size of the file (in bytes)"")
        error: str = SchemaField(description=""Error message if the file reading failed"")

    def __init__(self):
        super().__init__(
            id=""87ce6c27-5752-4bbc-8e26-6da40a3dcfd3"",
            description=""This block reads the content of a specified file from a GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadFileBlock.Input,
            output_schema=GithubReadFileBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""file_path"": ""path/to/file"",
                ""branch"": ""master"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""raw_content"", ""RmlsZSBjb250ZW50""),
                (""text_content"", ""File content""),
                (""size"", 13),
            ],
            test_mock={""read_file"": lambda *args, **kwargs: (""RmlsZSBjb250ZW50"", 13)},
        )

    @staticmethod
    def read_file(
        credentials: GithubCredentials, repo_url: str, file_path: str, branch: str
    ) -> tuple[str, int]:
        api = get_api(credentials)
        content_url = repo_url + f""/contents/{file_path}?ref={branch}""
        response = api.get(content_url)
        content = response.json()

        if isinstance(content, list):
            # Multiple entries of different types exist at this path
            if not (file := next((f for f in content if f[""type""] == ""file""), None)):
                raise TypeError(""Not a file"")
            content = file

        if content[""type""] != ""file"":
            raise TypeError(""Not a file"")

        return content[""content""], content[""size""]

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        raw_content, size = self.read_file(
            credentials,
            input_data.repo_url,
            input_data.file_path.lstrip(""/""),
            input_data.branch,
        )
        yield ""raw_content"", raw_content
        yield ""text_content"", base64.b64decode(raw_content).decode(""utf-8"")
        yield ""size"", size","Point(row=357, column=0)","Point(row=441, column=26)",,autogpt_platform/backend/backend/blocks/github/repo.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        file_path: str = SchemaField(
            description=""Path to the file in the repository"",
            placeholder=""path/to/file"",
        )
        branch: str = SchemaField(
            description=""Branch to read from"",
            placeholder=""branch_name"",
            default=""master"",
        )","Point(row=358, column=4)","Point(row=372, column=9)",,autogpt_platform/backend/backend/blocks/github/repo.py
Output,class,,"class Output(BlockSchema):
        text_content: str = SchemaField(
            description=""Content of the file (decoded as UTF-8 text)""
        )
        raw_content: str = SchemaField(
            description=""Raw base64-encoded content of the file""
        )
        size: int = SchemaField(description=""The size of the file (in bytes)"")
        error: str = SchemaField(description=""Error message if the file reading failed"")","Point(row=374, column=4)","Point(row=382, column=88)",,autogpt_platform/backend/backend/blocks/github/repo.py
GithubReadFileBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""87ce6c27-5752-4bbc-8e26-6da40a3dcfd3"",
            description=""This block reads the content of a specified file from a GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadFileBlock.Input,
            output_schema=GithubReadFileBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""file_path"": ""path/to/file"",
                ""branch"": ""master"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""raw_content"", ""RmlsZSBjb250ZW50""),
                (""text_content"", ""File content""),
                (""size"", 13),
            ],
            test_mock={""read_file"": lambda *args, **kwargs: (""RmlsZSBjb250ZW50"", 13)},
        )","Point(row=384, column=4)","Point(row=404, column=9)",GithubReadFileBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubReadFileBlock.read_file,function,,"def read_file(
        credentials: GithubCredentials, repo_url: str, file_path: str, branch: str
    ) -> tuple[str, int]:
        api = get_api(credentials)
        content_url = repo_url + f""/contents/{file_path}?ref={branch}""
        response = api.get(content_url)
        content = response.json()

        if isinstance(content, list):
            # Multiple entries of different types exist at this path
            if not (file := next((f for f in content if f[""type""] == ""file""), None)):
                raise TypeError(""Not a file"")
            content = file

        if content[""type""] != ""file"":
            raise TypeError(""Not a file"")

        return content[""content""], content[""size""]","Point(row=407, column=4)","Point(row=424, column=50)",GithubReadFileBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubReadFileBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        raw_content, size = self.read_file(
            credentials,
            input_data.repo_url,
            input_data.file_path.lstrip(""/""),
            input_data.branch,
        )
        yield ""raw_content"", raw_content
        yield ""text_content"", base64.b64decode(raw_content).decode(""utf-8"")
        yield ""size"", size","Point(row=426, column=4)","Point(row=441, column=26)",GithubReadFileBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubReadFolderBlock,class,,"class GithubReadFolderBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        folder_path: str = SchemaField(
            description=""Path to the folder in the repository"",
            placeholder=""path/to/folder"",
        )
        branch: str = SchemaField(
            description=""Branch name to read from (defaults to master)"",
            placeholder=""branch_name"",
            default=""master"",
        )

    class Output(BlockSchema):
        class DirEntry(TypedDict):
            name: str
            path: str

        class FileEntry(TypedDict):
            name: str
            path: str
            size: int

        file: FileEntry = SchemaField(description=""Files in the folder"")
        dir: DirEntry = SchemaField(description=""Directories in the folder"")
        error: str = SchemaField(
            description=""Error message if reading the folder failed""
        )

    def __init__(self):
        super().__init__(
            id=""1355f863-2db3-4d75-9fba-f91e8a8ca400"",
            description=""This block reads the content of a specified folder from a GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadFolderBlock.Input,
            output_schema=GithubReadFolderBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""folder_path"": ""path/to/folder"",
                ""branch"": ""master"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""file"",
                    {
                        ""name"": ""file1.txt"",
                        ""path"": ""path/to/folder/file1.txt"",
                        ""size"": 1337,
                    },
                ),
                (""dir"", {""name"": ""dir2"", ""path"": ""path/to/folder/dir2""}),
            ],
            test_mock={
                ""read_folder"": lambda *args, **kwargs: (
                    [
                        {
                            ""name"": ""file1.txt"",
                            ""path"": ""path/to/folder/file1.txt"",
                            ""size"": 1337,
                        }
                    ],
                    [{""name"": ""dir2"", ""path"": ""path/to/folder/dir2""}],
                )
            },
        )

    @staticmethod
    def read_folder(
        credentials: GithubCredentials, repo_url: str, folder_path: str, branch: str
    ) -> tuple[list[Output.FileEntry], list[Output.DirEntry]]:
        api = get_api(credentials)
        contents_url = repo_url + f""/contents/{folder_path}?ref={branch}""
        response = api.get(contents_url)
        content = response.json()

        if not isinstance(content, list):
            raise TypeError(""Not a folder"")

        files = [
            GithubReadFolderBlock.Output.FileEntry(
                name=entry[""name""],
                path=entry[""path""],
                size=entry[""size""],
            )
            for entry in content
            if entry[""type""] == ""file""
        ]
        dirs = [
            GithubReadFolderBlock.Output.DirEntry(
                name=entry[""name""],
                path=entry[""path""],
            )
            for entry in content
            if entry[""type""] == ""dir""
        ]

        return files, dirs

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        files, dirs = self.read_folder(
            credentials,
            input_data.repo_url,
            input_data.folder_path.lstrip(""/""),
            input_data.branch,
        )
        yield from ((""file"", file) for file in files)
        yield from ((""dir"", dir) for dir in dirs)","Point(row=444, column=0)","Point(row=562, column=49)",,autogpt_platform/backend/backend/blocks/github/repo.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        folder_path: str = SchemaField(
            description=""Path to the folder in the repository"",
            placeholder=""path/to/folder"",
        )
        branch: str = SchemaField(
            description=""Branch name to read from (defaults to master)"",
            placeholder=""branch_name"",
            default=""master"",
        )","Point(row=445, column=4)","Point(row=459, column=9)",,autogpt_platform/backend/backend/blocks/github/repo.py
Output,class,,"class Output(BlockSchema):
        class DirEntry(TypedDict):
            name: str
            path: str

        class FileEntry(TypedDict):
            name: str
            path: str
            size: int

        file: FileEntry = SchemaField(description=""Files in the folder"")
        dir: DirEntry = SchemaField(description=""Directories in the folder"")
        error: str = SchemaField(
            description=""Error message if reading the folder failed""
        )","Point(row=461, column=4)","Point(row=475, column=9)",,autogpt_platform/backend/backend/blocks/github/repo.py
DirEntry,class,,"class DirEntry(TypedDict):
            name: str
            path: str","Point(row=462, column=8)","Point(row=464, column=21)",,autogpt_platform/backend/backend/blocks/github/repo.py
FileEntry,class,,"class FileEntry(TypedDict):
            name: str
            path: str
            size: int","Point(row=466, column=8)","Point(row=469, column=21)",,autogpt_platform/backend/backend/blocks/github/repo.py
GithubReadFolderBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""1355f863-2db3-4d75-9fba-f91e8a8ca400"",
            description=""This block reads the content of a specified folder from a GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadFolderBlock.Input,
            output_schema=GithubReadFolderBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""folder_path"": ""path/to/folder"",
                ""branch"": ""master"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""file"",
                    {
                        ""name"": ""file1.txt"",
                        ""path"": ""path/to/folder/file1.txt"",
                        ""size"": 1337,
                    },
                ),
                (""dir"", {""name"": ""dir2"", ""path"": ""path/to/folder/dir2""}),
            ],
            test_mock={
                ""read_folder"": lambda *args, **kwargs: (
                    [
                        {
                            ""name"": ""file1.txt"",
                            ""path"": ""path/to/folder/file1.txt"",
                            ""size"": 1337,
                        }
                    ],
                    [{""name"": ""dir2"", ""path"": ""path/to/folder/dir2""}],
                )
            },
        )","Point(row=477, column=4)","Point(row=514, column=9)",GithubReadFolderBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubReadFolderBlock.read_folder,function,,"def read_folder(
        credentials: GithubCredentials, repo_url: str, folder_path: str, branch: str
    ) -> tuple[list[Output.FileEntry], list[Output.DirEntry]]:
        api = get_api(credentials)
        contents_url = repo_url + f""/contents/{folder_path}?ref={branch}""
        response = api.get(contents_url)
        content = response.json()

        if not isinstance(content, list):
            raise TypeError(""Not a folder"")

        files = [
            GithubReadFolderBlock.Output.FileEntry(
                name=entry[""name""],
                path=entry[""path""],
                size=entry[""size""],
            )
            for entry in content
            if entry[""type""] == ""file""
        ]
        dirs = [
            GithubReadFolderBlock.Output.DirEntry(
                name=entry[""name""],
                path=entry[""path""],
            )
            for entry in content
            if entry[""type""] == ""dir""
        ]

        return files, dirs","Point(row=517, column=4)","Point(row=546, column=26)",GithubReadFolderBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubReadFolderBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        files, dirs = self.read_folder(
            credentials,
            input_data.repo_url,
            input_data.folder_path.lstrip(""/""),
            input_data.branch,
        )
        yield from ((""file"", file) for file in files)
        yield from ((""dir"", dir) for dir in dirs)","Point(row=548, column=4)","Point(row=562, column=49)",GithubReadFolderBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubMakeBranchBlock,class,,"class GithubMakeBranchBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        new_branch: str = SchemaField(
            description=""Name of the new branch"",
            placeholder=""new_branch_name"",
        )
        source_branch: str = SchemaField(
            description=""Name of the source branch"",
            placeholder=""source_branch_name"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the branch creation operation"")
        error: str = SchemaField(
            description=""Error message if the branch creation failed""
        )

    def __init__(self):
        super().__init__(
            id=""944cc076-95e7-4d1b-b6b6-b15d8ee5448d"",
            description=""This block creates a new branch from a specified source branch."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubMakeBranchBlock.Input,
            output_schema=GithubMakeBranchBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""new_branch"": ""new_branch_name"",
                ""source_branch"": ""source_branch_name"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Branch created successfully"")],
            test_mock={
                ""create_branch"": lambda *args, **kwargs: ""Branch created successfully""
            },
        )

    @staticmethod
    def create_branch(
        credentials: GithubCredentials,
        repo_url: str,
        new_branch: str,
        source_branch: str,
    ) -> str:
        api = get_api(credentials)
        # Get the SHA of the source branch
        ref_url = repo_url + f""/git/refs/heads/{source_branch}""
        response = api.get(ref_url)
        sha = response.json()[""object""][""sha""]

        # Create the new branch
        create_ref_url = repo_url + ""/git/refs""
        data = {""ref"": f""refs/heads/{new_branch}"", ""sha"": sha}
        response = api.post(create_ref_url, json=data)
        return ""Branch created successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.create_branch(
            credentials,
            input_data.repo_url,
            input_data.new_branch,
            input_data.source_branch,
        )
        yield ""status"", status","Point(row=565, column=0)","Point(row=639, column=30)",,autogpt_platform/backend/backend/blocks/github/repo.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        new_branch: str = SchemaField(
            description=""Name of the new branch"",
            placeholder=""new_branch_name"",
        )
        source_branch: str = SchemaField(
            description=""Name of the source branch"",
            placeholder=""source_branch_name"",
        )","Point(row=566, column=4)","Point(row=579, column=9)",,autogpt_platform/backend/backend/blocks/github/repo.py
Output,class,,"class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the branch creation operation"")
        error: str = SchemaField(
            description=""Error message if the branch creation failed""
        )","Point(row=581, column=4)","Point(row=585, column=9)",,autogpt_platform/backend/backend/blocks/github/repo.py
GithubMakeBranchBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""944cc076-95e7-4d1b-b6b6-b15d8ee5448d"",
            description=""This block creates a new branch from a specified source branch."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubMakeBranchBlock.Input,
            output_schema=GithubMakeBranchBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""new_branch"": ""new_branch_name"",
                ""source_branch"": ""source_branch_name"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Branch created successfully"")],
            test_mock={
                ""create_branch"": lambda *args, **kwargs: ""Branch created successfully""
            },
        )","Point(row=587, column=4)","Point(row=605, column=9)",GithubMakeBranchBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubMakeBranchBlock.create_branch,function,,"def create_branch(
        credentials: GithubCredentials,
        repo_url: str,
        new_branch: str,
        source_branch: str,
    ) -> str:
        api = get_api(credentials)
        # Get the SHA of the source branch
        ref_url = repo_url + f""/git/refs/heads/{source_branch}""
        response = api.get(ref_url)
        sha = response.json()[""object""][""sha""]

        # Create the new branch
        create_ref_url = repo_url + ""/git/refs""
        data = {""ref"": f""refs/heads/{new_branch}"", ""sha"": sha}
        response = api.post(create_ref_url, json=data)
        return ""Branch created successfully""","Point(row=608, column=4)","Point(row=624, column=44)",GithubMakeBranchBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubMakeBranchBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.create_branch(
            credentials,
            input_data.repo_url,
            input_data.new_branch,
            input_data.source_branch,
        )
        yield ""status"", status","Point(row=626, column=4)","Point(row=639, column=30)",GithubMakeBranchBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubDeleteBranchBlock,class,,"class GithubDeleteBranchBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        branch: str = SchemaField(
            description=""Name of the branch to delete"",
            placeholder=""branch_name"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the branch deletion operation"")
        error: str = SchemaField(
            description=""Error message if the branch deletion failed""
        )

    def __init__(self):
        super().__init__(
            id=""0d4130f7-e0ab-4d55-adc3-0a40225e80f4"",
            description=""This block deletes a specified branch."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubDeleteBranchBlock.Input,
            output_schema=GithubDeleteBranchBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""branch"": ""branch_name"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Branch deleted successfully"")],
            test_mock={
                ""delete_branch"": lambda *args, **kwargs: ""Branch deleted successfully""
            },
        )

    @staticmethod
    def delete_branch(
        credentials: GithubCredentials, repo_url: str, branch: str
    ) -> str:
        api = get_api(credentials)
        ref_url = repo_url + f""/git/refs/heads/{branch}""
        api.delete(ref_url)
        return ""Branch deleted successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.delete_branch(
            credentials,
            input_data.repo_url,
            input_data.branch,
        )
        yield ""status"", status","Point(row=642, column=0)","Point(row=700, column=30)",,autogpt_platform/backend/backend/blocks/github/repo.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        branch: str = SchemaField(
            description=""Name of the branch to delete"",
            placeholder=""branch_name"",
        )","Point(row=643, column=4)","Point(row=652, column=9)",,autogpt_platform/backend/backend/blocks/github/repo.py
Output,class,,"class Output(BlockSchema):
        status: str = SchemaField(description=""Status of the branch deletion operation"")
        error: str = SchemaField(
            description=""Error message if the branch deletion failed""
        )","Point(row=654, column=4)","Point(row=658, column=9)",,autogpt_platform/backend/backend/blocks/github/repo.py
GithubDeleteBranchBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""0d4130f7-e0ab-4d55-adc3-0a40225e80f4"",
            description=""This block deletes a specified branch."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubDeleteBranchBlock.Input,
            output_schema=GithubDeleteBranchBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""branch"": ""branch_name"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Branch deleted successfully"")],
            test_mock={
                ""delete_branch"": lambda *args, **kwargs: ""Branch deleted successfully""
            },
        )","Point(row=660, column=4)","Point(row=677, column=9)",GithubDeleteBranchBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubDeleteBranchBlock.delete_branch,function,,"def delete_branch(
        credentials: GithubCredentials, repo_url: str, branch: str
    ) -> str:
        api = get_api(credentials)
        ref_url = repo_url + f""/git/refs/heads/{branch}""
        api.delete(ref_url)
        return ""Branch deleted successfully""","Point(row=680, column=4)","Point(row=686, column=44)",GithubDeleteBranchBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubDeleteBranchBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        status = self.delete_branch(
            credentials,
            input_data.repo_url,
            input_data.branch,
        )
        yield ""status"", status","Point(row=688, column=4)","Point(row=700, column=30)",GithubDeleteBranchBlock,autogpt_platform/backend/backend/blocks/github/repo.py
GithubListPullRequestsBlock,class,,"class GithubListPullRequestsBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )

    class Output(BlockSchema):
        class PRItem(TypedDict):
            title: str
            url: str

        pull_request: PRItem = SchemaField(
            title=""Pull Request"", description=""PRs with their title and URL""
        )
        error: str = SchemaField(description=""Error message if listing issues failed"")

    def __init__(self):
        super().__init__(
            id=""ffef3c4c-6cd0-48dd-817d-459f975219f4"",
            description=""This block lists all pull requests for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListPullRequestsBlock.Input,
            output_schema=GithubListPullRequestsBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""pull_request"",
                    {
                        ""title"": ""Pull request 1"",
                        ""url"": ""https://github.com/owner/repo/pull/1"",
                    },
                )
            ],
            test_mock={
                ""list_prs"": lambda *args, **kwargs: [
                    {
                        ""title"": ""Pull request 1"",
                        ""url"": ""https://github.com/owner/repo/pull/1"",
                    }
                ]
            },
        )

    @staticmethod
    def list_prs(credentials: GithubCredentials, repo_url: str) -> list[Output.PRItem]:
        api = get_api(credentials)
        pulls_url = repo_url + ""/pulls""
        response = api.get(pulls_url)
        data = response.json()
        pull_requests: list[GithubListPullRequestsBlock.Output.PRItem] = [
            {""title"": pr[""title""], ""url"": pr[""html_url""]} for pr in data
        ]
        return pull_requests

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        pull_requests = self.list_prs(
            credentials,
            input_data.repo_url,
        )
        yield from ((""pull_request"", pr) for pr in pull_requests)","Point(row=15, column=0)","Point(row=86, column=65)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )","Point(row=16, column=4)","Point(row=21, column=9)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
Output,class,,"class Output(BlockSchema):
        class PRItem(TypedDict):
            title: str
            url: str

        pull_request: PRItem = SchemaField(
            title=""Pull Request"", description=""PRs with their title and URL""
        )
        error: str = SchemaField(description=""Error message if listing issues failed"")","Point(row=23, column=4)","Point(row=31, column=86)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
PRItem,class,,"class PRItem(TypedDict):
            title: str
            url: str","Point(row=24, column=8)","Point(row=26, column=20)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubListPullRequestsBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""ffef3c4c-6cd0-48dd-817d-459f975219f4"",
            description=""This block lists all pull requests for a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListPullRequestsBlock.Input,
            output_schema=GithubListPullRequestsBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""pull_request"",
                    {
                        ""title"": ""Pull request 1"",
                        ""url"": ""https://github.com/owner/repo/pull/1"",
                    },
                )
            ],
            test_mock={
                ""list_prs"": lambda *args, **kwargs: [
                    {
                        ""title"": ""Pull request 1"",
                        ""url"": ""https://github.com/owner/repo/pull/1"",
                    }
                ]
            },
        )","Point(row=33, column=4)","Point(row=62, column=9)",GithubListPullRequestsBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubListPullRequestsBlock.list_prs,function,,"def list_prs(credentials: GithubCredentials, repo_url: str) -> list[Output.PRItem]:
        api = get_api(credentials)
        pulls_url = repo_url + ""/pulls""
        response = api.get(pulls_url)
        data = response.json()
        pull_requests: list[GithubListPullRequestsBlock.Output.PRItem] = [
            {""title"": pr[""title""], ""url"": pr[""html_url""]} for pr in data
        ]
        return pull_requests","Point(row=65, column=4)","Point(row=73, column=28)",GithubListPullRequestsBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubListPullRequestsBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        pull_requests = self.list_prs(
            credentials,
            input_data.repo_url,
        )
        yield from ((""pull_request"", pr) for pr in pull_requests)","Point(row=75, column=4)","Point(row=86, column=65)",GithubListPullRequestsBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubMakePullRequestBlock,class,,"class GithubMakePullRequestBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        title: str = SchemaField(
            description=""Title of the pull request"",
            placeholder=""Enter the pull request title"",
        )
        body: str = SchemaField(
            description=""Body of the pull request"",
            placeholder=""Enter the pull request body"",
        )
        head: str = SchemaField(
            description=(
                ""The name of the branch where your changes are implemented. ""
                ""For cross-repository pull requests in the same network, ""
                ""namespace head with a user like this: username:branch.""
            ),
            placeholder=""Enter the head branch"",
        )
        base: str = SchemaField(
            description=""The name of the branch you want the changes pulled into."",
            placeholder=""Enter the base branch"",
        )

    class Output(BlockSchema):
        number: int = SchemaField(description=""Number of the created pull request"")
        url: str = SchemaField(description=""URL of the created pull request"")
        error: str = SchemaField(
            description=""Error message if the pull request creation failed""
        )

    def __init__(self):
        super().__init__(
            id=""dfb987f8-f197-4b2e-bf19-111812afd692"",
            description=""This block creates a new pull request on a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubMakePullRequestBlock.Input,
            output_schema=GithubMakePullRequestBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""title"": ""Test Pull Request"",
                ""body"": ""This is a test pull request."",
                ""head"": ""feature-branch"",
                ""base"": ""main"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""number"", 1),
                (""url"", ""https://github.com/owner/repo/pull/1""),
            ],
            test_mock={
                ""create_pr"": lambda *args, **kwargs: (
                    1,
                    ""https://github.com/owner/repo/pull/1"",
                )
            },
        )

    @staticmethod
    def create_pr(
        credentials: GithubCredentials,
        repo_url: str,
        title: str,
        body: str,
        head: str,
        base: str,
    ) -> tuple[int, str]:
        api = get_api(credentials)
        pulls_url = repo_url + ""/pulls""
        data = {""title"": title, ""body"": body, ""head"": head, ""base"": base}
        response = api.post(pulls_url, json=data)
        pr_data = response.json()
        return pr_data[""number""], pr_data[""html_url""]

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            number, url = self.create_pr(
                credentials,
                input_data.repo_url,
                input_data.title,
                input_data.body,
                input_data.head,
                input_data.base,
            )
            yield ""number"", number
            yield ""url"", url
        except Exception as e:
            yield ""error"", str(e)","Point(row=89, column=0)","Point(row=187, column=33)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        repo_url: str = SchemaField(
            description=""URL of the GitHub repository"",
            placeholder=""https://github.com/owner/repo"",
        )
        title: str = SchemaField(
            description=""Title of the pull request"",
            placeholder=""Enter the pull request title"",
        )
        body: str = SchemaField(
            description=""Body of the pull request"",
            placeholder=""Enter the pull request body"",
        )
        head: str = SchemaField(
            description=(
                ""The name of the branch where your changes are implemented. ""
                ""For cross-repository pull requests in the same network, ""
                ""namespace head with a user like this: username:branch.""
            ),
            placeholder=""Enter the head branch"",
        )
        base: str = SchemaField(
            description=""The name of the branch you want the changes pulled into."",
            placeholder=""Enter the base branch"",
        )","Point(row=90, column=4)","Point(row=115, column=9)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
Output,class,,"class Output(BlockSchema):
        number: int = SchemaField(description=""Number of the created pull request"")
        url: str = SchemaField(description=""URL of the created pull request"")
        error: str = SchemaField(
            description=""Error message if the pull request creation failed""
        )","Point(row=117, column=4)","Point(row=122, column=9)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubMakePullRequestBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""dfb987f8-f197-4b2e-bf19-111812afd692"",
            description=""This block creates a new pull request on a specified GitHub repository."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubMakePullRequestBlock.Input,
            output_schema=GithubMakePullRequestBlock.Output,
            test_input={
                ""repo_url"": ""https://github.com/owner/repo"",
                ""title"": ""Test Pull Request"",
                ""body"": ""This is a test pull request."",
                ""head"": ""feature-branch"",
                ""base"": ""main"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""number"", 1),
                (""url"", ""https://github.com/owner/repo/pull/1""),
            ],
            test_mock={
                ""create_pr"": lambda *args, **kwargs: (
                    1,
                    ""https://github.com/owner/repo/pull/1"",
                )
            },
        )","Point(row=124, column=4)","Point(row=150, column=9)",GithubMakePullRequestBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubMakePullRequestBlock.create_pr,function,,"def create_pr(
        credentials: GithubCredentials,
        repo_url: str,
        title: str,
        body: str,
        head: str,
        base: str,
    ) -> tuple[int, str]:
        api = get_api(credentials)
        pulls_url = repo_url + ""/pulls""
        data = {""title"": title, ""body"": body, ""head"": head, ""base"": base}
        response = api.post(pulls_url, json=data)
        pr_data = response.json()
        return pr_data[""number""], pr_data[""html_url""]","Point(row=153, column=4)","Point(row=166, column=53)",GithubMakePullRequestBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubMakePullRequestBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            number, url = self.create_pr(
                credentials,
                input_data.repo_url,
                input_data.title,
                input_data.body,
                input_data.head,
                input_data.base,
            )
            yield ""number"", number
            yield ""url"", url
        except Exception as e:
            yield ""error"", str(e)","Point(row=168, column=4)","Point(row=187, column=33)",GithubMakePullRequestBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubReadPullRequestBlock,class,,"class GithubReadPullRequestBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )
        include_pr_changes: bool = SchemaField(
            description=""Whether to include the changes made in the pull request"",
            default=False,
        )

    class Output(BlockSchema):
        title: str = SchemaField(description=""Title of the pull request"")
        body: str = SchemaField(description=""Body of the pull request"")
        author: str = SchemaField(description=""User who created the pull request"")
        changes: str = SchemaField(description=""Changes made in the pull request"")
        error: str = SchemaField(
            description=""Error message if reading the pull request failed""
        )

    def __init__(self):
        super().__init__(
            id=""bf94b2a4-1a30-4600-a783-a8a44ee31301"",
            description=""This block reads the body, title, user, and changes of a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadPullRequestBlock.Input,
            output_schema=GithubReadPullRequestBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""include_pr_changes"": True,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""title"", ""Title of the pull request""),
                (""body"", ""This is the body of the pull request.""),
                (""author"", ""username""),
                (""changes"", ""List of changes made in the pull request.""),
            ],
            test_mock={
                ""read_pr"": lambda *args, **kwargs: (
                    ""Title of the pull request"",
                    ""This is the body of the pull request."",
                    ""username"",
                ),
                ""read_pr_changes"": lambda *args, **kwargs: ""List of changes made in the pull request."",
            },
        )

    @staticmethod
    def read_pr(credentials: GithubCredentials, pr_url: str) -> tuple[str, str, str]:
        api = get_api(credentials)
        # Adjust the URL to access the issue endpoint for PR metadata
        issue_url = pr_url.replace(""/pull/"", ""/issues/"")
        response = api.get(issue_url)
        data = response.json()
        title = data.get(""title"", ""No title found"")
        body = data.get(""body"", ""No body content found"")
        author = data.get(""user"", {}).get(""login"", ""No user found"")
        return title, body, author

    @staticmethod
    def read_pr_changes(credentials: GithubCredentials, pr_url: str) -> str:
        api = get_api(credentials)
        files_url = pr_url + ""/files""
        response = api.get(files_url)
        files = response.json()
        changes = []
        for file in files:
            filename = file.get(""filename"")
            patch = file.get(""patch"")
            if filename and patch:
                changes.append(f""File: {filename}\n{patch}"")
        return ""\n\n"".join(changes)

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        title, body, author = self.read_pr(
            credentials,
            input_data.pr_url,
        )
        yield ""title"", title
        yield ""body"", body
        yield ""author"", author

        if input_data.include_pr_changes:
            changes = self.read_pr_changes(
                credentials,
                input_data.pr_url,
            )
            yield ""changes"", changes","Point(row=190, column=0)","Point(row=286, column=36)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )
        include_pr_changes: bool = SchemaField(
            description=""Whether to include the changes made in the pull request"",
            default=False,
        )","Point(row=191, column=4)","Point(row=200, column=9)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
Output,class,,"class Output(BlockSchema):
        title: str = SchemaField(description=""Title of the pull request"")
        body: str = SchemaField(description=""Body of the pull request"")
        author: str = SchemaField(description=""User who created the pull request"")
        changes: str = SchemaField(description=""Changes made in the pull request"")
        error: str = SchemaField(
            description=""Error message if reading the pull request failed""
        )","Point(row=202, column=4)","Point(row=209, column=9)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubReadPullRequestBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""bf94b2a4-1a30-4600-a783-a8a44ee31301"",
            description=""This block reads the body, title, user, and changes of a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubReadPullRequestBlock.Input,
            output_schema=GithubReadPullRequestBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""include_pr_changes"": True,
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (""title"", ""Title of the pull request""),
                (""body"", ""This is the body of the pull request.""),
                (""author"", ""username""),
                (""changes"", ""List of changes made in the pull request.""),
            ],
            test_mock={
                ""read_pr"": lambda *args, **kwargs: (
                    ""Title of the pull request"",
                    ""This is the body of the pull request."",
                    ""username"",
                ),
                ""read_pr_changes"": lambda *args, **kwargs: ""List of changes made in the pull request."",
            },
        )","Point(row=211, column=4)","Point(row=238, column=9)",GithubReadPullRequestBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubReadPullRequestBlock.read_pr,function,,"def read_pr(credentials: GithubCredentials, pr_url: str) -> tuple[str, str, str]:
        api = get_api(credentials)
        # Adjust the URL to access the issue endpoint for PR metadata
        issue_url = pr_url.replace(""/pull/"", ""/issues/"")
        response = api.get(issue_url)
        data = response.json()
        title = data.get(""title"", ""No title found"")
        body = data.get(""body"", ""No body content found"")
        author = data.get(""user"", {}).get(""login"", ""No user found"")
        return title, body, author","Point(row=241, column=4)","Point(row=250, column=34)",GithubReadPullRequestBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubReadPullRequestBlock.read_pr_changes,function,,"def read_pr_changes(credentials: GithubCredentials, pr_url: str) -> str:
        api = get_api(credentials)
        files_url = pr_url + ""/files""
        response = api.get(files_url)
        files = response.json()
        changes = []
        for file in files:
            filename = file.get(""filename"")
            patch = file.get(""patch"")
            if filename and patch:
                changes.append(f""File: {filename}\n{patch}"")
        return ""\n\n"".join(changes)","Point(row=253, column=4)","Point(row=264, column=35)",GithubReadPullRequestBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubReadPullRequestBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        title, body, author = self.read_pr(
            credentials,
            input_data.pr_url,
        )
        yield ""title"", title
        yield ""body"", body
        yield ""author"", author

        if input_data.include_pr_changes:
            changes = self.read_pr_changes(
                credentials,
                input_data.pr_url,
            )
            yield ""changes"", changes","Point(row=266, column=4)","Point(row=286, column=36)",GithubReadPullRequestBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubAssignPRReviewerBlock,class,,"class GithubAssignPRReviewerBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )
        reviewer: str = SchemaField(
            description=""Username of the reviewer to assign"",
            placeholder=""Enter the reviewer's username"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the reviewer assignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the reviewer assignment failed""
        )

    def __init__(self):
        super().__init__(
            id=""c0d22c5e-e688-43e3-ba43-d5faba7927fd"",
            description=""This block assigns a reviewer to a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubAssignPRReviewerBlock.Input,
            output_schema=GithubAssignPRReviewerBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""reviewer"": ""reviewer_username"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Reviewer assigned successfully"")],
            test_mock={
                ""assign_reviewer"": lambda *args, **kwargs: ""Reviewer assigned successfully""
            },
        )

    @staticmethod
    def assign_reviewer(
        credentials: GithubCredentials, pr_url: str, reviewer: str
    ) -> str:
        api = get_api(credentials)
        reviewers_url = pr_url + ""/requested_reviewers""
        data = {""reviewers"": [reviewer]}
        api.post(reviewers_url, json=data)
        return ""Reviewer assigned successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            status = self.assign_reviewer(
                credentials,
                input_data.pr_url,
                input_data.reviewer,
            )
            yield ""status"", status
        except Exception as e:
            yield ""error"", str(e)","Point(row=289, column=0)","Point(row=353, column=33)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )
        reviewer: str = SchemaField(
            description=""Username of the reviewer to assign"",
            placeholder=""Enter the reviewer's username"",
        )","Point(row=290, column=4)","Point(row=299, column=9)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
Output,class,,"class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the reviewer assignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the reviewer assignment failed""
        )","Point(row=301, column=4)","Point(row=307, column=9)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubAssignPRReviewerBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""c0d22c5e-e688-43e3-ba43-d5faba7927fd"",
            description=""This block assigns a reviewer to a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubAssignPRReviewerBlock.Input,
            output_schema=GithubAssignPRReviewerBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""reviewer"": ""reviewer_username"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Reviewer assigned successfully"")],
            test_mock={
                ""assign_reviewer"": lambda *args, **kwargs: ""Reviewer assigned successfully""
            },
        )","Point(row=309, column=4)","Point(row=326, column=9)",GithubAssignPRReviewerBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubAssignPRReviewerBlock.assign_reviewer,function,,"def assign_reviewer(
        credentials: GithubCredentials, pr_url: str, reviewer: str
    ) -> str:
        api = get_api(credentials)
        reviewers_url = pr_url + ""/requested_reviewers""
        data = {""reviewers"": [reviewer]}
        api.post(reviewers_url, json=data)
        return ""Reviewer assigned successfully""","Point(row=329, column=4)","Point(row=336, column=47)",GithubAssignPRReviewerBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubAssignPRReviewerBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            status = self.assign_reviewer(
                credentials,
                input_data.pr_url,
                input_data.reviewer,
            )
            yield ""status"", status
        except Exception as e:
            yield ""error"", str(e)","Point(row=338, column=4)","Point(row=353, column=33)",GithubAssignPRReviewerBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubUnassignPRReviewerBlock,class,,"class GithubUnassignPRReviewerBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )
        reviewer: str = SchemaField(
            description=""Username of the reviewer to unassign"",
            placeholder=""Enter the reviewer's username"",
        )

    class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the reviewer unassignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the reviewer unassignment failed""
        )

    def __init__(self):
        super().__init__(
            id=""9637945d-c602-4875-899a-9c22f8fd30de"",
            description=""This block unassigns a reviewer from a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubUnassignPRReviewerBlock.Input,
            output_schema=GithubUnassignPRReviewerBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""reviewer"": ""reviewer_username"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Reviewer unassigned successfully"")],
            test_mock={
                ""unassign_reviewer"": lambda *args, **kwargs: ""Reviewer unassigned successfully""
            },
        )

    @staticmethod
    def unassign_reviewer(
        credentials: GithubCredentials, pr_url: str, reviewer: str
    ) -> str:
        api = get_api(credentials)
        reviewers_url = pr_url + ""/requested_reviewers""
        data = {""reviewers"": [reviewer]}
        api.delete(reviewers_url, json=data)
        return ""Reviewer unassigned successfully""

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            status = self.unassign_reviewer(
                credentials,
                input_data.pr_url,
                input_data.reviewer,
            )
            yield ""status"", status
        except Exception as e:
            yield ""error"", str(e)","Point(row=356, column=0)","Point(row=420, column=33)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )
        reviewer: str = SchemaField(
            description=""Username of the reviewer to unassign"",
            placeholder=""Enter the reviewer's username"",
        )","Point(row=357, column=4)","Point(row=366, column=9)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
Output,class,,"class Output(BlockSchema):
        status: str = SchemaField(
            description=""Status of the reviewer unassignment operation""
        )
        error: str = SchemaField(
            description=""Error message if the reviewer unassignment failed""
        )","Point(row=368, column=4)","Point(row=374, column=9)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubUnassignPRReviewerBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""9637945d-c602-4875-899a-9c22f8fd30de"",
            description=""This block unassigns a reviewer from a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubUnassignPRReviewerBlock.Input,
            output_schema=GithubUnassignPRReviewerBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""reviewer"": ""reviewer_username"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[(""status"", ""Reviewer unassigned successfully"")],
            test_mock={
                ""unassign_reviewer"": lambda *args, **kwargs: ""Reviewer unassigned successfully""
            },
        )","Point(row=376, column=4)","Point(row=393, column=9)",GithubUnassignPRReviewerBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubUnassignPRReviewerBlock.unassign_reviewer,function,,"def unassign_reviewer(
        credentials: GithubCredentials, pr_url: str, reviewer: str
    ) -> str:
        api = get_api(credentials)
        reviewers_url = pr_url + ""/requested_reviewers""
        data = {""reviewers"": [reviewer]}
        api.delete(reviewers_url, json=data)
        return ""Reviewer unassigned successfully""","Point(row=396, column=4)","Point(row=403, column=49)",GithubUnassignPRReviewerBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubUnassignPRReviewerBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        try:
            status = self.unassign_reviewer(
                credentials,
                input_data.pr_url,
                input_data.reviewer,
            )
            yield ""status"", status
        except Exception as e:
            yield ""error"", str(e)","Point(row=405, column=4)","Point(row=420, column=33)",GithubUnassignPRReviewerBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubListPRReviewersBlock,class,,"class GithubListPRReviewersBlock(Block):
    class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )

    class Output(BlockSchema):
        class ReviewerItem(TypedDict):
            username: str
            url: str

        reviewer: ReviewerItem = SchemaField(
            title=""Reviewer"",
            description=""Reviewers with their username and profile URL"",
        )
        error: str = SchemaField(
            description=""Error message if listing reviewers failed""
        )

    def __init__(self):
        super().__init__(
            id=""2646956e-96d5-4754-a3df-034017e7ed96"",
            description=""This block lists all reviewers for a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListPRReviewersBlock.Input,
            output_schema=GithubListPRReviewersBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""reviewer"",
                    {
                        ""username"": ""reviewer1"",
                        ""url"": ""https://github.com/reviewer1"",
                    },
                )
            ],
            test_mock={
                ""list_reviewers"": lambda *args, **kwargs: [
                    {
                        ""username"": ""reviewer1"",
                        ""url"": ""https://github.com/reviewer1"",
                    }
                ]
            },
        )

    @staticmethod
    def list_reviewers(
        credentials: GithubCredentials, pr_url: str
    ) -> list[Output.ReviewerItem]:
        api = get_api(credentials)
        reviewers_url = pr_url + ""/requested_reviewers""
        response = api.get(reviewers_url)
        data = response.json()
        reviewers: list[GithubListPRReviewersBlock.Output.ReviewerItem] = [
            {""username"": reviewer[""login""], ""url"": reviewer[""html_url""]}
            for reviewer in data.get(""users"", [])
        ]
        return reviewers

    def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        reviewers = self.list_reviewers(
            credentials,
            input_data.pr_url,
        )
        yield from ((""reviewer"", reviewer) for reviewer in reviewers)","Point(row=423, column=0)","Point(row=500, column=69)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
Input,class,,"class Input(BlockSchema):
        credentials: GithubCredentialsInput = GithubCredentialsField(""repo"")
        pr_url: str = SchemaField(
            description=""URL of the GitHub pull request"",
            placeholder=""https://github.com/owner/repo/pull/1"",
        )","Point(row=424, column=4)","Point(row=429, column=9)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
Output,class,,"class Output(BlockSchema):
        class ReviewerItem(TypedDict):
            username: str
            url: str

        reviewer: ReviewerItem = SchemaField(
            title=""Reviewer"",
            description=""Reviewers with their username and profile URL"",
        )
        error: str = SchemaField(
            description=""Error message if listing reviewers failed""
        )","Point(row=431, column=4)","Point(row=442, column=9)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
ReviewerItem,class,,"class ReviewerItem(TypedDict):
            username: str
            url: str","Point(row=432, column=8)","Point(row=434, column=20)",,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubListPRReviewersBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""2646956e-96d5-4754-a3df-034017e7ed96"",
            description=""This block lists all reviewers for a specified GitHub pull request."",
            categories={BlockCategory.DEVELOPER_TOOLS},
            input_schema=GithubListPRReviewersBlock.Input,
            output_schema=GithubListPRReviewersBlock.Output,
            test_input={
                ""pr_url"": ""https://github.com/owner/repo/pull/1"",
                ""credentials"": TEST_CREDENTIALS_INPUT,
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=[
                (
                    ""reviewer"",
                    {
                        ""username"": ""reviewer1"",
                        ""url"": ""https://github.com/reviewer1"",
                    },
                )
            ],
            test_mock={
                ""list_reviewers"": lambda *args, **kwargs: [
                    {
                        ""username"": ""reviewer1"",
                        ""url"": ""https://github.com/reviewer1"",
                    }
                ]
            },
        )","Point(row=444, column=4)","Point(row=473, column=9)",GithubListPRReviewersBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubListPRReviewersBlock.list_reviewers,function,,"def list_reviewers(
        credentials: GithubCredentials, pr_url: str
    ) -> list[Output.ReviewerItem]:
        api = get_api(credentials)
        reviewers_url = pr_url + ""/requested_reviewers""
        response = api.get(reviewers_url)
        data = response.json()
        reviewers: list[GithubListPRReviewersBlock.Output.ReviewerItem] = [
            {""username"": reviewer[""login""], ""url"": reviewer[""html_url""]}
            for reviewer in data.get(""users"", [])
        ]
        return reviewers","Point(row=476, column=4)","Point(row=487, column=24)",GithubListPRReviewersBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubListPRReviewersBlock.run,function,,"def run(
        self,
        input_data: Input,
        *,
        credentials: GithubCredentials,
        **kwargs,
    ) -> BlockOutput:
        reviewers = self.list_reviewers(
            credentials,
            input_data.pr_url,
        )
        yield from ((""reviewer"", reviewer) for reviewer in reviewers)","Point(row=489, column=4)","Point(row=500, column=69)",GithubListPRReviewersBlock,autogpt_platform/backend/backend/blocks/github/pull_requests.py
GithubCredentialsField,function,"
    Creates a GitHub credentials input on a block.

    Params:
        scope: The authorization scope needed for the block to work. ([list of available scopes](https://docs.github.com/en/apps/oauth-apps/building-oauth-apps/scopes-for-oauth-apps#available-scopes))
","def GithubCredentialsField(scope: str) -> GithubCredentialsInput:
    """"""
    Creates a GitHub credentials input on a block.

    Params:
        scope: The authorization scope needed for the block to work. ([list of available scopes](https://docs.github.com/en/apps/oauth-apps/building-oauth-apps/scopes-for-oauth-apps#available-scopes))
    """"""  # noqa
    return CredentialsField(
        provider=""github"",
        supported_credential_types=(
            {""api_key"", ""oauth2""} if GITHUB_OAUTH_IS_CONFIGURED else {""api_key""}
        ),
        required_scopes={scope},
        description=""The GitHub integration can be used with OAuth, ""
        ""or any API key with sufficient permissions for the blocks it is used on."",
    )","Point(row=23, column=0)","Point(row=38, column=5)",,autogpt_platform/backend/backend/blocks/github/_auth.py
JinaChunkingBlock,class,,"class JinaChunkingBlock(Block):
    class Input(BlockSchema):
        texts: list = SchemaField(description=""List of texts to chunk"")

        credentials: JinaCredentialsInput = JinaCredentialsField()
        max_chunk_length: int = SchemaField(
            description=""Maximum length of each chunk"", default=1000
        )
        return_tokens: bool = SchemaField(
            description=""Whether to return token information"", default=False
        )

    class Output(BlockSchema):
        chunks: list = SchemaField(description=""List of chunked texts"")
        tokens: list = SchemaField(
            description=""List of token information for each chunk"", optional=True
        )

    def __init__(self):
        super().__init__(
            id=""806fb15e-830f-4796-8692-557d300ff43c"",
            description=""Chunks texts using Jina AI's segmentation service"",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input_schema=JinaChunkingBlock.Input,
            output_schema=JinaChunkingBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: JinaCredentials, **kwargs
    ) -> BlockOutput:
        url = ""https://segment.jina.ai/""
        headers = {
            ""Content-Type"": ""application/json"",
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
        }

        all_chunks = []
        all_tokens = []

        for text in input_data.texts:
            data = {
                ""content"": text,
                ""return_tokens"": str(input_data.return_tokens).lower(),
                ""return_chunks"": ""true"",
                ""max_chunk_length"": str(input_data.max_chunk_length),
            }

            response = requests.post(url, headers=headers, json=data)
            result = response.json()

            all_chunks.extend(result.get(""chunks"", []))
            if input_data.return_tokens:
                all_tokens.extend(result.get(""tokens"", []))

        yield ""chunks"", all_chunks
        if input_data.return_tokens:
            yield ""tokens"", all_tokens","Point(row=10, column=0)","Point(row=66, column=38)",,autogpt_platform/backend/backend/blocks/jina/chunking.py
Input,class,,"class Input(BlockSchema):
        texts: list = SchemaField(description=""List of texts to chunk"")

        credentials: JinaCredentialsInput = JinaCredentialsField()
        max_chunk_length: int = SchemaField(
            description=""Maximum length of each chunk"", default=1000
        )
        return_tokens: bool = SchemaField(
            description=""Whether to return token information"", default=False
        )","Point(row=11, column=4)","Point(row=20, column=9)",,autogpt_platform/backend/backend/blocks/jina/chunking.py
Output,class,,"class Output(BlockSchema):
        chunks: list = SchemaField(description=""List of chunked texts"")
        tokens: list = SchemaField(
            description=""List of token information for each chunk"", optional=True
        )","Point(row=22, column=4)","Point(row=26, column=9)",,autogpt_platform/backend/backend/blocks/jina/chunking.py
JinaChunkingBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""806fb15e-830f-4796-8692-557d300ff43c"",
            description=""Chunks texts using Jina AI's segmentation service"",
            categories={BlockCategory.AI, BlockCategory.TEXT},
            input_schema=JinaChunkingBlock.Input,
            output_schema=JinaChunkingBlock.Output,
        )","Point(row=28, column=4)","Point(row=35, column=9)",JinaChunkingBlock,autogpt_platform/backend/backend/blocks/jina/chunking.py
JinaChunkingBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: JinaCredentials, **kwargs
    ) -> BlockOutput:
        url = ""https://segment.jina.ai/""
        headers = {
            ""Content-Type"": ""application/json"",
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
        }

        all_chunks = []
        all_tokens = []

        for text in input_data.texts:
            data = {
                ""content"": text,
                ""return_tokens"": str(input_data.return_tokens).lower(),
                ""return_chunks"": ""true"",
                ""max_chunk_length"": str(input_data.max_chunk_length),
            }

            response = requests.post(url, headers=headers, json=data)
            result = response.json()

            all_chunks.extend(result.get(""chunks"", []))
            if input_data.return_tokens:
                all_tokens.extend(result.get(""tokens"", []))

        yield ""chunks"", all_chunks
        if input_data.return_tokens:
            yield ""tokens"", all_tokens","Point(row=37, column=4)","Point(row=66, column=38)",JinaChunkingBlock,autogpt_platform/backend/backend/blocks/jina/chunking.py
JinaEmbeddingBlock,class,,"class JinaEmbeddingBlock(Block):
    class Input(BlockSchema):
        texts: list = SchemaField(description=""List of texts to embed"")
        credentials: JinaCredentialsInput = JinaCredentialsField()
        model: str = SchemaField(
            description=""Jina embedding model to use"",
            default=""jina-embeddings-v2-base-en"",
        )

    class Output(BlockSchema):
        embeddings: list = SchemaField(description=""List of embeddings"")

    def __init__(self):
        super().__init__(
            id=""7c56b3ab-62e7-43a2-a2dc-4ec4245660b6"",
            description=""Generates embeddings using Jina AI"",
            categories={BlockCategory.AI},
            input_schema=JinaEmbeddingBlock.Input,
            output_schema=JinaEmbeddingBlock.Output,
        )

    def run(
        self, input_data: Input, *, credentials: JinaCredentials, **kwargs
    ) -> BlockOutput:
        url = ""https://api.jina.ai/v1/embeddings""
        headers = {
            ""Content-Type"": ""application/json"",
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
        }
        data = {""input"": input_data.texts, ""model"": input_data.model}
        response = requests.post(url, headers=headers, json=data)
        embeddings = [e[""embedding""] for e in response.json()[""data""]]
        yield ""embeddings"", embeddings","Point(row=10, column=0)","Point(row=42, column=38)",,autogpt_platform/backend/backend/blocks/jina/embeddings.py
Input,class,,"class Input(BlockSchema):
        texts: list = SchemaField(description=""List of texts to embed"")
        credentials: JinaCredentialsInput = JinaCredentialsField()
        model: str = SchemaField(
            description=""Jina embedding model to use"",
            default=""jina-embeddings-v2-base-en"",
        )","Point(row=11, column=4)","Point(row=17, column=9)",,autogpt_platform/backend/backend/blocks/jina/embeddings.py
Output,class,,"class Output(BlockSchema):
        embeddings: list = SchemaField(description=""List of embeddings"")","Point(row=19, column=4)","Point(row=20, column=72)",,autogpt_platform/backend/backend/blocks/jina/embeddings.py
JinaEmbeddingBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""7c56b3ab-62e7-43a2-a2dc-4ec4245660b6"",
            description=""Generates embeddings using Jina AI"",
            categories={BlockCategory.AI},
            input_schema=JinaEmbeddingBlock.Input,
            output_schema=JinaEmbeddingBlock.Output,
        )","Point(row=22, column=4)","Point(row=29, column=9)",JinaEmbeddingBlock,autogpt_platform/backend/backend/blocks/jina/embeddings.py
JinaEmbeddingBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: JinaCredentials, **kwargs
    ) -> BlockOutput:
        url = ""https://api.jina.ai/v1/embeddings""
        headers = {
            ""Content-Type"": ""application/json"",
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
        }
        data = {""input"": input_data.texts, ""model"": input_data.model}
        response = requests.post(url, headers=headers, json=data)
        embeddings = [e[""embedding""] for e in response.json()[""data""]]
        yield ""embeddings"", embeddings","Point(row=31, column=4)","Point(row=42, column=38)",JinaEmbeddingBlock,autogpt_platform/backend/backend/blocks/jina/embeddings.py
SearchTheWebBlock,class,,"class SearchTheWebBlock(Block, GetRequest):
    class Input(BlockSchema):
        credentials: JinaCredentialsInput = JinaCredentialsField()
        query: str = SchemaField(description=""The search query to search the web for"")

    class Output(BlockSchema):
        results: str = SchemaField(
            description=""The search results including content from top 5 URLs""
        )
        error: str = SchemaField(description=""Error message if the search fails"")

    def __init__(self):
        super().__init__(
            id=""87840993-2053-44b7-8da4-187ad4ee518c"",
            description=""This block searches the internet for the given search query."",
            categories={BlockCategory.SEARCH},
            input_schema=SearchTheWebBlock.Input,
            output_schema=SearchTheWebBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""query"": ""Artificial Intelligence"",
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(""results"", ""search content""),
            test_mock={""get_request"": lambda *args, **kwargs: ""search content""},
        )

    def run(
        self, input_data: Input, *, credentials: JinaCredentials, **kwargs
    ) -> BlockOutput:
        # Encode the search query
        encoded_query = quote(input_data.query)
        headers = {
            ""Content-Type"": ""application/json"",
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
        }

        # Prepend the Jina Search URL to the encoded query
        jina_search_url = f""https://s.jina.ai/{encoded_query}""
        results = self.get_request(jina_search_url, headers=headers, json=False)

        # Output the search results
        yield ""results"", results","Point(row=14, column=0)","Point(row=56, column=32)",,autogpt_platform/backend/backend/blocks/jina/search.py
Input,class,,"class Input(BlockSchema):
        credentials: JinaCredentialsInput = JinaCredentialsField()
        query: str = SchemaField(description=""The search query to search the web for"")","Point(row=15, column=4)","Point(row=17, column=86)",,autogpt_platform/backend/backend/blocks/jina/search.py
Output,class,,"class Output(BlockSchema):
        results: str = SchemaField(
            description=""The search results including content from top 5 URLs""
        )
        error: str = SchemaField(description=""Error message if the search fails"")","Point(row=19, column=4)","Point(row=23, column=81)",,autogpt_platform/backend/backend/blocks/jina/search.py
SearchTheWebBlock.__init__,function,,"def __init__(self):
        super().__init__(
            id=""87840993-2053-44b7-8da4-187ad4ee518c"",
            description=""This block searches the internet for the given search query."",
            categories={BlockCategory.SEARCH},
            input_schema=SearchTheWebBlock.Input,
            output_schema=SearchTheWebBlock.Output,
            test_input={
                ""credentials"": TEST_CREDENTIALS_INPUT,
                ""query"": ""Artificial Intelligence"",
            },
            test_credentials=TEST_CREDENTIALS,
            test_output=(""results"", ""search content""),
            test_mock={""get_request"": lambda *args, **kwargs: ""search content""},
        )","Point(row=25, column=4)","Point(row=39, column=9)",SearchTheWebBlock,autogpt_platform/backend/backend/blocks/jina/search.py
SearchTheWebBlock.run,function,,"def run(
        self, input_data: Input, *, credentials: JinaCredentials, **kwargs
    ) -> BlockOutput:
        # Encode the search query
        encoded_query = quote(input_data.query)
        headers = {
            ""Content-Type"": ""application/json"",
            ""Authorization"": f""Bearer {credentials.api_key.get_secret_value()}"",
        }

        # Prepend the Jina Search URL to the encoded query
        jina_search_url = f""https://s.jina.ai/{encoded_query}""
        results = self.get_request(jina_search_url, headers=headers, json=False)

        # Output the search results
        yield ""results"", results","Point(row=41, column=4)","Point(row=56, column=32)",SearchTheWebBlock,autogpt_platform/backend/backend/blocks/jina/search.py
JinaCredentialsField,function,"
    Creates a Jina credentials input on a block.

","def JinaCredentialsField() -> JinaCredentialsInput:
    """"""
    Creates a Jina credentials input on a block.

    """"""
    return CredentialsField(
        provider=""jina"",
        supported_credential_types={""api_key""},
        description=""The Jina integration can be used with an API Key."",
    )","Point(row=28, column=0)","Point(row=37, column=5)",,autogpt_platform/backend/backend/blocks/jina/_auth.py
GetRequest,class,,"class GetRequest:
    @classmethod
    def get_request(
        cls, url: str, headers: Optional[dict] = None, json: bool = False
    ) -> Any:
        if headers is None:
            headers = {}
        response = requests.get(url, headers=headers)
        return response.json() if json else response.text","Point(row=5, column=0)","Point(row=13, column=57)",,autogpt_platform/backend/backend/blocks/helpers/http.py
GetRequest.get_request,function,,"def get_request(
        cls, url: str, headers: Optional[dict] = None, json: bool = False
    ) -> Any:
        if headers is None:
            headers = {}
        response = requests.get(url, headers=headers)
        return response.json() if json else response.text","Point(row=7, column=4)","Point(row=13, column=57)",GetRequest,autogpt_platform/backend/backend/blocks/helpers/http.py
configure_logging,function,,"def configure_logging():
    import logging

    import autogpt_libs.logging.config

    if (
        settings.config.behave_as == BehaveAs.LOCAL
        or settings.config.app_env == AppEnvironment.LOCAL
    ):
        autogpt_libs.logging.config.configure_logging(force_cloud_logging=False)
    else:
        autogpt_libs.logging.config.configure_logging(force_cloud_logging=True)

    # Silence httpx logger
    logging.getLogger(""httpx"").setLevel(logging.WARNING)","Point(row=5, column=0)","Point(row=19, column=56)",,autogpt_platform/backend/backend/util/logging.py
sentry_init,function,,"def sentry_init():
    sentry_dsn = Settings().secrets.sentry_dsn
    sentry_sdk.init(dsn=sentry_dsn, traces_sample_rate=1.0, profiles_sample_rate=1.0)","Point(row=5, column=0)","Point(row=7, column=85)",,autogpt_platform/backend/backend/util/metrics.py
expose,function,"
    Decorator to mark a method or class to be exposed for remote calls.

    ## ‚ö†Ô∏è Gotcha
    Aside from ""simple"" types, only Pydantic models are passed unscathed *if annotated*.
    Any other passed or returned class objects are converted to dictionaries by Pyro.
","def expose(func: C) -> C:
    """"""
    Decorator to mark a method or class to be exposed for remote calls.

    ## ‚ö†Ô∏è Gotcha
    Aside from ""simple"" types, only Pydantic models are passed unscathed *if annotated*.
    Any other passed or returned class objects are converted to dictionaries by Pyro.
    """"""

    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            msg = f""Error in {func.__name__}: {e.__str__()}""
            logger.exception(msg)
            raise

    # Register custom serializers and deserializers for annotated Pydantic models
    for name, annotation in func.__annotations__.items():
        try:
            pydantic_types = _pydantic_models_from_type_annotation(annotation)
        except Exception as e:
            raise TypeError(f""Error while exposing {func.__name__}: {e.__str__()}"")

        for model in pydantic_types:
            logger.debug(
                f""Registering Pyro (de)serializers for {func.__name__} annotation ""
                f""'{name}': {model.__qualname__}""
            )
            pyro.register_class_to_dict(model, _make_custom_serializer(model))
            pyro.register_dict_to_class(
                model.__qualname__, _make_custom_deserializer(model)
            )

    return pyro.expose(wrapper)  # type: ignore","Point(row=49, column=0)","Point(row=83, column=47)",,autogpt_platform/backend/backend/util/service.py
expose.wrapper,function,,"def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            msg = f""Error in {func.__name__}: {e.__str__()}""
            logger.exception(msg)
            raise","Point(row=58, column=4)","Point(row=64, column=17)",,autogpt_platform/backend/backend/util/service.py
_make_custom_serializer,function,,"def _make_custom_serializer(model: Type[BaseModel]):
    def custom_class_to_dict(obj):
        data = {
            ""__class__"": obj.__class__.__qualname__,
            **obj.model_dump(),
        }
        logger.debug(f""Serializing {obj.__class__.__qualname__} with data: {data}"")
        return data

    return custom_class_to_dict","Point(row=86, column=0)","Point(row=95, column=31)",,autogpt_platform/backend/backend/util/service.py
_make_custom_serializer.custom_class_to_dict,function,,"def custom_class_to_dict(obj):
        data = {
            ""__class__"": obj.__class__.__qualname__,
            **obj.model_dump(),
        }
        logger.debug(f""Serializing {obj.__class__.__qualname__} with data: {data}"")
        return data","Point(row=87, column=4)","Point(row=93, column=19)",,autogpt_platform/backend/backend/util/service.py
_make_custom_deserializer,function,,"def _make_custom_deserializer(model: Type[BaseModel]):
    def custom_dict_to_class(qualname, data: dict):
        logger.debug(f""Deserializing {model.__qualname__} from data: {data}"")
        return model(**data)

    return custom_dict_to_class","Point(row=98, column=0)","Point(row=103, column=31)",,autogpt_platform/backend/backend/util/service.py
_make_custom_deserializer.custom_dict_to_class,function,,"def custom_dict_to_class(qualname, data: dict):
        logger.debug(f""Deserializing {model.__qualname__} from data: {data}"")
        return model(**data)","Point(row=99, column=4)","Point(row=101, column=28)",,autogpt_platform/backend/backend/util/service.py
AppService,class,,"class AppService(AppProcess, ABC):
    shared_event_loop: asyncio.AbstractEventLoop
    use_db: bool = False
    use_redis: bool = False
    use_supabase: bool = False

    def __init__(self):
        self.uri = None

    @classmethod
    @abstractmethod
    def get_port(cls) -> int:
        pass

    @classmethod
    def get_host(cls) -> str:
        return os.environ.get(f""{cls.service_name.upper()}_HOST"", Config().pyro_host)

    def run_service(self) -> None:
        while True:
            time.sleep(10)

    def __run_async(self, coro: Coroutine[Any, Any, T]):
        return asyncio.run_coroutine_threadsafe(coro, self.shared_event_loop)

    def run_and_wait(self, coro: Coroutine[Any, Any, T]) -> T:
        future = self.__run_async(coro)
        return future.result()

    def run(self):
        self.shared_event_loop = asyncio.get_event_loop()
        if self.use_db:
            self.shared_event_loop.run_until_complete(db.connect())
        if self.use_redis:
            redis.connect()
        if self.use_supabase:
            from supabase import create_client

            secrets = Secrets()
            self.supabase = create_client(
                secrets.supabase_url, secrets.supabase_service_role_key
            )

        # Initialize the async loop.
        async_thread = threading.Thread(target=self.__start_async_loop)
        async_thread.daemon = True
        async_thread.start()

        # Initialize pyro service
        daemon_thread = threading.Thread(target=self.__start_pyro)
        daemon_thread.daemon = True
        daemon_thread.start()

        # Run the main service (if it's not implemented, just sleep).
        self.run_service()

    def cleanup(self):
        if self.use_db:
            logger.info(f""[{self.__class__.__name__}] ‚è≥ Disconnecting DB..."")
            self.run_and_wait(db.disconnect())
        if self.use_redis:
            logger.info(f""[{self.__class__.__name__}] ‚è≥ Disconnecting Redis..."")
            redis.disconnect()

    @conn_retry(""Pyro"", ""Starting Pyro Service"")
    def __start_pyro(self):
        conf = Config()
        maximum_connection_thread_count = max(
            Pyro5.config.THREADPOOL_SIZE,
            conf.num_node_workers * conf.num_graph_workers,
        )

        Pyro5.config.THREADPOOL_SIZE = maximum_connection_thread_count  # type: ignore
        daemon = Pyro5.api.Daemon(host=conf.pyro_host, port=self.get_port())
        self.uri = daemon.register(self, objectId=self.service_name)
        logger.info(f""[{self.service_name}] Connected to Pyro; URI = {self.uri}"")
        daemon.requestLoop()

    def __start_async_loop(self):
        self.shared_event_loop.run_forever()","Point(row=106, column=0)","Point(row=185, column=44)",,autogpt_platform/backend/backend/util/service.py
AppService.__init__,function,,"def __init__(self):
        self.uri = None","Point(row=112, column=4)","Point(row=113, column=23)",AppService,autogpt_platform/backend/backend/util/service.py
AppService.get_port,function,,"def get_port(cls) -> int:
        pass","Point(row=117, column=4)","Point(row=118, column=12)",AppService,autogpt_platform/backend/backend/util/service.py
AppService.get_host,function,,"def get_host(cls) -> str:
        return os.environ.get(f""{cls.service_name.upper()}_HOST"", Config().pyro_host)","Point(row=121, column=4)","Point(row=122, column=85)",AppService,autogpt_platform/backend/backend/util/service.py
AppService.run_service,function,,"def run_service(self) -> None:
        while True:
            time.sleep(10)","Point(row=124, column=4)","Point(row=126, column=26)",AppService,autogpt_platform/backend/backend/util/service.py
AppService.__run_async,function,,"def __run_async(self, coro: Coroutine[Any, Any, T]):
        return asyncio.run_coroutine_threadsafe(coro, self.shared_event_loop)","Point(row=128, column=4)","Point(row=129, column=77)",AppService,autogpt_platform/backend/backend/util/service.py
AppService.run_and_wait,function,,"def run_and_wait(self, coro: Coroutine[Any, Any, T]) -> T:
        future = self.__run_async(coro)
        return future.result()","Point(row=131, column=4)","Point(row=133, column=30)",AppService,autogpt_platform/backend/backend/util/service.py
AppService.run,function,,"def run(self):
        self.shared_event_loop = asyncio.get_event_loop()
        if self.use_db:
            self.shared_event_loop.run_until_complete(db.connect())
        if self.use_redis:
            redis.connect()
        if self.use_supabase:
            from supabase import create_client

            secrets = Secrets()
            self.supabase = create_client(
                secrets.supabase_url, secrets.supabase_service_role_key
            )

        # Initialize the async loop.
        async_thread = threading.Thread(target=self.__start_async_loop)
        async_thread.daemon = True
        async_thread.start()

        # Initialize pyro service
        daemon_thread = threading.Thread(target=self.__start_pyro)
        daemon_thread.daemon = True
        daemon_thread.start()

        # Run the main service (if it's not implemented, just sleep).
        self.run_service()","Point(row=135, column=4)","Point(row=160, column=26)",AppService,autogpt_platform/backend/backend/util/service.py
AppService.cleanup,function,,"def cleanup(self):
        if self.use_db:
            logger.info(f""[{self.__class__.__name__}] ‚è≥ Disconnecting DB..."")
            self.run_and_wait(db.disconnect())
        if self.use_redis:
            logger.info(f""[{self.__class__.__name__}] ‚è≥ Disconnecting Redis..."")
            redis.disconnect()","Point(row=162, column=4)","Point(row=168, column=30)",AppService,autogpt_platform/backend/backend/util/service.py
AppService.__start_pyro,function,,"def __start_pyro(self):
        conf = Config()
        maximum_connection_thread_count = max(
            Pyro5.config.THREADPOOL_SIZE,
            conf.num_node_workers * conf.num_graph_workers,
        )

        Pyro5.config.THREADPOOL_SIZE = maximum_connection_thread_count  # type: ignore
        daemon = Pyro5.api.Daemon(host=conf.pyro_host, port=self.get_port())
        self.uri = daemon.register(self, objectId=self.service_name)
        logger.info(f""[{self.service_name}] Connected to Pyro; URI = {self.uri}"")
        daemon.requestLoop()","Point(row=171, column=4)","Point(row=182, column=28)",AppService,autogpt_platform/backend/backend/util/service.py
AppService.__start_async_loop,function,,"def __start_async_loop(self):
        self.shared_event_loop.run_forever()","Point(row=184, column=4)","Point(row=185, column=44)",AppService,autogpt_platform/backend/backend/util/service.py
PyroClient,class,,"class PyroClient:
    proxy: Pyro5.api.Proxy","Point(row=194, column=0)","Point(row=195, column=26)",,autogpt_platform/backend/backend/util/service.py
close_service_client,function,,"def close_service_client(client: AppService) -> None:
    if isinstance(client, PyroClient):
        client.proxy._pyroRelease()
    else:
        raise RuntimeError(f""Client {client.__class__} is not a Pyro client."")","Point(row=198, column=0)","Point(row=202, column=78)",,autogpt_platform/backend/backend/util/service.py
get_service_client,function,,"def get_service_client(service_type: Type[AS]) -> AS:
    service_name = service_type.service_name

    class DynamicClient(PyroClient):
        @conn_retry(""Pyro"", f""Connecting to [{service_name}]"")
        def __init__(self):
            host = os.environ.get(f""{service_name.upper()}_HOST"", ""localhost"")
            uri = f""PYRO:{service_type.service_name}@{host}:{service_type.get_port()}""
            logger.debug(f""Connecting to service [{service_name}]. URI = {uri}"")
            self.proxy = Pyro5.api.Proxy(uri)
            # Attempt to bind to ensure the connection is established
            self.proxy._pyroBind()
            logger.debug(f""Successfully connected to service [{service_name}]"")

        def __getattr__(self, name: str) -> Callable[..., Any]:
            res = getattr(self.proxy, name)
            return res

    return cast(AS, DynamicClient())","Point(row=205, column=0)","Point(row=223, column=36)",,autogpt_platform/backend/backend/util/service.py
DynamicClient,class,,"class DynamicClient(PyroClient):
        @conn_retry(""Pyro"", f""Connecting to [{service_name}]"")
        def __init__(self):
            host = os.environ.get(f""{service_name.upper()}_HOST"", ""localhost"")
            uri = f""PYRO:{service_type.service_name}@{host}:{service_type.get_port()}""
            logger.debug(f""Connecting to service [{service_name}]. URI = {uri}"")
            self.proxy = Pyro5.api.Proxy(uri)
            # Attempt to bind to ensure the connection is established
            self.proxy._pyroBind()
            logger.debug(f""Successfully connected to service [{service_name}]"")

        def __getattr__(self, name: str) -> Callable[..., Any]:
            res = getattr(self.proxy, name)
            return res","Point(row=208, column=4)","Point(row=221, column=22)",,autogpt_platform/backend/backend/util/service.py
get_service_client.__init__,function,,"def __init__(self):
            host = os.environ.get(f""{service_name.upper()}_HOST"", ""localhost"")
            uri = f""PYRO:{service_type.service_name}@{host}:{service_type.get_port()}""
            logger.debug(f""Connecting to service [{service_name}]. URI = {uri}"")
            self.proxy = Pyro5.api.Proxy(uri)
            # Attempt to bind to ensure the connection is established
            self.proxy._pyroBind()
            logger.debug(f""Successfully connected to service [{service_name}]"")","Point(row=210, column=8)","Point(row=217, column=79)",DynamicClient,autogpt_platform/backend/backend/util/service.py
get_service_client.__getattr__,function,,"def __getattr__(self, name: str) -> Callable[..., Any]:
            res = getattr(self.proxy, name)
            return res","Point(row=219, column=8)","Point(row=221, column=22)",DynamicClient,autogpt_platform/backend/backend/util/service.py
_pydantic_models_from_type_annotation,function,,"def _pydantic_models_from_type_annotation(annotation) -> Iterator[type[BaseModel]]:
    # Peel Annotated parameters
    if (origin := get_origin(annotation)) and origin is Annotated:
        annotation = get_args(annotation)[0]

    origin = get_origin(annotation)
    args = get_args(annotation)

    if origin in (
        Union,
        UnionType,
        list,
        List,
        tuple,
        Tuple,
        set,
        Set,
        frozenset,
        FrozenSet,
    ):
        for arg in args:
            yield from _pydantic_models_from_type_annotation(arg)
    elif origin in (dict, Dict):
        key_type, value_type = args
        yield from _pydantic_models_from_type_annotation(key_type)
        yield from _pydantic_models_from_type_annotation(value_type)
    else:
        annotype = annotation if origin is None else origin

        # Exclude generic types and aliases
        if (
            annotype is not None
            and not hasattr(typing, getattr(annotype, ""__name__"", """"))
            and isinstance(annotype, type)
        ):
            if issubclass(annotype, BaseModel):
                yield annotype
            elif annotype not in builtin_types and not issubclass(annotype, Enum):
                raise TypeError(f""Unsupported type encountered: {annotype}"")","Point(row=229, column=0)","Point(row=267, column=76)",,autogpt_platform/backend/backend/util/service.py
JSONCryptor,class,,"class JSONCryptor:
    def __init__(self, key: Optional[str] = None):
        # Use provided key or get from environment
        self.key = key or ENCRYPTION_KEY
        if not self.key:
            raise ValueError(
                ""Encryption key must be provided or set in ENCRYPTION_KEY environment variable""
            )
        self.fernet = Fernet(
            self.key.encode() if isinstance(self.key, str) else self.key
        )

    def encrypt(self, data: dict) -> str:
        """"""Encrypt dictionary data to string""""""
        json_str = json.dumps(data)
        encrypted = self.fernet.encrypt(json_str.encode())
        return encrypted.decode()

    def decrypt(self, encrypted_str: str) -> dict:
        """"""Decrypt string to dictionary""""""
        if not encrypted_str:
            return {}
        decrypted = self.fernet.decrypt(encrypted_str.encode())
        return json.loads(decrypted.decode())","Point(row=10, column=0)","Point(row=33, column=45)",,autogpt_platform/backend/backend/util/encryption.py
JSONCryptor.__init__,function,,"def __init__(self, key: Optional[str] = None):
        # Use provided key or get from environment
        self.key = key or ENCRYPTION_KEY
        if not self.key:
            raise ValueError(
                ""Encryption key must be provided or set in ENCRYPTION_KEY environment variable""
            )
        self.fernet = Fernet(
            self.key.encode() if isinstance(self.key, str) else self.key
        )","Point(row=11, column=4)","Point(row=20, column=9)",JSONCryptor,autogpt_platform/backend/backend/util/encryption.py
JSONCryptor.encrypt,function,Encrypt dictionary data to string,"def encrypt(self, data: dict) -> str:
        """"""Encrypt dictionary data to string""""""
        json_str = json.dumps(data)
        encrypted = self.fernet.encrypt(json_str.encode())
        return encrypted.decode()","Point(row=22, column=4)","Point(row=26, column=33)",JSONCryptor,autogpt_platform/backend/backend/util/encryption.py
JSONCryptor.decrypt,function,Decrypt string to dictionary,"def decrypt(self, encrypted_str: str) -> dict:
        """"""Decrypt string to dictionary""""""
        if not encrypted_str:
            return {}
        decrypted = self.fernet.decrypt(encrypted_str.encode())
        return json.loads(decrypted.decode())","Point(row=28, column=4)","Point(row=33, column=45)",JSONCryptor,autogpt_platform/backend/backend/util/encryption.py
TimingInfo,class,,"class TimingInfo(BaseModel):
    cpu_time: float
    wall_time: float","Point(row=9, column=0)","Point(row=11, column=20)",,autogpt_platform/backend/backend/util/decorator.py
_start_measurement,function,,"def _start_measurement() -> Tuple[float, float]:
    return time.time(), os.times()[0] + os.times()[1]","Point(row=14, column=0)","Point(row=15, column=53)",,autogpt_platform/backend/backend/util/decorator.py
_end_measurement,function,,"def _end_measurement(
    start_wall_time: float, start_cpu_time: float
) -> Tuple[float, float]:
    end_wall_time = time.time()
    end_cpu_time = os.times()[0] + os.times()[1]
    return end_wall_time - start_wall_time, end_cpu_time - start_cpu_time","Point(row=18, column=0)","Point(row=23, column=73)",,autogpt_platform/backend/backend/util/decorator.py
time_measured,function,"
    Decorator to measure the time taken by a function to execute.
","def time_measured(func: Callable[P, T]) -> Callable[P, Tuple[TimingInfo, T]]:
    """"""
    Decorator to measure the time taken by a function to execute.
    """"""

    @functools.wraps(func)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> Tuple[TimingInfo, T]:
        start_wall_time, start_cpu_time = _start_measurement()
        try:
            result = func(*args, **kwargs)
        finally:
            wall_duration, cpu_duration = _end_measurement(
                start_wall_time, start_cpu_time
            )
            timing_info = TimingInfo(cpu_time=cpu_duration, wall_time=wall_duration)
        return timing_info, result

    return wrapper","Point(row=32, column=0)","Point(row=49, column=18)",,autogpt_platform/backend/backend/util/decorator.py
time_measured.wrapper,function,,"def wrapper(*args: P.args, **kwargs: P.kwargs) -> Tuple[TimingInfo, T]:
        start_wall_time, start_cpu_time = _start_measurement()
        try:
            result = func(*args, **kwargs)
        finally:
            wall_duration, cpu_duration = _end_measurement(
                start_wall_time, start_cpu_time
            )
            timing_info = TimingInfo(cpu_time=cpu_duration, wall_time=wall_duration)
        return timing_info, result","Point(row=38, column=4)","Point(row=47, column=34)",,autogpt_platform/backend/backend/util/decorator.py
error_logged,function,"
    Decorator to suppress and log any exceptions raised by a function.
","def error_logged(func: Callable[P, T]) -> Callable[P, T | None]:
    """"""
    Decorator to suppress and log any exceptions raised by a function.
    """"""

    @functools.wraps(func)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> T | None:
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.exception(
                f""Error when calling function {func.__name__} with arguments {args} {kwargs}: {e}""
            )

    return wrapper","Point(row=52, column=0)","Point(row=66, column=18)",,autogpt_platform/backend/backend/util/decorator.py
error_logged.wrapper,function,,"def wrapper(*args: P.args, **kwargs: P.kwargs) -> T | None:
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.exception(
                f""Error when calling function {func.__name__} with arguments {args} {kwargs}: {e}""
            )","Point(row=58, column=4)","Point(row=64, column=13)",,autogpt_platform/backend/backend/util/decorator.py
is_ip_blocked,function,"
    Checks if the IP address is in a blocked network.
","def is_ip_blocked(ip: str) -> bool:
    """"""
    Checks if the IP address is in a blocked network.
    """"""
    ip_addr = ipaddress.ip_address(ip)
    return any(ip_addr in network for network in BLOCKED_IP_NETWORKS)","Point(row=22, column=0)","Point(row=27, column=69)",,autogpt_platform/backend/backend/util/request.py
validate_url,function,"
    Validates the URL to prevent SSRF attacks by ensuring it does not point to a private
    or untrusted IP address, unless whitelisted.
","def validate_url(url: str, trusted_origins: list[str]) -> str:
    """"""
    Validates the URL to prevent SSRF attacks by ensuring it does not point to a private
    or untrusted IP address, unless whitelisted.
    """"""
    url = url.strip().strip(""/"")
    if not url.startswith((""http://"", ""https://"")):
        url = ""http://"" + url

    parsed_url = urlparse(url)
    hostname = parsed_url.hostname

    if not hostname:
        raise ValueError(f""Invalid URL: Unable to determine hostname from {url}"")

    if any(hostname == origin for origin in trusted_origins):
        return url

    # Resolve all IP addresses for the hostname
    ip_addresses = {result[4][0] for result in socket.getaddrinfo(hostname, None)}
    if not ip_addresses:
        raise ValueError(f""Unable to resolve IP address for {hostname}"")

    # Check if all IP addresses are global
    for ip in ip_addresses:
        if is_ip_blocked(ip):
            raise ValueError(
                f""Access to private IP address at {hostname}: {ip} is not allowed.""
            )

    return url","Point(row=30, column=0)","Point(row=60, column=14)",,autogpt_platform/backend/backend/util/request.py
Requests,class,"
    A wrapper around the requests library that validates URLs before making requests.
","class Requests:
    """"""
    A wrapper around the requests library that validates URLs before making requests.
    """"""

    def __init__(
        self,
        trusted_origins: list[str] | None = None,
        raise_for_status: bool = True,
        extra_url_validator: Callable[[str], str] | None = None,
        extra_headers: dict[str, str] | None = None,
    ):
        self.trusted_origins = []
        for url in trusted_origins or []:
            hostname = urlparse(url).hostname
            if not hostname:
                raise ValueError(f""Invalid URL: Unable to determine hostname of {url}"")
            self.trusted_origins.append(hostname)

        self.raise_for_status = raise_for_status
        self.extra_url_validator = extra_url_validator
        self.extra_headers = extra_headers

    def request(
        self, method, url, headers=None, allow_redirects=False, *args, **kwargs
    ) -> req.Response:
        if self.extra_headers is not None:
            headers = {**(headers or {}), **self.extra_headers}

        url = validate_url(url, self.trusted_origins)
        if self.extra_url_validator is not None:
            url = self.extra_url_validator(url)

        response = req.request(
            method,
            url,
            headers=headers,
            allow_redirects=allow_redirects,
            *args,
            **kwargs,
        )
        if self.raise_for_status:
            response.raise_for_status()

        return response

    def get(self, url, *args, **kwargs) -> req.Response:
        return self.request(""GET"", url, *args, **kwargs)

    def post(self, url, *args, **kwargs) -> req.Response:
        return self.request(""POST"", url, *args, **kwargs)

    def put(self, url, *args, **kwargs) -> req.Response:
        return self.request(""PUT"", url, *args, **kwargs)

    def delete(self, url, *args, **kwargs) -> req.Response:
        return self.request(""DELETE"", url, *args, **kwargs)

    def head(self, url, *args, **kwargs) -> req.Response:
        return self.request(""HEAD"", url, *args, **kwargs)

    def options(self, url, *args, **kwargs) -> req.Response:
        return self.request(""OPTIONS"", url, *args, **kwargs)

    def patch(self, url, *args, **kwargs) -> req.Response:
        return self.request(""PATCH"", url, *args, **kwargs)","Point(row=63, column=0)","Point(row=128, column=58)",,autogpt_platform/backend/backend/util/request.py
Requests.__init__,function,,"def __init__(
        self,
        trusted_origins: list[str] | None = None,
        raise_for_status: bool = True,
        extra_url_validator: Callable[[str], str] | None = None,
        extra_headers: dict[str, str] | None = None,
    ):
        self.trusted_origins = []
        for url in trusted_origins or []:
            hostname = urlparse(url).hostname
            if not hostname:
                raise ValueError(f""Invalid URL: Unable to determine hostname of {url}"")
            self.trusted_origins.append(hostname)

        self.raise_for_status = raise_for_status
        self.extra_url_validator = extra_url_validator
        self.extra_headers = extra_headers","Point(row=68, column=4)","Point(row=84, column=42)",Requests,autogpt_platform/backend/backend/util/request.py
Requests.request,function,,"def request(
        self, method, url, headers=None, allow_redirects=False, *args, **kwargs
    ) -> req.Response:
        if self.extra_headers is not None:
            headers = {**(headers or {}), **self.extra_headers}

        url = validate_url(url, self.trusted_origins)
        if self.extra_url_validator is not None:
            url = self.extra_url_validator(url)

        response = req.request(
            method,
            url,
            headers=headers,
            allow_redirects=allow_redirects,
            *args,
            **kwargs,
        )
        if self.raise_for_status:
            response.raise_for_status()

        return response","Point(row=86, column=4)","Point(row=107, column=23)",Requests,autogpt_platform/backend/backend/util/request.py
Requests.get,function,,"def get(self, url, *args, **kwargs) -> req.Response:
        return self.request(""GET"", url, *args, **kwargs)","Point(row=109, column=4)","Point(row=110, column=56)",Requests,autogpt_platform/backend/backend/util/request.py
Requests.post,function,,"def post(self, url, *args, **kwargs) -> req.Response:
        return self.request(""POST"", url, *args, **kwargs)","Point(row=112, column=4)","Point(row=113, column=57)",Requests,autogpt_platform/backend/backend/util/request.py
Requests.put,function,,"def put(self, url, *args, **kwargs) -> req.Response:
        return self.request(""PUT"", url, *args, **kwargs)","Point(row=115, column=4)","Point(row=116, column=56)",Requests,autogpt_platform/backend/backend/util/request.py
Requests.delete,function,,"def delete(self, url, *args, **kwargs) -> req.Response:
        return self.request(""DELETE"", url, *args, **kwargs)","Point(row=118, column=4)","Point(row=119, column=59)",Requests,autogpt_platform/backend/backend/util/request.py
Requests.head,function,,"def head(self, url, *args, **kwargs) -> req.Response:
        return self.request(""HEAD"", url, *args, **kwargs)","Point(row=121, column=4)","Point(row=122, column=57)",Requests,autogpt_platform/backend/backend/util/request.py
Requests.options,function,,"def options(self, url, *args, **kwargs) -> req.Response:
        return self.request(""OPTIONS"", url, *args, **kwargs)","Point(row=124, column=4)","Point(row=125, column=60)",Requests,autogpt_platform/backend/backend/util/request.py
Requests.patch,function,,"def patch(self, url, *args, **kwargs) -> req.Response:
        return self.request(""PATCH"", url, *args, **kwargs)","Point(row=127, column=4)","Point(row=128, column=58)",Requests,autogpt_platform/backend/backend/util/request.py
ConversionError,class,,"class ConversionError(Exception):
    pass","Point(row=4, column=0)","Point(row=5, column=8)",,autogpt_platform/backend/backend/util/type.py
__convert_list,function,,"def __convert_list(value: Any) -> list:
    if isinstance(value, (list, tuple, set)):
        return list(value)
    elif isinstance(value, dict):
        return list(value.items())
    elif isinstance(value, str):
        value = value.strip()
        if value.startswith(""["") and value.endswith(""]""):
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                return [value]
        else:
            return [value]
    else:
        return [value]","Point(row=8, column=0)","Point(row=23, column=22)",,autogpt_platform/backend/backend/util/type.py
__convert_dict,function,,"def __convert_dict(value: Any) -> dict:
    if isinstance(value, str):
        try:
            result = json.loads(value)
            if isinstance(result, dict):
                return result
            else:
                return {""value"": result}
        except json.JSONDecodeError:
            return {""value"": value}  # Fallback conversion
    elif isinstance(value, list):
        return {i: value[i] for i in range(len(value))}
    elif isinstance(value, tuple):
        return {i: value[i] for i in range(len(value))}
    elif isinstance(value, dict):
        return value
    else:
        return {""value"": value}","Point(row=26, column=0)","Point(row=43, column=31)",,autogpt_platform/backend/backend/util/type.py
__convert_tuple,function,,"def __convert_tuple(value: Any) -> tuple:
    if isinstance(value, (str, list, set)):
        return tuple(value)
    elif isinstance(value, dict):
        return tuple(value.items())
    elif isinstance(value, (int, float, bool)):
        return (value,)
    elif isinstance(value, tuple):
        return value
    else:
        return (value,)","Point(row=46, column=0)","Point(row=56, column=23)",,autogpt_platform/backend/backend/util/type.py
__convert_set,function,,"def __convert_set(value: Any) -> set:
    if isinstance(value, (str, list, tuple)):
        return set(value)
    elif isinstance(value, dict):
        return set(value.items())
    elif isinstance(value, set):
        return value
    else:
        return {value}","Point(row=59, column=0)","Point(row=67, column=22)",,autogpt_platform/backend/backend/util/type.py
__convert_str,function,,"def __convert_str(value: Any) -> str:
    if isinstance(value, str):
        return value
    else:
        return json.dumps(value)","Point(row=70, column=0)","Point(row=74, column=32)",,autogpt_platform/backend/backend/util/type.py
__convert_num,function,,"def __convert_num(value: Any, num_type: Type[NUM]) -> NUM:
    if isinstance(value, (list, dict, tuple, set)):
        return num_type(len(value))
    elif isinstance(value, num_type):
        return value
    else:
        try:
            return num_type(float(value))
        except (ValueError, TypeError):
            return num_type(0)  # Fallback conversion","Point(row=80, column=0)","Point(row=89, column=53)",,autogpt_platform/backend/backend/util/type.py
__convert_bool,function,,"def __convert_bool(value: Any) -> bool:
    if isinstance(value, bool):
        return value
    elif isinstance(value, str):
        if value.lower() in [""true"", ""1""]:
            return True
        else:
            return False
    else:
        return bool(value)","Point(row=92, column=0)","Point(row=101, column=26)",,autogpt_platform/backend/backend/util/type.py
convert,function,,"def convert(value: Any, target_type: Type):
    origin = get_origin(target_type)
    args = get_args(target_type)
    if origin is None:
        origin = target_type
    if origin not in [list, dict, tuple, str, set, int, float, bool]:
        return value

    # Handle the case when value is already of the target type
    if isinstance(value, origin):
        if not args:
            return value
        else:
            # Need to convert elements
            if origin is list:
                return [convert(v, args[0]) for v in value]
            elif origin is tuple:
                # Tuples can have multiple types
                if len(args) == 1:
                    return tuple(convert(v, args[0]) for v in value)
                else:
                    return tuple(convert(v, t) for v, t in zip(value, args))
            elif origin is dict:
                key_type, val_type = args
                return {
                    convert(k, key_type): convert(v, val_type) for k, v in value.items()
                }
            elif origin is set:
                return {convert(v, args[0]) for v in value}
            else:
                return value
    else:
        # Need to convert value to the origin type
        if origin is list:
            value = __convert_list(value)
            if args:
                return [convert(v, args[0]) for v in value]
            else:
                return value
        elif origin is dict:
            value = __convert_dict(value)
            if args:
                key_type, val_type = args
                return {
                    convert(k, key_type): convert(v, val_type) for k, v in value.items()
                }
            else:
                return value
        elif origin is tuple:
            value = __convert_tuple(value)
            if args:
                if len(args) == 1:
                    return tuple(convert(v, args[0]) for v in value)
                else:
                    return tuple(convert(v, t) for v, t in zip(value, args))
            else:
                return value
        elif origin is str:
            return __convert_str(value)
        elif origin is set:
            value = __convert_set(value)
            if args:
                return {convert(v, args[0]) for v in value}
            else:
                return value
        elif origin is int:
            return __convert_num(value, int)
        elif origin is float:
            return __convert_num(value, float)
        elif origin is bool:
            return __convert_bool(value)
        else:
            return value","Point(row=104, column=0)","Point(row=176, column=24)",,autogpt_platform/backend/backend/util/type.py
SpinTestServer,class,,"class SpinTestServer:
    def __init__(self):
        self.db_api = DatabaseManager()
        self.exec_manager = ExecutionManager()
        self.agent_server = AgentServer()
        self.scheduler = ExecutionScheduler()

    @staticmethod
    def test_get_user_id():
        return ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""

    async def __aenter__(self):
        self.setup_dependency_overrides()
        self.db_api.__enter__()
        self.agent_server.__enter__()
        self.exec_manager.__enter__()
        self.scheduler.__enter__()

        await db.connect()
        await initialize_blocks()
        await create_default_user()

        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await db.disconnect()

        self.scheduler.__exit__(exc_type, exc_val, exc_tb)
        self.exec_manager.__exit__(exc_type, exc_val, exc_tb)
        self.agent_server.__exit__(exc_type, exc_val, exc_tb)
        self.db_api.__exit__(exc_type, exc_val, exc_tb)

    def setup_dependency_overrides(self):
        # Override get_user_id for testing
        self.agent_server.set_test_dependency_overrides(
            {get_user_id: self.test_get_user_id}
        )","Point(row=15, column=0)","Point(row=51, column=9)",,autogpt_platform/backend/backend/util/test.py
SpinTestServer.__init__,function,,"def __init__(self):
        self.db_api = DatabaseManager()
        self.exec_manager = ExecutionManager()
        self.agent_server = AgentServer()
        self.scheduler = ExecutionScheduler()","Point(row=16, column=4)","Point(row=20, column=45)",SpinTestServer,autogpt_platform/backend/backend/util/test.py
SpinTestServer.test_get_user_id,function,,"def test_get_user_id():
        return ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""","Point(row=23, column=4)","Point(row=24, column=53)",SpinTestServer,autogpt_platform/backend/backend/util/test.py
SpinTestServer.__aenter__,function,,"async def __aenter__(self):
        self.setup_dependency_overrides()
        self.db_api.__enter__()
        self.agent_server.__enter__()
        self.exec_manager.__enter__()
        self.scheduler.__enter__()

        await db.connect()
        await initialize_blocks()
        await create_default_user()

        return self","Point(row=26, column=4)","Point(row=37, column=19)",SpinTestServer,autogpt_platform/backend/backend/util/test.py
SpinTestServer.__aexit__,function,,"async def __aexit__(self, exc_type, exc_val, exc_tb):
        await db.disconnect()

        self.scheduler.__exit__(exc_type, exc_val, exc_tb)
        self.exec_manager.__exit__(exc_type, exc_val, exc_tb)
        self.agent_server.__exit__(exc_type, exc_val, exc_tb)
        self.db_api.__exit__(exc_type, exc_val, exc_tb)","Point(row=39, column=4)","Point(row=45, column=55)",SpinTestServer,autogpt_platform/backend/backend/util/test.py
SpinTestServer.setup_dependency_overrides,function,,"def setup_dependency_overrides(self):
        # Override get_user_id for testing
        self.agent_server.set_test_dependency_overrides(
            {get_user_id: self.test_get_user_id}
        )","Point(row=47, column=4)","Point(row=51, column=9)",SpinTestServer,autogpt_platform/backend/backend/util/test.py
wait_execution,function,,"async def wait_execution(
    user_id: str,
    graph_id: str,
    graph_exec_id: str,
    timeout: int = 20,
) -> list:
    async def is_execution_completed():
        status = await AgentServer().test_get_graph_run_status(
            graph_id, graph_exec_id, user_id
        )
        log.info(f""Execution status: {status}"")
        if status == ExecutionStatus.FAILED:
            log.info(""Execution failed"")
            raise Exception(""Execution failed"")
        return status == ExecutionStatus.COMPLETED

    # Wait for the executions to complete
    for i in range(timeout):
        if await is_execution_completed():
            return await AgentServer().test_get_graph_run_node_execution_results(
                graph_id, graph_exec_id, user_id
            )
        time.sleep(1)

    assert False, ""Execution did not complete in time.""","Point(row=54, column=0)","Point(row=78, column=55)",,autogpt_platform/backend/backend/util/test.py
wait_execution.is_execution_completed,function,,"async def is_execution_completed():
        status = await AgentServer().test_get_graph_run_status(
            graph_id, graph_exec_id, user_id
        )
        log.info(f""Execution status: {status}"")
        if status == ExecutionStatus.FAILED:
            log.info(""Execution failed"")
            raise Exception(""Execution failed"")
        return status == ExecutionStatus.COMPLETED","Point(row=60, column=4)","Point(row=68, column=50)",,autogpt_platform/backend/backend/util/test.py
execute_block_test,function,,"def execute_block_test(block: Block):
    prefix = f""[Test-{block.name}]""

    if not block.test_input or not block.test_output:
        log.info(f""{prefix} No test data provided"")
        return
    if not isinstance(block.test_input, list):
        block.test_input = [block.test_input]
    if not isinstance(block.test_output, list):
        block.test_output = [block.test_output]

    output_index = 0
    log.info(f""{prefix} Executing {len(block.test_input)} tests..."")
    prefix = "" "" * 4 + prefix

    for mock_name, mock_obj in (block.test_mock or {}).items():
        log.info(f""{prefix} mocking {mock_name}..."")
        if hasattr(block, mock_name):
            setattr(block, mock_name, mock_obj)
        else:
            log.info(f""{prefix} mock {mock_name} not found in block"")

    extra_exec_kwargs = {}

    if CREDENTIALS_FIELD_NAME in block.input_schema.model_fields:
        if not block.test_credentials:
            raise ValueError(
                f""{prefix} requires credentials but has no test_credentials""
            )
        extra_exec_kwargs[CREDENTIALS_FIELD_NAME] = block.test_credentials

    for input_data in block.test_input:
        log.info(f""{prefix} in: {input_data}"")

        for output_name, output_data in block.execute(input_data, **extra_exec_kwargs):
            if output_index >= len(block.test_output):
                raise ValueError(f""{prefix} produced output more than expected"")
            ex_output_name, ex_output_data = block.test_output[output_index]

            def compare(data, expected_data):
                if data == expected_data:
                    is_matching = True
                elif isinstance(expected_data, type):
                    is_matching = isinstance(data, expected_data)
                elif callable(expected_data):
                    is_matching = expected_data(data)
                else:
                    is_matching = False

                mark = ""‚úÖ"" if is_matching else ""‚ùå""
                log.info(f""{prefix} {mark} comparing `{data}` vs `{expected_data}`"")
                if not is_matching:
                    raise ValueError(
                        f""{prefix}: wrong output {data} vs {expected_data}""
                    )

            compare(output_data, ex_output_data)
            compare(output_name, ex_output_name)
            output_index += 1

    if output_index < len(block.test_output):
        raise ValueError(
            f""{prefix} produced output less than expected. output_index={output_index}, len(block.test_output)={len(block.test_output)}""
        )","Point(row=81, column=0)","Point(row=144, column=9)",,autogpt_platform/backend/backend/util/test.py
execute_block_test.compare,function,,"def compare(data, expected_data):
                if data == expected_data:
                    is_matching = True
                elif isinstance(expected_data, type):
                    is_matching = isinstance(data, expected_data)
                elif callable(expected_data):
                    is_matching = expected_data(data)
                else:
                    is_matching = False

                mark = ""‚úÖ"" if is_matching else ""‚ùå""
                log.info(f""{prefix} {mark} comparing `{data}` vs `{expected_data}`"")
                if not is_matching:
                    raise ValueError(
                        f""{prefix}: wrong output {data} vs {expected_data}""
                    )","Point(row=120, column=12)","Point(row=135, column=21)",,autogpt_platform/backend/backend/util/test.py
_log_prefix,function,"
    Returns a prefix string for logging purposes.
    This needs to be called on the fly to get the current process ID & service name,
    not the parent process ID & service name.
","def _log_prefix(resource_name: str, conn_id: str):
    """"""
    Returns a prefix string for logging purposes.
    This needs to be called on the fly to get the current process ID & service name,
    not the parent process ID & service name.
    """"""
    return f""[PID-{os.getpid()}|{get_service_name()}|{resource_name}-{conn_id}]""","Point(row=12, column=0)","Point(row=18, column=80)",,autogpt_platform/backend/backend/util/retry.py
conn_retry,function,,"def conn_retry(resource_name: str, action_name: str, max_retry: int = 5):
    conn_id = str(uuid4())

    def on_retry(retry_state):
        prefix = _log_prefix(resource_name, conn_id)
        exception = retry_state.outcome.exception()
        logger.info(f""{prefix} {action_name} failed: {exception}. Retrying now..."")

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            prefix = _log_prefix(resource_name, conn_id)
            logger.info(f""{prefix} {action_name} started..."")

            # Define the retrying strategy
            retrying_func = retry(
                stop=stop_after_attempt(max_retry + 1),
                wait=wait_exponential(multiplier=1, min=1, max=30),
                before_sleep=on_retry,
                reraise=True,
            )(func)

            try:
                result = retrying_func(*args, **kwargs)
                logger.info(f""{prefix} {action_name} completed successfully."")
                return result
            except Exception as e:
                logger.error(f""{prefix} {action_name} failed after retries: {e}"")
                raise

        return wrapper

    return decorator","Point(row=21, column=0)","Point(row=53, column=20)",,autogpt_platform/backend/backend/util/retry.py
conn_retry.on_retry,function,,"def on_retry(retry_state):
        prefix = _log_prefix(resource_name, conn_id)
        exception = retry_state.outcome.exception()
        logger.info(f""{prefix} {action_name} failed: {exception}. Retrying now..."")","Point(row=24, column=4)","Point(row=27, column=83)",,autogpt_platform/backend/backend/util/retry.py
conn_retry.decorator,function,,"def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            prefix = _log_prefix(resource_name, conn_id)
            logger.info(f""{prefix} {action_name} started..."")

            # Define the retrying strategy
            retrying_func = retry(
                stop=stop_after_attempt(max_retry + 1),
                wait=wait_exponential(multiplier=1, min=1, max=30),
                before_sleep=on_retry,
                reraise=True,
            )(func)

            try:
                result = retrying_func(*args, **kwargs)
                logger.info(f""{prefix} {action_name} completed successfully."")
                return result
            except Exception as e:
                logger.error(f""{prefix} {action_name} failed after retries: {e}"")
                raise

        return wrapper","Point(row=29, column=4)","Point(row=51, column=22)",,autogpt_platform/backend/backend/util/retry.py
conn_retry.decorator.wrapper,function,,"def wrapper(*args, **kwargs):
            prefix = _log_prefix(resource_name, conn_id)
            logger.info(f""{prefix} {action_name} started..."")

            # Define the retrying strategy
            retrying_func = retry(
                stop=stop_after_attempt(max_retry + 1),
                wait=wait_exponential(multiplier=1, min=1, max=30),
                before_sleep=on_retry,
                reraise=True,
            )(func)

            try:
                result = retrying_func(*args, **kwargs)
                logger.info(f""{prefix} {action_name} completed successfully."")
                return result
            except Exception as e:
                logger.error(f""{prefix} {action_name} failed after retries: {e}"")
                raise","Point(row=31, column=8)","Point(row=49, column=21)",,autogpt_platform/backend/backend/util/retry.py
AppEnvironment,class,,"class AppEnvironment(str, Enum):
    LOCAL = ""local""
    DEVELOPMENT = ""dev""
    PRODUCTION = ""prod""","Point(row=18, column=0)","Point(row=21, column=23)",,autogpt_platform/backend/backend/util/settings.py
BehaveAs,class,,"class BehaveAs(str, Enum):
    LOCAL = ""local""
    CLOUD = ""cloud""","Point(row=24, column=0)","Point(row=26, column=19)",,autogpt_platform/backend/backend/util/settings.py
UpdateTrackingModel,class,,"class UpdateTrackingModel(BaseModel, Generic[T]):
    _updated_fields: Set[str] = PrivateAttr(default_factory=set)

    def __setattr__(self, name: str, value) -> None:
        if name in self.model_fields:
            self._updated_fields.add(name)
        super().__setattr__(name, value)

    def mark_updated(self, field_name: str) -> None:
        if field_name in self.model_fields:
            self._updated_fields.add(field_name)

    def clear_updates(self) -> None:
        self._updated_fields.clear()

    def get_updates(self) -> Dict[str, Any]:
        return {field: getattr(self, field) for field in self._updated_fields}

    @property
    def updated_fields(self):
        return self._updated_fields","Point(row=29, column=0)","Point(row=49, column=35)",,autogpt_platform/backend/backend/util/settings.py
UpdateTrackingModel.__setattr__,function,,"def __setattr__(self, name: str, value) -> None:
        if name in self.model_fields:
            self._updated_fields.add(name)
        super().__setattr__(name, value)","Point(row=32, column=4)","Point(row=35, column=40)",UpdateTrackingModel,autogpt_platform/backend/backend/util/settings.py
UpdateTrackingModel.mark_updated,function,,"def mark_updated(self, field_name: str) -> None:
        if field_name in self.model_fields:
            self._updated_fields.add(field_name)","Point(row=37, column=4)","Point(row=39, column=48)",UpdateTrackingModel,autogpt_platform/backend/backend/util/settings.py
UpdateTrackingModel.clear_updates,function,,"def clear_updates(self) -> None:
        self._updated_fields.clear()","Point(row=41, column=4)","Point(row=42, column=36)",UpdateTrackingModel,autogpt_platform/backend/backend/util/settings.py
UpdateTrackingModel.get_updates,function,,"def get_updates(self) -> Dict[str, Any]:
        return {field: getattr(self, field) for field in self._updated_fields}","Point(row=44, column=4)","Point(row=45, column=78)",UpdateTrackingModel,autogpt_platform/backend/backend/util/settings.py
UpdateTrackingModel.updated_fields,function,,"def updated_fields(self):
        return self._updated_fields","Point(row=48, column=4)","Point(row=49, column=35)",UpdateTrackingModel,autogpt_platform/backend/backend/util/settings.py
Config,class,Config for the server.,"class Config(UpdateTrackingModel[""Config""], BaseSettings):
    """"""Config for the server.""""""

    num_graph_workers: int = Field(
        default=10,
        ge=1,
        le=1000,
        description=""Maximum number of workers to use for graph execution."",
    )
    num_node_workers: int = Field(
        default=5,
        ge=1,
        le=1000,
        description=""Maximum number of workers to use for node execution within a single graph."",
    )
    pyro_host: str = Field(
        default=""localhost"",
        description=""The default hostname of the Pyro server."",
    )
    pyro_client_comm_timeout: float = Field(
        default=15,
        description=""The default timeout in seconds, for Pyro client connections."",
    )
    pyro_client_comm_retry: int = Field(
        default=3,
        description=""The default number of retries for Pyro client connections."",
    )
    enable_auth: bool = Field(
        default=True,
        description=""If authentication is enabled or not"",
    )
    enable_credit: str = Field(
        default=""false"",
        description=""If user credit system is enabled or not"",
    )
    num_user_credits_refill: int = Field(
        default=1500,
        description=""Number of credits to refill for each user"",
    )
    # Add more configuration fields as needed

    model_config = SettingsConfigDict(
        env_file="".env"",
        extra=""allow"",
    )

    websocket_server_host: str = Field(
        default=""0.0.0.0"",
        description=""The host for the websocket server to run on"",
    )

    websocket_server_port: int = Field(
        default=8001,
        description=""The port for the websocket server to run on"",
    )

    execution_manager_port: int = Field(
        default=8002,
        description=""The port for execution manager daemon to run on"",
    )

    execution_scheduler_port: int = Field(
        default=8003,
        description=""The port for execution scheduler daemon to run on"",
    )

    agent_server_port: int = Field(
        default=8004,
        description=""The port for agent server daemon to run on"",
    )

    database_api_port: int = Field(
        default=8005,
        description=""The port for database server API to run on"",
    )

    agent_api_host: str = Field(
        default=""0.0.0.0"",
        description=""The host for agent server API to run on"",
    )

    agent_api_port: int = Field(
        default=8006,
        description=""The port for agent server API to run on"",
    )

    frontend_base_url: str = Field(
        default=""http://localhost:3000"",
        description=""Can be used to explicitly set the base URL for the frontend. ""
        ""This value is then used to generate redirect URLs for OAuth flows."",
    )

    app_env: AppEnvironment = Field(
        default=AppEnvironment.LOCAL,
        description=""The name of the app environment: local or dev or prod"",
    )

    behave_as: BehaveAs = Field(
        default=BehaveAs.LOCAL,
        description=""What environment to behave as: local or cloud"",
    )

    execution_event_bus_name: str = Field(
        default=""execution_event"",
        description=""Name of the event bus"",
    )

    trust_endpoints_for_requests: List[str] = Field(
        default_factory=list,
        description=""A whitelist of trusted internal endpoints for the backend to make requests to."",
    )

    backend_cors_allow_origins: List[str] = Field(default_factory=list)

    @field_validator(""backend_cors_allow_origins"")
    @classmethod
    def validate_cors_allow_origins(cls, v: List[str]) -> List[str]:
        out = []
        port = None
        has_localhost = False
        has_127_0_0_1 = False
        for url in v:
            url = url.strip()
            if url.startswith((""http://"", ""https://"")):
                if ""localhost"" in url:
                    port = url.split("":"")[2]
                    has_localhost = True
                if ""127.0.0.1"" in url:
                    port = url.split("":"")[2]
                    has_127_0_0_1 = True
                out.append(url)
            else:
                raise ValueError(f""Invalid URL: {url}"")

        if has_127_0_0_1 and not has_localhost:
            out.append(f""http://localhost:{port}"")
        if has_localhost and not has_127_0_0_1:
            out.append(f""http://127.0.0.1:{port}"")

        return out

    @classmethod
    def settings_customise_sources(
        cls,
        settings_cls: Type[BaseSettings],
        init_settings: PydanticBaseSettingsSource,
        env_settings: PydanticBaseSettingsSource,
        dotenv_settings: PydanticBaseSettingsSource,
        file_secret_settings: PydanticBaseSettingsSource,
    ) -> Tuple[PydanticBaseSettingsSource, ...]:
        return (
            env_settings,
            file_secret_settings,
            dotenv_settings,
            JsonConfigSettingsSource(settings_cls),
            init_settings,
        )","Point(row=52, column=0)","Point(row=208, column=9)",,autogpt_platform/backend/backend/util/settings.py
Config.validate_cors_allow_origins,function,,"def validate_cors_allow_origins(cls, v: List[str]) -> List[str]:
        out = []
        port = None
        has_localhost = False
        has_127_0_0_1 = False
        for url in v:
            url = url.strip()
            if url.startswith((""http://"", ""https://"")):
                if ""localhost"" in url:
                    port = url.split("":"")[2]
                    has_localhost = True
                if ""127.0.0.1"" in url:
                    port = url.split("":"")[2]
                    has_127_0_0_1 = True
                out.append(url)
            else:
                raise ValueError(f""Invalid URL: {url}"")

        if has_127_0_0_1 and not has_localhost:
            out.append(f""http://localhost:{port}"")
        if has_localhost and not has_127_0_0_1:
            out.append(f""http://127.0.0.1:{port}"")

        return out","Point(row=168, column=4)","Point(row=191, column=18)",Config,autogpt_platform/backend/backend/util/settings.py
Config.settings_customise_sources,function,,"def settings_customise_sources(
        cls,
        settings_cls: Type[BaseSettings],
        init_settings: PydanticBaseSettingsSource,
        env_settings: PydanticBaseSettingsSource,
        dotenv_settings: PydanticBaseSettingsSource,
        file_secret_settings: PydanticBaseSettingsSource,
    ) -> Tuple[PydanticBaseSettingsSource, ...]:
        return (
            env_settings,
            file_secret_settings,
            dotenv_settings,
            JsonConfigSettingsSource(settings_cls),
            init_settings,
        )","Point(row=194, column=4)","Point(row=208, column=9)",Config,autogpt_platform/backend/backend/util/settings.py
Secrets,class,Secrets for the server.,"class Secrets(UpdateTrackingModel[""Secrets""], BaseSettings):
    """"""Secrets for the server.""""""

    supabase_url: str = Field(default="""", description=""Supabase URL"")
    supabase_service_role_key: str = Field(
        default="""", description=""Supabase service role key""
    )

    encryption_key: str = Field(default="""", description=""Encryption key"")

    # OAuth server credentials for integrations
    # --8<-- [start:OAuthServerCredentialsExample]
    github_client_id: str = Field(default="""", description=""GitHub OAuth client ID"")
    github_client_secret: str = Field(
        default="""", description=""GitHub OAuth client secret""
    )
    # --8<-- [end:OAuthServerCredentialsExample]
    google_client_id: str = Field(default="""", description=""Google OAuth client ID"")
    google_client_secret: str = Field(
        default="""", description=""Google OAuth client secret""
    )
    notion_client_id: str = Field(default="""", description=""Notion OAuth client ID"")
    notion_client_secret: str = Field(
        default="""", description=""Notion OAuth client secret""
    )

    openai_api_key: str = Field(default="""", description=""OpenAI API key"")
    anthropic_api_key: str = Field(default="""", description=""Anthropic API key"")
    groq_api_key: str = Field(default="""", description=""Groq API key"")

    reddit_client_id: str = Field(default="""", description=""Reddit client ID"")
    reddit_client_secret: str = Field(default="""", description=""Reddit client secret"")
    reddit_username: str = Field(default="""", description=""Reddit username"")
    reddit_password: str = Field(default="""", description=""Reddit password"")

    openweathermap_api_key: str = Field(
        default="""", description=""OpenWeatherMap API key""
    )

    medium_api_key: str = Field(default="""", description=""Medium API key"")
    medium_author_id: str = Field(default="""", description=""Medium author ID"")
    did_api_key: str = Field(default="""", description=""D-ID API Key"")
    revid_api_key: str = Field(default="""", description=""revid.ai API key"")
    discord_bot_token: str = Field(default="""", description=""Discord bot token"")

    smtp_server: str = Field(default="""", description=""SMTP server IP"")
    smtp_port: str = Field(default="""", description=""SMTP server port"")
    smtp_username: str = Field(default="""", description=""SMTP username"")
    smtp_password: str = Field(default="""", description=""SMTP password"")

    sentry_dsn: str = Field(default="""", description=""Sentry DSN"")

    google_maps_api_key: str = Field(default="""", description=""Google Maps API Key"")

    replicate_api_key: str = Field(default="""", description=""Replicate API Key"")
    unreal_speech_api_key: str = Field(default="""", description=""Unreal Speech API Key"")
    ideogram_api_key: str = Field(default="""", description=""Ideogram API Key"")
    jina_api_key: str = Field(default="""", description=""Jina API Key"")

    # Add more secret fields as needed

    model_config = SettingsConfigDict(
        secrets_dir=get_secrets_path(),
        env_file="".env"",
        env_file_encoding=""utf-8"",
        extra=""allow"",
    )","Point(row=211, column=0)","Point(row=277, column=5)",,autogpt_platform/backend/backend/util/settings.py
Settings,class,,"class Settings(BaseModel):
    config: Config = Config()
    secrets: Secrets = Secrets()

    def save(self) -> None:
        # Save updated config to JSON file
        if self.config.updated_fields:
            config_to_save = self.config.get_updates()
            config_path = os.path.join(get_data_path(), ""config.json"")
            if os.path.exists(config_path):
                with open(config_path, ""r+"") as f:
                    existing_config: Dict[str, Any] = json.load(f)
                    existing_config.update(config_to_save)
                    f.seek(0)
                    json.dump(existing_config, f, indent=2)
                    f.truncate()
            else:
                with open(config_path, ""w"") as f:
                    json.dump(config_to_save, f, indent=2)
            self.config.clear_updates()

        # Save updated secrets to individual files
        secrets_dir = get_secrets_path()
        for key in self.secrets.updated_fields:
            secret_file = os.path.join(secrets_dir, key)
            with open(secret_file, ""w"") as f:
                f.write(str(getattr(self.secrets, key)))
        self.secrets.clear_updates()","Point(row=280, column=0)","Point(row=307, column=36)",,autogpt_platform/backend/backend/util/settings.py
Settings.save,function,,"def save(self) -> None:
        # Save updated config to JSON file
        if self.config.updated_fields:
            config_to_save = self.config.get_updates()
            config_path = os.path.join(get_data_path(), ""config.json"")
            if os.path.exists(config_path):
                with open(config_path, ""r+"") as f:
                    existing_config: Dict[str, Any] = json.load(f)
                    existing_config.update(config_to_save)
                    f.seek(0)
                    json.dump(existing_config, f, indent=2)
                    f.truncate()
            else:
                with open(config_path, ""w"") as f:
                    json.dump(config_to_save, f, indent=2)
            self.config.clear_updates()

        # Save updated secrets to individual files
        secrets_dir = get_secrets_path()
        for key in self.secrets.updated_fields:
            secret_file = os.path.join(secrets_dir, key)
            with open(secret_file, ""w"") as f:
                f.write(str(getattr(self.secrets, key)))
        self.secrets.clear_updates()","Point(row=284, column=4)","Point(row=307, column=36)",Settings,autogpt_platform/backend/backend/util/settings.py
get_service_name,function,,"def get_service_name():
    return _SERVICE_NAME","Point(row=15, column=0)","Point(row=16, column=24)",,autogpt_platform/backend/backend/util/process.py
set_service_name,function,,"def set_service_name(name: str):
    global _SERVICE_NAME
    _SERVICE_NAME = name","Point(row=19, column=0)","Point(row=21, column=24)",,autogpt_platform/backend/backend/util/process.py
AppProcess,class,"
    A class to represent an object that can be executed in a background process.
","class AppProcess(ABC):
    """"""
    A class to represent an object that can be executed in a background process.
    """"""

    process: Optional[Process] = None

    set_start_method(""spawn"", force=True)
    configure_logging()
    sentry_init()

    # Methods that are executed INSIDE the process #

    @abstractmethod
    def run(self):
        """"""
        The method that will be executed in the process.
        """"""
        pass

    @classmethod
    @property
    def service_name(cls) -> str:
        return cls.__name__

    def cleanup(self):
        """"""
        Implement this method on a subclass to do post-execution cleanup,
        e.g. disconnecting from a database or terminating child processes.
        """"""
        pass

    def health_check(self):
        """"""
        A method to check the health of the process.
        """"""
        pass

    def execute_run_command(self, silent):
        signal.signal(signal.SIGTERM, self._self_terminate)

        try:
            if silent:
                sys.stdout = open(os.devnull, ""w"")
                sys.stderr = open(os.devnull, ""w"")

            set_service_name(self.service_name)
            logger.info(f""[{self.service_name}] Starting..."")
            self.run()
        except (KeyboardInterrupt, SystemExit) as e:
            logger.warning(f""[{self.service_name}] Terminated: {e}; quitting..."")

    def _self_terminate(self, signum: int, frame):
        self.cleanup()
        sys.exit(0)

    # Methods that are executed OUTSIDE the process #

    def __enter__(self):
        self.start(background=True)
        return self

    def __exit__(self, *args, **kwargs):
        self.stop()

    def start(self, background: bool = False, silent: bool = False, **proc_args) -> int:
        """"""
        Start the background process.
        Args:
            background: Whether to run the process in the background.
            silent: Whether to disable stdout and stderr.
            proc_args: Additional arguments to pass to the process.
        Returns:
            the process id or 0 if the process is not running in the background.
        """"""
        if not background:
            self.execute_run_command(silent)
            return 0

        self.process = Process(
            name=self.__class__.__name__,
            target=self.execute_run_command,
            args=(silent,),
            **proc_args,
        )
        self.process.start()
        self.health_check()
        return self.process.pid or 0

    def stop(self):
        """"""
        Stop the background process.
        """"""
        if not self.process:
            return

        self.process.terminate()
        self.process.join()
        self.process = None","Point(row=24, column=0)","Point(row=122, column=27)",,autogpt_platform/backend/backend/util/process.py
AppProcess.run,function,"
        The method that will be executed in the process.
","def run(self):
        """"""
        The method that will be executed in the process.
        """"""
        pass","Point(row=38, column=4)","Point(row=42, column=12)",AppProcess,autogpt_platform/backend/backend/util/process.py
AppProcess.service_name,function,,"def service_name(cls) -> str:
        return cls.__name__","Point(row=46, column=4)","Point(row=47, column=27)",AppProcess,autogpt_platform/backend/backend/util/process.py
AppProcess.cleanup,function,"
        Implement this method on a subclass to do post-execution cleanup,
        e.g. disconnecting from a database or terminating child processes.
","def cleanup(self):
        """"""
        Implement this method on a subclass to do post-execution cleanup,
        e.g. disconnecting from a database or terminating child processes.
        """"""
        pass","Point(row=49, column=4)","Point(row=54, column=12)",AppProcess,autogpt_platform/backend/backend/util/process.py
AppProcess.health_check,function,"
        A method to check the health of the process.
","def health_check(self):
        """"""
        A method to check the health of the process.
        """"""
        pass","Point(row=56, column=4)","Point(row=60, column=12)",AppProcess,autogpt_platform/backend/backend/util/process.py
AppProcess.execute_run_command,function,,"def execute_run_command(self, silent):
        signal.signal(signal.SIGTERM, self._self_terminate)

        try:
            if silent:
                sys.stdout = open(os.devnull, ""w"")
                sys.stderr = open(os.devnull, ""w"")

            set_service_name(self.service_name)
            logger.info(f""[{self.service_name}] Starting..."")
            self.run()
        except (KeyboardInterrupt, SystemExit) as e:
            logger.warning(f""[{self.service_name}] Terminated: {e}; quitting..."")","Point(row=62, column=4)","Point(row=74, column=81)",AppProcess,autogpt_platform/backend/backend/util/process.py
AppProcess._self_terminate,function,,"def _self_terminate(self, signum: int, frame):
        self.cleanup()
        sys.exit(0)","Point(row=76, column=4)","Point(row=78, column=19)",AppProcess,autogpt_platform/backend/backend/util/process.py
AppProcess.__enter__,function,,"def __enter__(self):
        self.start(background=True)
        return self","Point(row=82, column=4)","Point(row=84, column=19)",AppProcess,autogpt_platform/backend/backend/util/process.py
AppProcess.__exit__,function,,"def __exit__(self, *args, **kwargs):
        self.stop()","Point(row=86, column=4)","Point(row=87, column=19)",AppProcess,autogpt_platform/backend/backend/util/process.py
AppProcess.start,function,"
        Start the background process.
        Args:
            background: Whether to run the process in the background.
            silent: Whether to disable stdout and stderr.
            proc_args: Additional arguments to pass to the process.
        Returns:
            the process id or 0 if the process is not running in the background.
","def start(self, background: bool = False, silent: bool = False, **proc_args) -> int:
        """"""
        Start the background process.
        Args:
            background: Whether to run the process in the background.
            silent: Whether to disable stdout and stderr.
            proc_args: Additional arguments to pass to the process.
        Returns:
            the process id or 0 if the process is not running in the background.
        """"""
        if not background:
            self.execute_run_command(silent)
            return 0

        self.process = Process(
            name=self.__class__.__name__,
            target=self.execute_run_command,
            args=(silent,),
            **proc_args,
        )
        self.process.start()
        self.health_check()
        return self.process.pid or 0","Point(row=89, column=4)","Point(row=111, column=36)",AppProcess,autogpt_platform/backend/backend/util/process.py
AppProcess.stop,function,"
        Stop the background process.
","def stop(self):
        """"""
        Stop the background process.
        """"""
        if not self.process:
            return

        self.process.terminate()
        self.process.join()
        self.process = None","Point(row=113, column=4)","Point(row=122, column=27)",AppProcess,autogpt_platform/backend/backend/util/process.py
to_dict,function,,"def to_dict(data) -> dict:
    return jsonable_encoder(data)","Point(row=5, column=0)","Point(row=6, column=33)",,autogpt_platform/backend/backend/util/json.py
dumps,function,,"def dumps(data) -> str:
    return json.dumps(jsonable_encoder(data))","Point(row=9, column=0)","Point(row=10, column=45)",,autogpt_platform/backend/backend/util/json.py
get_secrets_path,function,,"def get_secrets_path() -> pathlib.Path:
    return get_data_path() / ""secrets""","Point(row=5, column=0)","Point(row=6, column=38)",,autogpt_platform/backend/backend/util/data.py
get_config_path,function,,"def get_config_path() -> pathlib.Path:
    return get_data_path()","Point(row=9, column=0)","Point(row=10, column=26)",,autogpt_platform/backend/backend/util/data.py
get_frontend_path,function,,"def get_frontend_path() -> pathlib.Path:
    if getattr(sys, ""frozen"", False):
        # The application is frozen
        datadir = pathlib.Path(os.path.dirname(sys.executable)) / ""example_files""
    else:
        # The application is not frozen
        # Change this bit to match where you store your data files:
        filedir = os.path.dirname(__file__)
        datadir = pathlib.Path(filedir).parent.parent.parent / ""example_files""
    return pathlib.Path(datadir)","Point(row=13, column=0)","Point(row=22, column=32)",,autogpt_platform/backend/backend/util/data.py
get_data_path,function,,"def get_data_path() -> pathlib.Path:
    if getattr(sys, ""frozen"", False):
        # The application is frozen
        datadir = os.path.dirname(sys.executable)
    else:
        # The application is not frozen
        # Change this bit to match where you store your data files:
        filedir = os.path.dirname(__file__)
        datadir = pathlib.Path(filedir).parent.parent
    return pathlib.Path(datadir)","Point(row=25, column=0)","Point(row=34, column=32)",,autogpt_platform/backend/backend/util/data.py
MockObject,class,,"class MockObject:
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)

    def __getattr__(self, name):
        return self.__dict__.get(name)

    def __call__(self, *args, **kwargs):
        return self

    def __setattr__(self, name, value):
        self.__dict__[name] = value","Point(row=0, column=0)","Point(row=11, column=35)",,autogpt_platform/backend/backend/util/mock.py
MockObject.__init__,function,,"def __init__(self, **kwargs):
        self.__dict__.update(kwargs)","Point(row=1, column=4)","Point(row=2, column=36)",MockObject,autogpt_platform/backend/backend/util/mock.py
MockObject.__getattr__,function,,"def __getattr__(self, name):
        return self.__dict__.get(name)","Point(row=4, column=4)","Point(row=5, column=38)",MockObject,autogpt_platform/backend/backend/util/mock.py
MockObject.__call__,function,,"def __call__(self, *args, **kwargs):
        return self","Point(row=7, column=4)","Point(row=8, column=19)",MockObject,autogpt_platform/backend/backend/util/mock.py
MockObject.__setattr__,function,,"def __setattr__(self, name, value):
        self.__dict__[name] = value","Point(row=10, column=4)","Point(row=11, column=35)",MockObject,autogpt_platform/backend/backend/util/mock.py
ConnectionManager,class,,"class ConnectionManager:
    def __init__(self):
        self.active_connections: Set[WebSocket] = set()
        self.subscriptions: Dict[str, Set[WebSocket]] = {}

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.add(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
        for subscribers in self.subscriptions.values():
            subscribers.discard(websocket)

    async def subscribe(self, graph_id: str, websocket: WebSocket):
        if graph_id not in self.subscriptions:
            self.subscriptions[graph_id] = set()
        self.subscriptions[graph_id].add(websocket)

    async def unsubscribe(self, graph_id: str, websocket: WebSocket):
        if graph_id in self.subscriptions:
            self.subscriptions[graph_id].discard(websocket)
            if not self.subscriptions[graph_id]:
                del self.subscriptions[graph_id]

    async def send_execution_result(self, result: execution.ExecutionResult):
        graph_id = result.graph_id
        if graph_id in self.subscriptions:
            message = WsMessage(
                method=Methods.EXECUTION_EVENT,
                channel=graph_id,
                data=result.model_dump(),
            ).model_dump_json()
            for connection in self.subscriptions[graph_id]:
                await connection.send_text(message)","Point(row=8, column=0)","Point(row=42, column=51)",,autogpt_platform/backend/backend/server/conn_manager.py
ConnectionManager.__init__,function,,"def __init__(self):
        self.active_connections: Set[WebSocket] = set()
        self.subscriptions: Dict[str, Set[WebSocket]] = {}","Point(row=9, column=4)","Point(row=11, column=58)",ConnectionManager,autogpt_platform/backend/backend/server/conn_manager.py
ConnectionManager.connect,function,,"async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.add(websocket)","Point(row=13, column=4)","Point(row=15, column=46)",ConnectionManager,autogpt_platform/backend/backend/server/conn_manager.py
ConnectionManager.disconnect,function,,"def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
        for subscribers in self.subscriptions.values():
            subscribers.discard(websocket)","Point(row=17, column=4)","Point(row=20, column=42)",ConnectionManager,autogpt_platform/backend/backend/server/conn_manager.py
ConnectionManager.subscribe,function,,"async def subscribe(self, graph_id: str, websocket: WebSocket):
        if graph_id not in self.subscriptions:
            self.subscriptions[graph_id] = set()
        self.subscriptions[graph_id].add(websocket)","Point(row=22, column=4)","Point(row=25, column=51)",ConnectionManager,autogpt_platform/backend/backend/server/conn_manager.py
ConnectionManager.unsubscribe,function,,"async def unsubscribe(self, graph_id: str, websocket: WebSocket):
        if graph_id in self.subscriptions:
            self.subscriptions[graph_id].discard(websocket)
            if not self.subscriptions[graph_id]:
                del self.subscriptions[graph_id]","Point(row=27, column=4)","Point(row=31, column=48)",ConnectionManager,autogpt_platform/backend/backend/server/conn_manager.py
ConnectionManager.send_execution_result,function,,"async def send_execution_result(self, result: execution.ExecutionResult):
        graph_id = result.graph_id
        if graph_id in self.subscriptions:
            message = WsMessage(
                method=Methods.EXECUTION_EVENT,
                channel=graph_id,
                data=result.model_dump(),
            ).model_dump_json()
            for connection in self.subscriptions[graph_id]:
                await connection.send_text(message)","Point(row=33, column=4)","Point(row=42, column=51)",ConnectionManager,autogpt_platform/backend/backend/server/conn_manager.py
lifespan,function,,"async def lifespan(app: FastAPI):
    manager = get_connection_manager()
    fut = asyncio.create_task(event_broadcaster(manager))
    fut.add_done_callback(lambda _: logger.info(""Event broadcaster stopped""))
    yield","Point(row=22, column=0)","Point(row=26, column=9)",,autogpt_platform/backend/backend/server/ws_api.py
get_connection_manager,function,,"def get_connection_manager():
    global _connection_manager
    if _connection_manager is None:
        _connection_manager = ConnectionManager()
    return _connection_manager","Point(row=34, column=0)","Point(row=38, column=30)",,autogpt_platform/backend/backend/server/ws_api.py
event_broadcaster,function,,"async def event_broadcaster(manager: ConnectionManager):
    try:
        redis.connect()
        event_queue = AsyncRedisExecutionEventBus()
        async for event in event_queue.listen():
            await manager.send_execution_result(event)
    except Exception as e:
        logger.exception(f""Event broadcaster error: {e}"")
        raise
    finally:
        redis.disconnect()","Point(row=41, column=0)","Point(row=51, column=26)",,autogpt_platform/backend/backend/server/ws_api.py
authenticate_websocket,function,,"async def authenticate_websocket(websocket: WebSocket) -> str:
    if settings.config.enable_auth:
        token = websocket.query_params.get(""token"")
        if not token:
            await websocket.close(code=4001, reason=""Missing authentication token"")
            return """"

        try:
            payload = parse_jwt_token(token)
            user_id = payload.get(""sub"")
            if not user_id:
                await websocket.close(code=4002, reason=""Invalid token"")
                return """"
            return user_id
        except ValueError:
            await websocket.close(code=4003, reason=""Invalid token"")
            return """"
    else:
        return DEFAULT_USER_ID","Point(row=54, column=0)","Point(row=72, column=30)",,autogpt_platform/backend/backend/server/ws_api.py
handle_subscribe,function,,"async def handle_subscribe(
    websocket: WebSocket, manager: ConnectionManager, message: WsMessage
):
    if not message.data:
        await websocket.send_text(
            WsMessage(
                method=Methods.ERROR,
                success=False,
                error=""Subscription data missing"",
            ).model_dump_json()
        )
    else:
        ex_sub = ExecutionSubscription.model_validate(message.data)
        await manager.subscribe(ex_sub.graph_id, websocket)
        logger.debug(f""New execution subscription for graph {ex_sub.graph_id}"")
        await websocket.send_text(
            WsMessage(
                method=Methods.SUBSCRIBE,
                success=True,
                channel=ex_sub.graph_id,
            ).model_dump_json()
        )","Point(row=75, column=0)","Point(row=96, column=9)",,autogpt_platform/backend/backend/server/ws_api.py
handle_unsubscribe,function,,"async def handle_unsubscribe(
    websocket: WebSocket, manager: ConnectionManager, message: WsMessage
):
    if not message.data:
        await websocket.send_text(
            WsMessage(
                method=Methods.ERROR,
                success=False,
                error=""Subscription data missing"",
            ).model_dump_json()
        )
    else:
        ex_sub = ExecutionSubscription.model_validate(message.data)
        await manager.unsubscribe(ex_sub.graph_id, websocket)
        logger.debug(f""Removed execution subscription for graph {ex_sub.graph_id}"")
        await websocket.send_text(
            WsMessage(
                method=Methods.UNSUBSCRIBE,
                success=True,
                channel=ex_sub.graph_id,
            ).model_dump_json()
        )","Point(row=99, column=0)","Point(row=120, column=9)",,autogpt_platform/backend/backend/server/ws_api.py
health,function,,"async def health():
    return {""status"": ""healthy""}","Point(row=124, column=0)","Point(row=125, column=32)",,autogpt_platform/backend/backend/server/ws_api.py
websocket_router,function,,"async def websocket_router(
    websocket: WebSocket, manager: ConnectionManager = Depends(get_connection_manager)
):
    user_id = await authenticate_websocket(websocket)
    if not user_id:
        return
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            message = WsMessage.model_validate_json(data)
            if message.method == Methods.SUBSCRIBE:
                await handle_subscribe(websocket, manager, message)

            elif message.method == Methods.UNSUBSCRIBE:
                await handle_unsubscribe(websocket, manager, message)

            elif message.method == Methods.ERROR:
                logger.error(f""WebSocket Error message received: {message.data}"")

            else:
                logger.warning(
                    f""Unknown WebSocket message type {message.method} received: ""
                    f""{message.data}""
                )
                await websocket.send_text(
                    WsMessage(
                        method=Methods.ERROR,
                        success=False,
                        error=""Message type is not processed by the server"",
                    ).model_dump_json()
                )

    except WebSocketDisconnect:
        manager.disconnect(websocket)
        logger.debug(""WebSocket client disconnected"")","Point(row=129, column=0)","Point(row=164, column=53)",,autogpt_platform/backend/backend/server/ws_api.py
WebsocketServer,class,,"class WebsocketServer(AppProcess):
    def run(self):
        logger.info(f""CORS allow origins: {settings.config.backend_cors_allow_origins}"")
        server_app = CORSMiddleware(
            app=app,
            allow_origins=settings.config.backend_cors_allow_origins,
            allow_credentials=True,
            allow_methods=[""*""],
            allow_headers=[""*""],
        )
        uvicorn.run(
            server_app,
            host=Config().websocket_server_host,
            port=Config().websocket_server_port,
        )","Point(row=167, column=0)","Point(row=181, column=9)",,autogpt_platform/backend/backend/server/ws_api.py
WebsocketServer.run,function,,"def run(self):
        logger.info(f""CORS allow origins: {settings.config.backend_cors_allow_origins}"")
        server_app = CORSMiddleware(
            app=app,
            allow_origins=settings.config.backend_cors_allow_origins,
            allow_credentials=True,
            allow_methods=[""*""],
            allow_headers=[""*""],
        )
        uvicorn.run(
            server_app,
            host=Config().websocket_server_host,
            port=Config().websocket_server_port,
        )","Point(row=168, column=4)","Point(row=181, column=9)",WebsocketServer,autogpt_platform/backend/backend/server/ws_api.py
Methods,class,,"class Methods(enum.Enum):
    SUBSCRIBE = ""subscribe""
    UNSUBSCRIBE = ""unsubscribe""
    EXECUTION_EVENT = ""execution_event""
    ERROR = ""error""","Point(row=8, column=0)","Point(row=12, column=19)",,autogpt_platform/backend/backend/server/model.py
WsMessage,class,,"class WsMessage(pydantic.BaseModel):
    method: Methods
    data: typing.Dict[str, typing.Any] | list[typing.Any] | None = None
    success: bool | None = None
    channel: str | None = None
    error: str | None = None","Point(row=15, column=0)","Point(row=20, column=28)",,autogpt_platform/backend/backend/server/model.py
ExecutionSubscription,class,,"class ExecutionSubscription(pydantic.BaseModel):
    graph_id: str","Point(row=23, column=0)","Point(row=24, column=17)",,autogpt_platform/backend/backend/server/model.py
SubscriptionDetails,class,,"class SubscriptionDetails(pydantic.BaseModel):
    event_type: str
    channel: str
    graph_id: str","Point(row=27, column=0)","Point(row=30, column=17)",,autogpt_platform/backend/backend/server/model.py
CreateGraph,class,,"class CreateGraph(pydantic.BaseModel):
    template_id: str | None = None
    template_version: int | None = None
    graph: backend.data.graph.Graph | None = None","Point(row=33, column=0)","Point(row=36, column=49)",,autogpt_platform/backend/backend/server/model.py
SetGraphActiveVersion,class,,"class SetGraphActiveVersion(pydantic.BaseModel):
    active_graph_version: int","Point(row=39, column=0)","Point(row=40, column=29)",,autogpt_platform/backend/backend/server/model.py
get_user_id,function,,"def get_user_id(payload: dict = Depends(auth_middleware)) -> str:
    if not payload:
        # This handles the case when authentication is disabled
        return DEFAULT_USER_ID

    user_id = payload.get(""sub"")
    if not user_id:
        raise HTTPException(status_code=401, detail=""User ID not found in token"")
    return user_id","Point(row=9, column=0)","Point(row=17, column=18)",,autogpt_platform/backend/backend/server/utils.py
lifespan_context,function,,"async def lifespan_context(app: fastapi.FastAPI):
    await backend.data.db.connect()
    await backend.data.block.initialize_blocks()
    await backend.data.user.migrate_and_encrypt_user_integrations()
    yield
    await backend.data.db.disconnect()","Point(row=21, column=0)","Point(row=26, column=38)",,autogpt_platform/backend/backend/server/rest_api.py
handle_internal_http_error,function,,"def handle_internal_http_error(status_code: int = 500, log_error: bool = True):
    def handler(request: fastapi.Request, exc: Exception):
        if log_error:
            logger.exception(f""{request.method} {request.url.path} failed: {exc}"")
        return fastapi.responses.JSONResponse(
            content={
                ""message"": f""{request.method} {request.url.path} failed"",
                ""detail"": str(exc),
            },
            status_code=status_code,
        )

    return handler","Point(row=29, column=0)","Point(row=41, column=18)",,autogpt_platform/backend/backend/server/rest_api.py
handle_internal_http_error.handler,function,,"def handler(request: fastapi.Request, exc: Exception):
        if log_error:
            logger.exception(f""{request.method} {request.url.path} failed: {exc}"")
        return fastapi.responses.JSONResponse(
            content={
                ""message"": f""{request.method} {request.url.path} failed"",
                ""detail"": str(exc),
            },
            status_code=status_code,
        )","Point(row=30, column=4)","Point(row=39, column=9)",,autogpt_platform/backend/backend/server/rest_api.py
health,function,,"async def health():
    return {""status"": ""healthy""}","Point(row=68, column=0)","Point(row=69, column=32)",,autogpt_platform/backend/backend/server/rest_api.py
AgentServer,class,,"class AgentServer(backend.util.service.AppProcess):
    def run(self):
        server_app = starlette.middleware.cors.CORSMiddleware(
            app=app,
            allow_origins=settings.config.backend_cors_allow_origins,
            allow_credentials=True,
            allow_methods=[""*""],  # Allows all methods
            allow_headers=[""*""],  # Allows all headers
        )
        uvicorn.run(
            server_app,
            host=backend.util.settings.Config().agent_api_host,
            port=backend.util.settings.Config().agent_api_port,
        )

    @staticmethod
    async def test_execute_graph(
        graph_id: str, node_input: dict[typing.Any, typing.Any], user_id: str
    ):
        return await backend.server.routers.v1.execute_graph(
            graph_id, node_input, user_id
        )

    @staticmethod
    async def test_create_graph(
        create_graph: backend.server.routers.v1.CreateGraph,
        user_id: str,
        is_template=False,
    ):
        return await backend.server.routers.v1.create_new_graph(create_graph, user_id)

    @staticmethod
    async def test_get_graph_run_status(
        graph_id: str, graph_exec_id: str, user_id: str
    ):
        return await backend.server.routers.v1.get_graph_run_status(
            graph_id, graph_exec_id, user_id
        )

    @staticmethod
    async def test_get_graph_run_node_execution_results(
        graph_id: str, graph_exec_id: str, user_id: str
    ):
        return await backend.server.routers.v1.get_graph_run_node_execution_results(
            graph_id, graph_exec_id, user_id
        )

    @staticmethod
    async def test_delete_graph(graph_id: str, user_id: str):
        return await backend.server.routers.v1.delete_graph(graph_id, user_id)

    def set_test_dependency_overrides(self, overrides: dict):
        app.dependency_overrides.update(overrides)","Point(row=72, column=0)","Point(row=124, column=50)",,autogpt_platform/backend/backend/server/rest_api.py
AgentServer.run,function,,"def run(self):
        server_app = starlette.middleware.cors.CORSMiddleware(
            app=app,
            allow_origins=settings.config.backend_cors_allow_origins,
            allow_credentials=True,
            allow_methods=[""*""],  # Allows all methods
            allow_headers=[""*""],  # Allows all headers
        )
        uvicorn.run(
            server_app,
            host=backend.util.settings.Config().agent_api_host,
            port=backend.util.settings.Config().agent_api_port,
        )","Point(row=73, column=4)","Point(row=85, column=9)",AgentServer,autogpt_platform/backend/backend/server/rest_api.py
AgentServer.test_execute_graph,function,,"async def test_execute_graph(
        graph_id: str, node_input: dict[typing.Any, typing.Any], user_id: str
    ):
        return await backend.server.routers.v1.execute_graph(
            graph_id, node_input, user_id
        )","Point(row=88, column=4)","Point(row=93, column=9)",AgentServer,autogpt_platform/backend/backend/server/rest_api.py
AgentServer.test_create_graph,function,,"async def test_create_graph(
        create_graph: backend.server.routers.v1.CreateGraph,
        user_id: str,
        is_template=False,
    ):
        return await backend.server.routers.v1.create_new_graph(create_graph, user_id)","Point(row=96, column=4)","Point(row=101, column=86)",AgentServer,autogpt_platform/backend/backend/server/rest_api.py
AgentServer.test_get_graph_run_status,function,,"async def test_get_graph_run_status(
        graph_id: str, graph_exec_id: str, user_id: str
    ):
        return await backend.server.routers.v1.get_graph_run_status(
            graph_id, graph_exec_id, user_id
        )","Point(row=104, column=4)","Point(row=109, column=9)",AgentServer,autogpt_platform/backend/backend/server/rest_api.py
AgentServer.test_get_graph_run_node_execution_results,function,,"async def test_get_graph_run_node_execution_results(
        graph_id: str, graph_exec_id: str, user_id: str
    ):
        return await backend.server.routers.v1.get_graph_run_node_execution_results(
            graph_id, graph_exec_id, user_id
        )","Point(row=112, column=4)","Point(row=117, column=9)",AgentServer,autogpt_platform/backend/backend/server/rest_api.py
AgentServer.test_delete_graph,function,,"async def test_delete_graph(graph_id: str, user_id: str):
        return await backend.server.routers.v1.delete_graph(graph_id, user_id)","Point(row=120, column=4)","Point(row=121, column=78)",AgentServer,autogpt_platform/backend/backend/server/rest_api.py
AgentServer.set_test_dependency_overrides,function,,"def set_test_dependency_overrides(self, overrides: dict):
        app.dependency_overrides.update(overrides)","Point(row=123, column=4)","Point(row=124, column=50)",AgentServer,autogpt_platform/backend/backend/server/rest_api.py
execution_manager_client,function,,"def execution_manager_client() -> ExecutionManager:
    return get_service_client(ExecutionManager)","Point(row=26, column=0)","Point(row=27, column=47)",,autogpt_platform/backend/backend/server/routers/v1.py
execution_scheduler_client,function,,"def execution_scheduler_client() -> ExecutionScheduler:
    return get_service_client(ExecutionScheduler)","Point(row=31, column=0)","Point(row=32, column=49)",,autogpt_platform/backend/backend/server/routers/v1.py
get_or_create_user_route,function,,"async def get_or_create_user_route(user_data: dict = Depends(auth_middleware)):
    user = await get_or_create_user(user_data)
    return user.model_dump()","Point(row=69, column=0)","Point(row=71, column=28)",,autogpt_platform/backend/backend/server/routers/v1.py
get_graph_blocks,function,,"def get_graph_blocks() -> list[dict[Any, Any]]:
    blocks = [block() for block in backend.data.block.get_blocks().values()]
    costs = get_block_costs()
    return [{**b.to_dict(), ""costs"": costs.get(b.id, [])} for b in blocks]","Point(row=80, column=0)","Point(row=83, column=74)",,autogpt_platform/backend/backend/server/routers/v1.py
execute_graph_block,function,,"def execute_graph_block(block_id: str, data: BlockInput) -> CompletedBlockOutput:
    obj = backend.data.block.get_block(block_id)
    if not obj:
        raise HTTPException(status_code=404, detail=f""Block #{block_id} not found."")

    output = defaultdict(list)
    for name, data in obj.execute(data):
        output[name].append(data)
    return output","Point(row=87, column=0)","Point(row=95, column=17)",,autogpt_platform/backend/backend/server/routers/v1.py
get_user_credits,function,,"async def get_user_credits(
    user_id: Annotated[str, Depends(get_user_id)]
) -> dict[str, int]:
    return {""credits"": await _user_credit_model.get_or_refill_credit(user_id)}","Point(row=104, column=0)","Point(row=107, column=78)",,autogpt_platform/backend/backend/server/routers/v1.py
DeleteGraphResponse,class,,"class DeleteGraphResponse(TypedDict):
    version_counts: int","Point(row=115, column=0)","Point(row=116, column=23)",,autogpt_platform/backend/backend/server/routers/v1.py
get_graphs,function,,"async def get_graphs(
    user_id: Annotated[str, Depends(get_user_id)],
    with_runs: bool = False,
) -> list[graph_db.Graph]:
    return await graph_db.get_graphs(
        include_executions=with_runs, filter_by=""active"", user_id=user_id
    )","Point(row=120, column=0)","Point(row=126, column=5)",,autogpt_platform/backend/backend/server/routers/v1.py
get_graph,function,,"async def get_graph(
    graph_id: str,
    user_id: Annotated[str, Depends(get_user_id)],
    version: int | None = None,
    hide_credentials: bool = False,
) -> graph_db.Graph:
    graph = await graph_db.get_graph(
        graph_id, version, user_id=user_id, hide_credentials=hide_credentials
    )
    if not graph:
        raise HTTPException(status_code=404, detail=f""Graph #{graph_id} not found."")
    return graph","Point(row=137, column=0)","Point(row=148, column=16)",,autogpt_platform/backend/backend/server/routers/v1.py
get_graph_all_versions,function,,"async def get_graph_all_versions(
    graph_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> list[graph_db.Graph]:
    graphs = await graph_db.get_graph_all_versions(graph_id, user_id=user_id)
    if not graphs:
        raise HTTPException(status_code=404, detail=f""Graph #{graph_id} not found."")
    return graphs","Point(row=161, column=0)","Point(row=167, column=17)",,autogpt_platform/backend/backend/server/routers/v1.py
delete_graph,function,,"async def delete_graph(
    graph_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> DeleteGraphResponse:
    return {""version_counts"": await graph_db.delete_graph(graph_id, user_id=user_id)}","Point(row=173, column=0)","Point(row=176, column=85)",,autogpt_platform/backend/backend/server/routers/v1.py
update_graph,function,,"async def update_graph(
    graph_id: str,
    graph: graph_db.Graph,
    user_id: Annotated[str, Depends(get_user_id)],
) -> graph_db.Graph:
    # Sanity check
    if graph.id and graph.id != graph_id:
        raise HTTPException(400, detail=""Graph ID does not match ID in URI"")

    # Determine new version
    existing_versions = await graph_db.get_graph_all_versions(graph_id, user_id=user_id)
    if not existing_versions:
        raise HTTPException(404, detail=f""Graph #{graph_id} not found"")
    latest_version_number = max(g.version for g in existing_versions)
    graph.version = latest_version_number + 1

    latest_version_graph = next(
        v for v in existing_versions if v.version == latest_version_number
    )
    if latest_version_graph.is_template != graph.is_template:
        raise HTTPException(
            400, detail=""Changing is_template on an existing graph is forbidden""
        )
    graph.is_active = not graph.is_template
    graph.reassign_ids()

    new_graph_version = await graph_db.create_graph(graph, user_id=user_id)

    if new_graph_version.is_active:
        # Ensure new version is the only active version
        await graph_db.set_graph_active_version(
            graph_id=graph_id, version=new_graph_version.version, user_id=user_id
        )

    return new_graph_version","Point(row=187, column=0)","Point(row=221, column=28)",,autogpt_platform/backend/backend/server/routers/v1.py
create_new_graph,function,,"async def create_new_graph(
    create_graph: CreateGraph, user_id: Annotated[str, Depends(get_user_id)]
) -> graph_db.Graph:
    return await do_create_graph(create_graph, is_template=False, user_id=user_id)","Point(row=227, column=0)","Point(row=230, column=82)",,autogpt_platform/backend/backend/server/routers/v1.py
set_graph_active_version,function,,"async def set_graph_active_version(
    graph_id: str,
    request_body: SetGraphActiveVersion,
    user_id: Annotated[str, Depends(get_user_id)],
):
    new_active_version = request_body.active_graph_version
    if not await graph_db.get_graph(graph_id, new_active_version, user_id=user_id):
        raise HTTPException(404, f""Graph #{graph_id} v{new_active_version} not found"")
    await graph_db.set_graph_active_version(
        graph_id=graph_id,
        version=request_body.active_graph_version,
        user_id=user_id,
    )","Point(row=238, column=0)","Point(row=250, column=5)",,autogpt_platform/backend/backend/server/routers/v1.py
execute_graph,function,,"async def execute_graph(
    graph_id: str,
    node_input: dict[Any, Any],
    user_id: Annotated[str, Depends(get_user_id)],
) -> dict[str, Any]:  # FIXME: add proper return type
    try:
        graph_exec = execution_manager_client().add_execution(
            graph_id, node_input, user_id=user_id
        )
        return {""id"": graph_exec[""graph_exec_id""]}
    except Exception as e:
        msg = e.__str__().encode().decode(""unicode_escape"")
        raise HTTPException(status_code=400, detail=msg)","Point(row=258, column=0)","Point(row=270, column=56)",,autogpt_platform/backend/backend/server/routers/v1.py
stop_graph_run,function,,"async def stop_graph_run(
    graph_exec_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> list[execution_db.ExecutionResult]:
    if not await execution_db.get_graph_execution(graph_exec_id, user_id):
        raise HTTPException(404, detail=f""Agent execution #{graph_exec_id} not found"")

    await asyncio.to_thread(
        lambda: execution_manager_client().cancel_execution(graph_exec_id)
    )

    # Retrieve & return canceled graph execution in its final state
    return await execution_db.get_execution_results(graph_exec_id)","Point(row=278, column=0)","Point(row=289, column=66)",,autogpt_platform/backend/backend/server/routers/v1.py
list_graph_runs,function,,"async def list_graph_runs(
    graph_id: str,
    user_id: Annotated[str, Depends(get_user_id)],
    graph_version: int | None = None,
) -> list[str]:
    graph = await graph_db.get_graph(graph_id, graph_version, user_id=user_id)
    if not graph:
        rev = """" if graph_version is None else f"" v{graph_version}""
        raise HTTPException(
            status_code=404, detail=f""Agent #{graph_id}{rev} not found.""
        )

    return await execution_db.list_executions(graph_id, graph_version)","Point(row=297, column=0)","Point(row=309, column=70)",,autogpt_platform/backend/backend/server/routers/v1.py
get_graph_run_node_execution_results,function,,"async def get_graph_run_node_execution_results(
    graph_id: str,
    graph_exec_id: str,
    user_id: Annotated[str, Depends(get_user_id)],
) -> list[execution_db.ExecutionResult]:
    graph = await graph_db.get_graph(graph_id, user_id=user_id)
    if not graph:
        raise HTTPException(status_code=404, detail=f""Graph #{graph_id} not found."")

    return await execution_db.get_execution_results(graph_exec_id)","Point(row=317, column=0)","Point(row=326, column=66)",,autogpt_platform/backend/backend/server/routers/v1.py
get_graph_run_status,function,,"async def get_graph_run_status(
    graph_id: str,
    graph_exec_id: str,
    user_id: Annotated[str, Depends(get_user_id)],
) -> execution_db.ExecutionStatus:
    graph = await graph_db.get_graph(graph_id, user_id=user_id)
    if not graph:
        raise HTTPException(status_code=404, detail=f""Graph #{graph_id} not found."")

    execution = await execution_db.get_graph_execution(graph_exec_id, user_id)
    if not execution:
        raise HTTPException(
            status_code=404, detail=f""Execution #{graph_exec_id} not found.""
        )

    return execution.executionStatus","Point(row=330, column=0)","Point(row=345, column=36)",,autogpt_platform/backend/backend/server/routers/v1.py
get_templates,function,,"async def get_templates(
    user_id: Annotated[str, Depends(get_user_id)]
) -> list[graph_db.Graph]:
    return await graph_db.get_graphs(filter_by=""template"", user_id=user_id)","Point(row=358, column=0)","Point(row=361, column=75)",,autogpt_platform/backend/backend/server/routers/v1.py
get_template,function,,"async def get_template(graph_id: str, version: int | None = None) -> graph_db.Graph:
    graph = await graph_db.get_graph(graph_id, version, template=True)
    if not graph:
        raise HTTPException(status_code=404, detail=f""Template #{graph_id} not found."")
    return graph","Point(row=369, column=0)","Point(row=373, column=16)",,autogpt_platform/backend/backend/server/routers/v1.py
do_create_graph,function,,"async def do_create_graph(
    create_graph: CreateGraph,
    is_template: bool,
    # user_id doesn't have to be annotated like on other endpoints,
    # because create_graph isn't used directly as an endpoint
    user_id: str,
) -> graph_db.Graph:
    if create_graph.graph:
        graph = create_graph.graph
    elif create_graph.template_id:
        # Create a new graph from a template
        graph = await graph_db.get_graph(
            create_graph.template_id,
            create_graph.template_version,
            template=True,
            user_id=user_id,
        )
        if not graph:
            raise HTTPException(
                400, detail=f""Template #{create_graph.template_id} not found""
            )
        graph.version = 1
    else:
        raise HTTPException(
            status_code=400, detail=""Either graph or template_id must be provided.""
        )

    graph.is_template = is_template
    graph.is_active = not is_template
    graph.reassign_ids(reassign_graph_id=True)

    return await graph_db.create_graph(graph, user_id=user_id)","Point(row=376, column=0)","Point(row=407, column=62)",,autogpt_platform/backend/backend/server/routers/v1.py
create_new_template,function,,"async def create_new_template(
    create_graph: CreateGraph, user_id: Annotated[str, Depends(get_user_id)]
) -> graph_db.Graph:
    return await do_create_graph(create_graph, is_template=True, user_id=user_id)","Point(row=415, column=0)","Point(row=418, column=81)",,autogpt_platform/backend/backend/server/routers/v1.py
create_schedule,function,,"async def create_schedule(
    graph_id: str,
    cron: str,
    input_data: dict[Any, Any],
    user_id: Annotated[str, Depends(get_user_id)],
) -> dict[Any, Any]:
    graph = await graph_db.get_graph(graph_id, user_id=user_id)
    if not graph:
        raise HTTPException(status_code=404, detail=f""Graph #{graph_id} not found."")

    return {
        ""id"": await asyncio.to_thread(
            lambda: execution_scheduler_client().add_execution_schedule(
                graph_id=graph_id,
                graph_version=graph.version,
                cron=cron,
                input_data=input_data,
                user_id=user_id,
            )
        )
    }","Point(row=431, column=0)","Point(row=451, column=5)",,autogpt_platform/backend/backend/server/routers/v1.py
update_schedule,function,,"async def update_schedule(
    schedule_id: str,
    input_data: dict[Any, Any],
    user_id: Annotated[str, Depends(get_user_id)],
) -> dict[Any, Any]:
    is_enabled = input_data.get(""is_enabled"", False)
    execution_scheduler_client().update_schedule(
        schedule_id, is_enabled, user_id=user_id
    )
    return {""id"": schedule_id}","Point(row=459, column=0)","Point(row=468, column=30)",,autogpt_platform/backend/backend/server/routers/v1.py
get_execution_schedules,function,,"async def get_execution_schedules(
    graph_id: str, user_id: Annotated[str, Depends(get_user_id)]
) -> dict[str, str]:
    return execution_scheduler_client().get_execution_schedules(graph_id, user_id)","Point(row=476, column=0)","Point(row=479, column=82)",,autogpt_platform/backend/backend/server/routers/v1.py
update_configuration,function,,"async def update_configuration(
    updated_settings: Annotated[
        Dict[str, Any],
        Body(
            examples=[
                {
                    ""config"": {
                        ""num_graph_workers"": 10,
                        ""num_node_workers"": 10,
                    }
                }
            ]
        ),
    ],
):
    settings = Settings()
    try:
        updated_fields: dict[Any, Any] = {""config"": [], ""secrets"": []}
        for key, value in updated_settings.get(""config"", {}).items():
            if hasattr(settings.config, key):
                setattr(settings.config, key, value)
                updated_fields[""config""].append(key)
        for key, value in updated_settings.get(""secrets"", {}).items():
            if hasattr(settings.secrets, key):
                setattr(settings.secrets, key, value)
                updated_fields[""secrets""].append(key)
        settings.save()
        return {
            ""message"": ""Settings updated successfully"",
            ""updated_fields"": updated_fields,
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))","Point(row=490, column=0)","Point(row=522, column=59)",,autogpt_platform/backend/backend/server/routers/v1.py
log_raw_metric,function,,"async def log_raw_metric(
    user_id: Annotated[str, fastapi.Depends(get_user_id)],
    metric_name: Annotated[str, fastapi.Body(..., embed=True)],
    metric_value: Annotated[float, fastapi.Body(..., embed=True)],
    data_string: Annotated[str, fastapi.Body(..., embed=True)],
):
    result = await backend.data.analytics.log_raw_metric(
        user_id=user_id,
        metric_name=metric_name,
        metric_value=metric_value,
        data_string=data_string,
    )
    return result.id","Point(row=13, column=0)","Point(row=25, column=20)",,autogpt_platform/backend/backend/server/routers/analytics.py
log_raw_analytics,function,,"async def log_raw_analytics(
    user_id: Annotated[str, fastapi.Depends(get_user_id)],
    type: Annotated[str, fastapi.Body(..., embed=True)],
    data: Annotated[
        dict,
        fastapi.Body(..., embed=True, description=""The data to log""),
    ],
    data_index: Annotated[
        str,
        fastapi.Body(
            ...,
            embed=True,
            description=""Indexable field for any count based analytical measures like page order clicking, tutorial step completion, etc."",
        ),
    ],
):
    result = await backend.data.analytics.log_raw_analytics(
        user_id, type, data, data_index
    )
    return result.id","Point(row=29, column=0)","Point(row=48, column=20)",,autogpt_platform/backend/backend/server/routers/analytics.py
get_supabase,function,,"def get_supabase() -> Client:
    return create_client(
        settings.secrets.supabase_url, settings.secrets.supabase_service_role_key
    )","Point(row=7, column=0)","Point(row=10, column=5)",,autogpt_platform/backend/backend/server/integrations/utils.py
LoginResponse,class,,"class LoginResponse(BaseModel):
    login_url: str
    state_token: str","Point(row=25, column=0)","Point(row=27, column=20)",,autogpt_platform/backend/backend/server/integrations/router.py
login,function,,"def login(
    provider: Annotated[str, Path(title=""The provider to initiate an OAuth flow for"")],
    user_id: Annotated[str, Depends(get_user_id)],
    request: Request,
    scopes: Annotated[
        str, Query(title=""Comma-separated list of authorization scopes"")
    ] = """",
) -> LoginResponse:
    handler = _get_provider_oauth_handler(request, provider)

    requested_scopes = scopes.split("","") if scopes else []

    # Generate and store a secure random state token along with the scopes
    state_token = creds_manager.store.store_state_token(
        user_id, provider, requested_scopes
    )

    login_url = handler.get_login_url(requested_scopes, state_token)

    return LoginResponse(login_url=login_url, state_token=state_token)","Point(row=31, column=0)","Point(row=50, column=70)",,autogpt_platform/backend/backend/server/integrations/router.py
CredentialsMetaResponse,class,,"class CredentialsMetaResponse(BaseModel):
    id: str
    type: CredentialsType
    title: str | None
    scopes: list[str] | None
    username: str | None","Point(row=53, column=0)","Point(row=58, column=24)",,autogpt_platform/backend/backend/server/integrations/router.py
callback,function,,"def callback(
    provider: Annotated[str, Path(title=""The target provider for this OAuth exchange"")],
    code: Annotated[str, Body(title=""Authorization code acquired by user login"")],
    state_token: Annotated[str, Body(title=""Anti-CSRF nonce"")],
    user_id: Annotated[str, Depends(get_user_id)],
    request: Request,
) -> CredentialsMetaResponse:
    logger.debug(f""Received OAuth callback for provider: {provider}"")
    handler = _get_provider_oauth_handler(request, provider)

    # Verify the state token
    if not creds_manager.store.verify_state_token(user_id, state_token, provider):
        logger.warning(f""Invalid or expired state token for user {user_id}"")
        raise HTTPException(status_code=400, detail=""Invalid or expired state token"")

    try:
        scopes = creds_manager.store.get_any_valid_scopes_from_state_token(
            user_id, state_token, provider
        )
        logger.debug(f""Retrieved scopes from state token: {scopes}"")

        scopes = handler.handle_default_scopes(scopes)

        credentials = handler.exchange_code_for_tokens(code, scopes)
        logger.debug(f""Received credentials with final scopes: {credentials.scopes}"")

        # Check if the granted scopes are sufficient for the requested scopes
        if not set(scopes).issubset(set(credentials.scopes)):
            # For now, we'll just log the warning and continue
            logger.warning(
                f""Granted scopes {credentials.scopes} for {provider}do not include all requested scopes {scopes}""
            )

    except Exception as e:
        logger.error(f""Code->Token exchange failed for provider {provider}: {e}"")
        raise HTTPException(
            status_code=400, detail=f""Failed to exchange code for tokens: {str(e)}""
        )

    # TODO: Allow specifying `title` to set on `credentials`
    creds_manager.create(user_id, credentials)

    logger.debug(
        f""Successfully processed OAuth callback for user {user_id} and provider {provider}""
    )
    return CredentialsMetaResponse(
        id=credentials.id,
        type=credentials.type,
        title=credentials.title,
        scopes=credentials.scopes,
        username=credentials.username,
    )","Point(row=62, column=0)","Point(row=113, column=5)",,autogpt_platform/backend/backend/server/integrations/router.py
list_credentials,function,,"def list_credentials(
    provider: Annotated[str, Path(title=""The provider to list credentials for"")],
    user_id: Annotated[str, Depends(get_user_id)],
) -> list[CredentialsMetaResponse]:
    credentials = creds_manager.store.get_creds_by_provider(user_id, provider)
    return [
        CredentialsMetaResponse(
            id=cred.id,
            type=cred.type,
            title=cred.title,
            scopes=cred.scopes if isinstance(cred, OAuth2Credentials) else None,
            username=cred.username if isinstance(cred, OAuth2Credentials) else None,
        )
        for cred in credentials
    ]","Point(row=117, column=0)","Point(row=131, column=5)",,autogpt_platform/backend/backend/server/integrations/router.py
get_credential,function,,"def get_credential(
    provider: Annotated[str, Path(title=""The provider to retrieve credentials for"")],
    cred_id: Annotated[str, Path(title=""The ID of the credentials to retrieve"")],
    user_id: Annotated[str, Depends(get_user_id)],
) -> Credentials:
    credential = creds_manager.get(user_id, cred_id)
    if not credential:
        raise HTTPException(status_code=404, detail=""Credentials not found"")
    if credential.provider != provider:
        raise HTTPException(
            status_code=404, detail=""Credentials do not match the specified provider""
        )
    return credential","Point(row=135, column=0)","Point(row=147, column=21)",,autogpt_platform/backend/backend/server/integrations/router.py
create_api_key_credentials,function,,"def create_api_key_credentials(
    user_id: Annotated[str, Depends(get_user_id)],
    provider: Annotated[str, Path(title=""The provider to create credentials for"")],
    api_key: Annotated[str, Body(title=""The API key to store"")],
    title: Annotated[str, Body(title=""Optional title for the credentials"")],
    expires_at: Annotated[
        int | None, Body(title=""Unix timestamp when the key expires"")
    ] = None,
) -> APIKeyCredentials:
    new_credentials = APIKeyCredentials(
        provider=provider,
        api_key=SecretStr(api_key),
        title=title,
        expires_at=expires_at,
    )

    try:
        creds_manager.create(user_id, new_credentials)
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f""Failed to store credentials: {str(e)}""
        )
    return new_credentials","Point(row=151, column=0)","Point(row=173, column=26)",,autogpt_platform/backend/backend/server/integrations/router.py
CredentialsDeletionResponse,class,,"class CredentialsDeletionResponse(BaseModel):
    deleted: Literal[True] = True
    revoked: bool | None = Field(
        description=""Indicates whether the credentials were also revoked by their ""
        ""provider. `None`/`null` if not applicable, e.g. when deleting ""
        ""non-revocable credentials such as API keys.""
    )","Point(row=176, column=0)","Point(row=182, column=5)",,autogpt_platform/backend/backend/server/integrations/router.py
delete_credentials,function,,"def delete_credentials(
    request: Request,
    provider: Annotated[str, Path(title=""The provider to delete credentials for"")],
    cred_id: Annotated[str, Path(title=""The ID of the credentials to delete"")],
    user_id: Annotated[str, Depends(get_user_id)],
) -> CredentialsDeletionResponse:
    creds = creds_manager.store.get_creds_by_id(user_id, cred_id)
    if not creds:
        raise HTTPException(status_code=404, detail=""Credentials not found"")
    if creds.provider != provider:
        raise HTTPException(
            status_code=404, detail=""Credentials do not match the specified provider""
        )

    creds_manager.delete(user_id, cred_id)

    tokens_revoked = None
    if isinstance(creds, OAuth2Credentials):
        handler = _get_provider_oauth_handler(request, provider)
        tokens_revoked = handler.revoke_tokens(creds)

    return CredentialsDeletionResponse(revoked=tokens_revoked)","Point(row=186, column=0)","Point(row=207, column=62)",,autogpt_platform/backend/backend/server/integrations/router.py
_get_provider_oauth_handler,function,,"def _get_provider_oauth_handler(req: Request, provider_name: str) -> BaseOAuthHandler:
    if provider_name not in HANDLERS_BY_NAME:
        raise HTTPException(
            status_code=404, detail=f""Unknown provider '{provider_name}'""
        )

    client_id = getattr(settings.secrets, f""{provider_name}_client_id"")
    client_secret = getattr(settings.secrets, f""{provider_name}_client_secret"")
    if not (client_id and client_secret):
        raise HTTPException(
            status_code=501,
            detail=f""Integration with provider '{provider_name}' is not configured"",
        )

    handler_class = HANDLERS_BY_NAME[provider_name]
    frontend_base_url = settings.config.frontend_base_url or str(req.base_url)
    return handler_class(
        client_id=client_id,
        client_secret=client_secret,
        redirect_uri=f""{frontend_base_url}/auth/integrations/oauth_callback"",
    )","Point(row=213, column=0)","Point(row=233, column=5)",,autogpt_platform/backend/backend/server/integrations/router.py
IntegrationCredentialsManager,class,"
    Handles the lifecycle of integration credentials.
    - Automatically refreshes requested credentials if needed.
    - Uses locking mechanisms to ensure system-wide consistency and
      prevent invalidation of in-use tokens.

    ### ‚ö†Ô∏è Gotcha
    With `acquire(..)`, credentials can only be in use in one place at a time (e.g. one
    block execution).

    ### Locking mechanism
    - Because *getting* credentials can result in a refresh (= *invalidation* +
      *replacement*) of the stored credentials, *getting* is an operation that
      potentially requires read/write access.
    - Checking whether a token has to be refreshed is subject to an additional `refresh`
      scoped lock to prevent unnecessary sequential refreshes when multiple executions
      try to access the same credentials simultaneously.
    - We MUST lock credentials while in use to prevent them from being invalidated while
      they are in use, e.g. because they are being refreshed by a different part
      of the system.
    - The `!time_sensitive` lock in `acquire(..)` is part of a two-tier locking
      mechanism in which *updating* gets priority over *getting* credentials.
      This is to prevent a long queue of waiting *get* requests from blocking essential
      credential refreshes or user-initiated updates.

    It is possible to implement a reader/writer locking system where either multiple
    readers or a single writer can have simultaneous access, but this would add a lot of
    complexity to the mechanism. I don't expect the current (""simple"") mechanism to
    cause so much latency that it's worth implementing.
","class IntegrationCredentialsManager:
    """"""
    Handles the lifecycle of integration credentials.
    - Automatically refreshes requested credentials if needed.
    - Uses locking mechanisms to ensure system-wide consistency and
      prevent invalidation of in-use tokens.

    ### ‚ö†Ô∏è Gotcha
    With `acquire(..)`, credentials can only be in use in one place at a time (e.g. one
    block execution).

    ### Locking mechanism
    - Because *getting* credentials can result in a refresh (= *invalidation* +
      *replacement*) of the stored credentials, *getting* is an operation that
      potentially requires read/write access.
    - Checking whether a token has to be refreshed is subject to an additional `refresh`
      scoped lock to prevent unnecessary sequential refreshes when multiple executions
      try to access the same credentials simultaneously.
    - We MUST lock credentials while in use to prevent them from being invalidated while
      they are in use, e.g. because they are being refreshed by a different part
      of the system.
    - The `!time_sensitive` lock in `acquire(..)` is part of a two-tier locking
      mechanism in which *updating* gets priority over *getting* credentials.
      This is to prevent a long queue of waiting *get* requests from blocking essential
      credential refreshes or user-initiated updates.

    It is possible to implement a reader/writer locking system where either multiple
    readers or a single writer can have simultaneous access, but this would add a lot of
    complexity to the mechanism. I don't expect the current (""simple"") mechanism to
    cause so much latency that it's worth implementing.
    """"""

    def __init__(self):
        redis_conn = redis.get_redis()
        self._locks = RedisKeyedMutex(redis_conn)
        self.store = SupabaseIntegrationCredentialsStore(redis=redis_conn)

    def create(self, user_id: str, credentials: Credentials) -> None:
        return self.store.add_creds(user_id, credentials)

    def exists(self, user_id: str, credentials_id: str) -> bool:
        return self.store.get_creds_by_id(user_id, credentials_id) is not None

    def get(
        self, user_id: str, credentials_id: str, lock: bool = True
    ) -> Credentials | None:
        credentials = self.store.get_creds_by_id(user_id, credentials_id)
        if not credentials:
            return None

        # Refresh OAuth credentials if needed
        if credentials.type == ""oauth2"" and credentials.access_token_expires_at:
            logger.debug(
                f""Credentials #{credentials.id} expire at ""
                f""{datetime.fromtimestamp(credentials.access_token_expires_at)}; ""
                f""current time is {datetime.now()}""
            )

            with self._locked(user_id, credentials_id, ""refresh""):
                oauth_handler = _get_provider_oauth_handler(credentials.provider)
                if oauth_handler.needs_refresh(credentials):
                    logger.debug(
                        f""Refreshing '{credentials.provider}' ""
                        f""credentials #{credentials.id}""
                    )
                    _lock = None
                    if lock:
                        # Wait until the credentials are no longer in use anywhere
                        _lock = self._acquire_lock(user_id, credentials_id)

                    fresh_credentials = oauth_handler.refresh_tokens(credentials)
                    self.store.update_creds(user_id, fresh_credentials)
                    if _lock:
                        _lock.release()

                    credentials = fresh_credentials
        else:
            logger.debug(f""Credentials #{credentials.id} never expire"")

        return credentials

    def acquire(
        self, user_id: str, credentials_id: str
    ) -> tuple[Credentials, RedisLock]:
        """"""
        ‚ö†Ô∏è WARNING: this locks credentials system-wide and blocks both acquiring
        and updating them elsewhere until the lock is released.
        See the class docstring for more info.
        """"""
        # Use a low-priority (!time_sensitive) locking queue on top of the general lock
        # to allow priority access for refreshing/updating the tokens.
        with self._locked(user_id, credentials_id, ""!time_sensitive""):
            lock = self._acquire_lock(user_id, credentials_id)
        credentials = self.get(user_id, credentials_id, lock=False)
        if not credentials:
            raise ValueError(
                f""Credentials #{credentials_id} for user #{user_id} not found""
            )
        return credentials, lock

    def update(self, user_id: str, updated: Credentials) -> None:
        with self._locked(user_id, updated.id):
            self.store.update_creds(user_id, updated)

    def delete(self, user_id: str, credentials_id: str) -> None:
        with self._locked(user_id, credentials_id):
            self.store.delete_creds_by_id(user_id, credentials_id)

    # -- Locking utilities -- #

    def _acquire_lock(self, user_id: str, credentials_id: str, *args: str) -> RedisLock:
        key = (
            self.store.db_manager,
            f""user:{user_id}"",
            f""credentials:{credentials_id}"",
            *args,
        )
        return self._locks.acquire(key)

    @contextmanager
    def _locked(self, user_id: str, credentials_id: str, *args: str):
        lock = self._acquire_lock(user_id, credentials_id, *args)
        try:
            yield
        finally:
            lock.release()

    def release_all_locks(self):
        """"""Call this on process termination to ensure all locks are released""""""
        self._locks.release_all_locks()
        self.store.locks.release_all_locks()","Point(row=19, column=0)","Point(row=149, column=44)",,autogpt_platform/backend/backend/integrations/creds_manager.py
IntegrationCredentialsManager.__init__,function,,"def __init__(self):
        redis_conn = redis.get_redis()
        self._locks = RedisKeyedMutex(redis_conn)
        self.store = SupabaseIntegrationCredentialsStore(redis=redis_conn)","Point(row=51, column=4)","Point(row=54, column=74)",IntegrationCredentialsManager,autogpt_platform/backend/backend/integrations/creds_manager.py
IntegrationCredentialsManager.create,function,,"def create(self, user_id: str, credentials: Credentials) -> None:
        return self.store.add_creds(user_id, credentials)","Point(row=56, column=4)","Point(row=57, column=57)",IntegrationCredentialsManager,autogpt_platform/backend/backend/integrations/creds_manager.py
IntegrationCredentialsManager.exists,function,,"def exists(self, user_id: str, credentials_id: str) -> bool:
        return self.store.get_creds_by_id(user_id, credentials_id) is not None","Point(row=59, column=4)","Point(row=60, column=78)",IntegrationCredentialsManager,autogpt_platform/backend/backend/integrations/creds_manager.py
IntegrationCredentialsManager.get,function,,"def get(
        self, user_id: str, credentials_id: str, lock: bool = True
    ) -> Credentials | None:
        credentials = self.store.get_creds_by_id(user_id, credentials_id)
        if not credentials:
            return None

        # Refresh OAuth credentials if needed
        if credentials.type == ""oauth2"" and credentials.access_token_expires_at:
            logger.debug(
                f""Credentials #{credentials.id} expire at ""
                f""{datetime.fromtimestamp(credentials.access_token_expires_at)}; ""
                f""current time is {datetime.now()}""
            )

            with self._locked(user_id, credentials_id, ""refresh""):
                oauth_handler = _get_provider_oauth_handler(credentials.provider)
                if oauth_handler.needs_refresh(credentials):
                    logger.debug(
                        f""Refreshing '{credentials.provider}' ""
                        f""credentials #{credentials.id}""
                    )
                    _lock = None
                    if lock:
                        # Wait until the credentials are no longer in use anywhere
                        _lock = self._acquire_lock(user_id, credentials_id)

                    fresh_credentials = oauth_handler.refresh_tokens(credentials)
                    self.store.update_creds(user_id, fresh_credentials)
                    if _lock:
                        _lock.release()

                    credentials = fresh_credentials
        else:
            logger.debug(f""Credentials #{credentials.id} never expire"")

        return credentials","Point(row=62, column=4)","Point(row=98, column=26)",IntegrationCredentialsManager,autogpt_platform/backend/backend/integrations/creds_manager.py
IntegrationCredentialsManager.acquire,function,"
        ‚ö†Ô∏è WARNING: this locks credentials system-wide and blocks both acquiring
        and updating them elsewhere until the lock is released.
        See the class docstring for more info.
","def acquire(
        self, user_id: str, credentials_id: str
    ) -> tuple[Credentials, RedisLock]:
        """"""
        ‚ö†Ô∏è WARNING: this locks credentials system-wide and blocks both acquiring
        and updating them elsewhere until the lock is released.
        See the class docstring for more info.
        """"""
        # Use a low-priority (!time_sensitive) locking queue on top of the general lock
        # to allow priority access for refreshing/updating the tokens.
        with self._locked(user_id, credentials_id, ""!time_sensitive""):
            lock = self._acquire_lock(user_id, credentials_id)
        credentials = self.get(user_id, credentials_id, lock=False)
        if not credentials:
            raise ValueError(
                f""Credentials #{credentials_id} for user #{user_id} not found""
            )
        return credentials, lock","Point(row=100, column=4)","Point(row=117, column=32)",IntegrationCredentialsManager,autogpt_platform/backend/backend/integrations/creds_manager.py
IntegrationCredentialsManager.update,function,,"def update(self, user_id: str, updated: Credentials) -> None:
        with self._locked(user_id, updated.id):
            self.store.update_creds(user_id, updated)","Point(row=119, column=4)","Point(row=121, column=53)",IntegrationCredentialsManager,autogpt_platform/backend/backend/integrations/creds_manager.py
IntegrationCredentialsManager.delete,function,,"def delete(self, user_id: str, credentials_id: str) -> None:
        with self._locked(user_id, credentials_id):
            self.store.delete_creds_by_id(user_id, credentials_id)","Point(row=123, column=4)","Point(row=125, column=66)",IntegrationCredentialsManager,autogpt_platform/backend/backend/integrations/creds_manager.py
IntegrationCredentialsManager._acquire_lock,function,,"def _acquire_lock(self, user_id: str, credentials_id: str, *args: str) -> RedisLock:
        key = (
            self.store.db_manager,
            f""user:{user_id}"",
            f""credentials:{credentials_id}"",
            *args,
        )
        return self._locks.acquire(key)","Point(row=129, column=4)","Point(row=136, column=39)",IntegrationCredentialsManager,autogpt_platform/backend/backend/integrations/creds_manager.py
IntegrationCredentialsManager._locked,function,,"def _locked(self, user_id: str, credentials_id: str, *args: str):
        lock = self._acquire_lock(user_id, credentials_id, *args)
        try:
            yield
        finally:
            lock.release()","Point(row=139, column=4)","Point(row=144, column=26)",IntegrationCredentialsManager,autogpt_platform/backend/backend/integrations/creds_manager.py
IntegrationCredentialsManager.release_all_locks,function,Call this on process termination to ensure all locks are released,"def release_all_locks(self):
        """"""Call this on process termination to ensure all locks are released""""""
        self._locks.release_all_locks()
        self.store.locks.release_all_locks()","Point(row=146, column=4)","Point(row=149, column=44)",IntegrationCredentialsManager,autogpt_platform/backend/backend/integrations/creds_manager.py
_get_provider_oauth_handler,function,,"def _get_provider_oauth_handler(provider_name: str) -> BaseOAuthHandler:
    if provider_name not in HANDLERS_BY_NAME:
        raise KeyError(f""Unknown provider '{provider_name}'"")

    client_id = getattr(settings.secrets, f""{provider_name}_client_id"")
    client_secret = getattr(settings.secrets, f""{provider_name}_client_secret"")
    if not (client_id and client_secret):
        raise Exception(  # TODO: ConfigError
            f""Integration with provider '{provider_name}' is not configured"",
        )

    handler_class = HANDLERS_BY_NAME[provider_name]
    frontend_base_url = settings.config.frontend_base_url
    return handler_class(
        client_id=client_id,
        client_secret=client_secret,
        redirect_uri=f""{frontend_base_url}/auth/integrations/oauth_callback"",
    )","Point(row=152, column=0)","Point(row=169, column=5)",,autogpt_platform/backend/backend/integrations/creds_manager.py
GoogleOAuthHandler,class,"
    Based on the documentation at https://developers.google.com/identity/protocols/oauth2/web-server
","class GoogleOAuthHandler(BaseOAuthHandler):
    """"""
    Based on the documentation at https://developers.google.com/identity/protocols/oauth2/web-server
    """"""  # noqa

    PROVIDER_NAME = ""google""
    EMAIL_ENDPOINT = ""https://www.googleapis.com/oauth2/v2/userinfo""
    DEFAULT_SCOPES = [
        ""https://www.googleapis.com/auth/userinfo.email"",
        ""https://www.googleapis.com/auth/userinfo.profile"",
        ""openid"",
    ]
    # --8<-- [end:GoogleOAuthHandlerExample]

    def __init__(self, client_id: str, client_secret: str, redirect_uri: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.redirect_uri = redirect_uri
        self.token_uri = ""https://oauth2.googleapis.com/token""
        self.revoke_uri = ""https://oauth2.googleapis.com/revoke""

    def get_login_url(self, scopes: list[str], state: str) -> str:
        all_scopes = list(set(scopes + self.DEFAULT_SCOPES))
        logger.debug(f""Setting up OAuth flow with scopes: {all_scopes}"")
        flow = self._setup_oauth_flow(all_scopes)
        flow.redirect_uri = self.redirect_uri
        authorization_url, _ = flow.authorization_url(
            access_type=""offline"",
            include_granted_scopes=""true"",
            state=state,
            prompt=""consent"",
        )
        return authorization_url

    def exchange_code_for_tokens(
        self, code: str, scopes: list[str]
    ) -> OAuth2Credentials:
        logger.debug(f""Exchanging code for tokens with scopes: {scopes}"")

        # Use the scopes from the initial request
        flow = self._setup_oauth_flow(scopes)
        flow.redirect_uri = self.redirect_uri

        logger.debug(""Fetching token from Google"")

        # Disable scope check in fetch_token
        flow.oauth2session.scope = None
        token = flow.fetch_token(code=code)
        logger.debug(""Token fetched successfully"")

        # Get the actual scopes granted by Google
        granted_scopes: list[str] = token.get(""scope"", [])

        logger.debug(f""Scopes granted by Google: {granted_scopes}"")

        google_creds = flow.credentials
        logger.debug(f""Received credentials: {google_creds}"")

        logger.debug(""Requesting user email"")
        username = self._request_email(google_creds)
        logger.debug(f""User email retrieved: {username}"")

        assert google_creds.token
        assert google_creds.refresh_token
        assert google_creds.expiry
        assert granted_scopes

        # Create OAuth2Credentials with the granted scopes
        credentials = OAuth2Credentials(
            provider=self.PROVIDER_NAME,
            title=None,
            username=username,
            access_token=SecretStr(google_creds.token),
            refresh_token=(SecretStr(google_creds.refresh_token)),
            access_token_expires_at=(
                int(google_creds.expiry.timestamp()) if google_creds.expiry else None
            ),
            refresh_token_expires_at=None,
            scopes=granted_scopes,
        )
        logger.debug(
            f""OAuth2Credentials object created successfully with scopes: {credentials.scopes}""
        )

        return credentials

    def revoke_tokens(self, credentials: OAuth2Credentials) -> bool:
        session = AuthorizedSession(credentials)
        session.post(
            self.revoke_uri,
            params={""token"": credentials.access_token.get_secret_value()},
            headers={""content-type"": ""application/x-www-form-urlencoded""},
        )
        return True

    def _request_email(
        self, creds: Credentials | ExternalAccountCredentials
    ) -> str | None:
        session = AuthorizedSession(creds)
        response = session.get(self.EMAIL_ENDPOINT)
        if not response.ok:
            logger.error(
                f""Failed to get user email. Status code: {response.status_code}""
            )
            return None
        return response.json()[""email""]

    def _refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        # Google credentials should ALWAYS have a refresh token
        assert credentials.refresh_token

        google_creds = Credentials(
            token=credentials.access_token.get_secret_value(),
            refresh_token=credentials.refresh_token.get_secret_value(),
            token_uri=self.token_uri,
            client_id=self.client_id,
            client_secret=self.client_secret,
            scopes=credentials.scopes,
        )
        # Google's OAuth library is poorly typed so we need some of these:
        assert google_creds.refresh_token
        assert google_creds.scopes

        google_creds.refresh(Request())
        assert google_creds.expiry

        return OAuth2Credentials(
            provider=self.PROVIDER_NAME,
            id=credentials.id,
            title=credentials.title,
            username=credentials.username,
            access_token=SecretStr(google_creds.token),
            refresh_token=SecretStr(google_creds.refresh_token),
            access_token_expires_at=int(google_creds.expiry.timestamp()),
            refresh_token_expires_at=None,
            scopes=google_creds.scopes,
        )

    def _setup_oauth_flow(self, scopes: list[str]) -> Flow:
        return Flow.from_client_config(
            {
                ""web"": {
                    ""client_id"": self.client_id,
                    ""client_secret"": self.client_secret,
                    ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",
                    ""token_uri"": self.token_uri,
                }
            },
            scopes=scopes,
        )","Point(row=17, column=0)","Point(row=166, column=9)",,autogpt_platform/backend/backend/integrations/oauth/google.py
GoogleOAuthHandler.__init__,function,,"def __init__(self, client_id: str, client_secret: str, redirect_uri: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.redirect_uri = redirect_uri
        self.token_uri = ""https://oauth2.googleapis.com/token""
        self.revoke_uri = ""https://oauth2.googleapis.com/revoke""","Point(row=31, column=4)","Point(row=36, column=64)",GoogleOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/google.py
GoogleOAuthHandler.get_login_url,function,,"def get_login_url(self, scopes: list[str], state: str) -> str:
        all_scopes = list(set(scopes + self.DEFAULT_SCOPES))
        logger.debug(f""Setting up OAuth flow with scopes: {all_scopes}"")
        flow = self._setup_oauth_flow(all_scopes)
        flow.redirect_uri = self.redirect_uri
        authorization_url, _ = flow.authorization_url(
            access_type=""offline"",
            include_granted_scopes=""true"",
            state=state,
            prompt=""consent"",
        )
        return authorization_url","Point(row=38, column=4)","Point(row=49, column=32)",GoogleOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/google.py
GoogleOAuthHandler.exchange_code_for_tokens,function,,"def exchange_code_for_tokens(
        self, code: str, scopes: list[str]
    ) -> OAuth2Credentials:
        logger.debug(f""Exchanging code for tokens with scopes: {scopes}"")

        # Use the scopes from the initial request
        flow = self._setup_oauth_flow(scopes)
        flow.redirect_uri = self.redirect_uri

        logger.debug(""Fetching token from Google"")

        # Disable scope check in fetch_token
        flow.oauth2session.scope = None
        token = flow.fetch_token(code=code)
        logger.debug(""Token fetched successfully"")

        # Get the actual scopes granted by Google
        granted_scopes: list[str] = token.get(""scope"", [])

        logger.debug(f""Scopes granted by Google: {granted_scopes}"")

        google_creds = flow.credentials
        logger.debug(f""Received credentials: {google_creds}"")

        logger.debug(""Requesting user email"")
        username = self._request_email(google_creds)
        logger.debug(f""User email retrieved: {username}"")

        assert google_creds.token
        assert google_creds.refresh_token
        assert google_creds.expiry
        assert granted_scopes

        # Create OAuth2Credentials with the granted scopes
        credentials = OAuth2Credentials(
            provider=self.PROVIDER_NAME,
            title=None,
            username=username,
            access_token=SecretStr(google_creds.token),
            refresh_token=(SecretStr(google_creds.refresh_token)),
            access_token_expires_at=(
                int(google_creds.expiry.timestamp()) if google_creds.expiry else None
            ),
            refresh_token_expires_at=None,
            scopes=granted_scopes,
        )
        logger.debug(
            f""OAuth2Credentials object created successfully with scopes: {credentials.scopes}""
        )

        return credentials","Point(row=51, column=4)","Point(row=101, column=26)",GoogleOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/google.py
GoogleOAuthHandler.revoke_tokens,function,,"def revoke_tokens(self, credentials: OAuth2Credentials) -> bool:
        session = AuthorizedSession(credentials)
        session.post(
            self.revoke_uri,
            params={""token"": credentials.access_token.get_secret_value()},
            headers={""content-type"": ""application/x-www-form-urlencoded""},
        )
        return True","Point(row=103, column=4)","Point(row=110, column=19)",GoogleOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/google.py
GoogleOAuthHandler._request_email,function,,"def _request_email(
        self, creds: Credentials | ExternalAccountCredentials
    ) -> str | None:
        session = AuthorizedSession(creds)
        response = session.get(self.EMAIL_ENDPOINT)
        if not response.ok:
            logger.error(
                f""Failed to get user email. Status code: {response.status_code}""
            )
            return None
        return response.json()[""email""]","Point(row=112, column=4)","Point(row=122, column=39)",GoogleOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/google.py
GoogleOAuthHandler._refresh_tokens,function,,"def _refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        # Google credentials should ALWAYS have a refresh token
        assert credentials.refresh_token

        google_creds = Credentials(
            token=credentials.access_token.get_secret_value(),
            refresh_token=credentials.refresh_token.get_secret_value(),
            token_uri=self.token_uri,
            client_id=self.client_id,
            client_secret=self.client_secret,
            scopes=credentials.scopes,
        )
        # Google's OAuth library is poorly typed so we need some of these:
        assert google_creds.refresh_token
        assert google_creds.scopes

        google_creds.refresh(Request())
        assert google_creds.expiry

        return OAuth2Credentials(
            provider=self.PROVIDER_NAME,
            id=credentials.id,
            title=credentials.title,
            username=credentials.username,
            access_token=SecretStr(google_creds.token),
            refresh_token=SecretStr(google_creds.refresh_token),
            access_token_expires_at=int(google_creds.expiry.timestamp()),
            refresh_token_expires_at=None,
            scopes=google_creds.scopes,
        )","Point(row=124, column=4)","Point(row=153, column=9)",GoogleOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/google.py
GoogleOAuthHandler._setup_oauth_flow,function,,"def _setup_oauth_flow(self, scopes: list[str]) -> Flow:
        return Flow.from_client_config(
            {
                ""web"": {
                    ""client_id"": self.client_id,
                    ""client_secret"": self.client_secret,
                    ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",
                    ""token_uri"": self.token_uri,
                }
            },
            scopes=scopes,
        )","Point(row=155, column=4)","Point(row=166, column=9)",GoogleOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/google.py
NotionOAuthHandler,class,"
    Based on the documentation at https://developers.notion.com/docs/authorization

    Notes:
    - Notion uses non-expiring access tokens and therefore doesn't have a refresh flow
    - Notion doesn't use scopes
","class NotionOAuthHandler(BaseOAuthHandler):
    """"""
    Based on the documentation at https://developers.notion.com/docs/authorization

    Notes:
    - Notion uses non-expiring access tokens and therefore doesn't have a refresh flow
    - Notion doesn't use scopes
    """"""

    PROVIDER_NAME = ""notion""

    def __init__(self, client_id: str, client_secret: str, redirect_uri: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.redirect_uri = redirect_uri
        self.auth_base_url = ""https://api.notion.com/v1/oauth/authorize""
        self.token_url = ""https://api.notion.com/v1/oauth/token""

    def get_login_url(self, scopes: list[str], state: str) -> str:
        params = {
            ""client_id"": self.client_id,
            ""redirect_uri"": self.redirect_uri,
            ""response_type"": ""code"",
            ""owner"": ""user"",
            ""state"": state,
        }
        return f""{self.auth_base_url}?{urlencode(params)}""

    def exchange_code_for_tokens(
        self, code: str, scopes: list[str]
    ) -> OAuth2Credentials:
        request_body = {
            ""grant_type"": ""authorization_code"",
            ""code"": code,
            ""redirect_uri"": self.redirect_uri,
        }
        auth_str = b64encode(f""{self.client_id}:{self.client_secret}"".encode()).decode()
        headers = {
            ""Authorization"": f""Basic {auth_str}"",
            ""Accept"": ""application/json"",
        }
        response = requests.post(self.token_url, json=request_body, headers=headers)
        token_data = response.json()
        # Email is only available for non-bot users
        email = (
            token_data[""owner""][""person""][""email""]
            if ""person"" in token_data[""owner""]
            and ""email"" in token_data[""owner""][""person""]
            else None
        )

        return OAuth2Credentials(
            provider=self.PROVIDER_NAME,
            title=token_data.get(""workspace_name""),
            username=email,
            access_token=token_data[""access_token""],
            refresh_token=None,
            access_token_expires_at=None,  # Notion tokens don't expire
            refresh_token_expires_at=None,
            scopes=[],
            metadata={
                ""owner"": token_data[""owner""],
                ""bot_id"": token_data[""bot_id""],
                ""workspace_id"": token_data[""workspace_id""],
                ""workspace_name"": token_data.get(""workspace_name""),
                ""workspace_icon"": token_data.get(""workspace_icon""),
            },
        )

    def revoke_tokens(self, credentials: OAuth2Credentials) -> bool:
        # Notion doesn't support token revocation
        return False

    def _refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        # Notion doesn't support token refresh
        return credentials

    def needs_refresh(self, credentials: OAuth2Credentials) -> bool:
        # Notion access tokens don't expire
        return False","Point(row=10, column=0)","Point(row=89, column=20)",,autogpt_platform/backend/backend/integrations/oauth/notion.py
NotionOAuthHandler.__init__,function,,"def __init__(self, client_id: str, client_secret: str, redirect_uri: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.redirect_uri = redirect_uri
        self.auth_base_url = ""https://api.notion.com/v1/oauth/authorize""
        self.token_url = ""https://api.notion.com/v1/oauth/token""","Point(row=21, column=4)","Point(row=26, column=64)",NotionOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/notion.py
NotionOAuthHandler.get_login_url,function,,"def get_login_url(self, scopes: list[str], state: str) -> str:
        params = {
            ""client_id"": self.client_id,
            ""redirect_uri"": self.redirect_uri,
            ""response_type"": ""code"",
            ""owner"": ""user"",
            ""state"": state,
        }
        return f""{self.auth_base_url}?{urlencode(params)}""","Point(row=28, column=4)","Point(row=36, column=58)",NotionOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/notion.py
NotionOAuthHandler.exchange_code_for_tokens,function,,"def exchange_code_for_tokens(
        self, code: str, scopes: list[str]
    ) -> OAuth2Credentials:
        request_body = {
            ""grant_type"": ""authorization_code"",
            ""code"": code,
            ""redirect_uri"": self.redirect_uri,
        }
        auth_str = b64encode(f""{self.client_id}:{self.client_secret}"".encode()).decode()
        headers = {
            ""Authorization"": f""Basic {auth_str}"",
            ""Accept"": ""application/json"",
        }
        response = requests.post(self.token_url, json=request_body, headers=headers)
        token_data = response.json()
        # Email is only available for non-bot users
        email = (
            token_data[""owner""][""person""][""email""]
            if ""person"" in token_data[""owner""]
            and ""email"" in token_data[""owner""][""person""]
            else None
        )

        return OAuth2Credentials(
            provider=self.PROVIDER_NAME,
            title=token_data.get(""workspace_name""),
            username=email,
            access_token=token_data[""access_token""],
            refresh_token=None,
            access_token_expires_at=None,  # Notion tokens don't expire
            refresh_token_expires_at=None,
            scopes=[],
            metadata={
                ""owner"": token_data[""owner""],
                ""bot_id"": token_data[""bot_id""],
                ""workspace_id"": token_data[""workspace_id""],
                ""workspace_name"": token_data.get(""workspace_name""),
                ""workspace_icon"": token_data.get(""workspace_icon""),
            },
        )","Point(row=38, column=4)","Point(row=77, column=9)",NotionOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/notion.py
NotionOAuthHandler.revoke_tokens,function,,"def revoke_tokens(self, credentials: OAuth2Credentials) -> bool:
        # Notion doesn't support token revocation
        return False","Point(row=79, column=4)","Point(row=81, column=20)",NotionOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/notion.py
NotionOAuthHandler._refresh_tokens,function,,"def _refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        # Notion doesn't support token refresh
        return credentials","Point(row=83, column=4)","Point(row=85, column=26)",NotionOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/notion.py
NotionOAuthHandler.needs_refresh,function,,"def needs_refresh(self, credentials: OAuth2Credentials) -> bool:
        # Notion access tokens don't expire
        return False","Point(row=87, column=4)","Point(row=89, column=20)",NotionOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/notion.py
GitHubOAuthHandler,class,"
    Based on the documentation at:
    - [Authorizing OAuth apps - GitHub Docs](https://docs.github.com/en/apps/oauth-apps/building-oauth-apps/authorizing-oauth-apps)
    - [Refreshing user access tokens - GitHub Docs](https://docs.github.com/en/apps/creating-github-apps/authenticating-with-a-github-app/refreshing-user-access-tokens)

    Notes:
    - By default, token expiration is disabled on GitHub Apps. This means the access
      token doesn't expire and no refresh token is returned by the authorization flow.
    - When token expiration gets enabled, any existing tokens will remain non-expiring.
    - When token expiration gets disabled, token refreshes will return a non-expiring
      access token *with no refresh token*.
","class GitHubOAuthHandler(BaseOAuthHandler):
    """"""
    Based on the documentation at:
    - [Authorizing OAuth apps - GitHub Docs](https://docs.github.com/en/apps/oauth-apps/building-oauth-apps/authorizing-oauth-apps)
    - [Refreshing user access tokens - GitHub Docs](https://docs.github.com/en/apps/creating-github-apps/authenticating-with-a-github-app/refreshing-user-access-tokens)

    Notes:
    - By default, token expiration is disabled on GitHub Apps. This means the access
      token doesn't expire and no refresh token is returned by the authorization flow.
    - When token expiration gets enabled, any existing tokens will remain non-expiring.
    - When token expiration gets disabled, token refreshes will return a non-expiring
      access token *with no refresh token*.
    """"""  # noqa

    PROVIDER_NAME = ""github""

    def __init__(self, client_id: str, client_secret: str, redirect_uri: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.redirect_uri = redirect_uri
        self.auth_base_url = ""https://github.com/login/oauth/authorize""
        self.token_url = ""https://github.com/login/oauth/access_token""
        self.revoke_url = ""https://api.github.com/applications/{client_id}/token""

    def get_login_url(self, scopes: list[str], state: str) -> str:
        params = {
            ""client_id"": self.client_id,
            ""redirect_uri"": self.redirect_uri,
            ""scope"": "" "".join(scopes),
            ""state"": state,
        }
        return f""{self.auth_base_url}?{urlencode(params)}""

    def exchange_code_for_tokens(
        self, code: str, scopes: list[str]
    ) -> OAuth2Credentials:
        return self._request_tokens({""code"": code, ""redirect_uri"": self.redirect_uri})

    def revoke_tokens(self, credentials: OAuth2Credentials) -> bool:
        if not credentials.access_token:
            raise ValueError(""No access token to revoke"")

        headers = {
            ""Accept"": ""application/vnd.github+json"",
            ""X-GitHub-Api-Version"": ""2022-11-28"",
        }

        requests.delete(
            url=self.revoke_url.format(client_id=self.client_id),
            auth=(self.client_id, self.client_secret),
            headers=headers,
            json={""access_token"": credentials.access_token.get_secret_value()},
        )
        return True

    def _refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        if not credentials.refresh_token:
            return credentials

        return self._request_tokens(
            {
                ""refresh_token"": credentials.refresh_token.get_secret_value(),
                ""grant_type"": ""refresh_token"",
            }
        )

    def _request_tokens(
        self,
        params: dict[str, str],
        current_credentials: Optional[OAuth2Credentials] = None,
    ) -> OAuth2Credentials:
        request_body = {
            ""client_id"": self.client_id,
            ""client_secret"": self.client_secret,
            **params,
        }
        headers = {""Accept"": ""application/json""}
        response = requests.post(self.token_url, data=request_body, headers=headers)
        token_data: dict = response.json()

        username = self._request_username(token_data[""access_token""])

        now = int(time.time())
        new_credentials = OAuth2Credentials(
            provider=self.PROVIDER_NAME,
            title=current_credentials.title if current_credentials else None,
            username=username,
            access_token=token_data[""access_token""],
            # Token refresh responses have an empty `scope` property (see docs),
            # so we have to get the scope from the existing credentials object.
            scopes=(
                token_data.get(""scope"", """").split("","")
                or (current_credentials.scopes if current_credentials else [])
            ),
            # Refresh token and expiration intervals are only given if token expiration
            # is enabled in the GitHub App's settings.
            refresh_token=token_data.get(""refresh_token""),
            access_token_expires_at=(
                now + expires_in
                if (expires_in := token_data.get(""expires_in"", None))
                else None
            ),
            refresh_token_expires_at=(
                now + expires_in
                if (expires_in := token_data.get(""refresh_token_expires_in"", None))
                else None
            ),
        )
        if current_credentials:
            new_credentials.id = current_credentials.id
        return new_credentials

    def _request_username(self, access_token: str) -> str | None:
        url = ""https://api.github.com/user""
        headers = {
            ""Accept"": ""application/vnd.github+json"",
            ""Authorization"": f""Bearer {access_token}"",
            ""X-GitHub-Api-Version"": ""2022-11-28"",
        }

        response = requests.get(url, headers=headers)

        if not response.ok:
            return None

        # Get the login (username)
        return response.json().get(""login"")","Point(row=12, column=0)","Point(row=138, column=43)",,autogpt_platform/backend/backend/integrations/oauth/github.py
GitHubOAuthHandler.__init__,function,,"def __init__(self, client_id: str, client_secret: str, redirect_uri: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.redirect_uri = redirect_uri
        self.auth_base_url = ""https://github.com/login/oauth/authorize""
        self.token_url = ""https://github.com/login/oauth/access_token""
        self.revoke_url = ""https://api.github.com/applications/{client_id}/token""","Point(row=28, column=4)","Point(row=34, column=81)",GitHubOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/github.py
GitHubOAuthHandler.get_login_url,function,,"def get_login_url(self, scopes: list[str], state: str) -> str:
        params = {
            ""client_id"": self.client_id,
            ""redirect_uri"": self.redirect_uri,
            ""scope"": "" "".join(scopes),
            ""state"": state,
        }
        return f""{self.auth_base_url}?{urlencode(params)}""","Point(row=36, column=4)","Point(row=43, column=58)",GitHubOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/github.py
GitHubOAuthHandler.exchange_code_for_tokens,function,,"def exchange_code_for_tokens(
        self, code: str, scopes: list[str]
    ) -> OAuth2Credentials:
        return self._request_tokens({""code"": code, ""redirect_uri"": self.redirect_uri})","Point(row=45, column=4)","Point(row=48, column=86)",GitHubOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/github.py
GitHubOAuthHandler.revoke_tokens,function,,"def revoke_tokens(self, credentials: OAuth2Credentials) -> bool:
        if not credentials.access_token:
            raise ValueError(""No access token to revoke"")

        headers = {
            ""Accept"": ""application/vnd.github+json"",
            ""X-GitHub-Api-Version"": ""2022-11-28"",
        }

        requests.delete(
            url=self.revoke_url.format(client_id=self.client_id),
            auth=(self.client_id, self.client_secret),
            headers=headers,
            json={""access_token"": credentials.access_token.get_secret_value()},
        )
        return True","Point(row=50, column=4)","Point(row=65, column=19)",GitHubOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/github.py
GitHubOAuthHandler._refresh_tokens,function,,"def _refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        if not credentials.refresh_token:
            return credentials

        return self._request_tokens(
            {
                ""refresh_token"": credentials.refresh_token.get_secret_value(),
                ""grant_type"": ""refresh_token"",
            }
        )","Point(row=67, column=4)","Point(row=76, column=9)",GitHubOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/github.py
GitHubOAuthHandler._request_tokens,function,,"def _request_tokens(
        self,
        params: dict[str, str],
        current_credentials: Optional[OAuth2Credentials] = None,
    ) -> OAuth2Credentials:
        request_body = {
            ""client_id"": self.client_id,
            ""client_secret"": self.client_secret,
            **params,
        }
        headers = {""Accept"": ""application/json""}
        response = requests.post(self.token_url, data=request_body, headers=headers)
        token_data: dict = response.json()

        username = self._request_username(token_data[""access_token""])

        now = int(time.time())
        new_credentials = OAuth2Credentials(
            provider=self.PROVIDER_NAME,
            title=current_credentials.title if current_credentials else None,
            username=username,
            access_token=token_data[""access_token""],
            # Token refresh responses have an empty `scope` property (see docs),
            # so we have to get the scope from the existing credentials object.
            scopes=(
                token_data.get(""scope"", """").split("","")
                or (current_credentials.scopes if current_credentials else [])
            ),
            # Refresh token and expiration intervals are only given if token expiration
            # is enabled in the GitHub App's settings.
            refresh_token=token_data.get(""refresh_token""),
            access_token_expires_at=(
                now + expires_in
                if (expires_in := token_data.get(""expires_in"", None))
                else None
            ),
            refresh_token_expires_at=(
                now + expires_in
                if (expires_in := token_data.get(""refresh_token_expires_in"", None))
                else None
            ),
        )
        if current_credentials:
            new_credentials.id = current_credentials.id
        return new_credentials","Point(row=78, column=4)","Point(row=122, column=30)",GitHubOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/github.py
GitHubOAuthHandler._request_username,function,,"def _request_username(self, access_token: str) -> str | None:
        url = ""https://api.github.com/user""
        headers = {
            ""Accept"": ""application/vnd.github+json"",
            ""Authorization"": f""Bearer {access_token}"",
            ""X-GitHub-Api-Version"": ""2022-11-28"",
        }

        response = requests.get(url, headers=headers)

        if not response.ok:
            return None

        # Get the login (username)
        return response.json().get(""login"")","Point(row=124, column=4)","Point(row=138, column=43)",GitHubOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/github.py
BaseOAuthHandler,class,,"class BaseOAuthHandler(ABC):
    # --8<-- [start:BaseOAuthHandler1]
    PROVIDER_NAME: ClassVar[str]
    DEFAULT_SCOPES: ClassVar[list[str]] = []
    # --8<-- [end:BaseOAuthHandler1]

    @abstractmethod
    # --8<-- [start:BaseOAuthHandler2]
    def __init__(self, client_id: str, client_secret: str, redirect_uri: str): ...

    # --8<-- [end:BaseOAuthHandler2]

    @abstractmethod
    # --8<-- [start:BaseOAuthHandler3]
    def get_login_url(self, scopes: list[str], state: str) -> str:
        # --8<-- [end:BaseOAuthHandler3]
        """"""Constructs a login URL that the user can be redirected to""""""
        ...

    @abstractmethod
    # --8<-- [start:BaseOAuthHandler4]
    def exchange_code_for_tokens(
        self, code: str, scopes: list[str]
    ) -> OAuth2Credentials:
        # --8<-- [end:BaseOAuthHandler4]
        """"""Exchanges the acquired authorization code from login for a set of tokens""""""
        ...

    @abstractmethod
    # --8<-- [start:BaseOAuthHandler5]
    def _refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        # --8<-- [end:BaseOAuthHandler5]
        """"""Implements the token refresh mechanism""""""
        ...

    @abstractmethod
    # --8<-- [start:BaseOAuthHandler6]
    def revoke_tokens(self, credentials: OAuth2Credentials) -> bool:
        # --8<-- [end:BaseOAuthHandler6]
        """"""Revokes the given token at provider,
        returns False provider does not support it""""""
        ...

    def refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        if credentials.provider != self.PROVIDER_NAME:
            raise ValueError(
                f""{self.__class__.__name__} can not refresh tokens ""
                f""for other provider '{credentials.provider}'""
            )
        return self._refresh_tokens(credentials)

    def get_access_token(self, credentials: OAuth2Credentials) -> str:
        """"""Returns a valid access token, refreshing it first if needed""""""
        if self.needs_refresh(credentials):
            credentials = self.refresh_tokens(credentials)
        return credentials.access_token.get_secret_value()

    def needs_refresh(self, credentials: OAuth2Credentials) -> bool:
        """"""Indicates whether the given tokens need to be refreshed""""""
        return (
            credentials.access_token_expires_at is not None
            and credentials.access_token_expires_at < int(time.time()) + 300
        )

    def handle_default_scopes(self, scopes: list[str]) -> list[str]:
        """"""Handles the default scopes for the provider""""""
        # If scopes are empty, use the default scopes for the provider
        if not scopes:
            logger.debug(f""Using default scopes for provider {self.PROVIDER_NAME}"")
            scopes = self.DEFAULT_SCOPES
        return scopes","Point(row=10, column=0)","Point(row=80, column=21)",,autogpt_platform/backend/backend/integrations/oauth/base.py
BaseOAuthHandler.__init__,function,,"def __init__(self, client_id: str, client_secret: str, redirect_uri: str): ...","Point(row=18, column=4)","Point(row=18, column=82)",BaseOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/base.py
BaseOAuthHandler.get_login_url,function,Constructs a login URL that the user can be redirected to,"def get_login_url(self, scopes: list[str], state: str) -> str:
        # --8<-- [end:BaseOAuthHandler3]
        """"""Constructs a login URL that the user can be redirected to""""""
        ...","Point(row=24, column=4)","Point(row=27, column=11)",BaseOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/base.py
BaseOAuthHandler.exchange_code_for_tokens,function,Exchanges the acquired authorization code from login for a set of tokens,"def exchange_code_for_tokens(
        self, code: str, scopes: list[str]
    ) -> OAuth2Credentials:
        # --8<-- [end:BaseOAuthHandler4]
        """"""Exchanges the acquired authorization code from login for a set of tokens""""""
        ...","Point(row=31, column=4)","Point(row=36, column=11)",BaseOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/base.py
BaseOAuthHandler._refresh_tokens,function,Implements the token refresh mechanism,"def _refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        # --8<-- [end:BaseOAuthHandler5]
        """"""Implements the token refresh mechanism""""""
        ...","Point(row=40, column=4)","Point(row=43, column=11)",BaseOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/base.py
BaseOAuthHandler.revoke_tokens,function,"Revokes the given token at provider,
        returns False provider does not support it","def revoke_tokens(self, credentials: OAuth2Credentials) -> bool:
        # --8<-- [end:BaseOAuthHandler6]
        """"""Revokes the given token at provider,
        returns False provider does not support it""""""
        ...","Point(row=47, column=4)","Point(row=51, column=11)",BaseOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/base.py
BaseOAuthHandler.refresh_tokens,function,,"def refresh_tokens(self, credentials: OAuth2Credentials) -> OAuth2Credentials:
        if credentials.provider != self.PROVIDER_NAME:
            raise ValueError(
                f""{self.__class__.__name__} can not refresh tokens ""
                f""for other provider '{credentials.provider}'""
            )
        return self._refresh_tokens(credentials)","Point(row=53, column=4)","Point(row=59, column=48)",BaseOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/base.py
BaseOAuthHandler.get_access_token,function,"Returns a valid access token, refreshing it first if needed","def get_access_token(self, credentials: OAuth2Credentials) -> str:
        """"""Returns a valid access token, refreshing it first if needed""""""
        if self.needs_refresh(credentials):
            credentials = self.refresh_tokens(credentials)
        return credentials.access_token.get_secret_value()","Point(row=61, column=4)","Point(row=65, column=58)",BaseOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/base.py
BaseOAuthHandler.needs_refresh,function,Indicates whether the given tokens need to be refreshed,"def needs_refresh(self, credentials: OAuth2Credentials) -> bool:
        """"""Indicates whether the given tokens need to be refreshed""""""
        return (
            credentials.access_token_expires_at is not None
            and credentials.access_token_expires_at < int(time.time()) + 300
        )","Point(row=67, column=4)","Point(row=72, column=9)",BaseOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/base.py
BaseOAuthHandler.handle_default_scopes,function,Handles the default scopes for the provider,"def handle_default_scopes(self, scopes: list[str]) -> list[str]:
        """"""Handles the default scopes for the provider""""""
        # If scopes are empty, use the default scopes for the provider
        if not scopes:
            logger.debug(f""Using default scopes for provider {self.PROVIDER_NAME}"")
            scopes = self.DEFAULT_SCOPES
        return scopes","Point(row=74, column=4)","Point(row=80, column=21)",BaseOAuthHandler,autogpt_platform/backend/backend/integrations/oauth/base.py
DatabaseManager,class,,"class DatabaseManager(AppService):
    def __init__(self):
        super().__init__()
        self.use_db = True
        self.use_redis = True
        self.event_queue = RedisExecutionEventBus()

    @classmethod
    def get_port(cls) -> int:
        return Config().database_api_port

    @expose
    def send_execution_update(self, execution_result_dict: dict[Any, Any]):
        self.event_queue.publish(ExecutionResult(**execution_result_dict))

    @staticmethod
    def exposed_run_and_wait(
        f: Callable[P, Coroutine[None, None, R]]
    ) -> Callable[Concatenate[object, P], R]:
        @expose
        @wraps(f)
        def wrapper(self, *args: P.args, **kwargs: P.kwargs) -> R:
            coroutine = f(*args, **kwargs)
            res = self.run_and_wait(coroutine)
            return res

        return wrapper

    # Executions
    create_graph_execution = exposed_run_and_wait(create_graph_execution)
    get_execution_results = exposed_run_and_wait(get_execution_results)
    get_incomplete_executions = exposed_run_and_wait(get_incomplete_executions)
    get_latest_execution = exposed_run_and_wait(get_latest_execution)
    update_execution_status = exposed_run_and_wait(update_execution_status)
    update_graph_execution_stats = exposed_run_and_wait(update_graph_execution_stats)
    update_node_execution_stats = exposed_run_and_wait(update_node_execution_stats)
    upsert_execution_input = exposed_run_and_wait(upsert_execution_input)
    upsert_execution_output = exposed_run_and_wait(upsert_execution_output)

    # Graphs
    get_node = exposed_run_and_wait(get_node)
    get_graph = exposed_run_and_wait(get_graph)

    # Credits
    user_credit_model = get_user_credit_model()
    get_or_refill_credit = cast(
        Callable[[Any, str], int],
        exposed_run_and_wait(user_credit_model.get_or_refill_credit),
    )
    spend_credits = cast(
        Callable[[Any, str, int, str, dict[str, str], float, float], int],
        exposed_run_and_wait(user_credit_model.spend_credits),
    )

    # User + User Metadata + User Integrations
    get_user_metadata = exposed_run_and_wait(get_user_metadata)
    update_user_metadata = exposed_run_and_wait(update_user_metadata)
    get_user_integrations = exposed_run_and_wait(get_user_integrations)
    update_user_integrations = exposed_run_and_wait(update_user_integrations)","Point(row=31, column=0)","Point(row=89, column=77)",,autogpt_platform/backend/backend/executor/database.py
DatabaseManager.__init__,function,,"def __init__(self):
        super().__init__()
        self.use_db = True
        self.use_redis = True
        self.event_queue = RedisExecutionEventBus()","Point(row=32, column=4)","Point(row=36, column=51)",DatabaseManager,autogpt_platform/backend/backend/executor/database.py
DatabaseManager.get_port,function,,"def get_port(cls) -> int:
        return Config().database_api_port","Point(row=39, column=4)","Point(row=40, column=41)",DatabaseManager,autogpt_platform/backend/backend/executor/database.py
DatabaseManager.send_execution_update,function,,"def send_execution_update(self, execution_result_dict: dict[Any, Any]):
        self.event_queue.publish(ExecutionResult(**execution_result_dict))","Point(row=43, column=4)","Point(row=44, column=74)",DatabaseManager,autogpt_platform/backend/backend/executor/database.py
DatabaseManager.exposed_run_and_wait,function,,"def exposed_run_and_wait(
        f: Callable[P, Coroutine[None, None, R]]
    ) -> Callable[Concatenate[object, P], R]:
        @expose
        @wraps(f)
        def wrapper(self, *args: P.args, **kwargs: P.kwargs) -> R:
            coroutine = f(*args, **kwargs)
            res = self.run_and_wait(coroutine)
            return res

        return wrapper","Point(row=47, column=4)","Point(row=57, column=22)",DatabaseManager,autogpt_platform/backend/backend/executor/database.py
DatabaseManager.exposed_run_and_wait.wrapper,function,,"def wrapper(self, *args: P.args, **kwargs: P.kwargs) -> R:
            coroutine = f(*args, **kwargs)
            res = self.run_and_wait(coroutine)
            return res","Point(row=52, column=8)","Point(row=55, column=22)",DatabaseManager,autogpt_platform/backend/backend/executor/database.py
log,function,,"def log(msg, **kwargs):
    logger.warning(""[ExecutionScheduler] "" + msg, **kwargs)","Point(row=23, column=0)","Point(row=24, column=59)",,autogpt_platform/backend/backend/executor/scheduler.py
ExecutionScheduler,class,,"class ExecutionScheduler(AppService):

    def __init__(self, refresh_interval=10):
        super().__init__()
        self.use_db = True
        self.last_check = datetime.min
        self.refresh_interval = refresh_interval

    @classmethod
    def get_port(cls) -> int:
        return Config().execution_scheduler_port

    @property
    @thread_cached
    def execution_client(self) -> ExecutionManager:
        return get_service_client(ExecutionManager)

    def run_service(self):
        scheduler = BackgroundScheduler()
        scheduler.start()
        while True:
            self.__refresh_jobs_from_db(scheduler)
            time.sleep(self.refresh_interval)

    def __refresh_jobs_from_db(self, scheduler: BackgroundScheduler):
        schedules = self.run_and_wait(get_active_schedules(self.last_check))
        for schedule in schedules:
            if schedule.last_updated:
                self.last_check = max(self.last_check, schedule.last_updated)

            if not schedule.is_enabled:
                log(f""Removing recurring job {schedule.id}: {schedule.schedule}"")
                scheduler.remove_job(schedule.id)
                continue

            log(f""Adding recurring job {schedule.id}: {schedule.schedule}"")
            scheduler.add_job(
                self.__execute_graph,
                CronTrigger.from_crontab(schedule.schedule),
                id=schedule.id,
                args=[schedule.graph_id, schedule.input_data, schedule.user_id],
                replace_existing=True,
            )

    def __execute_graph(self, graph_id: str, input_data: dict, user_id: str):
        try:
            log(f""Executing recurring job for graph #{graph_id}"")
            self.execution_client.add_execution(graph_id, input_data, user_id)
        except Exception as e:
            logger.exception(f""Error executing graph {graph_id}: {e}"")

    @expose
    def update_schedule(self, schedule_id: str, is_enabled: bool, user_id: str) -> str:
        self.run_and_wait(update_schedule(schedule_id, is_enabled, user_id))
        return schedule_id

    @expose
    def add_execution_schedule(
        self,
        graph_id: str,
        graph_version: int,
        cron: str,
        input_data: BlockInput,
        user_id: str,
    ) -> str:
        schedule = ExecutionSchedule(
            graph_id=graph_id,
            user_id=user_id,
            graph_version=graph_version,
            schedule=cron,
            input_data=input_data,
        )
        return self.run_and_wait(add_schedule(schedule)).id

    @expose
    def get_execution_schedules(self, graph_id: str, user_id: str) -> dict[str, str]:
        schedules = self.run_and_wait(get_schedules(graph_id, user_id=user_id))
        return {v.id: v.schedule for v in schedules}","Point(row=27, column=0)","Point(row=104, column=52)",,autogpt_platform/backend/backend/executor/scheduler.py
ExecutionScheduler.__init__,function,,"def __init__(self, refresh_interval=10):
        super().__init__()
        self.use_db = True
        self.last_check = datetime.min
        self.refresh_interval = refresh_interval","Point(row=29, column=4)","Point(row=33, column=48)",ExecutionScheduler,autogpt_platform/backend/backend/executor/scheduler.py
ExecutionScheduler.get_port,function,,"def get_port(cls) -> int:
        return Config().execution_scheduler_port","Point(row=36, column=4)","Point(row=37, column=48)",ExecutionScheduler,autogpt_platform/backend/backend/executor/scheduler.py
ExecutionScheduler.execution_client,function,,"def execution_client(self) -> ExecutionManager:
        return get_service_client(ExecutionManager)","Point(row=41, column=4)","Point(row=42, column=51)",ExecutionScheduler,autogpt_platform/backend/backend/executor/scheduler.py
ExecutionScheduler.run_service,function,,"def run_service(self):
        scheduler = BackgroundScheduler()
        scheduler.start()
        while True:
            self.__refresh_jobs_from_db(scheduler)
            time.sleep(self.refresh_interval)","Point(row=44, column=4)","Point(row=49, column=45)",ExecutionScheduler,autogpt_platform/backend/backend/executor/scheduler.py
ExecutionScheduler.__refresh_jobs_from_db,function,,"def __refresh_jobs_from_db(self, scheduler: BackgroundScheduler):
        schedules = self.run_and_wait(get_active_schedules(self.last_check))
        for schedule in schedules:
            if schedule.last_updated:
                self.last_check = max(self.last_check, schedule.last_updated)

            if not schedule.is_enabled:
                log(f""Removing recurring job {schedule.id}: {schedule.schedule}"")
                scheduler.remove_job(schedule.id)
                continue

            log(f""Adding recurring job {schedule.id}: {schedule.schedule}"")
            scheduler.add_job(
                self.__execute_graph,
                CronTrigger.from_crontab(schedule.schedule),
                id=schedule.id,
                args=[schedule.graph_id, schedule.input_data, schedule.user_id],
                replace_existing=True,
            )","Point(row=51, column=4)","Point(row=69, column=13)",ExecutionScheduler,autogpt_platform/backend/backend/executor/scheduler.py
ExecutionScheduler.__execute_graph,function,,"def __execute_graph(self, graph_id: str, input_data: dict, user_id: str):
        try:
            log(f""Executing recurring job for graph #{graph_id}"")
            self.execution_client.add_execution(graph_id, input_data, user_id)
        except Exception as e:
            logger.exception(f""Error executing graph {graph_id}: {e}"")","Point(row=71, column=4)","Point(row=76, column=70)",ExecutionScheduler,autogpt_platform/backend/backend/executor/scheduler.py
ExecutionScheduler.update_schedule,function,,"def update_schedule(self, schedule_id: str, is_enabled: bool, user_id: str) -> str:
        self.run_and_wait(update_schedule(schedule_id, is_enabled, user_id))
        return schedule_id","Point(row=79, column=4)","Point(row=81, column=26)",ExecutionScheduler,autogpt_platform/backend/backend/executor/scheduler.py
ExecutionScheduler.add_execution_schedule,function,,"def add_execution_schedule(
        self,
        graph_id: str,
        graph_version: int,
        cron: str,
        input_data: BlockInput,
        user_id: str,
    ) -> str:
        schedule = ExecutionSchedule(
            graph_id=graph_id,
            user_id=user_id,
            graph_version=graph_version,
            schedule=cron,
            input_data=input_data,
        )
        return self.run_and_wait(add_schedule(schedule)).id","Point(row=84, column=4)","Point(row=99, column=59)",ExecutionScheduler,autogpt_platform/backend/backend/executor/scheduler.py
ExecutionScheduler.get_execution_schedules,function,,"def get_execution_schedules(self, graph_id: str, user_id: str) -> dict[str, str]:
        schedules = self.run_and_wait(get_schedules(graph_id, user_id=user_id))
        return {v.id: v.schedule for v in schedules}","Point(row=102, column=4)","Point(row=104, column=52)",ExecutionScheduler,autogpt_platform/backend/backend/executor/scheduler.py
LogMetadata,class,,"class LogMetadata:
    def __init__(
        self,
        user_id: str,
        graph_eid: str,
        graph_id: str,
        node_eid: str,
        node_id: str,
        block_name: str,
    ):
        self.metadata = {
            ""component"": ""ExecutionManager"",
            ""user_id"": user_id,
            ""graph_eid"": graph_eid,
            ""graph_id"": graph_id,
            ""node_eid"": node_eid,
            ""node_id"": node_id,
            ""block_name"": block_name,
        }
        self.prefix = f""[ExecutionManager|uid:{user_id}|gid:{graph_id}|nid:{node_id}]|geid:{graph_eid}|nid:{node_eid}|{block_name}]""

    def info(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.info(msg, extra={""json_fields"": {**self.metadata, **extra}})

    def warning(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.warning(msg, extra={""json_fields"": {**self.metadata, **extra}})

    def error(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.error(msg, extra={""json_fields"": {**self.metadata, **extra}})

    def debug(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.debug(msg, extra={""json_fields"": {**self.metadata, **extra}})

    def exception(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.exception(msg, extra={""json_fields"": {**self.metadata, **extra}})

    def _wrap(self, msg: str, **extra):
        return f""{self.prefix} {msg} {extra}""","Point(row=51, column=0)","Point(row=93, column=45)",,autogpt_platform/backend/backend/executor/manager.py
LogMetadata.__init__,function,,"def __init__(
        self,
        user_id: str,
        graph_eid: str,
        graph_id: str,
        node_eid: str,
        node_id: str,
        block_name: str,
    ):
        self.metadata = {
            ""component"": ""ExecutionManager"",
            ""user_id"": user_id,
            ""graph_eid"": graph_eid,
            ""graph_id"": graph_id,
            ""node_eid"": node_eid,
            ""node_id"": node_id,
            ""block_name"": block_name,
        }
        self.prefix = f""[ExecutionManager|uid:{user_id}|gid:{graph_id}|nid:{node_id}]|geid:{graph_eid}|nid:{node_eid}|{block_name}]""","Point(row=52, column=4)","Point(row=70, column=132)",LogMetadata,autogpt_platform/backend/backend/executor/manager.py
LogMetadata.info,function,,"def info(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.info(msg, extra={""json_fields"": {**self.metadata, **extra}})","Point(row=72, column=4)","Point(row=74, column=75)",LogMetadata,autogpt_platform/backend/backend/executor/manager.py
LogMetadata.warning,function,,"def warning(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.warning(msg, extra={""json_fields"": {**self.metadata, **extra}})","Point(row=76, column=4)","Point(row=78, column=78)",LogMetadata,autogpt_platform/backend/backend/executor/manager.py
LogMetadata.error,function,,"def error(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.error(msg, extra={""json_fields"": {**self.metadata, **extra}})","Point(row=80, column=4)","Point(row=82, column=76)",LogMetadata,autogpt_platform/backend/backend/executor/manager.py
LogMetadata.debug,function,,"def debug(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.debug(msg, extra={""json_fields"": {**self.metadata, **extra}})","Point(row=84, column=4)","Point(row=86, column=76)",LogMetadata,autogpt_platform/backend/backend/executor/manager.py
LogMetadata.exception,function,,"def exception(self, msg: str, **extra):
        msg = self._wrap(msg, **extra)
        logger.exception(msg, extra={""json_fields"": {**self.metadata, **extra}})","Point(row=88, column=4)","Point(row=90, column=80)",LogMetadata,autogpt_platform/backend/backend/executor/manager.py
LogMetadata._wrap,function,,"def _wrap(self, msg: str, **extra):
        return f""{self.prefix} {msg} {extra}""","Point(row=92, column=4)","Point(row=93, column=45)",LogMetadata,autogpt_platform/backend/backend/executor/manager.py
execute_node,function,"
    Execute a node in the graph. This will trigger a block execution on a node,
    persist the execution result, and return the subsequent node to be executed.

    Args:
        db_client: The client to send execution updates to the server.
        creds_manager: The manager to acquire and release credentials.
        data: The execution data for executing the current node.
        execution_stats: The execution statistics to be updated.

    Returns:
        The subsequent node to be enqueued, or None if there is no subsequent node.
","def execute_node(
    db_client: ""DatabaseManager"",
    creds_manager: IntegrationCredentialsManager,
    data: NodeExecution,
    execution_stats: dict[str, Any] | None = None,
) -> ExecutionStream:
    """"""
    Execute a node in the graph. This will trigger a block execution on a node,
    persist the execution result, and return the subsequent node to be executed.

    Args:
        db_client: The client to send execution updates to the server.
        creds_manager: The manager to acquire and release credentials.
        data: The execution data for executing the current node.
        execution_stats: The execution statistics to be updated.

    Returns:
        The subsequent node to be enqueued, or None if there is no subsequent node.
    """"""
    user_id = data.user_id
    graph_exec_id = data.graph_exec_id
    graph_id = data.graph_id
    node_exec_id = data.node_exec_id
    node_id = data.node_id

    def update_execution(status: ExecutionStatus) -> ExecutionResult:
        exec_update = db_client.update_execution_status(node_exec_id, status)
        db_client.send_execution_update(exec_update.model_dump())
        return exec_update

    node = db_client.get_node(node_id)

    node_block = get_block(node.block_id)
    if not node_block:
        logger.error(f""Block {node.block_id} not found."")
        return

    # Sanity check: validate the execution input.
    log_metadata = LogMetadata(
        user_id=user_id,
        graph_eid=graph_exec_id,
        graph_id=graph_id,
        node_eid=node_exec_id,
        node_id=node_id,
        block_name=node_block.name,
    )
    input_data, error = validate_exec(node, data.data, resolve_input=False)
    if input_data is None:
        log_metadata.error(f""Skip execution, input validation error: {error}"")
        return

    # Execute the node
    input_data_str = json.dumps(input_data)
    input_size = len(input_data_str)
    log_metadata.info(""Executed node with input"", input=input_data_str)
    update_execution(ExecutionStatus.RUNNING)

    extra_exec_kwargs = {}
    # Last-minute fetch credentials + acquire a system-wide read-write lock to prevent
    # changes during execution. ‚ö†Ô∏è This means a set of credentials can only be used by
    # one (running) block at a time; simultaneous execution of blocks using same
    # credentials is not supported.
    creds_lock = None
    if CREDENTIALS_FIELD_NAME in input_data:
        credentials_meta = CredentialsMetaInput(**input_data[CREDENTIALS_FIELD_NAME])
        credentials, creds_lock = creds_manager.acquire(user_id, credentials_meta.id)
        extra_exec_kwargs[""credentials""] = credentials

    output_size = 0
    end_status = ExecutionStatus.COMPLETED
    credit = db_client.get_or_refill_credit(user_id)
    if credit < 0:
        raise ValueError(f""Insufficient credit: {credit}"")

    try:
        for output_name, output_data in node_block.execute(
            input_data, **extra_exec_kwargs
        ):
            output_size += len(json.dumps(output_data))
            log_metadata.info(""Node produced output"", output_name=output_data)
            db_client.upsert_execution_output(node_exec_id, output_name, output_data)

            for execution in _enqueue_next_nodes(
                db_client=db_client,
                node=node,
                output=(output_name, output_data),
                user_id=user_id,
                graph_exec_id=graph_exec_id,
                graph_id=graph_id,
                log_metadata=log_metadata,
            ):
                yield execution

    except Exception as e:
        end_status = ExecutionStatus.FAILED
        error_msg = str(e)
        log_metadata.exception(f""Node execution failed with error {error_msg}"")
        db_client.upsert_execution_output(node_exec_id, ""error"", error_msg)

        for execution in _enqueue_next_nodes(
            db_client=db_client,
            node=node,
            output=(""error"", error_msg),
            user_id=user_id,
            graph_exec_id=graph_exec_id,
            graph_id=graph_id,
            log_metadata=log_metadata,
        ):
            yield execution

        raise e
    finally:
        # Ensure credentials are released even if execution fails
        if creds_lock:
            try:
                creds_lock.release()
            except Exception as e:
                log_metadata.error(f""Failed to release credentials lock: {e}"")

        # Update execution status and spend credits
        res = update_execution(end_status)
        if end_status == ExecutionStatus.COMPLETED:
            s = input_size + output_size
            t = (
                (res.end_time - res.start_time).total_seconds()
                if res.end_time and res.start_time
                else 0
            )
            db_client.spend_credits(user_id, credit, node_block.id, input_data, s, t)

        # Update execution stats
        if execution_stats is not None:
            execution_stats.update(node_block.execution_stats)
            execution_stats[""input_size""] = input_size
            execution_stats[""output_size""] = output_size","Point(row=100, column=0)","Point(row=234, column=56)",,autogpt_platform/backend/backend/executor/manager.py
execute_node.update_execution,function,,"def update_execution(status: ExecutionStatus) -> ExecutionResult:
        exec_update = db_client.update_execution_status(node_exec_id, status)
        db_client.send_execution_update(exec_update.model_dump())
        return exec_update","Point(row=125, column=4)","Point(row=128, column=26)",,autogpt_platform/backend/backend/executor/manager.py
_enqueue_next_nodes,function,,"def _enqueue_next_nodes(
    db_client: ""DatabaseManager"",
    node: Node,
    output: BlockData,
    user_id: str,
    graph_exec_id: str,
    graph_id: str,
    log_metadata: LogMetadata,
) -> list[NodeExecution]:

    def add_enqueued_execution(
        node_exec_id: str, node_id: str, data: BlockInput
    ) -> NodeExecution:
        exec_update = db_client.update_execution_status(
            node_exec_id, ExecutionStatus.QUEUED, data
        )
        db_client.send_execution_update(exec_update.model_dump())
        return NodeExecution(
            user_id=user_id,
            graph_exec_id=graph_exec_id,
            graph_id=graph_id,
            node_exec_id=node_exec_id,
            node_id=node_id,
            data=data,
        )

    def register_next_executions(node_link: Link) -> list[NodeExecution]:
        enqueued_executions = []
        next_output_name = node_link.source_name
        next_input_name = node_link.sink_name
        next_node_id = node_link.sink_id

        next_data = parse_execution_output(output, next_output_name)
        if next_data is None:
            return enqueued_executions

        next_node = db_client.get_node(next_node_id)

        # Multiple node can register the same next node, we need this to be atomic
        # To avoid same execution to be enqueued multiple times,
        # Or the same input to be consumed multiple times.
        with synchronized(f""upsert_input-{next_node_id}-{graph_exec_id}""):
            # Add output data to the earliest incomplete execution, or create a new one.
            next_node_exec_id, next_node_input = db_client.upsert_execution_input(
                node_id=next_node_id,
                graph_exec_id=graph_exec_id,
                input_name=next_input_name,
                input_data=next_data,
            )

            # Complete missing static input pins data using the last execution input.
            static_link_names = {
                link.sink_name
                for link in next_node.input_links
                if link.is_static and link.sink_name not in next_node_input
            }
            if static_link_names and (
                latest_execution := db_client.get_latest_execution(
                    next_node_id, graph_exec_id
                )
            ):
                for name in static_link_names:
                    next_node_input[name] = latest_execution.input_data.get(name)

            # Validate the input data for the next node.
            next_node_input, validation_msg = validate_exec(next_node, next_node_input)
            suffix = f""{next_output_name}>{next_input_name}~{next_node_exec_id}:{validation_msg}""

            # Incomplete input data, skip queueing the execution.
            if not next_node_input:
                log_metadata.warning(f""Skipped queueing {suffix}"")
                return enqueued_executions

            # Input is complete, enqueue the execution.
            log_metadata.info(f""Enqueued {suffix}"")
            enqueued_executions.append(
                add_enqueued_execution(next_node_exec_id, next_node_id, next_node_input)
            )

            # Next execution stops here if the link is not static.
            if not node_link.is_static:
                return enqueued_executions

            # If link is static, there could be some incomplete executions waiting for it.
            # Load and complete the input missing input data, and try to re-enqueue them.
            for iexec in db_client.get_incomplete_executions(
                next_node_id, graph_exec_id
            ):
                idata = iexec.input_data
                ineid = iexec.node_exec_id

                static_link_names = {
                    link.sink_name
                    for link in next_node.input_links
                    if link.is_static and link.sink_name not in idata
                }
                for input_name in static_link_names:
                    idata[input_name] = next_node_input[input_name]

                idata, msg = validate_exec(next_node, idata)
                suffix = f""{next_output_name}>{next_input_name}~{ineid}:{msg}""
                if not idata:
                    log_metadata.info(f""Enqueueing static-link skipped: {suffix}"")
                    continue
                log_metadata.info(f""Enqueueing static-link execution {suffix}"")
                enqueued_executions.append(
                    add_enqueued_execution(iexec.node_exec_id, next_node_id, idata)
                )
            return enqueued_executions

    return [
        execution
        for link in node.output_links
        for execution in register_next_executions(link)
    ]","Point(row=237, column=0)","Point(row=351, column=5)",,autogpt_platform/backend/backend/executor/manager.py
_enqueue_next_nodes.add_enqueued_execution,function,,"def add_enqueued_execution(
        node_exec_id: str, node_id: str, data: BlockInput
    ) -> NodeExecution:
        exec_update = db_client.update_execution_status(
            node_exec_id, ExecutionStatus.QUEUED, data
        )
        db_client.send_execution_update(exec_update.model_dump())
        return NodeExecution(
            user_id=user_id,
            graph_exec_id=graph_exec_id,
            graph_id=graph_id,
            node_exec_id=node_exec_id,
            node_id=node_id,
            data=data,
        )","Point(row=247, column=4)","Point(row=261, column=9)",,autogpt_platform/backend/backend/executor/manager.py
_enqueue_next_nodes.register_next_executions,function,,"def register_next_executions(node_link: Link) -> list[NodeExecution]:
        enqueued_executions = []
        next_output_name = node_link.source_name
        next_input_name = node_link.sink_name
        next_node_id = node_link.sink_id

        next_data = parse_execution_output(output, next_output_name)
        if next_data is None:
            return enqueued_executions

        next_node = db_client.get_node(next_node_id)

        # Multiple node can register the same next node, we need this to be atomic
        # To avoid same execution to be enqueued multiple times,
        # Or the same input to be consumed multiple times.
        with synchronized(f""upsert_input-{next_node_id}-{graph_exec_id}""):
            # Add output data to the earliest incomplete execution, or create a new one.
            next_node_exec_id, next_node_input = db_client.upsert_execution_input(
                node_id=next_node_id,
                graph_exec_id=graph_exec_id,
                input_name=next_input_name,
                input_data=next_data,
            )

            # Complete missing static input pins data using the last execution input.
            static_link_names = {
                link.sink_name
                for link in next_node.input_links
                if link.is_static and link.sink_name not in next_node_input
            }
            if static_link_names and (
                latest_execution := db_client.get_latest_execution(
                    next_node_id, graph_exec_id
                )
            ):
                for name in static_link_names:
                    next_node_input[name] = latest_execution.input_data.get(name)

            # Validate the input data for the next node.
            next_node_input, validation_msg = validate_exec(next_node, next_node_input)
            suffix = f""{next_output_name}>{next_input_name}~{next_node_exec_id}:{validation_msg}""

            # Incomplete input data, skip queueing the execution.
            if not next_node_input:
                log_metadata.warning(f""Skipped queueing {suffix}"")
                return enqueued_executions

            # Input is complete, enqueue the execution.
            log_metadata.info(f""Enqueued {suffix}"")
            enqueued_executions.append(
                add_enqueued_execution(next_node_exec_id, next_node_id, next_node_input)
            )

            # Next execution stops here if the link is not static.
            if not node_link.is_static:
                return enqueued_executions

            # If link is static, there could be some incomplete executions waiting for it.
            # Load and complete the input missing input data, and try to re-enqueue them.
            for iexec in db_client.get_incomplete_executions(
                next_node_id, graph_exec_id
            ):
                idata = iexec.input_data
                ineid = iexec.node_exec_id

                static_link_names = {
                    link.sink_name
                    for link in next_node.input_links
                    if link.is_static and link.sink_name not in idata
                }
                for input_name in static_link_names:
                    idata[input_name] = next_node_input[input_name]

                idata, msg = validate_exec(next_node, idata)
                suffix = f""{next_output_name}>{next_input_name}~{ineid}:{msg}""
                if not idata:
                    log_metadata.info(f""Enqueueing static-link skipped: {suffix}"")
                    continue
                log_metadata.info(f""Enqueueing static-link execution {suffix}"")
                enqueued_executions.append(
                    add_enqueued_execution(iexec.node_exec_id, next_node_id, idata)
                )
            return enqueued_executions","Point(row=263, column=4)","Point(row=345, column=38)",,autogpt_platform/backend/backend/executor/manager.py
validate_exec,function,"
    Validate the input data for a node execution.

    Args:
        node: The node to execute.
        data: The input data for the node execution.
        resolve_input: Whether to resolve dynamic pins into dict/list/object.

    Returns:
        A tuple of the validated data and the block name.
        If the data is invalid, the first element will be None, and the second element
        will be an error message.
        If the data is valid, the first element will be the resolved input data, and
        the second element will be the block name.
","def validate_exec(
    node: Node,
    data: BlockInput,
    resolve_input: bool = True,
) -> tuple[BlockInput | None, str]:
    """"""
    Validate the input data for a node execution.

    Args:
        node: The node to execute.
        data: The input data for the node execution.
        resolve_input: Whether to resolve dynamic pins into dict/list/object.

    Returns:
        A tuple of the validated data and the block name.
        If the data is invalid, the first element will be None, and the second element
        will be an error message.
        If the data is valid, the first element will be the resolved input data, and
        the second element will be the block name.
    """"""
    node_block: Block | None = get_block(node.block_id)
    if not node_block:
        return None, f""Block for {node.block_id} not found.""

    error_prefix = f""Input data missing for {node_block.name}:""

    # Input data (without default values) should contain all required fields.
    input_fields_from_nodes = {link.sink_name for link in node.input_links}
    if not input_fields_from_nodes.issubset(data):
        return None, f""{error_prefix} {input_fields_from_nodes - set(data)}""

    # Merge input data with default values and resolve dynamic dict/list/object pins.
    data = {**node.input_default, **data}
    if resolve_input:
        data = merge_execution_input(data)

    # Input data post-merge should contain all required fields from the schema.
    input_fields_from_schema = node_block.input_schema.get_required_fields()
    if not input_fields_from_schema.issubset(data):
        return None, f""{error_prefix} {input_fields_from_schema - set(data)}""

    # Convert non-matching data types to the expected input schema.
    for name, data_type in node_block.input_schema.__annotations__.items():
        if (value := data.get(name)) and (type(value) is not data_type):
            data[name] = convert(value, data_type)

    # Last validation: Validate the input values against the schema.
    if error := node_block.input_schema.validate_data(data):
        error_message = f""Input data doesn't match {node_block.name}: {error}""
        logger.error(error_message)
        return None, error_message

    return data, node_block.name","Point(row=354, column=0)","Point(row=406, column=32)",,autogpt_platform/backend/backend/executor/manager.py
Executor,class,"
    This class contains event handlers for the process pool executor events.

    The main events are:
        on_node_executor_start: Initialize the process that executes the node.
        on_node_execution: Execution logic for a node.

        on_graph_executor_start: Initialize the process that executes the graph.
        on_graph_execution: Execution logic for a graph.

    The execution flow:
        1. Graph execution request is added to the queue.
        2. Graph executor loop picks the request from the queue.
        3. Graph executor loop submits the graph execution request to the executor pool.
      [on_graph_execution]
        4. Graph executor initialize the node execution queue.
        5. Graph executor adds the starting nodes to the node execution queue.
        6. Graph executor waits for all nodes to be executed.
      [on_node_execution]
        7. Node executor picks the node execution request from the queue.
        8. Node executor executes the node.
        9. Node executor enqueues the next executed nodes to the node execution queue.
","class Executor:
    """"""
    This class contains event handlers for the process pool executor events.

    The main events are:
        on_node_executor_start: Initialize the process that executes the node.
        on_node_execution: Execution logic for a node.

        on_graph_executor_start: Initialize the process that executes the graph.
        on_graph_execution: Execution logic for a graph.

    The execution flow:
        1. Graph execution request is added to the queue.
        2. Graph executor loop picks the request from the queue.
        3. Graph executor loop submits the graph execution request to the executor pool.
      [on_graph_execution]
        4. Graph executor initialize the node execution queue.
        5. Graph executor adds the starting nodes to the node execution queue.
        6. Graph executor waits for all nodes to be executed.
      [on_node_execution]
        7. Node executor picks the node execution request from the queue.
        8. Node executor executes the node.
        9. Node executor enqueues the next executed nodes to the node execution queue.
    """"""

    @classmethod
    def on_node_executor_start(cls):
        configure_logging()
        set_service_name(""NodeExecutor"")
        redis.connect()
        cls.pid = os.getpid()
        cls.db_client = get_db_client()
        cls.creds_manager = IntegrationCredentialsManager()

        # Set up shutdown handlers
        cls.shutdown_lock = threading.Lock()
        atexit.register(cls.on_node_executor_stop)  # handle regular shutdown
        signal.signal(  # handle termination
            signal.SIGTERM, lambda _, __: cls.on_node_executor_sigterm()
        )

    @classmethod
    def on_node_executor_stop(cls):
        if not cls.shutdown_lock.acquire(blocking=False):
            return  # already shutting down

        logger.info(f""[on_node_executor_stop {cls.pid}] ‚è≥ Releasing locks..."")
        cls.creds_manager.release_all_locks()
        logger.info(f""[on_node_executor_stop {cls.pid}] ‚è≥ Disconnecting Redis..."")
        redis.disconnect()
        logger.info(f""[on_node_executor_stop {cls.pid}] ‚è≥ Disconnecting DB manager..."")
        close_service_client(cls.db_client)
        logger.info(f""[on_node_executor_stop {cls.pid}] ‚úÖ Finished cleanup"")

    @classmethod
    def on_node_executor_sigterm(cls):
        llprint(f""[on_node_executor_sigterm {cls.pid}] ‚ö†Ô∏è SIGTERM received"")
        if not cls.shutdown_lock.acquire(blocking=False):
            return  # already shutting down

        llprint(f""[on_node_executor_stop {cls.pid}] ‚è≥ Releasing locks..."")
        cls.creds_manager.release_all_locks()
        llprint(f""[on_node_executor_stop {cls.pid}] ‚è≥ Disconnecting Redis..."")
        redis.disconnect()
        llprint(f""[on_node_executor_stop {cls.pid}] ‚úÖ Finished cleanup"")
        sys.exit(0)

    @classmethod
    @error_logged
    def on_node_execution(
        cls,
        q: ExecutionQueue[NodeExecution],
        node_exec: NodeExecution,
    ) -> dict[str, Any]:
        log_metadata = LogMetadata(
            user_id=node_exec.user_id,
            graph_eid=node_exec.graph_exec_id,
            graph_id=node_exec.graph_id,
            node_eid=node_exec.node_exec_id,
            node_id=node_exec.node_id,
            block_name=""-"",
        )

        execution_stats = {}
        timing_info, _ = cls._on_node_execution(
            q, node_exec, log_metadata, execution_stats
        )
        execution_stats[""walltime""] = timing_info.wall_time
        execution_stats[""cputime""] = timing_info.cpu_time

        cls.db_client.update_node_execution_stats(
            node_exec.node_exec_id, execution_stats
        )
        return execution_stats

    @classmethod
    @time_measured
    def _on_node_execution(
        cls,
        q: ExecutionQueue[NodeExecution],
        node_exec: NodeExecution,
        log_metadata: LogMetadata,
        stats: dict[str, Any] | None = None,
    ):
        try:
            log_metadata.info(f""Start node execution {node_exec.node_exec_id}"")
            for execution in execute_node(
                cls.db_client, cls.creds_manager, node_exec, stats
            ):
                q.add(execution)
            log_metadata.info(f""Finished node execution {node_exec.node_exec_id}"")
        except Exception as e:
            log_metadata.exception(
                f""Failed node execution {node_exec.node_exec_id}: {e}""
            )

    @classmethod
    def on_graph_executor_start(cls):
        configure_logging()
        set_service_name(""GraphExecutor"")

        cls.db_client = get_db_client()
        cls.pool_size = settings.config.num_node_workers
        cls.pid = os.getpid()
        cls._init_node_executor_pool()
        logger.info(
            f""Graph executor {cls.pid} started with {cls.pool_size} node workers""
        )

        # Set up shutdown handler
        atexit.register(cls.on_graph_executor_stop)

    @classmethod
    def on_graph_executor_stop(cls):
        prefix = f""[on_graph_executor_stop {cls.pid}]""
        logger.info(f""{prefix} ‚è≥ Terminating node executor pool..."")
        cls.executor.terminate()
        logger.info(f""{prefix} ‚è≥ Disconnecting DB manager..."")
        close_service_client(cls.db_client)
        logger.info(f""{prefix} ‚úÖ Finished cleanup"")

    @classmethod
    def _init_node_executor_pool(cls):
        cls.executor = Pool(
            processes=cls.pool_size,
            initializer=cls.on_node_executor_start,
        )

    @classmethod
    @error_logged
    def on_graph_execution(cls, graph_exec: GraphExecution, cancel: threading.Event):
        log_metadata = LogMetadata(
            user_id=graph_exec.user_id,
            graph_eid=graph_exec.graph_exec_id,
            graph_id=graph_exec.graph_id,
            node_id=""*"",
            node_eid=""*"",
            block_name=""-"",
        )
        timing_info, (exec_stats, error) = cls._on_graph_execution(
            graph_exec, cancel, log_metadata
        )
        exec_stats[""walltime""] = timing_info.wall_time
        exec_stats[""cputime""] = timing_info.cpu_time
        exec_stats[""error""] = str(error) if error else None
        cls.db_client.update_graph_execution_stats(
            graph_exec_id=graph_exec.graph_exec_id,
            stats=exec_stats,
        )

    @classmethod
    @time_measured
    def _on_graph_execution(
        cls,
        graph_exec: GraphExecution,
        cancel: threading.Event,
        log_metadata: LogMetadata,
    ) -> tuple[dict[str, Any], Exception | None]:
        """"""
        Returns:
            The execution statistics of the graph execution.
            The error that occurred during the execution.
        """"""
        log_metadata.info(f""Start graph execution {graph_exec.graph_exec_id}"")
        exec_stats = {
            ""nodes_walltime"": 0,
            ""nodes_cputime"": 0,
            ""node_count"": 0,
        }
        error = None
        finished = False

        def cancel_handler():
            while not cancel.is_set():
                cancel.wait(1)
            if finished:
                return
            cls.executor.terminate()
            log_metadata.info(f""Terminated graph execution {graph_exec.graph_exec_id}"")
            cls._init_node_executor_pool()

        cancel_thread = threading.Thread(target=cancel_handler)
        cancel_thread.start()

        try:
            queue = ExecutionQueue[NodeExecution]()
            for node_exec in graph_exec.start_node_execs:
                queue.add(node_exec)

            running_executions: dict[str, AsyncResult] = {}

            def make_exec_callback(exec_data: NodeExecution):
                node_id = exec_data.node_id

                def callback(result: object):
                    running_executions.pop(node_id)
                    nonlocal exec_stats
                    if isinstance(result, dict):
                        exec_stats[""node_count""] += 1
                        exec_stats[""nodes_cputime""] += result.get(""cputime"", 0)
                        exec_stats[""nodes_walltime""] += result.get(""walltime"", 0)

                return callback

            while not queue.empty():
                if cancel.is_set():
                    error = RuntimeError(""Execution is cancelled"")
                    return exec_stats, error

                exec_data = queue.get()

                # Avoid parallel execution of the same node.
                execution = running_executions.get(exec_data.node_id)
                if execution and not execution.ready():
                    # TODO (performance improvement):
                    #   Wait for the completion of the same node execution is blocking.
                    #   To improve this we need a separate queue for each node.
                    #   Re-enqueueing the data back to the queue will disrupt the order.
                    execution.wait()

                log_metadata.debug(
                    f""Dispatching node execution {exec_data.node_exec_id} ""
                    f""for node {exec_data.node_id}"",
                )
                running_executions[exec_data.node_id] = cls.executor.apply_async(
                    cls.on_node_execution,
                    (queue, exec_data),
                    callback=make_exec_callback(exec_data),
                )

                # Avoid terminating graph execution when some nodes are still running.
                while queue.empty() and running_executions:
                    log_metadata.debug(
                        f""Queue empty; running nodes: {list(running_executions.keys())}""
                    )
                    for node_id, execution in list(running_executions.items()):
                        if cancel.is_set():
                            error = RuntimeError(""Execution is cancelled"")
                            return exec_stats, error

                        if not queue.empty():
                            break  # yield to parent loop to execute new queue items

                        log_metadata.debug(f""Waiting on execution of node {node_id}"")
                        execution.wait(3)

            log_metadata.info(f""Finished graph execution {graph_exec.graph_exec_id}"")
        except Exception as e:
            log_metadata.exception(
                f""Failed graph execution {graph_exec.graph_exec_id}: {e}""
            )
            error = e
        finally:
            if not cancel.is_set():
                finished = True
                cancel.set()
            cancel_thread.join()
            return exec_stats, error","Point(row=409, column=0)","Point(row=686, column=36)",,autogpt_platform/backend/backend/executor/manager.py
Executor.on_node_executor_start,function,,"def on_node_executor_start(cls):
        configure_logging()
        set_service_name(""NodeExecutor"")
        redis.connect()
        cls.pid = os.getpid()
        cls.db_client = get_db_client()
        cls.creds_manager = IntegrationCredentialsManager()

        # Set up shutdown handlers
        cls.shutdown_lock = threading.Lock()
        atexit.register(cls.on_node_executor_stop)  # handle regular shutdown
        signal.signal(  # handle termination
            signal.SIGTERM, lambda _, __: cls.on_node_executor_sigterm()
        )","Point(row=435, column=4)","Point(row=448, column=9)",Executor,autogpt_platform/backend/backend/executor/manager.py
Executor.on_node_executor_stop,function,,"def on_node_executor_stop(cls):
        if not cls.shutdown_lock.acquire(blocking=False):
            return  # already shutting down

        logger.info(f""[on_node_executor_stop {cls.pid}] ‚è≥ Releasing locks..."")
        cls.creds_manager.release_all_locks()
        logger.info(f""[on_node_executor_stop {cls.pid}] ‚è≥ Disconnecting Redis..."")
        redis.disconnect()
        logger.info(f""[on_node_executor_stop {cls.pid}] ‚è≥ Disconnecting DB manager..."")
        close_service_client(cls.db_client)
        logger.info(f""[on_node_executor_stop {cls.pid}] ‚úÖ Finished cleanup"")","Point(row=451, column=4)","Point(row=461, column=78)",Executor,autogpt_platform/backend/backend/executor/manager.py
Executor.on_node_executor_sigterm,function,,"def on_node_executor_sigterm(cls):
        llprint(f""[on_node_executor_sigterm {cls.pid}] ‚ö†Ô∏è SIGTERM received"")
        if not cls.shutdown_lock.acquire(blocking=False):
            return  # already shutting down

        llprint(f""[on_node_executor_stop {cls.pid}] ‚è≥ Releasing locks..."")
        cls.creds_manager.release_all_locks()
        llprint(f""[on_node_executor_stop {cls.pid}] ‚è≥ Disconnecting Redis..."")
        redis.disconnect()
        llprint(f""[on_node_executor_stop {cls.pid}] ‚úÖ Finished cleanup"")
        sys.exit(0)","Point(row=464, column=4)","Point(row=474, column=19)",Executor,autogpt_platform/backend/backend/executor/manager.py
Executor.on_node_execution,function,,"def on_node_execution(
        cls,
        q: ExecutionQueue[NodeExecution],
        node_exec: NodeExecution,
    ) -> dict[str, Any]:
        log_metadata = LogMetadata(
            user_id=node_exec.user_id,
            graph_eid=node_exec.graph_exec_id,
            graph_id=node_exec.graph_id,
            node_eid=node_exec.node_exec_id,
            node_id=node_exec.node_id,
            block_name=""-"",
        )

        execution_stats = {}
        timing_info, _ = cls._on_node_execution(
            q, node_exec, log_metadata, execution_stats
        )
        execution_stats[""walltime""] = timing_info.wall_time
        execution_stats[""cputime""] = timing_info.cpu_time

        cls.db_client.update_node_execution_stats(
            node_exec.node_exec_id, execution_stats
        )
        return execution_stats","Point(row=478, column=4)","Point(row=502, column=30)",Executor,autogpt_platform/backend/backend/executor/manager.py
Executor._on_node_execution,function,,"def _on_node_execution(
        cls,
        q: ExecutionQueue[NodeExecution],
        node_exec: NodeExecution,
        log_metadata: LogMetadata,
        stats: dict[str, Any] | None = None,
    ):
        try:
            log_metadata.info(f""Start node execution {node_exec.node_exec_id}"")
            for execution in execute_node(
                cls.db_client, cls.creds_manager, node_exec, stats
            ):
                q.add(execution)
            log_metadata.info(f""Finished node execution {node_exec.node_exec_id}"")
        except Exception as e:
            log_metadata.exception(
                f""Failed node execution {node_exec.node_exec_id}: {e}""
            )","Point(row=506, column=4)","Point(row=523, column=13)",Executor,autogpt_platform/backend/backend/executor/manager.py
Executor.on_graph_executor_start,function,,"def on_graph_executor_start(cls):
        configure_logging()
        set_service_name(""GraphExecutor"")

        cls.db_client = get_db_client()
        cls.pool_size = settings.config.num_node_workers
        cls.pid = os.getpid()
        cls._init_node_executor_pool()
        logger.info(
            f""Graph executor {cls.pid} started with {cls.pool_size} node workers""
        )

        # Set up shutdown handler
        atexit.register(cls.on_graph_executor_stop)","Point(row=526, column=4)","Point(row=539, column=51)",Executor,autogpt_platform/backend/backend/executor/manager.py
Executor.on_graph_executor_stop,function,,"def on_graph_executor_stop(cls):
        prefix = f""[on_graph_executor_stop {cls.pid}]""
        logger.info(f""{prefix} ‚è≥ Terminating node executor pool..."")
        cls.executor.terminate()
        logger.info(f""{prefix} ‚è≥ Disconnecting DB manager..."")
        close_service_client(cls.db_client)
        logger.info(f""{prefix} ‚úÖ Finished cleanup"")","Point(row=542, column=4)","Point(row=548, column=53)",Executor,autogpt_platform/backend/backend/executor/manager.py
Executor._init_node_executor_pool,function,,"def _init_node_executor_pool(cls):
        cls.executor = Pool(
            processes=cls.pool_size,
            initializer=cls.on_node_executor_start,
        )","Point(row=551, column=4)","Point(row=555, column=9)",Executor,autogpt_platform/backend/backend/executor/manager.py
Executor.on_graph_execution,function,,"def on_graph_execution(cls, graph_exec: GraphExecution, cancel: threading.Event):
        log_metadata = LogMetadata(
            user_id=graph_exec.user_id,
            graph_eid=graph_exec.graph_exec_id,
            graph_id=graph_exec.graph_id,
            node_id=""*"",
            node_eid=""*"",
            block_name=""-"",
        )
        timing_info, (exec_stats, error) = cls._on_graph_execution(
            graph_exec, cancel, log_metadata
        )
        exec_stats[""walltime""] = timing_info.wall_time
        exec_stats[""cputime""] = timing_info.cpu_time
        exec_stats[""error""] = str(error) if error else None
        cls.db_client.update_graph_execution_stats(
            graph_exec_id=graph_exec.graph_exec_id,
            stats=exec_stats,
        )","Point(row=559, column=4)","Point(row=577, column=9)",Executor,autogpt_platform/backend/backend/executor/manager.py
Executor._on_graph_execution,function,"
        Returns:
            The execution statistics of the graph execution.
            The error that occurred during the execution.
","def _on_graph_execution(
        cls,
        graph_exec: GraphExecution,
        cancel: threading.Event,
        log_metadata: LogMetadata,
    ) -> tuple[dict[str, Any], Exception | None]:
        """"""
        Returns:
            The execution statistics of the graph execution.
            The error that occurred during the execution.
        """"""
        log_metadata.info(f""Start graph execution {graph_exec.graph_exec_id}"")
        exec_stats = {
            ""nodes_walltime"": 0,
            ""nodes_cputime"": 0,
            ""node_count"": 0,
        }
        error = None
        finished = False

        def cancel_handler():
            while not cancel.is_set():
                cancel.wait(1)
            if finished:
                return
            cls.executor.terminate()
            log_metadata.info(f""Terminated graph execution {graph_exec.graph_exec_id}"")
            cls._init_node_executor_pool()

        cancel_thread = threading.Thread(target=cancel_handler)
        cancel_thread.start()

        try:
            queue = ExecutionQueue[NodeExecution]()
            for node_exec in graph_exec.start_node_execs:
                queue.add(node_exec)

            running_executions: dict[str, AsyncResult] = {}

            def make_exec_callback(exec_data: NodeExecution):
                node_id = exec_data.node_id

                def callback(result: object):
                    running_executions.pop(node_id)
                    nonlocal exec_stats
                    if isinstance(result, dict):
                        exec_stats[""node_count""] += 1
                        exec_stats[""nodes_cputime""] += result.get(""cputime"", 0)
                        exec_stats[""nodes_walltime""] += result.get(""walltime"", 0)

                return callback

            while not queue.empty():
                if cancel.is_set():
                    error = RuntimeError(""Execution is cancelled"")
                    return exec_stats, error

                exec_data = queue.get()

                # Avoid parallel execution of the same node.
                execution = running_executions.get(exec_data.node_id)
                if execution and not execution.ready():
                    # TODO (performance improvement):
                    #   Wait for the completion of the same node execution is blocking.
                    #   To improve this we need a separate queue for each node.
                    #   Re-enqueueing the data back to the queue will disrupt the order.
                    execution.wait()

                log_metadata.debug(
                    f""Dispatching node execution {exec_data.node_exec_id} ""
                    f""for node {exec_data.node_id}"",
                )
                running_executions[exec_data.node_id] = cls.executor.apply_async(
                    cls.on_node_execution,
                    (queue, exec_data),
                    callback=make_exec_callback(exec_data),
                )

                # Avoid terminating graph execution when some nodes are still running.
                while queue.empty() and running_executions:
                    log_metadata.debug(
                        f""Queue empty; running nodes: {list(running_executions.keys())}""
                    )
                    for node_id, execution in list(running_executions.items()):
                        if cancel.is_set():
                            error = RuntimeError(""Execution is cancelled"")
                            return exec_stats, error

                        if not queue.empty():
                            break  # yield to parent loop to execute new queue items

                        log_metadata.debug(f""Waiting on execution of node {node_id}"")
                        execution.wait(3)

            log_metadata.info(f""Finished graph execution {graph_exec.graph_exec_id}"")
        except Exception as e:
            log_metadata.exception(
                f""Failed graph execution {graph_exec.graph_exec_id}: {e}""
            )
            error = e
        finally:
            if not cancel.is_set():
                finished = True
                cancel.set()
            cancel_thread.join()
            return exec_stats, error","Point(row=581, column=4)","Point(row=686, column=36)",Executor,autogpt_platform/backend/backend/executor/manager.py
Executor._on_graph_execution.cancel_handler,function,,"def cancel_handler():
            while not cancel.is_set():
                cancel.wait(1)
            if finished:
                return
            cls.executor.terminate()
            log_metadata.info(f""Terminated graph execution {graph_exec.graph_exec_id}"")
            cls._init_node_executor_pool()","Point(row=601, column=8)","Point(row=608, column=42)",Executor,autogpt_platform/backend/backend/executor/manager.py
Executor._on_graph_execution.make_exec_callback,function,,"def make_exec_callback(exec_data: NodeExecution):
                node_id = exec_data.node_id

                def callback(result: object):
                    running_executions.pop(node_id)
                    nonlocal exec_stats
                    if isinstance(result, dict):
                        exec_stats[""node_count""] += 1
                        exec_stats[""nodes_cputime""] += result.get(""cputime"", 0)
                        exec_stats[""nodes_walltime""] += result.get(""walltime"", 0)

                return callback","Point(row=620, column=12)","Point(row=631, column=31)",Executor,autogpt_platform/backend/backend/executor/manager.py
Executor._on_graph_execution.make_exec_callback.callback,function,,"def callback(result: object):
                    running_executions.pop(node_id)
                    nonlocal exec_stats
                    if isinstance(result, dict):
                        exec_stats[""node_count""] += 1
                        exec_stats[""nodes_cputime""] += result.get(""cputime"", 0)
                        exec_stats[""nodes_walltime""] += result.get(""walltime"", 0)","Point(row=623, column=16)","Point(row=629, column=81)",Executor,autogpt_platform/backend/backend/executor/manager.py
ExecutionManager,class,,"class ExecutionManager(AppService):

    def __init__(self):
        super().__init__()
        self.use_redis = True
        self.use_supabase = True
        self.pool_size = settings.config.num_graph_workers
        self.queue = ExecutionQueue[GraphExecution]()
        self.active_graph_runs: dict[str, tuple[Future, threading.Event]] = {}

    @classmethod
    def get_port(cls) -> int:
        return settings.config.execution_manager_port

    def run_service(self):
        from autogpt_libs.supabase_integration_credentials_store import (
            SupabaseIntegrationCredentialsStore,
        )

        self.credentials_store = SupabaseIntegrationCredentialsStore(
            redis=redis.get_redis()
        )
        self.executor = ProcessPoolExecutor(
            max_workers=self.pool_size,
            initializer=Executor.on_graph_executor_start,
        )
        sync_manager = multiprocessing.Manager()
        logger.info(
            f""[{self.service_name}] Started with max-{self.pool_size} graph workers""
        )
        while True:
            graph_exec_data = self.queue.get()
            graph_exec_id = graph_exec_data.graph_exec_id
            logger.debug(
                f""[ExecutionManager] Dispatching graph execution {graph_exec_id}""
            )
            cancel_event = sync_manager.Event()
            future = self.executor.submit(
                Executor.on_graph_execution, graph_exec_data, cancel_event
            )
            self.active_graph_runs[graph_exec_id] = (future, cancel_event)
            future.add_done_callback(
                lambda _: self.active_graph_runs.pop(graph_exec_id)
            )

    def cleanup(self):
        logger.info(f""[{__class__.__name__}] ‚è≥ Shutting down graph executor pool..."")
        self.executor.shutdown(cancel_futures=True)

        super().cleanup()

    @property
    def db_client(self) -> ""DatabaseManager"":
        return get_db_client()

    @expose
    def add_execution(
        self, graph_id: str, data: BlockInput, user_id: str
    ) -> dict[str, Any]:
        graph: Graph | None = self.db_client.get_graph(graph_id, user_id=user_id)
        if not graph:
            raise Exception(f""Graph #{graph_id} not found."")

        graph.validate_graph(for_run=True)
        self._validate_node_input_credentials(graph, user_id)

        nodes_input = []
        for node in graph.starting_nodes:
            input_data = {}
            block = get_block(node.block_id)

            # Invalid block & Note block should never be executed.
            if not block or block.block_type == BlockType.NOTE:
                continue

            # Extract request input data, and assign it to the input pin.
            if block.block_type == BlockType.INPUT:
                name = node.input_default.get(""name"")
                if name and name in data:
                    input_data = {""value"": data[name]}

            input_data, error = validate_exec(node, input_data)
            if input_data is None:
                raise Exception(error)
            else:
                nodes_input.append((node.id, input_data))

        graph_exec_id, node_execs = self.db_client.create_graph_execution(
            graph_id=graph_id,
            graph_version=graph.version,
            nodes_input=nodes_input,
            user_id=user_id,
        )

        starting_node_execs = []
        for node_exec in node_execs:
            starting_node_execs.append(
                NodeExecution(
                    user_id=user_id,
                    graph_exec_id=node_exec.graph_exec_id,
                    graph_id=node_exec.graph_id,
                    node_exec_id=node_exec.node_exec_id,
                    node_id=node_exec.node_id,
                    data=node_exec.input_data,
                )
            )
            exec_update = self.db_client.update_execution_status(
                node_exec.node_exec_id, ExecutionStatus.QUEUED, node_exec.input_data
            )
            self.db_client.send_execution_update(exec_update.model_dump())

        graph_exec = GraphExecution(
            user_id=user_id,
            graph_id=graph_id,
            graph_exec_id=graph_exec_id,
            start_node_execs=starting_node_execs,
        )
        self.queue.add(graph_exec)

        return graph_exec.model_dump()

    @expose
    def cancel_execution(self, graph_exec_id: str) -> None:
        """"""
        Mechanism:
        1. Set the cancel event
        2. Graph executor's cancel handler thread detects the event, terminates workers,
           reinitializes worker pool, and returns.
        3. Update execution statuses in DB and set `error` outputs to `""TERMINATED""`.
        """"""
        if graph_exec_id not in self.active_graph_runs:
            raise Exception(
                f""Graph execution #{graph_exec_id} not active/running: ""
                ""possibly already completed/cancelled.""
            )

        future, cancel_event = self.active_graph_runs[graph_exec_id]
        if cancel_event.is_set():
            return

        cancel_event.set()
        future.result()

        # Update the status of the unfinished node executions
        node_execs = self.db_client.get_execution_results(graph_exec_id)
        for node_exec in node_execs:
            if node_exec.status not in (
                ExecutionStatus.COMPLETED,
                ExecutionStatus.FAILED,
            ):
                self.db_client.upsert_execution_output(
                    node_exec.node_exec_id, ""error"", ""TERMINATED""
                )
                exec_update = self.db_client.update_execution_status(
                    node_exec.node_exec_id, ExecutionStatus.FAILED
                )
                self.db_client.send_execution_update(exec_update.model_dump())

    def _validate_node_input_credentials(self, graph: Graph, user_id: str):
        """"""Checks all credentials for all nodes of the graph""""""

        for node in graph.nodes:
            block = get_block(node.block_id)
            if not block:
                raise ValueError(f""Unknown block {node.block_id} for node #{node.id}"")

            # Find any fields of type CredentialsMetaInput
            model_fields = cast(type[BaseModel], block.input_schema).model_fields
            if CREDENTIALS_FIELD_NAME not in model_fields:
                continue

            field = model_fields[CREDENTIALS_FIELD_NAME]

            # The BlockSchema class enforces that a `credentials` field is always a
            # `CredentialsMetaInput`, so we can safely assume this here.
            credentials_meta_type = cast(CredentialsMetaInput, field.annotation)
            credentials_meta = credentials_meta_type.model_validate(
                node.input_default[CREDENTIALS_FIELD_NAME]
            )
            # Fetch the corresponding Credentials and perform sanity checks
            credentials = self.credentials_store.get_creds_by_id(
                user_id, credentials_meta.id
            )
            if not credentials:
                raise ValueError(
                    f""Unknown credentials #{credentials_meta.id} ""
                    f""for node #{node.id}""
                )
            if (
                credentials.provider != credentials_meta.provider
                or credentials.type != credentials_meta.type
            ):
                logger.warning(
                    f""Invalid credentials #{credentials.id} for node #{node.id}: ""
                    ""type/provider mismatch: ""
                    f""{credentials_meta.type}<>{credentials.type};""
                    f""{credentials_meta.provider}<>{credentials.provider}""
                )
                raise ValueError(
                    f""Invalid credentials #{credentials.id} for node #{node.id}: ""
                    ""type/provider mismatch""
                )","Point(row=689, column=0)","Point(row=890, column=17)",,autogpt_platform/backend/backend/executor/manager.py
ExecutionManager.__init__,function,,"def __init__(self):
        super().__init__()
        self.use_redis = True
        self.use_supabase = True
        self.pool_size = settings.config.num_graph_workers
        self.queue = ExecutionQueue[GraphExecution]()
        self.active_graph_runs: dict[str, tuple[Future, threading.Event]] = {}","Point(row=691, column=4)","Point(row=697, column=78)",ExecutionManager,autogpt_platform/backend/backend/executor/manager.py
ExecutionManager.get_port,function,,"def get_port(cls) -> int:
        return settings.config.execution_manager_port","Point(row=700, column=4)","Point(row=701, column=53)",ExecutionManager,autogpt_platform/backend/backend/executor/manager.py
ExecutionManager.run_service,function,,"def run_service(self):
        from autogpt_libs.supabase_integration_credentials_store import (
            SupabaseIntegrationCredentialsStore,
        )

        self.credentials_store = SupabaseIntegrationCredentialsStore(
            redis=redis.get_redis()
        )
        self.executor = ProcessPoolExecutor(
            max_workers=self.pool_size,
            initializer=Executor.on_graph_executor_start,
        )
        sync_manager = multiprocessing.Manager()
        logger.info(
            f""[{self.service_name}] Started with max-{self.pool_size} graph workers""
        )
        while True:
            graph_exec_data = self.queue.get()
            graph_exec_id = graph_exec_data.graph_exec_id
            logger.debug(
                f""[ExecutionManager] Dispatching graph execution {graph_exec_id}""
            )
            cancel_event = sync_manager.Event()
            future = self.executor.submit(
                Executor.on_graph_execution, graph_exec_data, cancel_event
            )
            self.active_graph_runs[graph_exec_id] = (future, cancel_event)
            future.add_done_callback(
                lambda _: self.active_graph_runs.pop(graph_exec_id)
            )","Point(row=703, column=4)","Point(row=732, column=13)",ExecutionManager,autogpt_platform/backend/backend/executor/manager.py
ExecutionManager.cleanup,function,,"def cleanup(self):
        logger.info(f""[{__class__.__name__}] ‚è≥ Shutting down graph executor pool..."")
        self.executor.shutdown(cancel_futures=True)

        super().cleanup()","Point(row=734, column=4)","Point(row=738, column=25)",ExecutionManager,autogpt_platform/backend/backend/executor/manager.py
ExecutionManager.db_client,function,,"def db_client(self) -> ""DatabaseManager"":
        return get_db_client()","Point(row=741, column=4)","Point(row=742, column=30)",ExecutionManager,autogpt_platform/backend/backend/executor/manager.py
ExecutionManager.add_execution,function,,"def add_execution(
        self, graph_id: str, data: BlockInput, user_id: str
    ) -> dict[str, Any]:
        graph: Graph | None = self.db_client.get_graph(graph_id, user_id=user_id)
        if not graph:
            raise Exception(f""Graph #{graph_id} not found."")

        graph.validate_graph(for_run=True)
        self._validate_node_input_credentials(graph, user_id)

        nodes_input = []
        for node in graph.starting_nodes:
            input_data = {}
            block = get_block(node.block_id)

            # Invalid block & Note block should never be executed.
            if not block or block.block_type == BlockType.NOTE:
                continue

            # Extract request input data, and assign it to the input pin.
            if block.block_type == BlockType.INPUT:
                name = node.input_default.get(""name"")
                if name and name in data:
                    input_data = {""value"": data[name]}

            input_data, error = validate_exec(node, input_data)
            if input_data is None:
                raise Exception(error)
            else:
                nodes_input.append((node.id, input_data))

        graph_exec_id, node_execs = self.db_client.create_graph_execution(
            graph_id=graph_id,
            graph_version=graph.version,
            nodes_input=nodes_input,
            user_id=user_id,
        )

        starting_node_execs = []
        for node_exec in node_execs:
            starting_node_execs.append(
                NodeExecution(
                    user_id=user_id,
                    graph_exec_id=node_exec.graph_exec_id,
                    graph_id=node_exec.graph_id,
                    node_exec_id=node_exec.node_exec_id,
                    node_id=node_exec.node_id,
                    data=node_exec.input_data,
                )
            )
            exec_update = self.db_client.update_execution_status(
                node_exec.node_exec_id, ExecutionStatus.QUEUED, node_exec.input_data
            )
            self.db_client.send_execution_update(exec_update.model_dump())

        graph_exec = GraphExecution(
            user_id=user_id,
            graph_id=graph_id,
            graph_exec_id=graph_exec_id,
            start_node_execs=starting_node_execs,
        )
        self.queue.add(graph_exec)

        return graph_exec.model_dump()","Point(row=745, column=4)","Point(row=808, column=38)",ExecutionManager,autogpt_platform/backend/backend/executor/manager.py
ExecutionManager.cancel_execution,function,"
        Mechanism:
        1. Set the cancel event
        2. Graph executor's cancel handler thread detects the event, terminates workers,
           reinitializes worker pool, and returns.
        3. Update execution statuses in DB and set `error` outputs to `""TERMINATED""`.
","def cancel_execution(self, graph_exec_id: str) -> None:
        """"""
        Mechanism:
        1. Set the cancel event
        2. Graph executor's cancel handler thread detects the event, terminates workers,
           reinitializes worker pool, and returns.
        3. Update execution statuses in DB and set `error` outputs to `""TERMINATED""`.
        """"""
        if graph_exec_id not in self.active_graph_runs:
            raise Exception(
                f""Graph execution #{graph_exec_id} not active/running: ""
                ""possibly already completed/cancelled.""
            )

        future, cancel_event = self.active_graph_runs[graph_exec_id]
        if cancel_event.is_set():
            return

        cancel_event.set()
        future.result()

        # Update the status of the unfinished node executions
        node_execs = self.db_client.get_execution_results(graph_exec_id)
        for node_exec in node_execs:
            if node_exec.status not in (
                ExecutionStatus.COMPLETED,
                ExecutionStatus.FAILED,
            ):
                self.db_client.upsert_execution_output(
                    node_exec.node_exec_id, ""error"", ""TERMINATED""
                )
                exec_update = self.db_client.update_execution_status(
                    node_exec.node_exec_id, ExecutionStatus.FAILED
                )
                self.db_client.send_execution_update(exec_update.model_dump())","Point(row=811, column=4)","Point(row=845, column=78)",ExecutionManager,autogpt_platform/backend/backend/executor/manager.py
ExecutionManager._validate_node_input_credentials,function,Checks all credentials for all nodes of the graph,"def _validate_node_input_credentials(self, graph: Graph, user_id: str):
        """"""Checks all credentials for all nodes of the graph""""""

        for node in graph.nodes:
            block = get_block(node.block_id)
            if not block:
                raise ValueError(f""Unknown block {node.block_id} for node #{node.id}"")

            # Find any fields of type CredentialsMetaInput
            model_fields = cast(type[BaseModel], block.input_schema).model_fields
            if CREDENTIALS_FIELD_NAME not in model_fields:
                continue

            field = model_fields[CREDENTIALS_FIELD_NAME]

            # The BlockSchema class enforces that a `credentials` field is always a
            # `CredentialsMetaInput`, so we can safely assume this here.
            credentials_meta_type = cast(CredentialsMetaInput, field.annotation)
            credentials_meta = credentials_meta_type.model_validate(
                node.input_default[CREDENTIALS_FIELD_NAME]
            )
            # Fetch the corresponding Credentials and perform sanity checks
            credentials = self.credentials_store.get_creds_by_id(
                user_id, credentials_meta.id
            )
            if not credentials:
                raise ValueError(
                    f""Unknown credentials #{credentials_meta.id} ""
                    f""for node #{node.id}""
                )
            if (
                credentials.provider != credentials_meta.provider
                or credentials.type != credentials_meta.type
            ):
                logger.warning(
                    f""Invalid credentials #{credentials.id} for node #{node.id}: ""
                    ""type/provider mismatch: ""
                    f""{credentials_meta.type}<>{credentials.type};""
                    f""{credentials_meta.provider}<>{credentials.provider}""
                )
                raise ValueError(
                    f""Invalid credentials #{credentials.id} for node #{node.id}: ""
                    ""type/provider mismatch""
                )","Point(row=847, column=4)","Point(row=890, column=17)",ExecutionManager,autogpt_platform/backend/backend/executor/manager.py
get_db_client,function,,"def get_db_client() -> ""DatabaseManager"":
    from backend.executor import DatabaseManager

    return get_service_client(DatabaseManager)","Point(row=897, column=0)","Point(row=900, column=46)",,autogpt_platform/backend/backend/executor/manager.py
synchronized,function,,"def synchronized(key: str, timeout: int = 60):
    lock: RedisLock = redis.get_redis().lock(f""lock:{key}"", timeout=timeout)
    try:
        lock.acquire()
        yield
    finally:
        lock.release()","Point(row=904, column=0)","Point(row=910, column=22)",,autogpt_platform/backend/backend/executor/manager.py
llprint,function,"
    Low-level print/log helper function for use in signal handlers.
    Regular log/print statements are not allowed in signal handlers.
","def llprint(message: str):
    """"""
    Low-level print/log helper function for use in signal handlers.
    Regular log/print statements are not allowed in signal handlers.
    """"""
    if logger.getEffectiveLevel() == logging.DEBUG:
        os.write(sys.stdout.fileno(), (message + ""\n"").encode())","Point(row=913, column=0)","Point(row=919, column=64)",,autogpt_platform/backend/backend/executor/manager.py
DateTimeEncoder,class,,"class DateTimeEncoder(json.JSONEncoder):
    def default(self, o):
        if isinstance(o, datetime):
            return o.isoformat()
        return super().default(o)","Point(row=18, column=0)","Point(row=22, column=33)",,autogpt_platform/backend/backend/data/queue.py
DateTimeEncoder.default,function,,"def default(self, o):
        if isinstance(o, datetime):
            return o.isoformat()
        return super().default(o)","Point(row=19, column=4)","Point(row=22, column=33)",DateTimeEncoder,autogpt_platform/backend/backend/data/queue.py
BaseRedisEventBus,class,,"class BaseRedisEventBus(Generic[M], ABC):
    Model: type[M]

    @property
    @abstractmethod
    def event_bus_name(self) -> str:
        pass

    def _serialize_message(self, item: M, channel_key: str) -> tuple[str, str]:
        message = json.dumps(item.model_dump(), cls=DateTimeEncoder)
        channel_name = f""{self.event_bus_name}-{channel_key}""
        logger.info(f""[{channel_name}] Publishing an event to Redis {message}"")
        return message, channel_name

    def _deserialize_message(self, msg: Any, channel_key: str) -> M | None:
        message_type = ""pmessage"" if ""*"" in channel_key else ""message""
        if msg[""type""] != message_type:
            return None
        try:
            data = json.loads(msg[""data""])
            logger.info(f""Consuming an event from Redis {data}"")
            return self.Model(**data)
        except Exception as e:
            logger.error(f""Failed to parse event result from Redis {msg} {e}"")

    def _subscribe(
        self, connection: redis.Redis | redis.AsyncRedis, channel_key: str
    ) -> tuple[PubSub | AsyncPubSub, str]:
        channel_name = f""{self.event_bus_name}-{channel_key}""
        pubsub = connection.pubsub()
        return pubsub, channel_name","Point(row=28, column=0)","Point(row=58, column=35)",,autogpt_platform/backend/backend/data/queue.py
BaseRedisEventBus.event_bus_name,function,,"def event_bus_name(self) -> str:
        pass","Point(row=33, column=4)","Point(row=34, column=12)",BaseRedisEventBus,autogpt_platform/backend/backend/data/queue.py
BaseRedisEventBus._serialize_message,function,,"def _serialize_message(self, item: M, channel_key: str) -> tuple[str, str]:
        message = json.dumps(item.model_dump(), cls=DateTimeEncoder)
        channel_name = f""{self.event_bus_name}-{channel_key}""
        logger.info(f""[{channel_name}] Publishing an event to Redis {message}"")
        return message, channel_name","Point(row=36, column=4)","Point(row=40, column=36)",BaseRedisEventBus,autogpt_platform/backend/backend/data/queue.py
BaseRedisEventBus._deserialize_message,function,,"def _deserialize_message(self, msg: Any, channel_key: str) -> M | None:
        message_type = ""pmessage"" if ""*"" in channel_key else ""message""
        if msg[""type""] != message_type:
            return None
        try:
            data = json.loads(msg[""data""])
            logger.info(f""Consuming an event from Redis {data}"")
            return self.Model(**data)
        except Exception as e:
            logger.error(f""Failed to parse event result from Redis {msg} {e}"")","Point(row=42, column=4)","Point(row=51, column=78)",BaseRedisEventBus,autogpt_platform/backend/backend/data/queue.py
BaseRedisEventBus._subscribe,function,,"def _subscribe(
        self, connection: redis.Redis | redis.AsyncRedis, channel_key: str
    ) -> tuple[PubSub | AsyncPubSub, str]:
        channel_name = f""{self.event_bus_name}-{channel_key}""
        pubsub = connection.pubsub()
        return pubsub, channel_name","Point(row=53, column=4)","Point(row=58, column=35)",BaseRedisEventBus,autogpt_platform/backend/backend/data/queue.py
RedisEventBus,class,,"class RedisEventBus(BaseRedisEventBus[M], ABC):
    Model: type[M]

    @property
    def connection(self) -> redis.Redis:
        return redis.get_redis()

    def publish_event(self, event: M, channel_key: str):
        message, channel_name = self._serialize_message(event, channel_key)
        self.connection.publish(channel_name, message)

    def listen_events(self, channel_key: str) -> Generator[M, None, None]:
        pubsub, channel_name = self._subscribe(self.connection, channel_key)
        assert isinstance(pubsub, PubSub)

        if ""*"" in channel_key:
            pubsub.psubscribe(channel_name)
        else:
            pubsub.subscribe(channel_name)

        for message in pubsub.listen():
            if event := self._deserialize_message(message, channel_key):
                yield event","Point(row=61, column=0)","Point(row=83, column=27)",,autogpt_platform/backend/backend/data/queue.py
RedisEventBus.connection,function,,"def connection(self) -> redis.Redis:
        return redis.get_redis()","Point(row=65, column=4)","Point(row=66, column=32)",RedisEventBus,autogpt_platform/backend/backend/data/queue.py
RedisEventBus.publish_event,function,,"def publish_event(self, event: M, channel_key: str):
        message, channel_name = self._serialize_message(event, channel_key)
        self.connection.publish(channel_name, message)","Point(row=68, column=4)","Point(row=70, column=54)",RedisEventBus,autogpt_platform/backend/backend/data/queue.py
RedisEventBus.listen_events,function,,"def listen_events(self, channel_key: str) -> Generator[M, None, None]:
        pubsub, channel_name = self._subscribe(self.connection, channel_key)
        assert isinstance(pubsub, PubSub)

        if ""*"" in channel_key:
            pubsub.psubscribe(channel_name)
        else:
            pubsub.subscribe(channel_name)

        for message in pubsub.listen():
            if event := self._deserialize_message(message, channel_key):
                yield event","Point(row=72, column=4)","Point(row=83, column=27)",RedisEventBus,autogpt_platform/backend/backend/data/queue.py
AsyncRedisEventBus,class,,"class AsyncRedisEventBus(BaseRedisEventBus[M], ABC):
    Model: type[M]

    @property
    async def connection(self) -> redis.AsyncRedis:
        return await redis.get_redis_async()

    async def publish_event(self, event: M, channel_key: str):
        message, channel_name = self._serialize_message(event, channel_key)
        connection = await self.connection
        await connection.publish(channel_name, message)

    async def listen_events(self, channel_key: str) -> AsyncGenerator[M, None]:
        pubsub, channel_name = self._subscribe(await self.connection, channel_key)
        assert isinstance(pubsub, AsyncPubSub)

        if ""*"" in channel_key:
            await pubsub.psubscribe(channel_name)
        else:
            await pubsub.subscribe(channel_name)

        async for message in pubsub.listen():
            if event := self._deserialize_message(message, channel_key):
                yield event","Point(row=86, column=0)","Point(row=109, column=27)",,autogpt_platform/backend/backend/data/queue.py
AsyncRedisEventBus.connection,function,,"async def connection(self) -> redis.AsyncRedis:
        return await redis.get_redis_async()","Point(row=90, column=4)","Point(row=91, column=44)",AsyncRedisEventBus,autogpt_platform/backend/backend/data/queue.py
AsyncRedisEventBus.publish_event,function,,"async def publish_event(self, event: M, channel_key: str):
        message, channel_name = self._serialize_message(event, channel_key)
        connection = await self.connection
        await connection.publish(channel_name, message)","Point(row=93, column=4)","Point(row=96, column=55)",AsyncRedisEventBus,autogpt_platform/backend/backend/data/queue.py
AsyncRedisEventBus.listen_events,function,,"async def listen_events(self, channel_key: str) -> AsyncGenerator[M, None]:
        pubsub, channel_name = self._subscribe(await self.connection, channel_key)
        assert isinstance(pubsub, AsyncPubSub)

        if ""*"" in channel_key:
            await pubsub.psubscribe(channel_name)
        else:
            await pubsub.subscribe(channel_name)

        async for message in pubsub.listen():
            if event := self._deserialize_message(message, channel_key):
                yield event","Point(row=98, column=4)","Point(row=109, column=27)",AsyncRedisEventBus,autogpt_platform/backend/backend/data/queue.py
RedisExecutionEventBus,class,,"class RedisExecutionEventBus(RedisEventBus[ExecutionResult]):
    Model = ExecutionResult

    @property
    def event_bus_name(self) -> str:
        return config.execution_event_bus_name

    def publish(self, res: ExecutionResult):
        self.publish_event(res, f""{res.graph_id}-{res.graph_exec_id}"")

    def listen(
        self, graph_id: str = ""*"", graph_exec_id: str = ""*""
    ) -> Generator[ExecutionResult, None, None]:
        for execution_result in self.listen_events(f""{graph_id}-{graph_exec_id}""):
            yield execution_result","Point(row=112, column=0)","Point(row=126, column=34)",,autogpt_platform/backend/backend/data/queue.py
RedisExecutionEventBus.event_bus_name,function,,"def event_bus_name(self) -> str:
        return config.execution_event_bus_name","Point(row=116, column=4)","Point(row=117, column=46)",RedisExecutionEventBus,autogpt_platform/backend/backend/data/queue.py
RedisExecutionEventBus.publish,function,,"def publish(self, res: ExecutionResult):
        self.publish_event(res, f""{res.graph_id}-{res.graph_exec_id}"")","Point(row=119, column=4)","Point(row=120, column=70)",RedisExecutionEventBus,autogpt_platform/backend/backend/data/queue.py
RedisExecutionEventBus.listen,function,,"def listen(
        self, graph_id: str = ""*"", graph_exec_id: str = ""*""
    ) -> Generator[ExecutionResult, None, None]:
        for execution_result in self.listen_events(f""{graph_id}-{graph_exec_id}""):
            yield execution_result","Point(row=122, column=4)","Point(row=126, column=34)",RedisExecutionEventBus,autogpt_platform/backend/backend/data/queue.py
AsyncRedisExecutionEventBus,class,,"class AsyncRedisExecutionEventBus(AsyncRedisEventBus[ExecutionResult]):
    Model = ExecutionResult

    @property
    def event_bus_name(self) -> str:
        return config.execution_event_bus_name

    async def publish(self, res: ExecutionResult):
        await self.publish_event(res, f""{res.graph_id}-{res.graph_exec_id}"")

    async def listen(
        self, graph_id: str = ""*"", graph_exec_id: str = ""*""
    ) -> AsyncGenerator[ExecutionResult, None]:
        async for execution_result in self.listen_events(f""{graph_id}-{graph_exec_id}""):
            yield execution_result","Point(row=129, column=0)","Point(row=143, column=34)",,autogpt_platform/backend/backend/data/queue.py
AsyncRedisExecutionEventBus.event_bus_name,function,,"def event_bus_name(self) -> str:
        return config.execution_event_bus_name","Point(row=133, column=4)","Point(row=134, column=46)",AsyncRedisExecutionEventBus,autogpt_platform/backend/backend/data/queue.py
AsyncRedisExecutionEventBus.publish,function,,"async def publish(self, res: ExecutionResult):
        await self.publish_event(res, f""{res.graph_id}-{res.graph_exec_id}"")","Point(row=136, column=4)","Point(row=137, column=76)",AsyncRedisExecutionEventBus,autogpt_platform/backend/backend/data/queue.py
AsyncRedisExecutionEventBus.listen,function,,"async def listen(
        self, graph_id: str = ""*"", graph_exec_id: str = ""*""
    ) -> AsyncGenerator[ExecutionResult, None]:
        async for execution_result in self.listen_events(f""{graph_id}-{graph_exec_id}""):
            yield execution_result","Point(row=139, column=4)","Point(row=143, column=34)",AsyncRedisExecutionEventBus,autogpt_platform/backend/backend/data/queue.py
connect,function,,"async def connect():
    if prisma.is_connected():
        return
    await prisma.connect()","Point(row=22, column=0)","Point(row=25, column=26)",,autogpt_platform/backend/backend/data/db.py
disconnect,function,,"async def disconnect():
    if not prisma.is_connected():
        return
    await prisma.disconnect()","Point(row=29, column=0)","Point(row=32, column=29)",,autogpt_platform/backend/backend/data/db.py
transaction,function,,"async def transaction():
    async with prisma.tx() as tx:
        yield tx","Point(row=36, column=0)","Point(row=38, column=16)",,autogpt_platform/backend/backend/data/db.py
BaseDbModel,class,,"class BaseDbModel(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid4()))

    @field_validator(""id"", mode=""before"")
    def set_model_id(cls, id: str) -> str:
        # In case an empty ID is submitted
        return id or str(uuid4())","Point(row=41, column=0)","Point(row=47, column=33)",,autogpt_platform/backend/backend/data/db.py
BaseDbModel.set_model_id,function,,"def set_model_id(cls, id: str) -> str:
        # In case an empty ID is submitted
        return id or str(uuid4())","Point(row=45, column=4)","Point(row=47, column=33)",BaseDbModel,autogpt_platform/backend/backend/data/db.py
get_or_create_user,function,,"async def get_or_create_user(user_data: dict) -> User:
    user_id = user_data.get(""sub"")
    if not user_id:
        raise HTTPException(status_code=401, detail=""User ID not found in token"")

    user_email = user_data.get(""email"")
    if not user_email:
        raise HTTPException(status_code=401, detail=""Email not found in token"")

    user = await prisma.user.find_unique(where={""id"": user_id})
    if not user:
        user = await prisma.user.create(
            data={
                ""id"": user_id,
                ""email"": user_email,
                ""name"": user_data.get(""user_metadata"", {}).get(""name""),
            }
        )
    return User.model_validate(user)","Point(row=21, column=0)","Point(row=39, column=36)",,autogpt_platform/backend/backend/data/user.py
get_user_by_id,function,,"async def get_user_by_id(user_id: str) -> Optional[User]:
    user = await prisma.user.find_unique(where={""id"": user_id})
    return User.model_validate(user) if user else None","Point(row=42, column=0)","Point(row=44, column=54)",,autogpt_platform/backend/backend/data/user.py
create_default_user,function,,"async def create_default_user() -> Optional[User]:
    user = await prisma.user.find_unique(where={""id"": DEFAULT_USER_ID})
    if not user:
        user = await prisma.user.create(
            data={
                ""id"": DEFAULT_USER_ID,
                ""email"": ""default@example.com"",
                ""name"": ""Default User"",
            }
        )
    return User.model_validate(user)","Point(row=47, column=0)","Point(row=57, column=36)",,autogpt_platform/backend/backend/data/user.py
get_user_metadata,function,,"async def get_user_metadata(user_id: str) -> UserMetadata:
    user = await User.prisma().find_unique_or_raise(
        where={""id"": user_id},
    )

    metadata = cast(UserMetadataRaw, user.metadata)
    return UserMetadata.model_validate(metadata)","Point(row=60, column=0)","Point(row=66, column=48)",,autogpt_platform/backend/backend/data/user.py
update_user_metadata,function,,"async def update_user_metadata(user_id: str, metadata: UserMetadata):
    await User.prisma().update(
        where={""id"": user_id},
        data={""metadata"": Json(metadata.model_dump())},
    )","Point(row=69, column=0)","Point(row=73, column=5)",,autogpt_platform/backend/backend/data/user.py
get_user_integrations,function,,"async def get_user_integrations(user_id: str) -> UserIntegrations:
    user = await User.prisma().find_unique_or_raise(
        where={""id"": user_id},
    )

    encrypted_integrations = user.integrations
    if not encrypted_integrations:
        return UserIntegrations()
    else:
        return UserIntegrations.model_validate(
            JSONCryptor().decrypt(encrypted_integrations)
        )","Point(row=76, column=0)","Point(row=87, column=9)",,autogpt_platform/backend/backend/data/user.py
update_user_integrations,function,,"async def update_user_integrations(user_id: str, data: UserIntegrations):
    encrypted_data = JSONCryptor().encrypt(data.model_dump())
    await User.prisma().update(
        where={""id"": user_id},
        data={""integrations"": encrypted_data},
    )","Point(row=90, column=0)","Point(row=95, column=5)",,autogpt_platform/backend/backend/data/user.py
migrate_and_encrypt_user_integrations,function,Migrate integration credentials and OAuth states from metadata to integrations column.,"async def migrate_and_encrypt_user_integrations():
    """"""Migrate integration credentials and OAuth states from metadata to integrations column.""""""
    users = await User.prisma().find_many(
        where={
            ""metadata"": {
                ""path"": [""integration_credentials""],
                ""not"": Json({""a"": ""yolo""}),  # bogus value works to check if key exists
            }  # type: ignore
        }
    )
    logger.info(f""Migrating integration credentials for {len(users)} users"")

    for user in users:
        raw_metadata = cast(UserMetadataRaw, user.metadata)
        metadata = UserMetadata.model_validate(raw_metadata)

        # Get existing integrations data
        integrations = await get_user_integrations(user_id=user.id)

        # Copy credentials and oauth states from metadata if they exist
        if metadata.integration_credentials and not integrations.credentials:
            integrations.credentials = metadata.integration_credentials
        if metadata.integration_oauth_states:
            integrations.oauth_states = metadata.integration_oauth_states

        # Save to integrations column
        await update_user_integrations(user_id=user.id, data=integrations)

        # Remove from metadata
        raw_metadata = dict(raw_metadata)
        raw_metadata.pop(""integration_credentials"", None)
        raw_metadata.pop(""integration_oauth_states"", None)

        # Update metadata without integration data
        await User.prisma().update(
            where={""id"": user.id},
            data={""metadata"": Json(raw_metadata)},
        )","Point(row=98, column=0)","Point(row=135, column=9)",,autogpt_platform/backend/backend/data/user.py
GraphExecution,class,,"class GraphExecution(BaseModel):
    user_id: str
    graph_exec_id: str
    graph_id: str
    start_node_execs: list[""NodeExecution""]","Point(row=19, column=0)","Point(row=23, column=43)",,autogpt_platform/backend/backend/data/execution.py
NodeExecution,class,,"class NodeExecution(BaseModel):
    user_id: str
    graph_exec_id: str
    graph_id: str
    node_exec_id: str
    node_id: str
    data: BlockInput","Point(row=26, column=0)","Point(row=32, column=20)",,autogpt_platform/backend/backend/data/execution.py
ExecutionQueue,class,"
    Queue for managing the execution of agents.
    This will be shared between different processes
","class ExecutionQueue(Generic[T]):
    """"""
    Queue for managing the execution of agents.
    This will be shared between different processes
    """"""

    def __init__(self):
        self.queue = Manager().Queue()

    def add(self, execution: T) -> T:
        self.queue.put(execution)
        return execution

    def get(self) -> T:
        return self.queue.get()

    def empty(self) -> bool:
        return self.queue.empty()","Point(row=40, column=0)","Point(row=57, column=33)",,autogpt_platform/backend/backend/data/execution.py
ExecutionQueue.__init__,function,,"def __init__(self):
        self.queue = Manager().Queue()","Point(row=46, column=4)","Point(row=47, column=38)",ExecutionQueue,autogpt_platform/backend/backend/data/execution.py
ExecutionQueue.add,function,,"def add(self, execution: T) -> T:
        self.queue.put(execution)
        return execution","Point(row=49, column=4)","Point(row=51, column=24)",ExecutionQueue,autogpt_platform/backend/backend/data/execution.py
ExecutionQueue.get,function,,"def get(self) -> T:
        return self.queue.get()","Point(row=53, column=4)","Point(row=54, column=31)",ExecutionQueue,autogpt_platform/backend/backend/data/execution.py
ExecutionQueue.empty,function,,"def empty(self) -> bool:
        return self.queue.empty()","Point(row=56, column=4)","Point(row=57, column=33)",ExecutionQueue,autogpt_platform/backend/backend/data/execution.py
ExecutionResult,class,,"class ExecutionResult(BaseModel):
    graph_id: str
    graph_version: int
    graph_exec_id: str
    node_exec_id: str
    node_id: str
    status: ExecutionStatus
    input_data: BlockInput
    output_data: CompletedBlockOutput
    add_time: datetime
    queue_time: datetime | None
    start_time: datetime | None
    end_time: datetime | None

    @staticmethod
    def from_db(execution: AgentNodeExecution):
        if execution.executionData:
            # Execution that has been queued for execution will persist its data.
            input_data = json.loads(execution.executionData)
        else:
            # For incomplete execution, executionData will not be yet available.
            input_data: BlockInput = defaultdict()
            for data in execution.Input or []:
                input_data[data.name] = json.loads(data.data)

        output_data: CompletedBlockOutput = defaultdict(list)
        for data in execution.Output or []:
            output_data[data.name].append(json.loads(data.data))

        graph_execution: AgentGraphExecution | None = execution.AgentGraphExecution

        return ExecutionResult(
            graph_id=graph_execution.agentGraphId if graph_execution else """",
            graph_version=graph_execution.agentGraphVersion if graph_execution else 0,
            graph_exec_id=execution.agentGraphExecutionId,
            node_exec_id=execution.id,
            node_id=execution.agentNodeId,
            status=ExecutionStatus(execution.executionStatus),
            input_data=input_data,
            output_data=output_data,
            add_time=execution.addedTime,
            queue_time=execution.queuedTime,
            start_time=execution.startedTime,
            end_time=execution.endedTime,
        )","Point(row=60, column=0)","Point(row=104, column=9)",,autogpt_platform/backend/backend/data/execution.py
ExecutionResult.from_db,function,,"def from_db(execution: AgentNodeExecution):
        if execution.executionData:
            # Execution that has been queued for execution will persist its data.
            input_data = json.loads(execution.executionData)
        else:
            # For incomplete execution, executionData will not be yet available.
            input_data: BlockInput = defaultdict()
            for data in execution.Input or []:
                input_data[data.name] = json.loads(data.data)

        output_data: CompletedBlockOutput = defaultdict(list)
        for data in execution.Output or []:
            output_data[data.name].append(json.loads(data.data))

        graph_execution: AgentGraphExecution | None = execution.AgentGraphExecution

        return ExecutionResult(
            graph_id=graph_execution.agentGraphId if graph_execution else """",
            graph_version=graph_execution.agentGraphVersion if graph_execution else 0,
            graph_exec_id=execution.agentGraphExecutionId,
            node_exec_id=execution.id,
            node_id=execution.agentNodeId,
            status=ExecutionStatus(execution.executionStatus),
            input_data=input_data,
            output_data=output_data,
            add_time=execution.addedTime,
            queue_time=execution.queuedTime,
            start_time=execution.startedTime,
            end_time=execution.endedTime,
        )","Point(row=75, column=4)","Point(row=104, column=9)",ExecutionResult,autogpt_platform/backend/backend/data/execution.py
create_graph_execution,function,"
    Create a new AgentGraphExecution record.
    Returns:
        The id of the AgentGraphExecution and the list of ExecutionResult for each node.
","async def create_graph_execution(
    graph_id: str,
    graph_version: int,
    nodes_input: list[tuple[str, BlockInput]],
    user_id: str,
) -> tuple[str, list[ExecutionResult]]:
    """"""
    Create a new AgentGraphExecution record.
    Returns:
        The id of the AgentGraphExecution and the list of ExecutionResult for each node.
    """"""
    result = await AgentGraphExecution.prisma().create(
        data={
            ""agentGraphId"": graph_id,
            ""agentGraphVersion"": graph_version,
            ""executionStatus"": ExecutionStatus.QUEUED,
            ""AgentNodeExecutions"": {
                ""create"": [  # type: ignore
                    {
                        ""agentNodeId"": node_id,
                        ""executionStatus"": ExecutionStatus.INCOMPLETE,
                        ""Input"": {
                            ""create"": [
                                {""name"": name, ""data"": json.dumps(data)}
                                for name, data in node_input.items()
                            ]
                        },
                    }
                    for node_id, node_input in nodes_input
                ]
            },
            ""userId"": user_id,
        },
        include=GRAPH_EXECUTION_INCLUDE,
    )

    return result.id, [
        ExecutionResult.from_db(execution)
        for execution in result.AgentNodeExecutions or []
    ]","Point(row=110, column=0)","Point(row=149, column=5)",,autogpt_platform/backend/backend/data/execution.py
upsert_execution_input,function,"
    Insert AgentNodeExecutionInputOutput record for as one of AgentNodeExecution.Input.
    If there is no AgentNodeExecution that has no `input_name` as input, create new one.

    Args:
        node_id: The id of the AgentNode.
        graph_exec_id: The id of the AgentGraphExecution.
        input_name: The name of the input data.
        input_data: The input data to be inserted.
        node_exec_id: [Optional] The id of the AgentNodeExecution that has no `input_name` as input. If not provided, it will find the eligible incomplete AgentNodeExecution or create a new one.

    Returns:
        * The id of the created or existing AgentNodeExecution.
        * Dict of node input data, key is the input name, value is the input data.
","async def upsert_execution_input(
    node_id: str,
    graph_exec_id: str,
    input_name: str,
    input_data: Any,
    node_exec_id: str | None = None,
) -> tuple[str, BlockInput]:
    """"""
    Insert AgentNodeExecutionInputOutput record for as one of AgentNodeExecution.Input.
    If there is no AgentNodeExecution that has no `input_name` as input, create new one.

    Args:
        node_id: The id of the AgentNode.
        graph_exec_id: The id of the AgentGraphExecution.
        input_name: The name of the input data.
        input_data: The input data to be inserted.
        node_exec_id: [Optional] The id of the AgentNodeExecution that has no `input_name` as input. If not provided, it will find the eligible incomplete AgentNodeExecution or create a new one.

    Returns:
        * The id of the created or existing AgentNodeExecution.
        * Dict of node input data, key is the input name, value is the input data.
    """"""
    existing_execution = await AgentNodeExecution.prisma().find_first(
        where={  # type: ignore
            **({""id"": node_exec_id} if node_exec_id else {}),
            ""agentNodeId"": node_id,
            ""agentGraphExecutionId"": graph_exec_id,
            ""executionStatus"": ExecutionStatus.INCOMPLETE,
            ""Input"": {""every"": {""name"": {""not"": input_name}}},
        },
        order={""addedTime"": ""asc""},
        include={""Input"": True},
    )
    json_input_data = json.dumps(input_data)

    if existing_execution:
        await AgentNodeExecutionInputOutput.prisma().create(
            data={
                ""name"": input_name,
                ""data"": json_input_data,
                ""referencedByInputExecId"": existing_execution.id,
            }
        )
        return existing_execution.id, {
            **{
                input_data.name: json.loads(input_data.data)
                for input_data in existing_execution.Input or []
            },
            input_name: input_data,
        }

    elif not node_exec_id:
        result = await AgentNodeExecution.prisma().create(
            data={
                ""agentNodeId"": node_id,
                ""agentGraphExecutionId"": graph_exec_id,
                ""executionStatus"": ExecutionStatus.INCOMPLETE,
                ""Input"": {""create"": {""name"": input_name, ""data"": json_input_data}},
            }
        )
        return result.id, {input_name: input_data}

    else:
        raise ValueError(
            f""NodeExecution {node_exec_id} not found or already has input {input_name}.""
        )","Point(row=152, column=0)","Point(row=217, column=9)",,autogpt_platform/backend/backend/data/execution.py
upsert_execution_output,function,"
    Insert AgentNodeExecutionInputOutput record for as one of AgentNodeExecution.Output.
","async def upsert_execution_output(
    node_exec_id: str,
    output_name: str,
    output_data: Any,
) -> None:
    """"""
    Insert AgentNodeExecutionInputOutput record for as one of AgentNodeExecution.Output.
    """"""
    await AgentNodeExecutionInputOutput.prisma().create(
        data={
            ""name"": output_name,
            ""data"": json.dumps(output_data),
            ""referencedByOutputExecId"": node_exec_id,
        }
    )","Point(row=220, column=0)","Point(row=234, column=5)",,autogpt_platform/backend/backend/data/execution.py
update_graph_execution_start_time,function,,"async def update_graph_execution_start_time(graph_exec_id: str):
    await AgentGraphExecution.prisma().update(
        where={""id"": graph_exec_id},
        data={
            ""executionStatus"": ExecutionStatus.RUNNING,
            ""startedAt"": datetime.now(tz=timezone.utc),
        },
    )","Point(row=237, column=0)","Point(row=244, column=5)",,autogpt_platform/backend/backend/data/execution.py
update_graph_execution_stats,function,,"async def update_graph_execution_stats(
    graph_exec_id: str,
    stats: dict[str, Any],
):
    status = ExecutionStatus.FAILED if stats.get(""error"") else ExecutionStatus.COMPLETED
    await AgentGraphExecution.prisma().update(
        where={""id"": graph_exec_id},
        data={
            ""executionStatus"": status,
            ""stats"": json.dumps(stats),
        },
    )","Point(row=247, column=0)","Point(row=258, column=5)",,autogpt_platform/backend/backend/data/execution.py
update_node_execution_stats,function,,"async def update_node_execution_stats(node_exec_id: str, stats: dict[str, Any]):
    await AgentNodeExecution.prisma().update(
        where={""id"": node_exec_id},
        data={""stats"": json.dumps(stats)},
    )","Point(row=261, column=0)","Point(row=265, column=5)",,autogpt_platform/backend/backend/data/execution.py
update_execution_status,function,,"async def update_execution_status(
    node_exec_id: str,
    status: ExecutionStatus,
    execution_data: BlockInput | None = None,
    stats: dict[str, Any] | None = None,
) -> ExecutionResult:
    if status == ExecutionStatus.QUEUED and execution_data is None:
        raise ValueError(""Execution data must be provided when queuing an execution."")

    now = datetime.now(tz=timezone.utc)
    data = {
        **({""executionStatus"": status}),
        **({""queuedTime"": now} if status == ExecutionStatus.QUEUED else {}),
        **({""startedTime"": now} if status == ExecutionStatus.RUNNING else {}),
        **({""endedTime"": now} if status == ExecutionStatus.FAILED else {}),
        **({""endedTime"": now} if status == ExecutionStatus.COMPLETED else {}),
        **({""executionData"": json.dumps(execution_data)} if execution_data else {}),
        **({""stats"": json.dumps(stats)} if stats else {}),
    }

    res = await AgentNodeExecution.prisma().update(
        where={""id"": node_exec_id},
        data=data,  # type: ignore
        include=EXECUTION_RESULT_INCLUDE,
    )
    if not res:
        raise ValueError(f""Execution {node_exec_id} not found."")

    return ExecutionResult.from_db(res)","Point(row=268, column=0)","Point(row=296, column=39)",,autogpt_platform/backend/backend/data/execution.py
get_graph_execution,function,"
    Retrieve a specific graph execution by its ID.

    Args:
        graph_exec_id (str): The ID of the graph execution to retrieve.
        user_id (str): The ID of the user to whom the graph (execution) belongs.

    Returns:
        AgentGraphExecution | None: The graph execution if found, None otherwise.
","async def get_graph_execution(
    graph_exec_id: str, user_id: str
) -> AgentGraphExecution | None:
    """"""
    Retrieve a specific graph execution by its ID.

    Args:
        graph_exec_id (str): The ID of the graph execution to retrieve.
        user_id (str): The ID of the user to whom the graph (execution) belongs.

    Returns:
        AgentGraphExecution | None: The graph execution if found, None otherwise.
    """"""
    execution = await AgentGraphExecution.prisma().find_first(
        where={""id"": graph_exec_id, ""userId"": user_id},
        include=GRAPH_EXECUTION_INCLUDE,
    )
    return execution","Point(row=299, column=0)","Point(row=316, column=20)",,autogpt_platform/backend/backend/data/execution.py
list_executions,function,,"async def list_executions(graph_id: str, graph_version: int | None = None) -> list[str]:
    where: AgentGraphExecutionWhereInput = {""agentGraphId"": graph_id}
    if graph_version is not None:
        where[""agentGraphVersion""] = graph_version
    executions = await AgentGraphExecution.prisma().find_many(where=where)
    return [execution.id for execution in executions]","Point(row=319, column=0)","Point(row=324, column=53)",,autogpt_platform/backend/backend/data/execution.py
get_execution_results,function,,"async def get_execution_results(graph_exec_id: str) -> list[ExecutionResult]:
    executions = await AgentNodeExecution.prisma().find_many(
        where={""agentGraphExecutionId"": graph_exec_id},
        include=EXECUTION_RESULT_INCLUDE,
        order=[
            {""queuedTime"": ""asc""},
            {""addedTime"": ""asc""},  # Fallback: Incomplete execs has no queuedTime.
        ],
    )
    res = [ExecutionResult.from_db(execution) for execution in executions]
    return res","Point(row=327, column=0)","Point(row=337, column=14)",,autogpt_platform/backend/backend/data/execution.py
parse_execution_output,function,,"def parse_execution_output(output: BlockData, name: str) -> Any | None:
    # Allow extracting partial output data by name.
    output_name, output_data = output

    if name == output_name:
        return output_data

    if name.startswith(f""{output_name}{LIST_SPLIT}""):
        index = int(name.split(LIST_SPLIT)[1])
        if not isinstance(output_data, list) or len(output_data) <= index:
            return None
        return output_data[int(name.split(LIST_SPLIT)[1])]

    if name.startswith(f""{output_name}{DICT_SPLIT}""):
        index = name.split(DICT_SPLIT)[1]
        if not isinstance(output_data, dict) or index not in output_data:
            return None
        return output_data[index]

    if name.startswith(f""{output_name}{OBJC_SPLIT}""):
        index = name.split(OBJC_SPLIT)[1]
        if isinstance(output_data, object) and hasattr(output_data, index):
            return getattr(output_data, index)
        return None

    return None","Point(row=345, column=0)","Point(row=370, column=15)",,autogpt_platform/backend/backend/data/execution.py
merge_execution_input,function,"
    Merge all dynamic input pins which described by the following pattern:
    - <input_name>_$_<index> for list input.
    - <input_name>_#_<index> for dict input.
    - <input_name>_@_<index> for object input.
    This function will construct pins with the same name into a single list/dict/object.
","def merge_execution_input(data: BlockInput) -> BlockInput:
    """"""
    Merge all dynamic input pins which described by the following pattern:
    - <input_name>_$_<index> for list input.
    - <input_name>_#_<index> for dict input.
    - <input_name>_@_<index> for object input.
    This function will construct pins with the same name into a single list/dict/object.
    """"""

    # Merge all input with <input_name>_$_<index> into a single list.
    items = list(data.items())

    for key, value in items:
        if LIST_SPLIT not in key:
            continue
        name, index = key.split(LIST_SPLIT)
        if not index.isdigit():
            raise ValueError(f""Invalid key: {key}, #{index} index must be an integer."")

        data[name] = data.get(name, [])
        if int(index) >= len(data[name]):
            # Pad list with empty string on missing indices.
            data[name].extend([""""] * (int(index) - len(data[name]) + 1))
        data[name][int(index)] = value

    # Merge all input with <input_name>_#_<index> into a single dict.
    for key, value in items:
        if DICT_SPLIT not in key:
            continue
        name, index = key.split(DICT_SPLIT)
        data[name] = data.get(name, {})
        data[name][index] = value

    # Merge all input with <input_name>_@_<index> into a single object.
    for key, value in items:
        if OBJC_SPLIT not in key:
            continue
        name, index = key.split(OBJC_SPLIT)
        if name not in data or not isinstance(data[name], object):
            data[name] = mock.MockObject()
        setattr(data[name], index, value)

    return data","Point(row=373, column=0)","Point(row=415, column=15)",,autogpt_platform/backend/backend/data/execution.py
get_latest_execution,function,,"async def get_latest_execution(node_id: str, graph_eid: str) -> ExecutionResult | None:
    execution = await AgentNodeExecution.prisma().find_first(
        where={
            ""agentNodeId"": node_id,
            ""agentGraphExecutionId"": graph_eid,
            ""executionStatus"": {""not"": ExecutionStatus.INCOMPLETE},
            ""executionData"": {""not"": None},  # type: ignore
        },
        order={""queuedTime"": ""desc""},
        include=EXECUTION_RESULT_INCLUDE,
    )
    if not execution:
        return None
    return ExecutionResult.from_db(execution)","Point(row=418, column=0)","Point(row=431, column=45)",,autogpt_platform/backend/backend/data/execution.py
get_incomplete_executions,function,,"async def get_incomplete_executions(
    node_id: str, graph_eid: str
) -> list[ExecutionResult]:
    executions = await AgentNodeExecution.prisma().find_many(
        where={
            ""agentNodeId"": node_id,
            ""agentGraphExecutionId"": graph_eid,
            ""executionStatus"": ExecutionStatus.INCOMPLETE,
        },
        include=EXECUTION_RESULT_INCLUDE,
    )
    return [ExecutionResult.from_db(execution) for execution in executions]","Point(row=434, column=0)","Point(row=445, column=75)",,autogpt_platform/backend/backend/data/execution.py
Link,class,,"class Link(BaseDbModel):
    source_id: str
    sink_id: str
    source_name: str
    sink_name: str
    is_static: bool = False

    @staticmethod
    def from_db(link: AgentNodeLink):
        return Link(
            id=link.id,
            source_name=link.sourceName,
            source_id=link.agentNodeSourceId,
            sink_name=link.sinkName,
            sink_id=link.agentNodeSinkId,
            is_static=link.isStatic,
        )

    def __hash__(self):
        return hash((self.source_id, self.sink_id, self.source_name, self.sink_name))","Point(row=21, column=0)","Point(row=40, column=85)",,autogpt_platform/backend/backend/data/graph.py
Link.from_db,function,,"def from_db(link: AgentNodeLink):
        return Link(
            id=link.id,
            source_name=link.sourceName,
            source_id=link.agentNodeSourceId,
            sink_name=link.sinkName,
            sink_id=link.agentNodeSinkId,
            is_static=link.isStatic,
        )","Point(row=29, column=4)","Point(row=37, column=9)",Link,autogpt_platform/backend/backend/data/graph.py
Link.__hash__,function,,"def __hash__(self):
        return hash((self.source_id, self.sink_id, self.source_name, self.sink_name))","Point(row=39, column=4)","Point(row=40, column=85)",Link,autogpt_platform/backend/backend/data/graph.py
Node,class,,"class Node(BaseDbModel):
    block_id: str
    input_default: BlockInput = {}  # dict[input_name, default_value]
    metadata: dict[str, Any] = {}
    input_links: list[Link] = []
    output_links: list[Link] = []

    @staticmethod
    def from_db(node: AgentNode):
        if not node.AgentBlock:
            raise ValueError(f""Invalid node {node.id}, invalid AgentBlock."")
        obj = Node(
            id=node.id,
            block_id=node.AgentBlock.id,
            input_default=json.loads(node.constantInput),
            metadata=json.loads(node.metadata),
        )
        obj.input_links = [Link.from_db(link) for link in node.Input or []]
        obj.output_links = [Link.from_db(link) for link in node.Output or []]
        return obj","Point(row=43, column=0)","Point(row=62, column=18)",,autogpt_platform/backend/backend/data/graph.py
Node.from_db,function,,"def from_db(node: AgentNode):
        if not node.AgentBlock:
            raise ValueError(f""Invalid node {node.id}, invalid AgentBlock."")
        obj = Node(
            id=node.id,
            block_id=node.AgentBlock.id,
            input_default=json.loads(node.constantInput),
            metadata=json.loads(node.metadata),
        )
        obj.input_links = [Link.from_db(link) for link in node.Input or []]
        obj.output_links = [Link.from_db(link) for link in node.Output or []]
        return obj","Point(row=51, column=4)","Point(row=62, column=18)",Node,autogpt_platform/backend/backend/data/graph.py
GraphExecution,class,,"class GraphExecution(BaseDbModel):
    execution_id: str
    started_at: datetime
    ended_at: datetime
    duration: float
    total_run_time: float
    status: ExecutionStatus

    @staticmethod
    def from_db(execution: AgentGraphExecution):
        now = datetime.now(timezone.utc)
        start_time = execution.startedAt or execution.createdAt
        end_time = execution.updatedAt or now
        duration = (end_time - start_time).total_seconds()
        total_run_time = duration

        if execution.stats:
            stats = json.loads(execution.stats)
            duration = stats.get(""walltime"", duration)
            total_run_time = stats.get(""nodes_walltime"", total_run_time)

        return GraphExecution(
            id=execution.id,
            execution_id=execution.id,
            started_at=start_time,
            ended_at=end_time,
            duration=duration,
            total_run_time=total_run_time,
            status=ExecutionStatus(execution.executionStatus),
        )","Point(row=65, column=0)","Point(row=94, column=9)",,autogpt_platform/backend/backend/data/graph.py
GraphExecution.from_db,function,,"def from_db(execution: AgentGraphExecution):
        now = datetime.now(timezone.utc)
        start_time = execution.startedAt or execution.createdAt
        end_time = execution.updatedAt or now
        duration = (end_time - start_time).total_seconds()
        total_run_time = duration

        if execution.stats:
            stats = json.loads(execution.stats)
            duration = stats.get(""walltime"", duration)
            total_run_time = stats.get(""nodes_walltime"", total_run_time)

        return GraphExecution(
            id=execution.id,
            execution_id=execution.id,
            started_at=start_time,
            ended_at=end_time,
            duration=duration,
            total_run_time=total_run_time,
            status=ExecutionStatus(execution.executionStatus),
        )","Point(row=74, column=4)","Point(row=94, column=9)",GraphExecution,autogpt_platform/backend/backend/data/graph.py
Graph,class,,"class Graph(BaseDbModel):
    version: int = 1
    is_active: bool = True
    is_template: bool = False
    name: str
    description: str
    executions: list[GraphExecution] = []
    nodes: list[Node] = []
    links: list[Link] = []

    @staticmethod
    def _generate_schema(
        type_class: Type[AgentInputBlock.Input] | Type[AgentOutputBlock.Input],
        data: list[dict],
    ) -> dict[str, Any]:
        props = []
        for p in data:
            try:
                props.append(type_class(**p))
            except Exception as e:
                logger.warning(f""Invalid {type_class}: {p}, {e}"")

        return {
            ""type"": ""object"",
            ""properties"": {
                p.name: {
                    ""secret"": p.secret,
                    ""advanced"": p.advanced,
                    ""title"": p.title or p.name,
                    **({""description"": p.description} if p.description else {}),
                    **({""default"": p.value} if p.value is not None else {}),
                }
                for p in props
            },
            ""required"": [p.name for p in props if p.value is None],
        }

    @computed_field
    @property
    def input_schema(self) -> dict[str, Any]:
        return self._generate_schema(
            AgentInputBlock.Input,
            [
                node.input_default
                for node in self.nodes
                if (b := get_block(node.block_id))
                and b.block_type == BlockType.INPUT
                and ""name"" in node.input_default
            ],
        )

    @computed_field
    @property
    def output_schema(self) -> dict[str, Any]:
        return self._generate_schema(
            AgentOutputBlock.Input,
            [
                node.input_default
                for node in self.nodes
                if (b := get_block(node.block_id))
                and b.block_type == BlockType.OUTPUT
                and ""name"" in node.input_default
            ],
        )

    @property
    def starting_nodes(self) -> list[Node]:
        outbound_nodes = {link.sink_id for link in self.links}
        input_nodes = {
            v.id
            for v in self.nodes
            if (b := get_block(v.block_id)) and b.block_type == BlockType.INPUT
        }
        return [
            node
            for node in self.nodes
            if node.id not in outbound_nodes or node.id in input_nodes
        ]

    def reassign_ids(self, reassign_graph_id: bool = False):
        """"""
        Reassigns all IDs in the graph to new UUIDs.
        This method can be used before storing a new graph to the database.
        """"""
        self.validate_graph()

        id_map = {node.id: str(uuid.uuid4()) for node in self.nodes}
        if reassign_graph_id:
            self.id = str(uuid.uuid4())

        for node in self.nodes:
            node.id = id_map[node.id]

        for link in self.links:
            link.source_id = id_map[link.source_id]
            link.sink_id = id_map[link.sink_id]

    def validate_graph(self, for_run: bool = False):
        def sanitize(name):
            return name.split(""_#_"")[0].split(""_@_"")[0].split(""_$_"")[0]

        input_links = defaultdict(list)
        for link in self.links:
            input_links[link.sink_id].append(link)

        # Nodes: required fields are filled or connected
        for node in self.nodes:
            block = get_block(node.block_id)
            if block is None:
                raise ValueError(f""Invalid block {node.block_id} for node #{node.id}"")

            provided_inputs = set(
                [sanitize(name) for name in node.input_default]
                + [sanitize(link.sink_name) for link in input_links.get(node.id, [])]
            )
            for name in block.input_schema.get_required_fields():
                if name not in provided_inputs and (
                    for_run  # Skip input completion validation, unless when executing.
                    or block.block_type == BlockType.INPUT
                    or block.block_type == BlockType.OUTPUT
                ):
                    raise ValueError(
                        f""Node {block.name} #{node.id} required input missing: `{name}`""
                    )

        node_map = {v.id: v for v in self.nodes}

        def is_static_output_block(nid: str) -> bool:
            bid = node_map[nid].block_id
            b = get_block(bid)
            return b.static_output if b else False

        # Links: links are connected and the connected pin data type are compatible.
        for link in self.links:
            source = (link.source_id, link.source_name)
            sink = (link.sink_id, link.sink_name)
            suffix = f""Link {source} <-> {sink}""

            for i, (node_id, name) in enumerate([source, sink]):
                node = node_map.get(node_id)
                if not node:
                    raise ValueError(
                        f""{suffix}, {node_id} is invalid node id, available nodes: {node_map.keys()}""
                    )

                block = get_block(node.block_id)
                if not block:
                    blocks = {v().id: v().name for v in get_blocks().values()}
                    raise ValueError(
                        f""{suffix}, {node.block_id} is invalid block id, available blocks: {blocks}""
                    )

                sanitized_name = sanitize(name)
                if i == 0:
                    fields = f""Valid output fields: {block.output_schema.get_fields()}""
                else:
                    fields = f""Valid input fields: {block.input_schema.get_fields()}""
                if sanitized_name not in fields:
                    raise ValueError(f""{suffix}, `{name}` invalid, {fields}"")

            if is_static_output_block(link.source_id):
                link.is_static = True  # Each value block output should be static.

            # TODO: Add type compatibility check here.

    @staticmethod
    def from_db(graph: AgentGraph, hide_credentials: bool = False):
        executions = [
            GraphExecution.from_db(execution)
            for execution in graph.AgentGraphExecution or []
        ]
        nodes = graph.AgentNodes or []

        return Graph(
            id=graph.id,
            version=graph.version,
            is_active=graph.isActive,
            is_template=graph.isTemplate,
            name=graph.name or """",
            description=graph.description or """",
            executions=executions,
            nodes=[Graph._process_node(node, hide_credentials) for node in nodes],
            links=list(
                {
                    Link.from_db(link)
                    for node in nodes
                    for link in (node.Input or []) + (node.Output or [])
                }
            ),
        )

    @staticmethod
    def _process_node(node: AgentNode, hide_credentials: bool) -> Node:
        node_dict = node.model_dump()
        if hide_credentials and ""constantInput"" in node_dict:
            constant_input = json.loads(node_dict[""constantInput""])
            constant_input = Graph._hide_credentials_in_input(constant_input)
            node_dict[""constantInput""] = json.dumps(constant_input)
        return Node.from_db(AgentNode(**node_dict))

    @staticmethod
    def _hide_credentials_in_input(input_data: dict[str, Any]) -> dict[str, Any]:
        sensitive_keys = [""credentials"", ""api_key"", ""password"", ""token"", ""secret""]
        result = {}
        for key, value in input_data.items():
            if isinstance(value, dict):
                result[key] = Graph._hide_credentials_in_input(value)
            elif isinstance(value, str) and any(
                sensitive_key in key.lower() for sensitive_key in sensitive_keys
            ):
                # Skip this key-value pair in the result
                continue
            else:
                result[key] = value
        return result","Point(row=97, column=0)","Point(row=311, column=21)",,autogpt_platform/backend/backend/data/graph.py
Graph._generate_schema,function,,"def _generate_schema(
        type_class: Type[AgentInputBlock.Input] | Type[AgentOutputBlock.Input],
        data: list[dict],
    ) -> dict[str, Any]:
        props = []
        for p in data:
            try:
                props.append(type_class(**p))
            except Exception as e:
                logger.warning(f""Invalid {type_class}: {p}, {e}"")

        return {
            ""type"": ""object"",
            ""properties"": {
                p.name: {
                    ""secret"": p.secret,
                    ""advanced"": p.advanced,
                    ""title"": p.title or p.name,
                    **({""description"": p.description} if p.description else {}),
                    **({""default"": p.value} if p.value is not None else {}),
                }
                for p in props
            },
            ""required"": [p.name for p in props if p.value is None],
        }","Point(row=108, column=4)","Point(row=132, column=9)",Graph,autogpt_platform/backend/backend/data/graph.py
Graph.input_schema,function,,"def input_schema(self) -> dict[str, Any]:
        return self._generate_schema(
            AgentInputBlock.Input,
            [
                node.input_default
                for node in self.nodes
                if (b := get_block(node.block_id))
                and b.block_type == BlockType.INPUT
                and ""name"" in node.input_default
            ],
        )","Point(row=136, column=4)","Point(row=146, column=9)",Graph,autogpt_platform/backend/backend/data/graph.py
Graph.output_schema,function,,"def output_schema(self) -> dict[str, Any]:
        return self._generate_schema(
            AgentOutputBlock.Input,
            [
                node.input_default
                for node in self.nodes
                if (b := get_block(node.block_id))
                and b.block_type == BlockType.OUTPUT
                and ""name"" in node.input_default
            ],
        )","Point(row=150, column=4)","Point(row=160, column=9)",Graph,autogpt_platform/backend/backend/data/graph.py
Graph.starting_nodes,function,,"def starting_nodes(self) -> list[Node]:
        outbound_nodes = {link.sink_id for link in self.links}
        input_nodes = {
            v.id
            for v in self.nodes
            if (b := get_block(v.block_id)) and b.block_type == BlockType.INPUT
        }
        return [
            node
            for node in self.nodes
            if node.id not in outbound_nodes or node.id in input_nodes
        ]","Point(row=163, column=4)","Point(row=174, column=9)",Graph,autogpt_platform/backend/backend/data/graph.py
Graph.reassign_ids,function,"
        Reassigns all IDs in the graph to new UUIDs.
        This method can be used before storing a new graph to the database.
","def reassign_ids(self, reassign_graph_id: bool = False):
        """"""
        Reassigns all IDs in the graph to new UUIDs.
        This method can be used before storing a new graph to the database.
        """"""
        self.validate_graph()

        id_map = {node.id: str(uuid.uuid4()) for node in self.nodes}
        if reassign_graph_id:
            self.id = str(uuid.uuid4())

        for node in self.nodes:
            node.id = id_map[node.id]

        for link in self.links:
            link.source_id = id_map[link.source_id]
            link.sink_id = id_map[link.sink_id]","Point(row=176, column=4)","Point(row=192, column=47)",Graph,autogpt_platform/backend/backend/data/graph.py
Graph.validate_graph,function,,"def validate_graph(self, for_run: bool = False):
        def sanitize(name):
            return name.split(""_#_"")[0].split(""_@_"")[0].split(""_$_"")[0]

        input_links = defaultdict(list)
        for link in self.links:
            input_links[link.sink_id].append(link)

        # Nodes: required fields are filled or connected
        for node in self.nodes:
            block = get_block(node.block_id)
            if block is None:
                raise ValueError(f""Invalid block {node.block_id} for node #{node.id}"")

            provided_inputs = set(
                [sanitize(name) for name in node.input_default]
                + [sanitize(link.sink_name) for link in input_links.get(node.id, [])]
            )
            for name in block.input_schema.get_required_fields():
                if name not in provided_inputs and (
                    for_run  # Skip input completion validation, unless when executing.
                    or block.block_type == BlockType.INPUT
                    or block.block_type == BlockType.OUTPUT
                ):
                    raise ValueError(
                        f""Node {block.name} #{node.id} required input missing: `{name}`""
                    )

        node_map = {v.id: v for v in self.nodes}

        def is_static_output_block(nid: str) -> bool:
            bid = node_map[nid].block_id
            b = get_block(bid)
            return b.static_output if b else False

        # Links: links are connected and the connected pin data type are compatible.
        for link in self.links:
            source = (link.source_id, link.source_name)
            sink = (link.sink_id, link.sink_name)
            suffix = f""Link {source} <-> {sink}""

            for i, (node_id, name) in enumerate([source, sink]):
                node = node_map.get(node_id)
                if not node:
                    raise ValueError(
                        f""{suffix}, {node_id} is invalid node id, available nodes: {node_map.keys()}""
                    )

                block = get_block(node.block_id)
                if not block:
                    blocks = {v().id: v().name for v in get_blocks().values()}
                    raise ValueError(
                        f""{suffix}, {node.block_id} is invalid block id, available blocks: {blocks}""
                    )

                sanitized_name = sanitize(name)
                if i == 0:
                    fields = f""Valid output fields: {block.output_schema.get_fields()}""
                else:
                    fields = f""Valid input fields: {block.input_schema.get_fields()}""
                if sanitized_name not in fields:
                    raise ValueError(f""{suffix}, `{name}` invalid, {fields}"")

            if is_static_output_block(link.source_id):
                link.is_static = True  # Each value block output should be static.

            # TODO: Add type compatibility check here.","Point(row=194, column=4)","Point(row=260, column=54)",Graph,autogpt_platform/backend/backend/data/graph.py
Graph.validate_graph.sanitize,function,,"def sanitize(name):
            return name.split(""_#_"")[0].split(""_@_"")[0].split(""_$_"")[0]","Point(row=195, column=8)","Point(row=196, column=71)",Graph,autogpt_platform/backend/backend/data/graph.py
Graph.validate_graph.is_static_output_block,function,,"def is_static_output_block(nid: str) -> bool:
            bid = node_map[nid].block_id
            b = get_block(bid)
            return b.static_output if b else False","Point(row=224, column=8)","Point(row=227, column=50)",Graph,autogpt_platform/backend/backend/data/graph.py
Graph.from_db,function,,"def from_db(graph: AgentGraph, hide_credentials: bool = False):
        executions = [
            GraphExecution.from_db(execution)
            for execution in graph.AgentGraphExecution or []
        ]
        nodes = graph.AgentNodes or []

        return Graph(
            id=graph.id,
            version=graph.version,
            is_active=graph.isActive,
            is_template=graph.isTemplate,
            name=graph.name or """",
            description=graph.description or """",
            executions=executions,
            nodes=[Graph._process_node(node, hide_credentials) for node in nodes],
            links=list(
                {
                    Link.from_db(link)
                    for node in nodes
                    for link in (node.Input or []) + (node.Output or [])
                }
            ),
        )","Point(row=263, column=4)","Point(row=286, column=9)",Graph,autogpt_platform/backend/backend/data/graph.py
Graph._process_node,function,,"def _process_node(node: AgentNode, hide_credentials: bool) -> Node:
        node_dict = node.model_dump()
        if hide_credentials and ""constantInput"" in node_dict:
            constant_input = json.loads(node_dict[""constantInput""])
            constant_input = Graph._hide_credentials_in_input(constant_input)
            node_dict[""constantInput""] = json.dumps(constant_input)
        return Node.from_db(AgentNode(**node_dict))","Point(row=289, column=4)","Point(row=295, column=51)",Graph,autogpt_platform/backend/backend/data/graph.py
Graph._hide_credentials_in_input,function,,"def _hide_credentials_in_input(input_data: dict[str, Any]) -> dict[str, Any]:
        sensitive_keys = [""credentials"", ""api_key"", ""password"", ""token"", ""secret""]
        result = {}
        for key, value in input_data.items():
            if isinstance(value, dict):
                result[key] = Graph._hide_credentials_in_input(value)
            elif isinstance(value, str) and any(
                sensitive_key in key.lower() for sensitive_key in sensitive_keys
            ):
                # Skip this key-value pair in the result
                continue
            else:
                result[key] = value
        return result","Point(row=298, column=4)","Point(row=311, column=21)",Graph,autogpt_platform/backend/backend/data/graph.py
get_node,function,,"async def get_node(node_id: str) -> Node:
    node = await AgentNode.prisma().find_unique_or_raise(
        where={""id"": node_id},
        include=AGENT_NODE_INCLUDE,
    )
    return Node.from_db(node)","Point(row=317, column=0)","Point(row=322, column=29)",,autogpt_platform/backend/backend/data/graph.py
get_graphs,function,"
    Retrieves graph metadata objects.
    Default behaviour is to get all currently active graphs.

    Args:
        include_executions: Whether to include executions in the graph metadata.
        filter_by: An optional filter to either select templates or active graphs.
        user_id: The ID of the user that owns the graph.

    Returns:
        list[Graph]: A list of objects representing the retrieved graph metadata.
","async def get_graphs(
    user_id: str,
    include_executions: bool = False,
    filter_by: Literal[""active"", ""template""] | None = ""active"",
) -> list[Graph]:
    """"""
    Retrieves graph metadata objects.
    Default behaviour is to get all currently active graphs.

    Args:
        include_executions: Whether to include executions in the graph metadata.
        filter_by: An optional filter to either select templates or active graphs.
        user_id: The ID of the user that owns the graph.

    Returns:
        list[Graph]: A list of objects representing the retrieved graph metadata.
    """"""
    where_clause: AgentGraphWhereInput = {}

    if filter_by == ""active"":
        where_clause[""isActive""] = True
    elif filter_by == ""template"":
        where_clause[""isTemplate""] = True

    where_clause[""userId""] = user_id

    graph_include = AGENT_GRAPH_INCLUDE
    graph_include[""AgentGraphExecution""] = include_executions

    graphs = await AgentGraph.prisma().find_many(
        where=where_clause,
        distinct=[""id""],
        order={""version"": ""desc""},
        include=graph_include,
    )

    return [Graph.from_db(graph) for graph in graphs]","Point(row=325, column=0)","Point(row=361, column=53)",,autogpt_platform/backend/backend/data/graph.py
get_graph,function,"
    Retrieves a graph from the DB.
    Defaults to the version with `is_active` if `version` is not passed,
    or the latest version with `is_template` if `template=True`.

    Returns `None` if the record is not found.
","async def get_graph(
    graph_id: str,
    version: int | None = None,
    template: bool = False,
    user_id: str | None = None,
    hide_credentials: bool = False,
) -> Graph | None:
    """"""
    Retrieves a graph from the DB.
    Defaults to the version with `is_active` if `version` is not passed,
    or the latest version with `is_template` if `template=True`.

    Returns `None` if the record is not found.
    """"""
    where_clause: AgentGraphWhereInput = {
        ""id"": graph_id,
        ""isTemplate"": template,
    }
    if version is not None:
        where_clause[""version""] = version
    elif not template:
        where_clause[""isActive""] = True

    if user_id is not None and not template:
        where_clause[""userId""] = user_id

    graph = await AgentGraph.prisma().find_first(
        where=where_clause,
        include=AGENT_GRAPH_INCLUDE,
        order={""version"": ""desc""},
    )
    return Graph.from_db(graph, hide_credentials) if graph else None","Point(row=364, column=0)","Point(row=395, column=68)",,autogpt_platform/backend/backend/data/graph.py
set_graph_active_version,function,,"async def set_graph_active_version(graph_id: str, version: int, user_id: str) -> None:
    # Check if the graph belongs to the user
    graph = await AgentGraph.prisma().find_first(
        where={
            ""id"": graph_id,
            ""version"": version,
            ""userId"": user_id,
        }
    )
    if not graph:
        raise Exception(f""Graph #{graph_id} v{version} not found or not owned by user"")

    updated_graph = await AgentGraph.prisma().update(
        data={""isActive"": True},
        where={
            ""graphVersionId"": {""id"": graph_id, ""version"": version},
        },
    )
    if not updated_graph:
        raise Exception(f""Graph #{graph_id} v{version} not found"")

    # Deactivate all other versions
    await AgentGraph.prisma().update_many(
        data={""isActive"": False},
        where={""id"": graph_id, ""version"": {""not"": version}, ""userId"": user_id},
    )","Point(row=398, column=0)","Point(row=423, column=5)",,autogpt_platform/backend/backend/data/graph.py
get_graph_all_versions,function,,"async def get_graph_all_versions(graph_id: str, user_id: str) -> list[Graph]:
    graph_versions = await AgentGraph.prisma().find_many(
        where={""id"": graph_id, ""userId"": user_id},
        order={""version"": ""desc""},
        include=AGENT_GRAPH_INCLUDE,
    )

    if not graph_versions:
        return []

    return [Graph.from_db(graph) for graph in graph_versions]","Point(row=426, column=0)","Point(row=436, column=61)",,autogpt_platform/backend/backend/data/graph.py
delete_graph,function,,"async def delete_graph(graph_id: str, user_id: str) -> int:
    entries_count = await AgentGraph.prisma().delete_many(
        where={""id"": graph_id, ""userId"": user_id}
    )
    if entries_count:
        logger.info(f""Deleted {entries_count} graph entries for Graph #{graph_id}"")
    return entries_count","Point(row=439, column=0)","Point(row=445, column=24)",,autogpt_platform/backend/backend/data/graph.py
create_graph,function,,"async def create_graph(graph: Graph, user_id: str) -> Graph:
    async with transaction() as tx:
        await __create_graph(tx, graph, user_id)

    if created_graph := await get_graph(
        graph.id, graph.version, graph.is_template, user_id=user_id
    ):
        return created_graph

    raise ValueError(f""Created graph {graph.id} v{graph.version} is not in DB"")","Point(row=448, column=0)","Point(row=457, column=79)",,autogpt_platform/backend/backend/data/graph.py
__create_graph,function,,"async def __create_graph(tx, graph: Graph, user_id: str):
    await AgentGraph.prisma(tx).create(
        data={
            ""id"": graph.id,
            ""version"": graph.version,
            ""name"": graph.name,
            ""description"": graph.description,
            ""isTemplate"": graph.is_template,
            ""isActive"": graph.is_active,
            ""userId"": user_id,
        }
    )

    await asyncio.gather(
        *[
            AgentNode.prisma(tx).create(
                {
                    ""id"": node.id,
                    ""agentBlockId"": node.block_id,
                    ""agentGraphId"": graph.id,
                    ""agentGraphVersion"": graph.version,
                    ""constantInput"": json.dumps(node.input_default),
                    ""metadata"": json.dumps(node.metadata),
                }
            )
            for node in graph.nodes
        ]
    )

    await asyncio.gather(
        *[
            AgentNodeLink.prisma(tx).create(
                {
                    ""id"": str(uuid.uuid4()),
                    ""sourceName"": link.source_name,
                    ""sinkName"": link.sink_name,
                    ""agentNodeSourceId"": link.source_id,
                    ""agentNodeSinkId"": link.sink_id,
                    ""isStatic"": link.is_static,
                }
            )
            for link in graph.links
        ]
    )","Point(row=460, column=0)","Point(row=503, column=5)",,autogpt_platform/backend/backend/data/graph.py
BlockSecret,class,,"class BlockSecret:
    def __init__(self, key: Optional[str] = None, value: Optional[str] = None):
        if value is not None:
            trimmed_value = value.strip()
            if value != trimmed_value:
                logger.debug(BlockSecret.TRIMMING_VALUE_MSG)
            self._value = trimmed_value
            return

        self._value = self.__get_secret(key)
        if self._value is None:
            raise ValueError(f""Secret {key} not found."")
        trimmed_value = self._value.strip()
        if self._value != trimmed_value:
            logger.debug(BlockSecret.TRIMMING_VALUE_MSG)
        self._value = trimmed_value

    TRIMMING_VALUE_MSG: ClassVar[str] = ""Provided secret value got trimmed.""
    STR: ClassVar[str] = ""<secret>""
    SECRETS: ClassVar[Secrets] = Secrets()

    def __repr__(self):
        return BlockSecret.STR

    def __str__(self):
        return BlockSecret.STR

    @staticmethod
    def __get_secret(key: str | None):
        if not key or not hasattr(BlockSecret.SECRETS, key):
            return None
        return getattr(BlockSecret.SECRETS, key)

    def get_secret_value(self):
        trimmed_value = str(self._value).strip()
        if self._value != trimmed_value:
            logger.info(BlockSecret.TRIMMING_VALUE_MSG)
        return trimmed_value

    @classmethod
    def parse_value(cls, value: Any) -> BlockSecret:
        if isinstance(value, BlockSecret):
            return value
        return BlockSecret(value=value)

    @classmethod
    def __get_pydantic_json_schema__(
        cls, source_type: Any, handler: GetCoreSchemaHandler
    ) -> dict[str, Any]:
        return {
            ""type"": ""string"",
        }

    @classmethod
    def __get_pydantic_core_schema__(
        cls, source_type: Any, handler: GetCoreSchemaHandler
    ) -> CoreSchema:
        validate_fun = core_schema.no_info_plain_validator_function(cls.parse_value)
        return core_schema.json_or_python_schema(
            json_schema=validate_fun,
            python_schema=validate_fun,
            serialization=core_schema.plain_serializer_function_ser_schema(
                lambda val: BlockSecret.STR
            ),
        )","Point(row=20, column=0)","Point(row=84, column=9)",,autogpt_platform/backend/backend/data/model.py
BlockSecret.__init__,function,,"def __init__(self, key: Optional[str] = None, value: Optional[str] = None):
        if value is not None:
            trimmed_value = value.strip()
            if value != trimmed_value:
                logger.debug(BlockSecret.TRIMMING_VALUE_MSG)
            self._value = trimmed_value
            return

        self._value = self.__get_secret(key)
        if self._value is None:
            raise ValueError(f""Secret {key} not found."")
        trimmed_value = self._value.strip()
        if self._value != trimmed_value:
            logger.debug(BlockSecret.TRIMMING_VALUE_MSG)
        self._value = trimmed_value","Point(row=21, column=4)","Point(row=35, column=35)",BlockSecret,autogpt_platform/backend/backend/data/model.py
BlockSecret.__repr__,function,,"def __repr__(self):
        return BlockSecret.STR","Point(row=41, column=4)","Point(row=42, column=30)",BlockSecret,autogpt_platform/backend/backend/data/model.py
BlockSecret.__str__,function,,"def __str__(self):
        return BlockSecret.STR","Point(row=44, column=4)","Point(row=45, column=30)",BlockSecret,autogpt_platform/backend/backend/data/model.py
BlockSecret.__get_secret,function,,"def __get_secret(key: str | None):
        if not key or not hasattr(BlockSecret.SECRETS, key):
            return None
        return getattr(BlockSecret.SECRETS, key)","Point(row=48, column=4)","Point(row=51, column=48)",BlockSecret,autogpt_platform/backend/backend/data/model.py
BlockSecret.get_secret_value,function,,"def get_secret_value(self):
        trimmed_value = str(self._value).strip()
        if self._value != trimmed_value:
            logger.info(BlockSecret.TRIMMING_VALUE_MSG)
        return trimmed_value","Point(row=53, column=4)","Point(row=57, column=28)",BlockSecret,autogpt_platform/backend/backend/data/model.py
BlockSecret.parse_value,function,,"def parse_value(cls, value: Any) -> BlockSecret:
        if isinstance(value, BlockSecret):
            return value
        return BlockSecret(value=value)","Point(row=60, column=4)","Point(row=63, column=39)",BlockSecret,autogpt_platform/backend/backend/data/model.py
BlockSecret.__get_pydantic_json_schema__,function,,"def __get_pydantic_json_schema__(
        cls, source_type: Any, handler: GetCoreSchemaHandler
    ) -> dict[str, Any]:
        return {
            ""type"": ""string"",
        }","Point(row=66, column=4)","Point(row=71, column=9)",BlockSecret,autogpt_platform/backend/backend/data/model.py
BlockSecret.__get_pydantic_core_schema__,function,,"def __get_pydantic_core_schema__(
        cls, source_type: Any, handler: GetCoreSchemaHandler
    ) -> CoreSchema:
        validate_fun = core_schema.no_info_plain_validator_function(cls.parse_value)
        return core_schema.json_or_python_schema(
            json_schema=validate_fun,
            python_schema=validate_fun,
            serialization=core_schema.plain_serializer_function_ser_schema(
                lambda val: BlockSecret.STR
            ),
        )","Point(row=74, column=4)","Point(row=84, column=9)",BlockSecret,autogpt_platform/backend/backend/data/model.py
SecretField,function,,"def SecretField(
    value: Optional[str] = None,
    key: Optional[str] = None,
    title: Optional[str] = None,
    description: Optional[str] = None,
    placeholder: Optional[str] = None,
    **kwargs,
) -> BlockSecret:
    return SchemaField(
        BlockSecret(key=key, value=value),
        title=title,
        description=description,
        placeholder=placeholder,
        secret=True,
        **kwargs,
    )","Point(row=87, column=0)","Point(row=102, column=5)",,autogpt_platform/backend/backend/data/model.py
SchemaField,function,,"def SchemaField(
    default: T | PydanticUndefinedType = PydanticUndefined,
    *args,
    default_factory: Optional[Callable[[], T]] = None,
    title: Optional[str] = None,
    description: Optional[str] = None,
    placeholder: Optional[str] = None,
    advanced: Optional[bool] = None,
    secret: bool = False,
    exclude: bool = False,
    **kwargs,
) -> T:
    json_extra = {
        k: v
        for k, v in {
            ""placeholder"": placeholder,
            ""secret"": secret,
            ""advanced"": advanced,
        }.items()
        if v is not None
    }

    return Field(
        default,
        *args,
        default_factory=default_factory,
        title=title,
        description=description,
        exclude=exclude,
        json_schema_extra=json_extra,
        **kwargs,
    )","Point(row=105, column=0)","Point(row=136, column=5)",,autogpt_platform/backend/backend/data/model.py
CredentialsMetaInput,class,,"class CredentialsMetaInput(BaseModel, Generic[CP, CT]):
    id: str
    title: Optional[str] = None
    provider: CP
    type: CT","Point(row=146, column=0)","Point(row=150, column=12)",,autogpt_platform/backend/backend/data/model.py
CredentialsField,function,"
    `CredentialsField` must and can only be used on fields named `credentials`.
    This is enforced by the `BlockSchema` base class.
","def CredentialsField(
    provider: CP,
    supported_credential_types: set[CT],
    required_scopes: set[str] = set(),
    *,
    title: Optional[str] = None,
    description: Optional[str] = None,
    **kwargs,
) -> CredentialsMetaInput[CP, CT]:
    """"""
    `CredentialsField` must and can only be used on fields named `credentials`.
    This is enforced by the `BlockSchema` base class.
    """"""
    json_extra = {
        k: v
        for k, v in {
            ""credentials_provider"": provider,
            ""credentials_scopes"": list(required_scopes) or None,  # omit if empty
            ""credentials_types"": list(supported_credential_types),
        }.items()
        if v is not None
    }

    return Field(
        title=title,
        description=description,
        json_schema_extra=json_extra,
        **kwargs,
    )","Point(row=153, column=0)","Point(row=181, column=5)",,autogpt_platform/backend/backend/data/model.py
ContributorDetails,class,,"class ContributorDetails(BaseModel):
    name: str = Field(title=""Name"", description=""The name of the contributor."")","Point(row=184, column=0)","Point(row=185, column=79)",,autogpt_platform/backend/backend/data/model.py
connect,function,,"def connect() -> Redis:
    global connection
    if connection:
        return connection

    c = Redis(
        host=HOST,
        port=PORT,
        password=PASSWORD,
        decode_responses=True,
    )
    c.ping()
    connection = c
    return connection","Point(row=21, column=0)","Point(row=34, column=21)",,autogpt_platform/backend/backend/data/redis.py
disconnect,function,,"def disconnect():
    global connection
    if connection:
        connection.close()
    connection = None","Point(row=38, column=0)","Point(row=42, column=21)",,autogpt_platform/backend/backend/data/redis.py
get_redis,function,,"def get_redis(auto_connect: bool = True) -> Redis:
    if connection:
        return connection
    if auto_connect:
        return connect()
    raise RuntimeError(""Redis connection is not established"")","Point(row=45, column=0)","Point(row=50, column=61)",,autogpt_platform/backend/backend/data/redis.py
connect_async,function,,"async def connect_async() -> AsyncRedis:
    global connection_async
    if connection_async:
        return connection_async

    c = AsyncRedis(
        host=HOST,
        port=PORT,
        password=PASSWORD,
        decode_responses=True,
    )
    await c.ping()
    connection_async = c
    return connection_async","Point(row=54, column=0)","Point(row=67, column=27)",,autogpt_platform/backend/backend/data/redis.py
disconnect_async,function,,"async def disconnect_async():
    global connection_async
    if connection_async:
        await connection_async.close()
    connection_async = None","Point(row=71, column=0)","Point(row=75, column=27)",,autogpt_platform/backend/backend/data/redis.py
get_redis_async,function,,"async def get_redis_async(auto_connect: bool = True) -> AsyncRedis:
    if connection_async:
        return connection_async
    if auto_connect:
        return await connect_async()
    raise RuntimeError(""AsyncRedis connection is not established"")","Point(row=78, column=0)","Point(row=83, column=66)",,autogpt_platform/backend/backend/data/redis.py
log_raw_analytics,function,,"async def log_raw_analytics(
    user_id: str,
    type: str,
    data: dict,
    data_index: str,
):
    details = await prisma.models.AnalyticsDetails.prisma().create(
        data={
            ""userId"": user_id,
            ""type"": type,
            ""data"": prisma.Json(data),
            ""dataIndex"": data_index,
        }
    )
    return details","Point(row=7, column=0)","Point(row=21, column=18)",,autogpt_platform/backend/backend/data/analytics.py
log_raw_metric,function,,"async def log_raw_metric(
    user_id: str,
    metric_name: str,
    metric_value: float,
    data_string: str,
):
    if metric_value < 0:
        raise ValueError(""metric_value must be non-negative"")

    result = await prisma.models.AnalyticsMetrics.prisma().create(
        data={
            ""value"": metric_value,
            ""analyticMetric"": metric_name,
            ""userId"": user_id,
            ""dataString"": data_string,
        },
    )

    return result","Point(row=24, column=0)","Point(row=42, column=17)",,autogpt_platform/backend/backend/data/analytics.py
ExecutionSchedule,class,,"class ExecutionSchedule(BaseDbModel):
    graph_id: str
    user_id: str
    graph_version: int
    schedule: str
    is_enabled: bool
    input_data: BlockInput
    last_updated: Optional[datetime] = None

    def __init__(self, is_enabled: Optional[bool] = None, **kwargs):
        kwargs[""is_enabled""] = (is_enabled is None) or is_enabled
        super().__init__(**kwargs)

    @staticmethod
    def from_db(schedule: AgentGraphExecutionSchedule):
        return ExecutionSchedule(
            id=schedule.id,
            graph_id=schedule.agentGraphId,
            user_id=schedule.userId,
            graph_version=schedule.agentGraphVersion,
            schedule=schedule.schedule,
            is_enabled=schedule.isEnabled,
            last_updated=schedule.lastUpdated.replace(tzinfo=None),
            input_data=json.loads(schedule.inputData),
        )","Point(row=10, column=0)","Point(row=34, column=9)",,autogpt_platform/backend/backend/data/schedule.py
ExecutionSchedule.__init__,function,,"def __init__(self, is_enabled: Optional[bool] = None, **kwargs):
        kwargs[""is_enabled""] = (is_enabled is None) or is_enabled
        super().__init__(**kwargs)","Point(row=19, column=4)","Point(row=21, column=34)",ExecutionSchedule,autogpt_platform/backend/backend/data/schedule.py
ExecutionSchedule.from_db,function,,"def from_db(schedule: AgentGraphExecutionSchedule):
        return ExecutionSchedule(
            id=schedule.id,
            graph_id=schedule.agentGraphId,
            user_id=schedule.userId,
            graph_version=schedule.agentGraphVersion,
            schedule=schedule.schedule,
            is_enabled=schedule.isEnabled,
            last_updated=schedule.lastUpdated.replace(tzinfo=None),
            input_data=json.loads(schedule.inputData),
        )","Point(row=24, column=4)","Point(row=34, column=9)",ExecutionSchedule,autogpt_platform/backend/backend/data/schedule.py
get_active_schedules,function,,"async def get_active_schedules(last_fetch_time: datetime) -> list[ExecutionSchedule]:
    query = AgentGraphExecutionSchedule.prisma().find_many(
        where={""isEnabled"": True, ""lastUpdated"": {""gt"": last_fetch_time}},
        order={""lastUpdated"": ""asc""},
    )
    return [ExecutionSchedule.from_db(schedule) for schedule in await query]","Point(row=37, column=0)","Point(row=42, column=76)",,autogpt_platform/backend/backend/data/schedule.py
disable_schedule,function,,"async def disable_schedule(schedule_id: str):
    await AgentGraphExecutionSchedule.prisma().update(
        where={""id"": schedule_id}, data={""isEnabled"": False}
    )","Point(row=45, column=0)","Point(row=48, column=5)",,autogpt_platform/backend/backend/data/schedule.py
get_schedules,function,,"async def get_schedules(graph_id: str, user_id: str) -> list[ExecutionSchedule]:
    query = AgentGraphExecutionSchedule.prisma().find_many(
        where={
            ""isEnabled"": True,
            ""agentGraphId"": graph_id,
            ""userId"": user_id,
        },
    )
    return [ExecutionSchedule.from_db(schedule) for schedule in await query]","Point(row=51, column=0)","Point(row=59, column=76)",,autogpt_platform/backend/backend/data/schedule.py
add_schedule,function,,"async def add_schedule(schedule: ExecutionSchedule) -> ExecutionSchedule:
    obj = await AgentGraphExecutionSchedule.prisma().create(
        data={
            ""id"": schedule.id,
            ""userId"": schedule.user_id,
            ""agentGraphId"": schedule.graph_id,
            ""agentGraphVersion"": schedule.graph_version,
            ""schedule"": schedule.schedule,
            ""isEnabled"": schedule.is_enabled,
            ""inputData"": json.dumps(schedule.input_data),
        }
    )
    return ExecutionSchedule.from_db(obj)","Point(row=62, column=0)","Point(row=74, column=41)",,autogpt_platform/backend/backend/data/schedule.py
update_schedule,function,,"async def update_schedule(schedule_id: str, is_enabled: bool, user_id: str):
    await AgentGraphExecutionSchedule.prisma().update(
        where={""id"": schedule_id}, data={""isEnabled"": is_enabled}
    )","Point(row=77, column=0)","Point(row=80, column=5)",,autogpt_platform/backend/backend/data/schedule.py
BlockType,class,,"class BlockType(Enum):
    STANDARD = ""Standard""
    INPUT = ""Input""
    OUTPUT = ""Output""
    NOTE = ""Note""","Point(row=31, column=0)","Point(row=35, column=17)",,autogpt_platform/backend/backend/data/block.py
BlockCategory,class,,"class BlockCategory(Enum):
    AI = ""Block that leverages AI to perform a task.""
    SOCIAL = ""Block that interacts with social media platforms.""
    TEXT = ""Block that processes text data.""
    SEARCH = ""Block that searches or extracts information from the internet.""
    BASIC = ""Block that performs basic operations.""
    INPUT = ""Block that interacts with input of the graph.""
    OUTPUT = ""Block that interacts with output of the graph.""
    LOGIC = ""Programming logic to control the flow of your agent""
    COMMUNICATION = ""Block that interacts with communication platforms.""
    DEVELOPER_TOOLS = ""Developer tools such as GitHub blocks.""
    DATA = ""Block that interacts with structured data.""

    def dict(self) -> dict[str, str]:
        return {""category"": self.name, ""description"": self.value}","Point(row=38, column=0)","Point(row=52, column=65)",,autogpt_platform/backend/backend/data/block.py
BlockCategory.dict,function,,"def dict(self) -> dict[str, str]:
        return {""category"": self.name, ""description"": self.value}","Point(row=51, column=4)","Point(row=52, column=65)",BlockCategory,autogpt_platform/backend/backend/data/block.py
BlockSchema,class,,"class BlockSchema(BaseModel):
    cached_jsonschema: ClassVar[dict[str, Any]] = {}

    @classmethod
    def jsonschema(cls) -> dict[str, Any]:
        if cls.cached_jsonschema:
            return cls.cached_jsonschema

        model = jsonref.replace_refs(cls.model_json_schema(), merge_props=True)

        def ref_to_dict(obj):
            if isinstance(obj, dict):
                # OpenAPI <3.1 does not support sibling fields that has a $ref key
                # So sometimes, the schema has an ""allOf""/""anyOf""/""oneOf"" with 1 item.
                keys = {""allOf"", ""anyOf"", ""oneOf""}
                one_key = next((k for k in keys if k in obj and len(obj[k]) == 1), None)
                if one_key:
                    obj.update(obj[one_key][0])

                return {
                    key: ref_to_dict(value)
                    for key, value in obj.items()
                    if not key.startswith(""$"") and key != one_key
                }
            elif isinstance(obj, list):
                return [ref_to_dict(item) for item in obj]
            return obj

        cls.cached_jsonschema = cast(dict[str, Any], ref_to_dict(model))

        # Set default properties values
        for field in cls.cached_jsonschema.get(""properties"", {}).values():
            if isinstance(field, dict) and ""advanced"" not in field:
                field[""advanced""] = True

        return cls.cached_jsonschema

    @classmethod
    def validate_data(cls, data: BlockInput) -> str | None:
        """"""
        Validate the data against the schema.
        Returns the validation error message if the data does not match the schema.
        """"""
        try:
            jsonschema.validate(data, cls.jsonschema())
            return None
        except jsonschema.ValidationError as e:
            return str(e)

    @classmethod
    def validate_field(cls, field_name: str, data: BlockInput) -> str | None:
        """"""
        Validate the data against a specific property (one of the input/output name).
        Returns the validation error message if the data does not match the schema.
        """"""
        model_schema = cls.jsonschema().get(""properties"", {})
        if not model_schema:
            return f""Invalid model schema {cls}""

        property_schema = model_schema.get(field_name)
        if not property_schema:
            return f""Invalid property name {field_name}""

        try:
            jsonschema.validate(json.to_dict(data), property_schema)
            return None
        except jsonschema.ValidationError as e:
            return str(e)

    @classmethod
    def get_fields(cls) -> set[str]:
        return set(cls.model_fields.keys())

    @classmethod
    def get_required_fields(cls) -> set[str]:
        return {
            field
            for field, field_info in cls.model_fields.items()
            if field_info.is_required()
        }

    @classmethod
    def __pydantic_init_subclass__(cls, **kwargs):
        """"""Validates the schema definition. Rules:
        - Only one `CredentialsMetaInput` field may be present.
          - This field MUST be called `credentials`.
        - A field that is called `credentials` MUST be a `CredentialsMetaInput`.
        """"""
        super().__pydantic_init_subclass__(**kwargs)
        credentials_fields = [
            field_name
            for field_name, info in cls.model_fields.items()
            if (
                inspect.isclass(info.annotation)
                and issubclass(
                    get_origin(info.annotation) or info.annotation,
                    CredentialsMetaInput,
                )
            )
        ]
        if len(credentials_fields) > 1:
            raise ValueError(
                f""{cls.__qualname__} can only have one CredentialsMetaInput field""
            )
        elif (
            len(credentials_fields) == 1
            and credentials_fields[0] != CREDENTIALS_FIELD_NAME
        ):
            raise ValueError(
                f""CredentialsMetaInput field on {cls.__qualname__} ""
                ""must be named 'credentials'""
            )
        elif (
            len(credentials_fields) == 0
            and CREDENTIALS_FIELD_NAME in cls.model_fields.keys()
        ):
            raise TypeError(
                f""Field 'credentials' on {cls.__qualname__} ""
                f""must be of type {CredentialsMetaInput.__name__}""
            )","Point(row=55, column=0)","Point(row=174, column=13)",,autogpt_platform/backend/backend/data/block.py
BlockSchema.jsonschema,function,,"def jsonschema(cls) -> dict[str, Any]:
        if cls.cached_jsonschema:
            return cls.cached_jsonschema

        model = jsonref.replace_refs(cls.model_json_schema(), merge_props=True)

        def ref_to_dict(obj):
            if isinstance(obj, dict):
                # OpenAPI <3.1 does not support sibling fields that has a $ref key
                # So sometimes, the schema has an ""allOf""/""anyOf""/""oneOf"" with 1 item.
                keys = {""allOf"", ""anyOf"", ""oneOf""}
                one_key = next((k for k in keys if k in obj and len(obj[k]) == 1), None)
                if one_key:
                    obj.update(obj[one_key][0])

                return {
                    key: ref_to_dict(value)
                    for key, value in obj.items()
                    if not key.startswith(""$"") and key != one_key
                }
            elif isinstance(obj, list):
                return [ref_to_dict(item) for item in obj]
            return obj

        cls.cached_jsonschema = cast(dict[str, Any], ref_to_dict(model))

        # Set default properties values
        for field in cls.cached_jsonschema.get(""properties"", {}).values():
            if isinstance(field, dict) and ""advanced"" not in field:
                field[""advanced""] = True

        return cls.cached_jsonschema","Point(row=59, column=4)","Point(row=90, column=36)",BlockSchema,autogpt_platform/backend/backend/data/block.py
BlockSchema.jsonschema.ref_to_dict,function,,"def ref_to_dict(obj):
            if isinstance(obj, dict):
                # OpenAPI <3.1 does not support sibling fields that has a $ref key
                # So sometimes, the schema has an ""allOf""/""anyOf""/""oneOf"" with 1 item.
                keys = {""allOf"", ""anyOf"", ""oneOf""}
                one_key = next((k for k in keys if k in obj and len(obj[k]) == 1), None)
                if one_key:
                    obj.update(obj[one_key][0])

                return {
                    key: ref_to_dict(value)
                    for key, value in obj.items()
                    if not key.startswith(""$"") and key != one_key
                }
            elif isinstance(obj, list):
                return [ref_to_dict(item) for item in obj]
            return obj","Point(row=65, column=8)","Point(row=81, column=22)",BlockSchema,autogpt_platform/backend/backend/data/block.py
BlockSchema.validate_data,function,"
        Validate the data against the schema.
        Returns the validation error message if the data does not match the schema.
","def validate_data(cls, data: BlockInput) -> str | None:
        """"""
        Validate the data against the schema.
        Returns the validation error message if the data does not match the schema.
        """"""
        try:
            jsonschema.validate(data, cls.jsonschema())
            return None
        except jsonschema.ValidationError as e:
            return str(e)","Point(row=93, column=4)","Point(row=102, column=25)",BlockSchema,autogpt_platform/backend/backend/data/block.py
BlockSchema.validate_field,function,"
        Validate the data against a specific property (one of the input/output name).
        Returns the validation error message if the data does not match the schema.
","def validate_field(cls, field_name: str, data: BlockInput) -> str | None:
        """"""
        Validate the data against a specific property (one of the input/output name).
        Returns the validation error message if the data does not match the schema.
        """"""
        model_schema = cls.jsonschema().get(""properties"", {})
        if not model_schema:
            return f""Invalid model schema {cls}""

        property_schema = model_schema.get(field_name)
        if not property_schema:
            return f""Invalid property name {field_name}""

        try:
            jsonschema.validate(json.to_dict(data), property_schema)
            return None
        except jsonschema.ValidationError as e:
            return str(e)","Point(row=105, column=4)","Point(row=122, column=25)",BlockSchema,autogpt_platform/backend/backend/data/block.py
BlockSchema.get_fields,function,,"def get_fields(cls) -> set[str]:
        return set(cls.model_fields.keys())","Point(row=125, column=4)","Point(row=126, column=43)",BlockSchema,autogpt_platform/backend/backend/data/block.py
BlockSchema.get_required_fields,function,,"def get_required_fields(cls) -> set[str]:
        return {
            field
            for field, field_info in cls.model_fields.items()
            if field_info.is_required()
        }","Point(row=129, column=4)","Point(row=134, column=9)",BlockSchema,autogpt_platform/backend/backend/data/block.py
BlockSchema.__pydantic_init_subclass__,function,"Validates the schema definition. Rules:
        - Only one `CredentialsMetaInput` field may be present.
          - This field MUST be called `credentials`.
        - A field that is called `credentials` MUST be a `CredentialsMetaInput`.
","def __pydantic_init_subclass__(cls, **kwargs):
        """"""Validates the schema definition. Rules:
        - Only one `CredentialsMetaInput` field may be present.
          - This field MUST be called `credentials`.
        - A field that is called `credentials` MUST be a `CredentialsMetaInput`.
        """"""
        super().__pydantic_init_subclass__(**kwargs)
        credentials_fields = [
            field_name
            for field_name, info in cls.model_fields.items()
            if (
                inspect.isclass(info.annotation)
                and issubclass(
                    get_origin(info.annotation) or info.annotation,
                    CredentialsMetaInput,
                )
            )
        ]
        if len(credentials_fields) > 1:
            raise ValueError(
                f""{cls.__qualname__} can only have one CredentialsMetaInput field""
            )
        elif (
            len(credentials_fields) == 1
            and credentials_fields[0] != CREDENTIALS_FIELD_NAME
        ):
            raise ValueError(
                f""CredentialsMetaInput field on {cls.__qualname__} ""
                ""must be named 'credentials'""
            )
        elif (
            len(credentials_fields) == 0
            and CREDENTIALS_FIELD_NAME in cls.model_fields.keys()
        ):
            raise TypeError(
                f""Field 'credentials' on {cls.__qualname__} ""
                f""must be of type {CredentialsMetaInput.__name__}""
            )","Point(row=137, column=4)","Point(row=174, column=13)",BlockSchema,autogpt_platform/backend/backend/data/block.py
EmptySchema,class,,"class EmptySchema(BlockSchema):
    pass","Point(row=181, column=0)","Point(row=182, column=8)",,autogpt_platform/backend/backend/data/block.py
Block,class,,"class Block(ABC, Generic[BlockSchemaInputType, BlockSchemaOutputType]):
    def __init__(
        self,
        id: str = """",
        description: str = """",
        contributors: list[ContributorDetails] = [],
        categories: set[BlockCategory] | None = None,
        input_schema: Type[BlockSchemaInputType] = EmptySchema,
        output_schema: Type[BlockSchemaOutputType] = EmptySchema,
        test_input: BlockInput | list[BlockInput] | None = None,
        test_output: BlockData | list[BlockData] | None = None,
        test_mock: dict[str, Any] | None = None,
        test_credentials: Optional[Credentials] = None,
        disabled: bool = False,
        static_output: bool = False,
        block_type: BlockType = BlockType.STANDARD,
    ):
        """"""
        Initialize the block with the given schema.

        Args:
            id: The unique identifier for the block, this value will be persisted in the
                DB. So it should be a unique and constant across the application run.
                Use the UUID format for the ID.
            description: The description of the block, explaining what the block does.
            contributors: The list of contributors who contributed to the block.
            input_schema: The schema, defined as a Pydantic model, for the input data.
            output_schema: The schema, defined as a Pydantic model, for the output data.
            test_input: The list or single sample input data for the block, for testing.
            test_output: The list or single expected output if the test_input is run.
            test_mock: function names on the block implementation to mock on test run.
            disabled: If the block is disabled, it will not be available for execution.
            static_output: Whether the output links of the block are static by default.
        """"""
        self.id = id
        self.input_schema = input_schema
        self.output_schema = output_schema
        self.test_input = test_input
        self.test_output = test_output
        self.test_mock = test_mock
        self.test_credentials = test_credentials
        self.description = description
        self.categories = categories or set()
        self.contributors = contributors or set()
        self.disabled = disabled
        self.static_output = static_output
        self.block_type = block_type
        self.execution_stats = {}

    @classmethod
    def create(cls: Type[""Block""]) -> ""Block"":
        return cls()

    @abstractmethod
    def run(self, input_data: BlockSchemaInputType, **kwargs) -> BlockOutput:
        """"""
        Run the block with the given input data.
        Args:
            input_data: The input data with the structure of input_schema.
        Returns:
            A Generator that yields (output_name, output_data).
            output_name: One of the output name defined in Block's output_schema.
            output_data: The data for the output_name, matching the defined schema.
        """"""
        pass

    def run_once(self, input_data: BlockSchemaInputType, output: str, **kwargs) -> Any:
        for name, data in self.run(input_data, **kwargs):
            if name == output:
                return data
        raise ValueError(f""{self.name} did not produce any output for {output}"")

    def merge_stats(self, stats: dict[str, Any]) -> dict[str, Any]:
        for key, value in stats.items():
            if isinstance(value, dict):
                self.execution_stats.setdefault(key, {}).update(value)
            elif isinstance(value, (int, float)):
                self.execution_stats.setdefault(key, 0)
                self.execution_stats[key] += value
            elif isinstance(value, list):
                self.execution_stats.setdefault(key, [])
                self.execution_stats[key].extend(value)
            else:
                self.execution_stats[key] = value
        return self.execution_stats

    @property
    def name(self):
        return self.__class__.__name__

    def to_dict(self):
        return {
            ""id"": self.id,
            ""name"": self.name,
            ""inputSchema"": self.input_schema.jsonschema(),
            ""outputSchema"": self.output_schema.jsonschema(),
            ""description"": self.description,
            ""categories"": [category.dict() for category in self.categories],
            ""contributors"": [
                contributor.model_dump() for contributor in self.contributors
            ],
            ""staticOutput"": self.static_output,
            ""uiType"": self.block_type.value,
        }

    def execute(self, input_data: BlockInput, **kwargs) -> BlockOutput:
        if error := self.input_schema.validate_data(input_data):
            raise ValueError(
                f""Unable to execute block with invalid input data: {error}""
            )

        for output_name, output_data in self.run(
            self.input_schema(**input_data), **kwargs
        ):
            if output_name == ""error"":
                raise RuntimeError(output_data)
            if error := self.output_schema.validate_field(output_name, output_data):
                raise ValueError(f""Block produced an invalid output data: {error}"")
            yield output_name, output_data","Point(row=185, column=0)","Point(row=303, column=42)",,autogpt_platform/backend/backend/data/block.py
Block.__init__,function,"
        Initialize the block with the given schema.

        Args:
            id: The unique identifier for the block, this value will be persisted in the
                DB. So it should be a unique and constant across the application run.
                Use the UUID format for the ID.
            description: The description of the block, explaining what the block does.
            contributors: The list of contributors who contributed to the block.
            input_schema: The schema, defined as a Pydantic model, for the input data.
            output_schema: The schema, defined as a Pydantic model, for the output data.
            test_input: The list or single sample input data for the block, for testing.
            test_output: The list or single expected output if the test_input is run.
            test_mock: function names on the block implementation to mock on test run.
            disabled: If the block is disabled, it will not be available for execution.
            static_output: Whether the output links of the block are static by default.
","def __init__(
        self,
        id: str = """",
        description: str = """",
        contributors: list[ContributorDetails] = [],
        categories: set[BlockCategory] | None = None,
        input_schema: Type[BlockSchemaInputType] = EmptySchema,
        output_schema: Type[BlockSchemaOutputType] = EmptySchema,
        test_input: BlockInput | list[BlockInput] | None = None,
        test_output: BlockData | list[BlockData] | None = None,
        test_mock: dict[str, Any] | None = None,
        test_credentials: Optional[Credentials] = None,
        disabled: bool = False,
        static_output: bool = False,
        block_type: BlockType = BlockType.STANDARD,
    ):
        """"""
        Initialize the block with the given schema.

        Args:
            id: The unique identifier for the block, this value will be persisted in the
                DB. So it should be a unique and constant across the application run.
                Use the UUID format for the ID.
            description: The description of the block, explaining what the block does.
            contributors: The list of contributors who contributed to the block.
            input_schema: The schema, defined as a Pydantic model, for the input data.
            output_schema: The schema, defined as a Pydantic model, for the output data.
            test_input: The list or single sample input data for the block, for testing.
            test_output: The list or single expected output if the test_input is run.
            test_mock: function names on the block implementation to mock on test run.
            disabled: If the block is disabled, it will not be available for execution.
            static_output: Whether the output links of the block are static by default.
        """"""
        self.id = id
        self.input_schema = input_schema
        self.output_schema = output_schema
        self.test_input = test_input
        self.test_output = test_output
        self.test_mock = test_mock
        self.test_credentials = test_credentials
        self.description = description
        self.categories = categories or set()
        self.contributors = contributors or set()
        self.disabled = disabled
        self.static_output = static_output
        self.block_type = block_type
        self.execution_stats = {}","Point(row=186, column=4)","Point(row=232, column=33)",Block,autogpt_platform/backend/backend/data/block.py
Block.create,function,,"def create(cls: Type[""Block""]) -> ""Block"":
        return cls()","Point(row=235, column=4)","Point(row=236, column=20)",Block,autogpt_platform/backend/backend/data/block.py
Block.run,function,"
        Run the block with the given input data.
        Args:
            input_data: The input data with the structure of input_schema.
        Returns:
            A Generator that yields (output_name, output_data).
            output_name: One of the output name defined in Block's output_schema.
            output_data: The data for the output_name, matching the defined schema.
","def run(self, input_data: BlockSchemaInputType, **kwargs) -> BlockOutput:
        """"""
        Run the block with the given input data.
        Args:
            input_data: The input data with the structure of input_schema.
        Returns:
            A Generator that yields (output_name, output_data).
            output_name: One of the output name defined in Block's output_schema.
            output_data: The data for the output_name, matching the defined schema.
        """"""
        pass","Point(row=239, column=4)","Point(row=249, column=12)",Block,autogpt_platform/backend/backend/data/block.py
Block.run_once,function,,"def run_once(self, input_data: BlockSchemaInputType, output: str, **kwargs) -> Any:
        for name, data in self.run(input_data, **kwargs):
            if name == output:
                return data
        raise ValueError(f""{self.name} did not produce any output for {output}"")","Point(row=251, column=4)","Point(row=255, column=80)",Block,autogpt_platform/backend/backend/data/block.py
Block.merge_stats,function,,"def merge_stats(self, stats: dict[str, Any]) -> dict[str, Any]:
        for key, value in stats.items():
            if isinstance(value, dict):
                self.execution_stats.setdefault(key, {}).update(value)
            elif isinstance(value, (int, float)):
                self.execution_stats.setdefault(key, 0)
                self.execution_stats[key] += value
            elif isinstance(value, list):
                self.execution_stats.setdefault(key, [])
                self.execution_stats[key].extend(value)
            else:
                self.execution_stats[key] = value
        return self.execution_stats","Point(row=257, column=4)","Point(row=269, column=35)",Block,autogpt_platform/backend/backend/data/block.py
Block.name,function,,"def name(self):
        return self.__class__.__name__","Point(row=272, column=4)","Point(row=273, column=38)",Block,autogpt_platform/backend/backend/data/block.py
Block.to_dict,function,,"def to_dict(self):
        return {
            ""id"": self.id,
            ""name"": self.name,
            ""inputSchema"": self.input_schema.jsonschema(),
            ""outputSchema"": self.output_schema.jsonschema(),
            ""description"": self.description,
            ""categories"": [category.dict() for category in self.categories],
            ""contributors"": [
                contributor.model_dump() for contributor in self.contributors
            ],
            ""staticOutput"": self.static_output,
            ""uiType"": self.block_type.value,
        }","Point(row=275, column=4)","Point(row=288, column=9)",Block,autogpt_platform/backend/backend/data/block.py
Block.execute,function,,"def execute(self, input_data: BlockInput, **kwargs) -> BlockOutput:
        if error := self.input_schema.validate_data(input_data):
            raise ValueError(
                f""Unable to execute block with invalid input data: {error}""
            )

        for output_name, output_data in self.run(
            self.input_schema(**input_data), **kwargs
        ):
            if output_name == ""error"":
                raise RuntimeError(output_data)
            if error := self.output_schema.validate_field(output_name, output_data):
                raise ValueError(f""Block produced an invalid output data: {error}"")
            yield output_name, output_data","Point(row=290, column=4)","Point(row=303, column=42)",Block,autogpt_platform/backend/backend/data/block.py
get_blocks,function,,"def get_blocks() -> dict[str, Type[Block]]:
    from backend.blocks import AVAILABLE_BLOCKS  # noqa: E402

    return AVAILABLE_BLOCKS","Point(row=309, column=0)","Point(row=312, column=27)",,autogpt_platform/backend/backend/data/block.py
initialize_blocks,function,,"async def initialize_blocks() -> None:
    for cls in get_blocks().values():
        block = cls()
        existing_block = await AgentBlock.prisma().find_first(
            where={""OR"": [{""id"": block.id}, {""name"": block.name}]}
        )
        if not existing_block:
            await AgentBlock.prisma().create(
                data={
                    ""id"": block.id,
                    ""name"": block.name,
                    ""inputSchema"": json.dumps(block.input_schema.jsonschema()),
                    ""outputSchema"": json.dumps(block.output_schema.jsonschema()),
                }
            )
            continue

        input_schema = json.dumps(block.input_schema.jsonschema())
        output_schema = json.dumps(block.output_schema.jsonschema())
        if (
            block.id != existing_block.id
            or block.name != existing_block.name
            or input_schema != existing_block.inputSchema
            or output_schema != existing_block.outputSchema
        ):
            await AgentBlock.prisma().update(
                where={""id"": existing_block.id},
                data={
                    ""id"": block.id,
                    ""name"": block.name,
                    ""inputSchema"": input_schema,
                    ""outputSchema"": output_schema,
                },
            )","Point(row=315, column=0)","Point(row=348, column=13)",,autogpt_platform/backend/backend/data/block.py
get_block,function,,"def get_block(block_id: str) -> Block | None:
    cls = get_blocks().get(block_id)
    return cls() if cls else None","Point(row=351, column=0)","Point(row=353, column=33)",,autogpt_platform/backend/backend/data/block.py
BlockCostType,class,,"class BlockCostType(str, Enum):
    RUN = ""run""  # cost X credits per run
    BYTE = ""byte""  # cost X credits per byte
    SECOND = ""second""  # cost X credits per second","Point(row=39, column=0)","Point(row=42, column=50)",,autogpt_platform/backend/backend/data/credit.py
BlockCost,class,,"class BlockCost(BaseModel):
    cost_amount: int
    cost_filter: BlockInput
    cost_type: BlockCostType

    def __init__(
        self,
        cost_amount: int,
        cost_type: BlockCostType = BlockCostType.RUN,
        cost_filter: Optional[BlockInput] = None,
        **data: Any,
    ) -> None:
        super().__init__(
            cost_amount=cost_amount,
            cost_filter=cost_filter or {},
            cost_type=cost_type,
            **data,
        )","Point(row=45, column=0)","Point(row=62, column=9)",,autogpt_platform/backend/backend/data/credit.py
BlockCost.__init__,function,,"def __init__(
        self,
        cost_amount: int,
        cost_type: BlockCostType = BlockCostType.RUN,
        cost_filter: Optional[BlockInput] = None,
        **data: Any,
    ) -> None:
        super().__init__(
            cost_amount=cost_amount,
            cost_filter=cost_filter or {},
            cost_type=cost_type,
            **data,
        )","Point(row=50, column=4)","Point(row=62, column=9)",BlockCost,autogpt_platform/backend/backend/data/credit.py
UserCreditBase,class,,"class UserCreditBase(ABC):
    def __init__(self, num_user_credits_refill: int):
        self.num_user_credits_refill = num_user_credits_refill

    @abstractmethod
    async def get_or_refill_credit(self, user_id: str) -> int:
        """"""
        Get the current credit for the user and refill if no transaction has been made in the current cycle.

        Returns:
            int: The current credit for the user.
        """"""
        pass

    @abstractmethod
    async def spend_credits(
        self,
        user_id: str,
        user_credit: int,
        block_id: str,
        input_data: BlockInput,
        data_size: float,
        run_time: float,
    ) -> int:
        """"""
        Spend the credits for the user based on the block usage.

        Args:
            user_id (str): The user ID.
            user_credit (int): The current credit for the user.
            block_id (str): The block ID.
            input_data (BlockInput): The input data for the block.
            data_size (float): The size of the data being processed.
            run_time (float): The time taken to run the block.

        Returns:
            int: amount of credit spent
        """"""
        pass

    @abstractmethod
    async def top_up_credits(self, user_id: str, amount: int):
        """"""
        Top up the credits for the user.

        Args:
            user_id (str): The user ID.
            amount (int): The amount to top up.
        """"""
        pass","Point(row=201, column=0)","Point(row=250, column=12)",,autogpt_platform/backend/backend/data/credit.py
UserCreditBase.__init__,function,,"def __init__(self, num_user_credits_refill: int):
        self.num_user_credits_refill = num_user_credits_refill","Point(row=202, column=4)","Point(row=203, column=62)",UserCreditBase,autogpt_platform/backend/backend/data/credit.py
UserCreditBase.get_or_refill_credit,function,"
        Get the current credit for the user and refill if no transaction has been made in the current cycle.

        Returns:
            int: The current credit for the user.
","async def get_or_refill_credit(self, user_id: str) -> int:
        """"""
        Get the current credit for the user and refill if no transaction has been made in the current cycle.

        Returns:
            int: The current credit for the user.
        """"""
        pass","Point(row=206, column=4)","Point(row=213, column=12)",UserCreditBase,autogpt_platform/backend/backend/data/credit.py
UserCreditBase.spend_credits,function,"
        Spend the credits for the user based on the block usage.

        Args:
            user_id (str): The user ID.
            user_credit (int): The current credit for the user.
            block_id (str): The block ID.
            input_data (BlockInput): The input data for the block.
            data_size (float): The size of the data being processed.
            run_time (float): The time taken to run the block.

        Returns:
            int: amount of credit spent
","async def spend_credits(
        self,
        user_id: str,
        user_credit: int,
        block_id: str,
        input_data: BlockInput,
        data_size: float,
        run_time: float,
    ) -> int:
        """"""
        Spend the credits for the user based on the block usage.

        Args:
            user_id (str): The user ID.
            user_credit (int): The current credit for the user.
            block_id (str): The block ID.
            input_data (BlockInput): The input data for the block.
            data_size (float): The size of the data being processed.
            run_time (float): The time taken to run the block.

        Returns:
            int: amount of credit spent
        """"""
        pass","Point(row=216, column=4)","Point(row=239, column=12)",UserCreditBase,autogpt_platform/backend/backend/data/credit.py
UserCreditBase.top_up_credits,function,"
        Top up the credits for the user.

        Args:
            user_id (str): The user ID.
            amount (int): The amount to top up.
","async def top_up_credits(self, user_id: str, amount: int):
        """"""
        Top up the credits for the user.

        Args:
            user_id (str): The user ID.
            amount (int): The amount to top up.
        """"""
        pass","Point(row=242, column=4)","Point(row=250, column=12)",UserCreditBase,autogpt_platform/backend/backend/data/credit.py
UserCredit,class,,"class UserCredit(UserCreditBase):
    async def get_or_refill_credit(self, user_id: str) -> int:
        cur_time = self.time_now()
        cur_month = cur_time.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        nxt_month = cur_month.replace(month=cur_month.month + 1)

        user_credit = await UserBlockCredit.prisma().group_by(
            by=[""userId""],
            sum={""amount"": True},
            where={
                ""userId"": user_id,
                ""createdAt"": {""gte"": cur_month, ""lt"": nxt_month},
                ""isActive"": True,
            },
        )

        if user_credit:
            credit_sum = user_credit[0].get(""_sum"") or {}
            return credit_sum.get(""amount"", 0)

        key = f""MONTHLY-CREDIT-TOP-UP-{cur_month}""

        try:
            await UserBlockCredit.prisma().create(
                data={
                    ""amount"": self.num_user_credits_refill,
                    ""type"": UserBlockCreditType.TOP_UP,
                    ""userId"": user_id,
                    ""transactionKey"": key,
                    ""createdAt"": self.time_now(),
                }
            )
        except prisma.errors.UniqueViolationError:
            pass  # Already refilled this month

        return self.num_user_credits_refill

    @staticmethod
    def time_now():
        return datetime.now(timezone.utc)

    @staticmethod
    def _block_usage_cost(
        block: Block,
        input_data: BlockInput,
        data_size: float,
        run_time: float,
    ) -> tuple[int, BlockInput]:
        block_costs = BLOCK_COSTS.get(type(block))
        if not block_costs:
            return 0, {}

        for block_cost in block_costs:
            if all(
                # None, [], {}, """", are considered the same value.
                input_data.get(k) == b or (not input_data.get(k) and not b)
                for k, b in block_cost.cost_filter.items()
            ):
                if block_cost.cost_type == BlockCostType.RUN:
                    return block_cost.cost_amount, block_cost.cost_filter

                if block_cost.cost_type == BlockCostType.SECOND:
                    return (
                        int(run_time * block_cost.cost_amount),
                        block_cost.cost_filter,
                    )

                if block_cost.cost_type == BlockCostType.BYTE:
                    return (
                        int(data_size * block_cost.cost_amount),
                        block_cost.cost_filter,
                    )

        return 0, {}

    async def spend_credits(
        self,
        user_id: str,
        user_credit: int,
        block_id: str,
        input_data: BlockInput,
        data_size: float,
        run_time: float,
        validate_balance: bool = True,
    ) -> int:
        block = get_block(block_id)
        if not block:
            raise ValueError(f""Block not found: {block_id}"")

        cost, matching_filter = self._block_usage_cost(
            block=block, input_data=input_data, data_size=data_size, run_time=run_time
        )
        if cost <= 0:
            return 0

        if validate_balance and user_credit < cost:
            raise ValueError(f""Insufficient credit: {user_credit} < {cost}"")

        await UserBlockCredit.prisma().create(
            data={
                ""userId"": user_id,
                ""amount"": -cost,
                ""type"": UserBlockCreditType.USAGE,
                ""blockId"": block.id,
                ""metadata"": Json(
                    {
                        ""block"": block.name,
                        ""input"": matching_filter,
                    }
                ),
                ""createdAt"": self.time_now(),
            }
        )
        return cost

    async def top_up_credits(self, user_id: str, amount: int):
        await UserBlockCredit.prisma().create(
            data={
                ""userId"": user_id,
                ""amount"": amount,
                ""type"": UserBlockCreditType.TOP_UP,
                ""createdAt"": self.time_now(),
            }
        )","Point(row=253, column=0)","Point(row=376, column=9)",,autogpt_platform/backend/backend/data/credit.py
UserCredit.get_or_refill_credit,function,,"async def get_or_refill_credit(self, user_id: str) -> int:
        cur_time = self.time_now()
        cur_month = cur_time.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
        nxt_month = cur_month.replace(month=cur_month.month + 1)

        user_credit = await UserBlockCredit.prisma().group_by(
            by=[""userId""],
            sum={""amount"": True},
            where={
                ""userId"": user_id,
                ""createdAt"": {""gte"": cur_month, ""lt"": nxt_month},
                ""isActive"": True,
            },
        )

        if user_credit:
            credit_sum = user_credit[0].get(""_sum"") or {}
            return credit_sum.get(""amount"", 0)

        key = f""MONTHLY-CREDIT-TOP-UP-{cur_month}""

        try:
            await UserBlockCredit.prisma().create(
                data={
                    ""amount"": self.num_user_credits_refill,
                    ""type"": UserBlockCreditType.TOP_UP,
                    ""userId"": user_id,
                    ""transactionKey"": key,
                    ""createdAt"": self.time_now(),
                }
            )
        except prisma.errors.UniqueViolationError:
            pass  # Already refilled this month

        return self.num_user_credits_refill","Point(row=254, column=4)","Point(row=288, column=43)",UserCredit,autogpt_platform/backend/backend/data/credit.py
UserCredit.time_now,function,,"def time_now():
        return datetime.now(timezone.utc)","Point(row=291, column=4)","Point(row=292, column=41)",UserCredit,autogpt_platform/backend/backend/data/credit.py
UserCredit._block_usage_cost,function,,"def _block_usage_cost(
        block: Block,
        input_data: BlockInput,
        data_size: float,
        run_time: float,
    ) -> tuple[int, BlockInput]:
        block_costs = BLOCK_COSTS.get(type(block))
        if not block_costs:
            return 0, {}

        for block_cost in block_costs:
            if all(
                # None, [], {}, """", are considered the same value.
                input_data.get(k) == b or (not input_data.get(k) and not b)
                for k, b in block_cost.cost_filter.items()
            ):
                if block_cost.cost_type == BlockCostType.RUN:
                    return block_cost.cost_amount, block_cost.cost_filter

                if block_cost.cost_type == BlockCostType.SECOND:
                    return (
                        int(run_time * block_cost.cost_amount),
                        block_cost.cost_filter,
                    )

                if block_cost.cost_type == BlockCostType.BYTE:
                    return (
                        int(data_size * block_cost.cost_amount),
                        block_cost.cost_filter,
                    )

        return 0, {}","Point(row=295, column=4)","Point(row=326, column=20)",UserCredit,autogpt_platform/backend/backend/data/credit.py
UserCredit.spend_credits,function,,"async def spend_credits(
        self,
        user_id: str,
        user_credit: int,
        block_id: str,
        input_data: BlockInput,
        data_size: float,
        run_time: float,
        validate_balance: bool = True,
    ) -> int:
        block = get_block(block_id)
        if not block:
            raise ValueError(f""Block not found: {block_id}"")

        cost, matching_filter = self._block_usage_cost(
            block=block, input_data=input_data, data_size=data_size, run_time=run_time
        )
        if cost <= 0:
            return 0

        if validate_balance and user_credit < cost:
            raise ValueError(f""Insufficient credit: {user_credit} < {cost}"")

        await UserBlockCredit.prisma().create(
            data={
                ""userId"": user_id,
                ""amount"": -cost,
                ""type"": UserBlockCreditType.USAGE,
                ""blockId"": block.id,
                ""metadata"": Json(
                    {
                        ""block"": block.name,
                        ""input"": matching_filter,
                    }
                ),
                ""createdAt"": self.time_now(),
            }
        )
        return cost","Point(row=328, column=4)","Point(row=366, column=19)",UserCredit,autogpt_platform/backend/backend/data/credit.py
UserCredit.top_up_credits,function,,"async def top_up_credits(self, user_id: str, amount: int):
        await UserBlockCredit.prisma().create(
            data={
                ""userId"": user_id,
                ""amount"": amount,
                ""type"": UserBlockCreditType.TOP_UP,
                ""createdAt"": self.time_now(),
            }
        )","Point(row=368, column=4)","Point(row=376, column=9)",UserCredit,autogpt_platform/backend/backend/data/credit.py
DisabledUserCredit,class,,"class DisabledUserCredit(UserCreditBase):
    async def get_or_refill_credit(self, *args, **kwargs) -> int:
        return 0

    async def spend_credits(self, *args, **kwargs) -> int:
        return 0

    async def top_up_credits(self, *args, **kwargs):
        pass","Point(row=379, column=0)","Point(row=387, column=12)",,autogpt_platform/backend/backend/data/credit.py
DisabledUserCredit.get_or_refill_credit,function,,"async def get_or_refill_credit(self, *args, **kwargs) -> int:
        return 0","Point(row=380, column=4)","Point(row=381, column=16)",DisabledUserCredit,autogpt_platform/backend/backend/data/credit.py
DisabledUserCredit.spend_credits,function,,"async def spend_credits(self, *args, **kwargs) -> int:
        return 0","Point(row=383, column=4)","Point(row=384, column=16)",DisabledUserCredit,autogpt_platform/backend/backend/data/credit.py
DisabledUserCredit.top_up_credits,function,,"async def top_up_credits(self, *args, **kwargs):
        pass","Point(row=386, column=4)","Point(row=387, column=12)",DisabledUserCredit,autogpt_platform/backend/backend/data/credit.py
get_user_credit_model,function,,"def get_user_credit_model() -> UserCreditBase:
    config = Config()
    if config.enable_credit.lower() == ""true"":
        return UserCredit(config.num_user_credits_refill)
    else:
        return DisabledUserCredit(0)","Point(row=390, column=0)","Point(row=395, column=36)",,autogpt_platform/backend/backend/data/credit.py
get_block_costs,function,,"def get_block_costs() -> dict[str, list[BlockCost]]:
    return {block().id: costs for block, costs in BLOCK_COSTS.items()}","Point(row=398, column=0)","Point(row=399, column=70)",,autogpt_platform/backend/backend/data/credit.py
create_test_graph,function,"
                    subreddit
                       ||
                        v
        GetRedditPostsBlock (post_id, post_title, post_body)
                  //     ||     \\
              post_id  post_title  post_body
                 ||       ||        ||
                 v        v         v
              FillTextTemplateBlock (format)
                      ||
                      v
            AIStructuredResponseBlock / TextRelevancy
                 ||       ||       ||
            post_id  is_relevant  marketing_text
               ||       ||        ||
               v        v         v
                 MatchTextPatternBlock
                 ||       ||
              positive  negative
                ||
                v
        PostRedditCommentBlock
","def create_test_graph() -> Graph:
    """"""
                    subreddit
                       ||
                        v
        GetRedditPostsBlock (post_id, post_title, post_body)
                  //     ||     \\
              post_id  post_title  post_body
                 ||       ||        ||
                 v        v         v
              FillTextTemplateBlock (format)
                      ||
                      v
            AIStructuredResponseBlock / TextRelevancy
                 ||       ||       ||
            post_id  is_relevant  marketing_text
               ||       ||        ||
               v        v         v
                 MatchTextPatternBlock
                 ||       ||
              positive  negative
                ||
                v
        PostRedditCommentBlock
    """"""
    # Hardcoded inputs
    reddit_get_post_input = {
        ""post_limit"": 3,
    }
    text_formatter_input = {
        ""format"": """"""
Based on the following post, write your marketing comment:
* Post ID: {id}
* Post Subreddit: {subreddit}
* Post Title: {title}
* Post Body: {body}"""""".strip(),
    }
    llm_call_input = {
        ""sys_prompt"": """"""
You are an expert at marketing.
You have been tasked with picking Reddit posts that are relevant to your product.
The product you are marketing is: Auto-GPT an autonomous AI agent utilizing GPT model.
You reply the post that you find it relevant to be replied with marketing text.
Make sure to only comment on a relevant post.
"""""",
        ""expected_format"": {
            ""post_id"": ""str, the reddit post id"",
            ""is_relevant"": ""bool, whether the post is relevant for marketing"",
            ""marketing_text"": ""str, marketing text, this is empty on irrelevant posts"",
        },
    }
    text_matcher_input = {""match"": ""true"", ""case_sensitive"": False}
    reddit_comment_input = {}

    # Nodes
    reddit_get_post_node = Node(
        block_id=GetRedditPostsBlock().id,
        input_default=reddit_get_post_input,
    )
    text_formatter_node = Node(
        block_id=FillTextTemplateBlock().id,
        input_default=text_formatter_input,
    )
    llm_call_node = Node(
        block_id=AIStructuredResponseGeneratorBlock().id, input_default=llm_call_input
    )
    text_matcher_node = Node(
        block_id=MatchTextPatternBlock().id,
        input_default=text_matcher_input,
    )
    reddit_comment_node = Node(
        block_id=PostRedditCommentBlock().id,
        input_default=reddit_comment_input,
    )

    nodes = [
        reddit_get_post_node,
        text_formatter_node,
        llm_call_node,
        text_matcher_node,
        reddit_comment_node,
    ]

    # Links
    links = [
        Link(
            source_id=reddit_get_post_node.id,
            sink_id=text_formatter_node.id,
            source_name=""post"",
            sink_name=""values"",
        ),
        Link(
            source_id=text_formatter_node.id,
            sink_id=llm_call_node.id,
            source_name=""output"",
            sink_name=""prompt"",
        ),
        Link(
            source_id=llm_call_node.id,
            sink_id=text_matcher_node.id,
            source_name=""response"",
            sink_name=""data"",
        ),
        Link(
            source_id=llm_call_node.id,
            sink_id=text_matcher_node.id,
            source_name=""response_#_is_relevant"",
            sink_name=""text"",
        ),
        Link(
            source_id=text_matcher_node.id,
            sink_id=reddit_comment_node.id,
            source_name=""positive_#_post_id"",
            sink_name=""data_#_post_id"",
        ),
        Link(
            source_id=text_matcher_node.id,
            sink_id=reddit_comment_node.id,
            source_name=""positive_#_marketing_text"",
            sink_name=""data_#_comment"",
        ),
    ]

    # Create graph
    test_graph = Graph(
        name=""RedditMarketingAgent"",
        description=""Reddit marketing agent"",
        nodes=nodes,
        links=links,
    )
    return test_graph","Point(row=10, column=0)","Point(row=140, column=21)",,autogpt_platform/backend/backend/usecases/reddit_marketing.py
create_test_user,function,,"async def create_test_user() -> User:
    test_user_data = {
        ""sub"": ""ef3b97d7-1161-4eb4-92b2-10c24fb154c1"",
        ""email"": ""testuser@example.com"",
        ""name"": ""Test User"",
    }
    user = await get_or_create_user(test_user_data)
    return user","Point(row=143, column=0)","Point(row=150, column=15)",,autogpt_platform/backend/backend/usecases/reddit_marketing.py
reddit_marketing_agent,function,,"async def reddit_marketing_agent():
    async with SpinTestServer() as server:
        test_user = await create_test_user()
        test_graph = await create_graph(create_test_graph(), user_id=test_user.id)
        input_data = {""subreddit"": ""AutoGPT""}
        response = await server.agent_server.test_execute_graph(
            test_graph.id, input_data, test_user.id
        )
        print(response)
        result = await wait_execution(test_user.id, test_graph.id, response[""id""], 120)
        print(result)","Point(row=153, column=0)","Point(row=163, column=21)",,autogpt_platform/backend/backend/usecases/reddit_marketing.py
create_test_user,function,,"async def create_test_user() -> User:
    test_user_data = {
        ""sub"": ""ef3b97d7-1161-4eb4-92b2-10c24fb154c1"",
        ""email"": ""testuser#example.com"",
        ""name"": ""Test User"",
    }
    user = await get_or_create_user(test_user_data)
    return user","Point(row=10, column=0)","Point(row=17, column=15)",,autogpt_platform/backend/backend/usecases/sample.py
create_test_graph,function,"
    InputBlock
               \
                 ---- FillTextTemplateBlock ---- PrintToConsoleBlock
               /
    InputBlock
","def create_test_graph() -> graph.Graph:
    """"""
    InputBlock
               \
                 ---- FillTextTemplateBlock ---- PrintToConsoleBlock
               /
    InputBlock
    """"""
    nodes = [
        graph.Node(
            block_id=AgentInputBlock().id,
            input_default={""name"": ""input_1""},
        ),
        graph.Node(
            block_id=AgentInputBlock().id,
            input_default={""name"": ""input_2""},
        ),
        graph.Node(
            block_id=FillTextTemplateBlock().id,
            input_default={
                ""format"": ""{a}, {b}{c}"",
                ""values_#_c"": ""!!!"",
            },
        ),
        graph.Node(block_id=PrintToConsoleBlock().id),
    ]
    links = [
        graph.Link(
            source_id=nodes[0].id,
            sink_id=nodes[2].id,
            source_name=""result"",
            sink_name=""values_#_a"",
        ),
        graph.Link(
            source_id=nodes[1].id,
            sink_id=nodes[2].id,
            source_name=""result"",
            sink_name=""values_#_b"",
        ),
        graph.Link(
            source_id=nodes[2].id,
            sink_id=nodes[3].id,
            source_name=""output"",
            sink_name=""text"",
        ),
    ]

    return graph.Graph(
        name=""TestGraph"",
        description=""Test graph"",
        nodes=nodes,
        links=links,
    )","Point(row=20, column=0)","Point(row=72, column=5)",,autogpt_platform/backend/backend/usecases/sample.py
sample_agent,function,,"async def sample_agent():
    async with SpinTestServer() as server:
        test_user = await create_test_user()
        test_graph = await create_graph(create_test_graph(), test_user.id)
        input_data = {""input_1"": ""Hello"", ""input_2"": ""World""}
        response = await server.agent_server.test_execute_graph(
            test_graph.id, input_data, test_user.id
        )
        print(response)
        result = await wait_execution(test_user.id, test_graph.id, response[""id""], 10)
        print(result)","Point(row=75, column=0)","Point(row=85, column=21)",,autogpt_platform/backend/backend/usecases/sample.py
create_test_user,function,,"async def create_test_user() -> User:
    test_user_data = {
        ""sub"": ""ef3b97d7-1161-4eb4-92b2-10c24fb154c1"",
        ""email"": ""testuser@example.com"",
        ""name"": ""Test User"",
    }
    user = await get_or_create_user(test_user_data)
    return user","Point(row=28, column=0)","Point(row=35, column=15)",,autogpt_platform/backend/backend/usecases/block_autogen.py
create_test_graph,function,"
            StoreValueBlock (input)
                 ||
                 v
        FillTextTemplateBlock (input query)
                 ||
                 v
         SendWebRequestBlock (browse)
                 ||
                 v
     ------> StoreValueBlock===============
    |           |  |                    ||
    |            --                     ||
    |                                   ||
    |                                   ||
    |                                    v
    |        AITextGeneratorBlock  <===== FillTextTemplateBlock (query)
    |            ||                      ^
    |            v                      ||
    |       ExtractTextInformationBlock             ||
    |            ||                     ||
    |            v                      ||
    ------ BlockInstallationBlock  ======
","def create_test_graph() -> Graph:
    """"""
            StoreValueBlock (input)
                 ||
                 v
        FillTextTemplateBlock (input query)
                 ||
                 v
         SendWebRequestBlock (browse)
                 ||
                 v
     ------> StoreValueBlock===============
    |           |  |                    ||
    |            --                     ||
    |                                   ||
    |                                   ||
    |                                    v
    |        AITextGeneratorBlock  <===== FillTextTemplateBlock (query)
    |            ||                      ^
    |            v                      ||
    |       ExtractTextInformationBlock             ||
    |            ||                     ||
    |            v                      ||
    ------ BlockInstallationBlock  ======
    """"""
    # ======= Nodes ========= #
    input_data = Node(block_id=StoreValueBlock().id)
    input_query_constant = Node(
        block_id=StoreValueBlock().id,
        input_default={""data"": None},
    )
    input_text_formatter = Node(
        block_id=FillTextTemplateBlock().id,
        input_default={
            ""format"": ""Show me how to make a python code for this query: `{query}`"",
        },
    )
    search_http_request = Node(
        block_id=SendWebRequestBlock().id,
        input_default={
            ""url"": ""https://osit-v2.bentlybro.com/search"",
        },
    )
    search_result_constant = Node(
        block_id=StoreValueBlock().id,
        input_default={
            ""data"": None,
        },
    )
    prompt_text_formatter = Node(
        block_id=FillTextTemplateBlock().id,
        input_default={
            ""format"": """"""
Write me a full Block implementation for this query: `{query}`

Here is the information I get to write a Python code for that:
{search_result}

Here is your previous attempt:
{previous_attempt}
"""""",
            ""values_#_previous_attempt"": ""No previous attempt found."",
        },
    )
    code_gen_llm_call = Node(
        block_id=AITextGeneratorBlock().id,
        input_default={
            ""sys_prompt"": f""""""
You are a software engineer and you are asked to write the full class implementation.
The class that you are implementing is extending a class called `Block`.
This class will be used as a node in a graph of other blocks to build a complex system.
This class has a method called `run` that takes an input and returns an output.
It also has an `id` attribute that is a UUID, input_schema, and output_schema.
For UUID, you have to hardcode it, like `d2e2ecd2-9ae6-422d-8dfe-ceca500ce6a6`,
don't use any automatic UUID generation, because it needs to be consistent.
To validate the correctness of your implementation, you can also define a test.
There is `test_input` and `test_output` you can use to validate your implementation.
There is also `test_mock` to mock a helper function on your block class for testing.

Feel free to start your answer by explaining your plan what's required how to test, etc.
But make sure to produce the fully working implementation at the end,
and it should be enclosed within this block format:
```python
<Your implementation here>
```

Here are a couple of sample of the Block class implementation:

{""--------------"".join([sample_block_codes[v] for v in sample_block_modules])}
"""""",
        },
    )
    code_text_parser = Node(
        block_id=ExtractTextInformationBlock().id,
        input_default={
            ""pattern"": ""```python\n(.+?)\n```"",
            ""group"": 1,
        },
    )
    block_installation = Node(
        block_id=BlockInstallationBlock().id,
    )
    nodes = [
        input_data,
        input_query_constant,
        input_text_formatter,
        search_http_request,
        search_result_constant,
        prompt_text_formatter,
        code_gen_llm_call,
        code_text_parser,
        block_installation,
    ]

    # ======= Links ========= #
    links = [
        Link(
            source_id=input_data.id,
            sink_id=input_query_constant.id,
            source_name=""output"",
            sink_name=""input"",
        ),
        Link(
            source_id=input_data.id,
            sink_id=input_text_formatter.id,
            source_name=""output"",
            sink_name=""values_#_query"",
        ),
        Link(
            source_id=input_query_constant.id,
            sink_id=input_query_constant.id,
            source_name=""output"",
            sink_name=""data"",
        ),
        Link(
            source_id=input_text_formatter.id,
            sink_id=search_http_request.id,
            source_name=""output"",
            sink_name=""body_#_query"",
        ),
        Link(
            source_id=search_http_request.id,
            sink_id=search_result_constant.id,
            source_name=""response_#_reply"",
            sink_name=""input"",
        ),
        Link(  # Loopback for constant block
            source_id=search_result_constant.id,
            sink_id=search_result_constant.id,
            source_name=""output"",
            sink_name=""data"",
        ),
        Link(
            source_id=search_result_constant.id,
            sink_id=prompt_text_formatter.id,
            source_name=""output"",
            sink_name=""values_#_search_result"",
        ),
        Link(
            source_id=input_query_constant.id,
            sink_id=prompt_text_formatter.id,
            source_name=""output"",
            sink_name=""values_#_query"",
        ),
        Link(
            source_id=prompt_text_formatter.id,
            sink_id=code_gen_llm_call.id,
            source_name=""output"",
            sink_name=""prompt"",
        ),
        Link(
            source_id=code_gen_llm_call.id,
            sink_id=code_text_parser.id,
            source_name=""response"",
            sink_name=""text"",
        ),
        Link(
            source_id=code_text_parser.id,
            sink_id=block_installation.id,
            source_name=""positive"",
            sink_name=""code"",
        ),
        Link(
            source_id=block_installation.id,
            sink_id=prompt_text_formatter.id,
            source_name=""error"",
            sink_name=""values_#_previous_attempt"",
        ),
        Link(  # Re-trigger search result.
            source_id=block_installation.id,
            sink_id=search_result_constant.id,
            source_name=""error"",
            sink_name=""input"",
        ),
        Link(  # Re-trigger search result.
            source_id=block_installation.id,
            sink_id=input_query_constant.id,
            source_name=""error"",
            sink_name=""input"",
        ),
    ]

    # ======= Graph ========= #
    return Graph(
        name=""BlockAutoGen"",
        description=""Block auto generation agent"",
        nodes=nodes,
        links=links,
    )","Point(row=38, column=0)","Point(row=246, column=5)",,autogpt_platform/backend/backend/usecases/block_autogen.py
block_autogen_agent,function,,"async def block_autogen_agent():
    async with SpinTestServer() as server:
        test_user = await create_test_user()
        test_graph = await create_graph(create_test_graph(), user_id=test_user.id)
        input_data = {""input"": ""Write me a block that writes a string into a file.""}
        response = await server.agent_server.test_execute_graph(
            test_graph.id, input_data, test_user.id
        )
        print(response)
        result = await wait_execution(
            graph_id=test_graph.id,
            graph_exec_id=response[""id""],
            timeout=1200,
            user_id=test_user.id,
        )
        print(result)","Point(row=249, column=0)","Point(row=264, column=21)",,autogpt_platform/backend/backend/usecases/block_autogen.py
SupabaseIntegrationCredentialsStore,class,,"class SupabaseIntegrationCredentialsStore:
    def __init__(self, redis: ""Redis""):
        self.locks = RedisKeyedMutex(redis)

    @property
    @thread_cached
    def db_manager(self) -> ""DatabaseManager"":
        from backend.executor.database import DatabaseManager
        from backend.util.service import get_service_client

        return get_service_client(DatabaseManager)

    def add_creds(self, user_id: str, credentials: Credentials) -> None:
        with self.locked_user_integrations(user_id):
            if self.get_creds_by_id(user_id, credentials.id):
                raise ValueError(
                    f""Can not re-create existing credentials #{credentials.id} ""
                    f""for user #{user_id}""
                )
            self._set_user_integration_creds(
                user_id, [*self.get_all_creds(user_id), credentials]
            )

    def get_all_creds(self, user_id: str) -> list[Credentials]:
        users_credentials = self._get_user_integrations(user_id).credentials
        all_credentials = users_credentials
        if settings.secrets.revid_api_key:
            all_credentials.append(revid_credentials)
        if settings.secrets.ideogram_api_key:
            all_credentials.append(ideogram_credentials)
        if settings.secrets.groq_api_key:
            all_credentials.append(groq_credentials)
        if settings.secrets.replicate_api_key:
            all_credentials.append(replicate_credentials)
        if settings.secrets.openai_api_key:
            all_credentials.append(openai_credentials)
        if settings.secrets.anthropic_api_key:
            all_credentials.append(anthropic_credentials)
        if settings.secrets.did_api_key:
            all_credentials.append(did_credentials)
        if settings.secrets.jina_api_key:
            all_credentials.append(jina_credentials)
        return all_credentials

    def get_creds_by_id(self, user_id: str, credentials_id: str) -> Credentials | None:
        all_credentials = self.get_all_creds(user_id)
        return next((c for c in all_credentials if c.id == credentials_id), None)

    def get_creds_by_provider(self, user_id: str, provider: str) -> list[Credentials]:
        credentials = self.get_all_creds(user_id)
        return [c for c in credentials if c.provider == provider]

    def get_authorized_providers(self, user_id: str) -> list[str]:
        credentials = self.get_all_creds(user_id)
        return list(set(c.provider for c in credentials))

    def update_creds(self, user_id: str, updated: Credentials) -> None:
        with self.locked_user_integrations(user_id):
            current = self.get_creds_by_id(user_id, updated.id)
            if not current:
                raise ValueError(
                    f""Credentials with ID {updated.id} ""
                    f""for user with ID {user_id} not found""
                )
            if type(current) is not type(updated):
                raise TypeError(
                    f""Can not update credentials with ID {updated.id} ""
                    f""from type {type(current)} ""
                    f""to type {type(updated)}""
                )

            # Ensure no scopes are removed when updating credentials
            if (
                isinstance(updated, OAuth2Credentials)
                and isinstance(current, OAuth2Credentials)
                and not set(updated.scopes).issuperset(current.scopes)
            ):
                raise ValueError(
                    f""Can not update credentials with ID {updated.id} ""
                    f""and scopes {current.scopes} ""
                    f""to more restrictive set of scopes {updated.scopes}""
                )

            # Update the credentials
            updated_credentials_list = [
                updated if c.id == updated.id else c
                for c in self.get_all_creds(user_id)
            ]
            self._set_user_integration_creds(user_id, updated_credentials_list)

    def delete_creds_by_id(self, user_id: str, credentials_id: str) -> None:
        with self.locked_user_integrations(user_id):
            filtered_credentials = [
                c for c in self.get_all_creds(user_id) if c.id != credentials_id
            ]
            self._set_user_integration_creds(user_id, filtered_credentials)

    def store_state_token(self, user_id: str, provider: str, scopes: list[str]) -> str:
        token = secrets.token_urlsafe(32)
        expires_at = datetime.now(timezone.utc) + timedelta(minutes=10)

        state = OAuthState(
            token=token,
            provider=provider,
            expires_at=int(expires_at.timestamp()),
            scopes=scopes,
        )

        with self.locked_user_integrations(user_id):
            user_integrations = self._get_user_integrations(user_id)
            oauth_states = user_integrations.oauth_states
            oauth_states.append(state)
            user_integrations.oauth_states = oauth_states

            self.db_manager.update_user_integrations(
                user_id=user_id, data=user_integrations
            )

        return token

    def get_any_valid_scopes_from_state_token(
        self, user_id: str, token: str, provider: str
    ) -> list[str]:
        """"""
        Get the valid scopes from the OAuth state token. This will return any valid scopes
        from any OAuth state token for the given provider. If no valid scopes are found,
        an empty list is returned. DO NOT RELY ON THIS TOKEN TO AUTHENTICATE A USER, AS IT
        IS TO CHECK IF THE USER HAS GIVEN PERMISSIONS TO THE APPLICATION BEFORE EXCHANGING
        THE CODE FOR TOKENS.
        """"""
        user_integrations = self._get_user_integrations(user_id)
        oauth_states = user_integrations.oauth_states

        now = datetime.now(timezone.utc)
        valid_state = next(
            (
                state
                for state in oauth_states
                if state.token == token
                and state.provider == provider
                and state.expires_at > now.timestamp()
            ),
            None,
        )

        if valid_state:
            return valid_state.scopes

        return []

    def verify_state_token(self, user_id: str, token: str, provider: str) -> bool:
        with self.locked_user_integrations(user_id):
            user_integrations = self._get_user_integrations(user_id)
            oauth_states = user_integrations.oauth_states

            now = datetime.now(timezone.utc)
            valid_state = next(
                (
                    state
                    for state in oauth_states
                    if state.token == token
                    and state.provider == provider
                    and state.expires_at > now.timestamp()
                ),
                None,
            )

            if valid_state:
                # Remove the used state
                oauth_states.remove(valid_state)
                user_integrations.oauth_states = oauth_states
                self.db_manager.update_user_integrations(user_id, user_integrations)
                return True

        return False

    def _set_user_integration_creds(
        self, user_id: str, credentials: list[Credentials]
    ) -> None:
        integrations = self._get_user_integrations(user_id)
        # Remove default credentials from the list
        credentials = [c for c in credentials if c not in DEFAULT_CREDENTIALS]
        integrations.credentials = credentials
        self.db_manager.update_user_integrations(user_id, integrations)

    def _get_user_integrations(self, user_id: str) -> UserIntegrations:
        integrations: UserIntegrations = self.db_manager.get_user_integrations(
            user_id=user_id
        )
        return integrations

    def locked_user_integrations(self, user_id: str):
        key = (self.db_manager, f""user:{user_id}"", ""integrations"")
        return self.locks.locked(key)","Point(row=95, column=0)","Point(row=288, column=37)",,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
SupabaseIntegrationCredentialsStore.__init__,function,,"def __init__(self, redis: ""Redis""):
        self.locks = RedisKeyedMutex(redis)","Point(row=96, column=4)","Point(row=97, column=43)",SupabaseIntegrationCredentialsStore,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
SupabaseIntegrationCredentialsStore.db_manager,function,,"def db_manager(self) -> ""DatabaseManager"":
        from backend.executor.database import DatabaseManager
        from backend.util.service import get_service_client

        return get_service_client(DatabaseManager)","Point(row=101, column=4)","Point(row=105, column=50)",SupabaseIntegrationCredentialsStore,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
SupabaseIntegrationCredentialsStore.add_creds,function,,"def add_creds(self, user_id: str, credentials: Credentials) -> None:
        with self.locked_user_integrations(user_id):
            if self.get_creds_by_id(user_id, credentials.id):
                raise ValueError(
                    f""Can not re-create existing credentials #{credentials.id} ""
                    f""for user #{user_id}""
                )
            self._set_user_integration_creds(
                user_id, [*self.get_all_creds(user_id), credentials]
            )","Point(row=107, column=4)","Point(row=116, column=13)",SupabaseIntegrationCredentialsStore,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
SupabaseIntegrationCredentialsStore.get_all_creds,function,,"def get_all_creds(self, user_id: str) -> list[Credentials]:
        users_credentials = self._get_user_integrations(user_id).credentials
        all_credentials = users_credentials
        if settings.secrets.revid_api_key:
            all_credentials.append(revid_credentials)
        if settings.secrets.ideogram_api_key:
            all_credentials.append(ideogram_credentials)
        if settings.secrets.groq_api_key:
            all_credentials.append(groq_credentials)
        if settings.secrets.replicate_api_key:
            all_credentials.append(replicate_credentials)
        if settings.secrets.openai_api_key:
            all_credentials.append(openai_credentials)
        if settings.secrets.anthropic_api_key:
            all_credentials.append(anthropic_credentials)
        if settings.secrets.did_api_key:
            all_credentials.append(did_credentials)
        if settings.secrets.jina_api_key:
            all_credentials.append(jina_credentials)
        return all_credentials","Point(row=118, column=4)","Point(row=137, column=30)",SupabaseIntegrationCredentialsStore,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
SupabaseIntegrationCredentialsStore.get_creds_by_id,function,,"def get_creds_by_id(self, user_id: str, credentials_id: str) -> Credentials | None:
        all_credentials = self.get_all_creds(user_id)
        return next((c for c in all_credentials if c.id == credentials_id), None)","Point(row=139, column=4)","Point(row=141, column=81)",SupabaseIntegrationCredentialsStore,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
SupabaseIntegrationCredentialsStore.get_creds_by_provider,function,,"def get_creds_by_provider(self, user_id: str, provider: str) -> list[Credentials]:
        credentials = self.get_all_creds(user_id)
        return [c for c in credentials if c.provider == provider]","Point(row=143, column=4)","Point(row=145, column=65)",SupabaseIntegrationCredentialsStore,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
SupabaseIntegrationCredentialsStore.get_authorized_providers,function,,"def get_authorized_providers(self, user_id: str) -> list[str]:
        credentials = self.get_all_creds(user_id)
        return list(set(c.provider for c in credentials))","Point(row=147, column=4)","Point(row=149, column=57)",SupabaseIntegrationCredentialsStore,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
SupabaseIntegrationCredentialsStore.update_creds,function,,"def update_creds(self, user_id: str, updated: Credentials) -> None:
        with self.locked_user_integrations(user_id):
            current = self.get_creds_by_id(user_id, updated.id)
            if not current:
                raise ValueError(
                    f""Credentials with ID {updated.id} ""
                    f""for user with ID {user_id} not found""
                )
            if type(current) is not type(updated):
                raise TypeError(
                    f""Can not update credentials with ID {updated.id} ""
                    f""from type {type(current)} ""
                    f""to type {type(updated)}""
                )

            # Ensure no scopes are removed when updating credentials
            if (
                isinstance(updated, OAuth2Credentials)
                and isinstance(current, OAuth2Credentials)
                and not set(updated.scopes).issuperset(current.scopes)
            ):
                raise ValueError(
                    f""Can not update credentials with ID {updated.id} ""
                    f""and scopes {current.scopes} ""
                    f""to more restrictive set of scopes {updated.scopes}""
                )

            # Update the credentials
            updated_credentials_list = [
                updated if c.id == updated.id else c
                for c in self.get_all_creds(user_id)
            ]
            self._set_user_integration_creds(user_id, updated_credentials_list)","Point(row=151, column=4)","Point(row=183, column=79)",SupabaseIntegrationCredentialsStore,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
SupabaseIntegrationCredentialsStore.delete_creds_by_id,function,,"def delete_creds_by_id(self, user_id: str, credentials_id: str) -> None:
        with self.locked_user_integrations(user_id):
            filtered_credentials = [
                c for c in self.get_all_creds(user_id) if c.id != credentials_id
            ]
            self._set_user_integration_creds(user_id, filtered_credentials)","Point(row=185, column=4)","Point(row=190, column=75)",SupabaseIntegrationCredentialsStore,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
SupabaseIntegrationCredentialsStore.store_state_token,function,,"def store_state_token(self, user_id: str, provider: str, scopes: list[str]) -> str:
        token = secrets.token_urlsafe(32)
        expires_at = datetime.now(timezone.utc) + timedelta(minutes=10)

        state = OAuthState(
            token=token,
            provider=provider,
            expires_at=int(expires_at.timestamp()),
            scopes=scopes,
        )

        with self.locked_user_integrations(user_id):
            user_integrations = self._get_user_integrations(user_id)
            oauth_states = user_integrations.oauth_states
            oauth_states.append(state)
            user_integrations.oauth_states = oauth_states

            self.db_manager.update_user_integrations(
                user_id=user_id, data=user_integrations
            )

        return token","Point(row=192, column=4)","Point(row=213, column=20)",SupabaseIntegrationCredentialsStore,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
SupabaseIntegrationCredentialsStore.get_any_valid_scopes_from_state_token,function,"
        Get the valid scopes from the OAuth state token. This will return any valid scopes
        from any OAuth state token for the given provider. If no valid scopes are found,
        an empty list is returned. DO NOT RELY ON THIS TOKEN TO AUTHENTICATE A USER, AS IT
        IS TO CHECK IF THE USER HAS GIVEN PERMISSIONS TO THE APPLICATION BEFORE EXCHANGING
        THE CODE FOR TOKENS.
","def get_any_valid_scopes_from_state_token(
        self, user_id: str, token: str, provider: str
    ) -> list[str]:
        """"""
        Get the valid scopes from the OAuth state token. This will return any valid scopes
        from any OAuth state token for the given provider. If no valid scopes are found,
        an empty list is returned. DO NOT RELY ON THIS TOKEN TO AUTHENTICATE A USER, AS IT
        IS TO CHECK IF THE USER HAS GIVEN PERMISSIONS TO THE APPLICATION BEFORE EXCHANGING
        THE CODE FOR TOKENS.
        """"""
        user_integrations = self._get_user_integrations(user_id)
        oauth_states = user_integrations.oauth_states

        now = datetime.now(timezone.utc)
        valid_state = next(
            (
                state
                for state in oauth_states
                if state.token == token
                and state.provider == provider
                and state.expires_at > now.timestamp()
            ),
            None,
        )

        if valid_state:
            return valid_state.scopes

        return []","Point(row=215, column=4)","Point(row=243, column=17)",SupabaseIntegrationCredentialsStore,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
SupabaseIntegrationCredentialsStore.verify_state_token,function,,"def verify_state_token(self, user_id: str, token: str, provider: str) -> bool:
        with self.locked_user_integrations(user_id):
            user_integrations = self._get_user_integrations(user_id)
            oauth_states = user_integrations.oauth_states

            now = datetime.now(timezone.utc)
            valid_state = next(
                (
                    state
                    for state in oauth_states
                    if state.token == token
                    and state.provider == provider
                    and state.expires_at > now.timestamp()
                ),
                None,
            )

            if valid_state:
                # Remove the used state
                oauth_states.remove(valid_state)
                user_integrations.oauth_states = oauth_states
                self.db_manager.update_user_integrations(user_id, user_integrations)
                return True

        return False","Point(row=245, column=4)","Point(row=269, column=20)",SupabaseIntegrationCredentialsStore,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
SupabaseIntegrationCredentialsStore._set_user_integration_creds,function,,"def _set_user_integration_creds(
        self, user_id: str, credentials: list[Credentials]
    ) -> None:
        integrations = self._get_user_integrations(user_id)
        # Remove default credentials from the list
        credentials = [c for c in credentials if c not in DEFAULT_CREDENTIALS]
        integrations.credentials = credentials
        self.db_manager.update_user_integrations(user_id, integrations)","Point(row=271, column=4)","Point(row=278, column=71)",SupabaseIntegrationCredentialsStore,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
SupabaseIntegrationCredentialsStore._get_user_integrations,function,,"def _get_user_integrations(self, user_id: str) -> UserIntegrations:
        integrations: UserIntegrations = self.db_manager.get_user_integrations(
            user_id=user_id
        )
        return integrations","Point(row=280, column=4)","Point(row=284, column=27)",SupabaseIntegrationCredentialsStore,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
SupabaseIntegrationCredentialsStore.locked_user_integrations,function,,"def locked_user_integrations(self, user_id: str):
        key = (self.db_manager, f""user:{user_id}"", ""integrations"")
        return self.locks.locked(key)","Point(row=286, column=4)","Point(row=288, column=37)",SupabaseIntegrationCredentialsStore,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/store.py
_BaseCredentials,class,,"class _BaseCredentials(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid4()))
    provider: str
    title: Optional[str]

    @field_serializer(""*"")
    def dump_secret_strings(value: Any, _info):
        if isinstance(value, SecretStr):
            return value.get_secret_value()
        return value","Point(row=6, column=0)","Point(row=15, column=20)",,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/types.py
_BaseCredentials.dump_secret_strings,function,,"def dump_secret_strings(value: Any, _info):
        if isinstance(value, SecretStr):
            return value.get_secret_value()
        return value","Point(row=12, column=4)","Point(row=15, column=20)",_BaseCredentials,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/types.py
OAuth2Credentials,class,,"class OAuth2Credentials(_BaseCredentials):
    type: Literal[""oauth2""] = ""oauth2""
    username: Optional[str]
    """"""Username of the third-party service user that these credentials belong to""""""
    access_token: SecretStr
    access_token_expires_at: Optional[int]
    """"""Unix timestamp (seconds) indicating when the access token expires (if at all)""""""
    refresh_token: Optional[SecretStr]
    refresh_token_expires_at: Optional[int]
    """"""Unix timestamp (seconds) indicating when the refresh token expires (if at all)""""""
    scopes: list[str]
    metadata: dict[str, Any] = Field(default_factory=dict)

    def bearer(self) -> str:
        return f""Bearer {self.access_token.get_secret_value()}""","Point(row=18, column=0)","Point(row=32, column=63)",,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/types.py
OAuth2Credentials.bearer,function,,"def bearer(self) -> str:
        return f""Bearer {self.access_token.get_secret_value()}""","Point(row=31, column=4)","Point(row=32, column=63)",OAuth2Credentials,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/types.py
APIKeyCredentials,class,,"class APIKeyCredentials(_BaseCredentials):
    type: Literal[""api_key""] = ""api_key""
    api_key: SecretStr
    expires_at: Optional[int]
    """"""Unix timestamp (seconds) indicating when the API key expires (if at all)""""""

    def bearer(self) -> str:
        return f""Bearer {self.api_key.get_secret_value()}""","Point(row=35, column=0)","Point(row=42, column=58)",,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/types.py
APIKeyCredentials.bearer,function,,"def bearer(self) -> str:
        return f""Bearer {self.api_key.get_secret_value()}""","Point(row=41, column=4)","Point(row=42, column=58)",APIKeyCredentials,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/types.py
OAuthState,class,,"class OAuthState(BaseModel):
    token: str
    provider: str
    expires_at: int
    scopes: list[str]
    """"""Unix timestamp (seconds) indicating when this OAuth state expires""""""","Point(row=54, column=0)","Point(row=59, column=75)",,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/types.py
UserMetadata,class,,"class UserMetadata(BaseModel):
    integration_credentials: list[Credentials] = Field(default_factory=list)
    integration_oauth_states: list[OAuthState] = Field(default_factory=list)","Point(row=62, column=0)","Point(row=64, column=76)",,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/types.py
UserMetadataRaw,class,,"class UserMetadataRaw(TypedDict, total=False):
    integration_credentials: list[dict]
    integration_oauth_states: list[dict]","Point(row=67, column=0)","Point(row=69, column=40)",,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/types.py
UserIntegrations,class,,"class UserIntegrations(BaseModel):
    credentials: list[Credentials] = Field(default_factory=list)
    oauth_states: list[OAuthState] = Field(default_factory=list)","Point(row=72, column=0)","Point(row=74, column=64)",,autogpt_platform/autogpt_libs/autogpt_libs/supabase_integration_credentials_store/types.py
Settings,class,,"class Settings:
    JWT_SECRET_KEY: str = os.getenv(""SUPABASE_JWT_SECRET"", """")
    ENABLE_AUTH: bool = os.getenv(""ENABLE_AUTH"", ""false"").lower() == ""true""
    JWT_ALGORITHM: str = ""HS256""

    @property
    def is_configured(self) -> bool:
        return bool(self.JWT_SECRET_KEY)","Point(row=7, column=0)","Point(row=14, column=40)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/config.py
Settings.is_configured,function,,"def is_configured(self) -> bool:
        return bool(self.JWT_SECRET_KEY)","Point(row=13, column=4)","Point(row=14, column=40)",Settings,autogpt_platform/autogpt_libs/autogpt_libs/auth/config.py
User,class,,"class User:
    user_id: str
    email: str
    phone_number: str
    role: str

    @classmethod
    def from_payload(cls, payload):
        return cls(
            user_id=payload[""sub""],
            email=payload.get(""email"", """"),
            phone_number=payload.get(""phone"", """"),
            role=payload[""role""],
        )","Point(row=5, column=0)","Point(row=18, column=9)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/models.py
User.from_payload,function,,"def from_payload(cls, payload):
        return cls(
            user_id=payload[""sub""],
            email=payload.get(""email"", """"),
            phone_number=payload.get(""phone"", """"),
            role=payload[""role""],
        )","Point(row=12, column=4)","Point(row=18, column=9)",User,autogpt_platform/autogpt_libs/autogpt_libs/auth/models.py
requires_user,function,,"def requires_user(payload: dict = fastapi.Depends(auth_middleware)) -> User:
    return verify_user(payload, admin_only=False)","Point(row=6, column=0)","Point(row=7, column=49)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/depends.py
requires_admin_user,function,,"def requires_admin_user(
    payload: dict = fastapi.Depends(auth_middleware),
) -> User:
    return verify_user(payload, admin_only=True)","Point(row=10, column=0)","Point(row=13, column=48)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/depends.py
verify_user,function,,"def verify_user(payload: dict | None, admin_only: bool) -> User:
    if not payload:
        # This handles the case when authentication is disabled
        payload = {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""admin""}

    user_id = payload.get(""sub"")

    if not user_id:
        raise fastapi.HTTPException(
            status_code=401, detail=""User ID not found in token""
        )

    if admin_only and payload[""role""] != ""admin"":
        raise fastapi.HTTPException(status_code=403, detail=""Admin access required"")

    return User.from_payload(payload)","Point(row=16, column=0)","Point(row=31, column=37)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/depends.py
test_verify_user_no_payload,function,,"def test_verify_user_no_payload():
    user = verify_user(None, admin_only=False)
    assert user.user_id == ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
    assert user.role == ""admin""","Point(row=5, column=0)","Point(row=8, column=31)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/depends_tests.py
test_verify_user_no_user_id,function,,"def test_verify_user_no_user_id():
    with pytest.raises(Exception):
        verify_user({""role"": ""admin""}, admin_only=False)","Point(row=11, column=0)","Point(row=13, column=56)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/depends_tests.py
test_verify_user_not_admin,function,,"def test_verify_user_not_admin():
    with pytest.raises(Exception):
        verify_user(
            {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""user""},
            admin_only=True,
        )","Point(row=16, column=0)","Point(row=21, column=9)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/depends_tests.py
test_verify_user_with_admin_role,function,,"def test_verify_user_with_admin_role():
    user = verify_user(
        {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""admin""},
        admin_only=True,
    )
    assert user.user_id == ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
    assert user.role == ""admin""","Point(row=24, column=0)","Point(row=30, column=31)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/depends_tests.py
test_verify_user_with_user_role,function,,"def test_verify_user_with_user_role():
    user = verify_user(
        {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""user""},
        admin_only=False,
    )
    assert user.user_id == ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
    assert user.role == ""user""","Point(row=33, column=0)","Point(row=39, column=30)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/depends_tests.py
test_requires_user,function,,"def test_requires_user():
    user = requires_user(
        {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""user""}
    )
    assert user.user_id == ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
    assert user.role == ""user""","Point(row=42, column=0)","Point(row=47, column=30)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/depends_tests.py
test_requires_user_no_user_id,function,,"def test_requires_user_no_user_id():
    with pytest.raises(Exception):
        requires_user({""role"": ""user""})","Point(row=50, column=0)","Point(row=52, column=39)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/depends_tests.py
test_requires_admin_user,function,,"def test_requires_admin_user():
    user = requires_admin_user(
        {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""admin""}
    )
    assert user.user_id == ""3e53486c-cf57-477e-ba2a-cb02dc828e1a""
    assert user.role == ""admin""","Point(row=55, column=0)","Point(row=60, column=31)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/depends_tests.py
test_requires_admin_user_not_admin,function,,"def test_requires_admin_user_not_admin():
    with pytest.raises(Exception):
        requires_admin_user(
            {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""user""}
        )","Point(row=63, column=0)","Point(row=67, column=9)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/depends_tests.py
parse_jwt_token,function,"
    Parse and validate a JWT token.

    :param token: The token to parse
    :return: The decoded payload
    :raises ValueError: If the token is invalid or expired
","def parse_jwt_token(token: str) -> Dict[str, Any]:
    """"""
    Parse and validate a JWT token.

    :param token: The token to parse
    :return: The decoded payload
    :raises ValueError: If the token is invalid or expired
    """"""
    try:
        payload = jwt.decode(
            token,
            settings.JWT_SECRET_KEY,
            algorithms=[settings.JWT_ALGORITHM],
            audience=""authenticated"",
        )
        return payload
    except jwt.ExpiredSignatureError:
        raise ValueError(""Token has expired"")
    except jwt.InvalidTokenError as e:
        raise ValueError(f""Invalid token: {str(e)}"")","Point(row=7, column=0)","Point(row=26, column=52)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/jwt_utils.py
auth_middleware,function,,"async def auth_middleware(request: Request):
    if not settings.ENABLE_AUTH:
        # If authentication is disabled, allow the request to proceed
        logger.warn(""Auth disabled"")
        return {}

    security = HTTPBearer()
    credentials = await security(request)

    if not credentials:
        raise HTTPException(status_code=401, detail=""Authorization header is missing"")

    try:
        payload = parse_jwt_token(credentials.credentials)
        request.state.user = payload
        logger.debug(""Token decoded successfully"")
    except ValueError as e:
        raise HTTPException(status_code=401, detail=str(e))
    return payload","Point(row=12, column=0)","Point(row=30, column=18)",,autogpt_platform/autogpt_libs/autogpt_libs/auth/middleware.py
thread_cached,function,,"def thread_cached(func: Callable[P, R]) -> Callable[P, R]:
    thread_local = threading.local()

    def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:
        cache = getattr(thread_local, ""cache"", None)
        if cache is None:
            cache = thread_local.cache = {}
        key = (args, tuple(sorted(kwargs.items())))
        if key not in cache:
            cache[key] = func(*args, **kwargs)
        return cache[key]

    return wrapper","Point(row=7, column=0)","Point(row=19, column=18)",,autogpt_platform/autogpt_libs/autogpt_libs/utils/cache.py
thread_cached.wrapper,function,,"def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:
        cache = getattr(thread_local, ""cache"", None)
        if cache is None:
            cache = thread_local.cache = {}
        key = (args, tuple(sorted(kwargs.items())))
        if key not in cache:
            cache[key] = func(*args, **kwargs)
        return cache[key]","Point(row=10, column=4)","Point(row=17, column=25)",,autogpt_platform/autogpt_libs/autogpt_libs/utils/cache.py
RedisKeyedMutex,class,"
    This class provides a mutex that can be locked and unlocked by a specific key,
    using Redis as a distributed locking provider.
    It uses an ExpiringDict to automatically clear the mutex after a specified timeout,
    in case the key is not unlocked for a specified duration, to prevent memory leaks.
","class RedisKeyedMutex:
    """"""
    This class provides a mutex that can be locked and unlocked by a specific key,
    using Redis as a distributed locking provider.
    It uses an ExpiringDict to automatically clear the mutex after a specified timeout,
    in case the key is not unlocked for a specified duration, to prevent memory leaks.
    """"""

    def __init__(self, redis: ""Redis"", timeout: int | None = 60):
        self.redis = redis
        self.timeout = timeout
        self.locks: dict[Any, ""RedisLock""] = ExpiringDict(
            max_len=6000, max_age_seconds=self.timeout
        )
        self.locks_lock = Lock()

    @contextmanager
    def locked(self, key: Any):
        lock = self.acquire(key)
        try:
            yield
        finally:
            lock.release()

    def acquire(self, key: Any) -> ""RedisLock"":
        """"""Acquires and returns a lock with the given key""""""
        with self.locks_lock:
            if key not in self.locks:
                self.locks[key] = self.redis.lock(
                    str(key), self.timeout, thread_local=False
                )
            lock = self.locks[key]
        lock.acquire()
        return lock

    def release(self, key: Any):
        if lock := self.locks.get(key):
            lock.release()

    def release_all_locks(self):
        """"""Call this on process termination to ensure all locks are released""""""
        self.locks_lock.acquire(blocking=False)
        for lock in self.locks.values():
            if lock.locked() and lock.owned():
                lock.release()","Point(row=11, column=0)","Point(row=55, column=30)",,autogpt_platform/autogpt_libs/autogpt_libs/utils/synchronize.py
RedisKeyedMutex.__init__,function,,"def __init__(self, redis: ""Redis"", timeout: int | None = 60):
        self.redis = redis
        self.timeout = timeout
        self.locks: dict[Any, ""RedisLock""] = ExpiringDict(
            max_len=6000, max_age_seconds=self.timeout
        )
        self.locks_lock = Lock()","Point(row=19, column=4)","Point(row=25, column=32)",RedisKeyedMutex,autogpt_platform/autogpt_libs/autogpt_libs/utils/synchronize.py
RedisKeyedMutex.locked,function,,"def locked(self, key: Any):
        lock = self.acquire(key)
        try:
            yield
        finally:
            lock.release()","Point(row=28, column=4)","Point(row=33, column=26)",RedisKeyedMutex,autogpt_platform/autogpt_libs/autogpt_libs/utils/synchronize.py
RedisKeyedMutex.acquire,function,Acquires and returns a lock with the given key,"def acquire(self, key: Any) -> ""RedisLock"":
        """"""Acquires and returns a lock with the given key""""""
        with self.locks_lock:
            if key not in self.locks:
                self.locks[key] = self.redis.lock(
                    str(key), self.timeout, thread_local=False
                )
            lock = self.locks[key]
        lock.acquire()
        return lock","Point(row=35, column=4)","Point(row=44, column=19)",RedisKeyedMutex,autogpt_platform/autogpt_libs/autogpt_libs/utils/synchronize.py
RedisKeyedMutex.release,function,,"def release(self, key: Any):
        if lock := self.locks.get(key):
            lock.release()","Point(row=46, column=4)","Point(row=48, column=26)",RedisKeyedMutex,autogpt_platform/autogpt_libs/autogpt_libs/utils/synchronize.py
RedisKeyedMutex.release_all_locks,function,Call this on process termination to ensure all locks are released,"def release_all_locks(self):
        """"""Call this on process termination to ensure all locks are released""""""
        self.locks_lock.acquire(blocking=False)
        for lock in self.locks.values():
            if lock.locked() and lock.owned():
                lock.release()","Point(row=50, column=4)","Point(row=55, column=30)",RedisKeyedMutex,autogpt_platform/autogpt_libs/autogpt_libs/utils/synchronize.py
test_remove_color_codes,function,,"def test_remove_color_codes(raw_text, clean_text):
    assert remove_color_codes(raw_text) == clean_text","Point(row=34, column=0)","Point(row=35, column=53)",,autogpt_platform/autogpt_libs/autogpt_libs/logging/test_utils.py
LoggingConfig,class,,"class LoggingConfig(BaseSettings):

    level: str = Field(
        default=""INFO"",
        description=""Logging level"",
        validation_alias=""LOG_LEVEL"",
    )

    enable_cloud_logging: bool = Field(
        default=False,
        description=""Enable logging to Google Cloud Logging"",
    )

    enable_file_logging: bool = Field(
        default=False,
        description=""Enable logging to file"",
    )
    # File output
    log_dir: Path = Field(
        default=LOG_DIR,
        description=""Log directory"",
    )

    model_config = SettingsConfigDict(
        env_prefix="""",
        env_file="".env"",
        env_file_encoding=""utf-8"",
        extra=""ignore"",
    )

    @field_validator(""level"", mode=""before"")
    @classmethod
    def parse_log_level(cls, v):
        if isinstance(v, str):
            v = v.upper()
            if v not in [""DEBUG"", ""INFO"", ""WARNING"", ""ERROR"", ""CRITICAL""]:
                raise ValueError(f""Invalid log level: {v}"")
            return v
        return v","Point(row=23, column=0)","Point(row=61, column=16)",,autogpt_platform/autogpt_libs/autogpt_libs/logging/config.py
LoggingConfig.parse_log_level,function,,"def parse_log_level(cls, v):
        if isinstance(v, str):
            v = v.upper()
            if v not in [""DEBUG"", ""INFO"", ""WARNING"", ""ERROR"", ""CRITICAL""]:
                raise ValueError(f""Invalid log level: {v}"")
            return v
        return v","Point(row=55, column=4)","Point(row=61, column=16)",LoggingConfig,autogpt_platform/autogpt_libs/autogpt_libs/logging/config.py
configure_logging,function,"Configure the native logging module based on the LoggingConfig settings.

    This function sets up logging handlers and formatters according to the
    configuration specified in the LoggingConfig object. It supports various
    logging outputs including console, file, cloud, and JSON logging.

    The function uses the LoggingConfig object to determine which logging
    features to enable and how to configure them. This includes setting
    log levels, log formats, and output destinations.

    No arguments are required as the function creates its own LoggingConfig
    instance internally.

    Note: This function is typically called at the start of the application
    to set up the logging infrastructure.
","def configure_logging(force_cloud_logging: bool = False) -> None:
    """"""Configure the native logging module based on the LoggingConfig settings.

    This function sets up logging handlers and formatters according to the
    configuration specified in the LoggingConfig object. It supports various
    logging outputs including console, file, cloud, and JSON logging.

    The function uses the LoggingConfig object to determine which logging
    features to enable and how to configure them. This includes setting
    log levels, log formats, and output destinations.

    No arguments are required as the function creates its own LoggingConfig
    instance internally.

    Note: This function is typically called at the start of the application
    to set up the logging infrastructure.
    """"""

    config = LoggingConfig()

    log_handlers: list[logging.Handler] = []

    # Cloud logging setup
    if config.enable_cloud_logging or force_cloud_logging:
        import google.cloud.logging
        from google.cloud.logging.handlers import CloudLoggingHandler
        from google.cloud.logging_v2.handlers.transports.sync import SyncTransport

        client = google.cloud.logging.Client()
        cloud_handler = CloudLoggingHandler(
            client,
            name=""autogpt_logs"",
            transport=SyncTransport,
        )
        cloud_handler.setLevel(config.level)
        cloud_handler.setFormatter(StructuredLoggingFormatter())
        log_handlers.append(cloud_handler)
        print(""Cloud logging enabled"")
    else:
        # Console output handlers
        stdout = logging.StreamHandler(stream=sys.stdout)
        stdout.setLevel(config.level)
        stdout.addFilter(BelowLevelFilter(logging.WARNING))
        if config.level == logging.DEBUG:
            stdout.setFormatter(AGPTFormatter(DEBUG_LOG_FORMAT))
        else:
            stdout.setFormatter(AGPTFormatter(SIMPLE_LOG_FORMAT))

        stderr = logging.StreamHandler()
        stderr.setLevel(logging.WARNING)
        if config.level == logging.DEBUG:
            stderr.setFormatter(AGPTFormatter(DEBUG_LOG_FORMAT))
        else:
            stderr.setFormatter(AGPTFormatter(SIMPLE_LOG_FORMAT))

        log_handlers += [stdout, stderr]
        print(""Console logging enabled"")

    # File logging setup
    if config.enable_file_logging:
        # create log directory if it doesn't exist
        if not config.log_dir.exists():
            config.log_dir.mkdir(parents=True, exist_ok=True)

        print(f""Log directory: {config.log_dir}"")

        # Activity log handler (INFO and above)
        activity_log_handler = logging.FileHandler(
            config.log_dir / LOG_FILE, ""a"", ""utf-8""
        )
        activity_log_handler.setLevel(config.level)
        activity_log_handler.setFormatter(
            AGPTFormatter(SIMPLE_LOG_FORMAT, no_color=True)
        )
        log_handlers.append(activity_log_handler)

        if config.level == logging.DEBUG:
            # Debug log handler (all levels)
            debug_log_handler = logging.FileHandler(
                config.log_dir / DEBUG_LOG_FILE, ""a"", ""utf-8""
            )
            debug_log_handler.setLevel(logging.DEBUG)
            debug_log_handler.setFormatter(
                AGPTFormatter(DEBUG_LOG_FORMAT, no_color=True)
            )
            log_handlers.append(debug_log_handler)

        # Error log handler (ERROR and above)
        error_log_handler = logging.FileHandler(
            config.log_dir / ERROR_LOG_FILE, ""a"", ""utf-8""
        )
        error_log_handler.setLevel(logging.ERROR)
        error_log_handler.setFormatter(AGPTFormatter(DEBUG_LOG_FORMAT, no_color=True))
        log_handlers.append(error_log_handler)
        print(""File logging enabled"")

    # Configure the root logger
    logging.basicConfig(
        format=DEBUG_LOG_FORMAT if config.level == logging.DEBUG else SIMPLE_LOG_FORMAT,
        level=config.level,
        handlers=log_handlers,
    )","Point(row=64, column=0)","Point(row=165, column=5)",,autogpt_platform/autogpt_libs/autogpt_libs/logging/config.py
FancyConsoleFormatter,class,"
    A custom logging formatter designed for console output.

    This formatter enhances the standard logging output with color coding. The color
    coding is based on the level of the log message, making it easier to distinguish
    between different types of messages in the console output.

    The color for each level is defined in the LEVEL_COLOR_MAP class attribute.
","class FancyConsoleFormatter(logging.Formatter):
    """"""
    A custom logging formatter designed for console output.

    This formatter enhances the standard logging output with color coding. The color
    coding is based on the level of the log message, making it easier to distinguish
    between different types of messages in the console output.

    The color for each level is defined in the LEVEL_COLOR_MAP class attribute.
    """"""

    # level -> (level & text color, title color)
    LEVEL_COLOR_MAP = {
        logging.DEBUG: Fore.LIGHTBLACK_EX,
        logging.INFO: Fore.BLUE,
        logging.WARNING: Fore.YELLOW,
        logging.ERROR: Fore.RED,
        logging.CRITICAL: Fore.RED + Style.BRIGHT,
    }

    def format(self, record: logging.LogRecord) -> str:
        # Make sure `msg` is a string
        if not hasattr(record, ""msg""):
            record.msg = """"
        elif type(record.msg) is not str:
            record.msg = str(record.msg)

        # Determine default color based on error level
        level_color = """"
        if record.levelno in self.LEVEL_COLOR_MAP:
            level_color = self.LEVEL_COLOR_MAP[record.levelno]
            record.levelname = f""{level_color}{record.levelname}{Style.RESET_ALL}""

        # Determine color for message
        color = getattr(record, ""color"", level_color)
        color_is_specified = hasattr(record, ""color"")

        # Don't color INFO messages unless the color is explicitly specified.
        if color and (record.levelno != logging.INFO or color_is_specified):
            record.msg = f""{color}{record.msg}{Style.RESET_ALL}""

        return super().format(record)","Point(row=8, column=0)","Point(row=49, column=37)",,autogpt_platform/autogpt_libs/autogpt_libs/logging/formatters.py
FancyConsoleFormatter.format,function,,"def format(self, record: logging.LogRecord) -> str:
        # Make sure `msg` is a string
        if not hasattr(record, ""msg""):
            record.msg = """"
        elif type(record.msg) is not str:
            record.msg = str(record.msg)

        # Determine default color based on error level
        level_color = """"
        if record.levelno in self.LEVEL_COLOR_MAP:
            level_color = self.LEVEL_COLOR_MAP[record.levelno]
            record.levelname = f""{level_color}{record.levelname}{Style.RESET_ALL}""

        # Determine color for message
        color = getattr(record, ""color"", level_color)
        color_is_specified = hasattr(record, ""color"")

        # Don't color INFO messages unless the color is explicitly specified.
        if color and (record.levelno != logging.INFO or color_is_specified):
            record.msg = f""{color}{record.msg}{Style.RESET_ALL}""

        return super().format(record)","Point(row=28, column=4)","Point(row=49, column=37)",FancyConsoleFormatter,autogpt_platform/autogpt_libs/autogpt_libs/logging/formatters.py
AGPTFormatter,class,,"class AGPTFormatter(FancyConsoleFormatter):
    def __init__(self, *args, no_color: bool = False, **kwargs):
        super().__init__(*args, **kwargs)
        self.no_color = no_color

    def format(self, record: logging.LogRecord) -> str:
        # Make sure `msg` is a string
        if not hasattr(record, ""msg""):
            record.msg = """"
        elif type(record.msg) is not str:
            record.msg = str(record.msg)

        # Strip color from the message to prevent color spoofing
        if record.msg and not getattr(record, ""preserve_color"", False):
            record.msg = remove_color_codes(record.msg)

        # Determine color for title
        title = getattr(record, ""title"", """")
        title_color = getattr(record, ""title_color"", """") or self.LEVEL_COLOR_MAP.get(
            record.levelno, """"
        )
        if title and title_color:
            title = f""{title_color + Style.BRIGHT}{title}{Style.RESET_ALL}""
        # Make sure record.title is set, and padded with a space if not empty
        record.title = f""{title} "" if title else """"

        if self.no_color:
            return remove_color_codes(super().format(record))
        else:
            return super().format(record)","Point(row=52, column=0)","Point(row=81, column=41)",,autogpt_platform/autogpt_libs/autogpt_libs/logging/formatters.py
AGPTFormatter.__init__,function,,"def __init__(self, *args, no_color: bool = False, **kwargs):
        super().__init__(*args, **kwargs)
        self.no_color = no_color","Point(row=53, column=4)","Point(row=55, column=32)",AGPTFormatter,autogpt_platform/autogpt_libs/autogpt_libs/logging/formatters.py
AGPTFormatter.format,function,,"def format(self, record: logging.LogRecord) -> str:
        # Make sure `msg` is a string
        if not hasattr(record, ""msg""):
            record.msg = """"
        elif type(record.msg) is not str:
            record.msg = str(record.msg)

        # Strip color from the message to prevent color spoofing
        if record.msg and not getattr(record, ""preserve_color"", False):
            record.msg = remove_color_codes(record.msg)

        # Determine color for title
        title = getattr(record, ""title"", """")
        title_color = getattr(record, ""title_color"", """") or self.LEVEL_COLOR_MAP.get(
            record.levelno, """"
        )
        if title and title_color:
            title = f""{title_color + Style.BRIGHT}{title}{Style.RESET_ALL}""
        # Make sure record.title is set, and padded with a space if not empty
        record.title = f""{title} "" if title else """"

        if self.no_color:
            return remove_color_codes(super().format(record))
        else:
            return super().format(record)","Point(row=57, column=4)","Point(row=81, column=41)",AGPTFormatter,autogpt_platform/autogpt_libs/autogpt_libs/logging/formatters.py
StructuredLoggingFormatter,class,,"class StructuredLoggingFormatter(StructuredLogHandler, logging.Formatter):
    def __init__(self):
        # Set up CloudLoggingFilter to add diagnostic info to the log records
        self.cloud_logging_filter = CloudLoggingFilter()

        # Init StructuredLogHandler
        super().__init__()

    def format(self, record: logging.LogRecord) -> str:
        self.cloud_logging_filter.filter(record)
        return super().format(record)","Point(row=84, column=0)","Point(row=94, column=37)",,autogpt_platform/autogpt_libs/autogpt_libs/logging/formatters.py
StructuredLoggingFormatter.__init__,function,,"def __init__(self):
        # Set up CloudLoggingFilter to add diagnostic info to the log records
        self.cloud_logging_filter = CloudLoggingFilter()

        # Init StructuredLogHandler
        super().__init__()","Point(row=85, column=4)","Point(row=90, column=26)",StructuredLoggingFormatter,autogpt_platform/autogpt_libs/autogpt_libs/logging/formatters.py
StructuredLoggingFormatter.format,function,,"def format(self, record: logging.LogRecord) -> str:
        self.cloud_logging_filter.filter(record)
        return super().format(record)","Point(row=92, column=4)","Point(row=94, column=37)",StructuredLoggingFormatter,autogpt_platform/autogpt_libs/autogpt_libs/logging/formatters.py
JsonFileHandler,class,,"class JsonFileHandler(logging.FileHandler):
    def format(self, record: logging.LogRecord) -> str:
        record.json_data = json.loads(record.getMessage())
        return json.dumps(getattr(record, ""json_data""), ensure_ascii=False, indent=4)

    def emit(self, record: logging.LogRecord) -> None:
        with open(self.baseFilename, ""w"", encoding=""utf-8"") as f:
            f.write(self.format(record))","Point(row=6, column=0)","Point(row=13, column=40)",,autogpt_platform/autogpt_libs/autogpt_libs/logging/handlers.py
JsonFileHandler.format,function,,"def format(self, record: logging.LogRecord) -> str:
        record.json_data = json.loads(record.getMessage())
        return json.dumps(getattr(record, ""json_data""), ensure_ascii=False, indent=4)","Point(row=7, column=4)","Point(row=9, column=85)",JsonFileHandler,autogpt_platform/autogpt_libs/autogpt_libs/logging/handlers.py
JsonFileHandler.emit,function,,"def emit(self, record: logging.LogRecord) -> None:
        with open(self.baseFilename, ""w"", encoding=""utf-8"") as f:
            f.write(self.format(record))","Point(row=11, column=4)","Point(row=13, column=40)",JsonFileHandler,autogpt_platform/autogpt_libs/autogpt_libs/logging/handlers.py
remove_color_codes,function,,"def remove_color_codes(s: str) -> str:
    return re.sub(r""\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])"", """", s)","Point(row=7, column=0)","Point(row=8, column=66)",,autogpt_platform/autogpt_libs/autogpt_libs/logging/utils.py
fmt_kwargs,function,,"def fmt_kwargs(kwargs: dict) -> str:
    return "", "".join(f""{n}={repr(v)}"" for n, v in kwargs.items())","Point(row=11, column=0)","Point(row=12, column=65)",,autogpt_platform/autogpt_libs/autogpt_libs/logging/utils.py
print_attribute,function,,"def print_attribute(
    title: str, value: Any, title_color: str = Fore.GREEN, value_color: str = """"
) -> None:
    logger = logging.getLogger()
    logger.info(
        str(value),
        extra={
            ""title"": f""{title.rstrip(':')}:"",
            ""title_color"": title_color,
            ""color"": value_color,
        },
    )","Point(row=15, column=0)","Point(row=26, column=5)",,autogpt_platform/autogpt_libs/autogpt_libs/logging/utils.py
BelowLevelFilter,class,Filter for logging levels below a certain threshold.,"class BelowLevelFilter(logging.Filter):
    """"""Filter for logging levels below a certain threshold.""""""

    def __init__(self, below_level: int):
        super().__init__()
        self.below_level = below_level

    def filter(self, record: logging.LogRecord):
        return record.levelno < self.below_level","Point(row=3, column=0)","Point(row=11, column=48)",,autogpt_platform/autogpt_libs/autogpt_libs/logging/filters.py
BelowLevelFilter.__init__,function,,"def __init__(self, below_level: int):
        super().__init__()
        self.below_level = below_level","Point(row=6, column=4)","Point(row=8, column=38)",BelowLevelFilter,autogpt_platform/autogpt_libs/autogpt_libs/logging/filters.py
BelowLevelFilter.filter,function,,"def filter(self, record: logging.LogRecord):
        return record.levelno < self.below_level","Point(row=10, column=4)","Point(row=11, column=48)",BelowLevelFilter,autogpt_platform/autogpt_libs/autogpt_libs/logging/filters.py
run,function,,"def run(*command: str) -> None:
    print(f"">>>>> Running poetry run {' '.join(command)}"")
    subprocess.run([""poetry"", ""run""] + list(command), cwd=directory, check=True)","Point(row=6, column=0)","Point(row=8, column=80)",,autogpt_platform/market/scripts.py
lint,function,,"def lint():
    try:
        run(""ruff"", ""check"", ""."", ""--exit-zero"")
        run(""isort"", ""--diff"", ""--check"", ""--profile"", ""black"", ""."")
        run(""black"", ""--diff"", ""--check"", ""."")
        run(""pyright"")
    except subprocess.CalledProcessError as e:
        print(""Lint failed, try running `poetry run format` to fix the issues: "", e)
        raise e","Point(row=11, column=0)","Point(row=19, column=15)",,autogpt_platform/market/scripts.py
populate_database,function,,"def populate_database():
    import glob
    import json
    import pathlib

    import requests

    import market.model

    templates = pathlib.Path(__file__).parent.parent / ""graph_templates""

    all_files = glob.glob(str(templates / ""*.json""))

    for file in all_files:
        with open(file, ""r"") as f:
            data = f.read()
            req = market.model.AddAgentRequest(
                graph=json.loads(data),
                author=""Populate DB"",
                categories=[""Pre-Populated""],
                keywords=[""test""],
            )
            response = requests.post(
                ""http://localhost:8015/api/v1/market/admin/agent"", json=req.model_dump()
            )
            print(response.text)","Point(row=22, column=0)","Point(row=47, column=32)",,autogpt_platform/market/scripts.py
format,function,,"def format():
    run(""ruff"", ""check"", ""--fix"", ""."")
    run(""isort"", ""--profile"", ""black"", ""."")
    run(""black"", ""."")
    run(""pyright"", ""."")","Point(row=50, column=0)","Point(row=54, column=23)",,autogpt_platform/market/scripts.py
app,function,,"def app():
    port = os.getenv(""PORT"", ""8015"")
    run(""uvicorn"", ""market.app:app"", ""--reload"", ""--port"", port, ""--host"", ""0.0.0.0"")","Point(row=57, column=0)","Point(row=59, column=85)",,autogpt_platform/market/scripts.py
setup,function,,"def setup():
    run(""prisma"", ""generate"")
    run(""prisma"", ""migrate"", ""deploy"")","Point(row=62, column=0)","Point(row=64, column=38)",,autogpt_platform/market/scripts.py
test_client,function,,"def test_client():
    return TestClient(app)","Point(row=10, column=0)","Point(row=11, column=26)",,autogpt_platform/market/tests/test_agents.py
test_list_agents,function,,"async def test_list_agents(test_client):
    response = test_client.get(""/agents"")
    assert response.status_code == 200
    data = response.json()
    assert len(data[""agents""]) == 2
    assert data[""total_count""] == 2","Point(row=47, column=0)","Point(row=52, column=35)",,autogpt_platform/market/tests/test_agents.py
test_list_agents_with_filters,function,,"async def test_list_agents_with_filters(test_client):
    response = await test_client.get(""/agents?name=Agent 1&keyword=AI&category=general"")
    assert response.status_code == 200
    data = response.json()
    assert len(data[""agents""]) == 1
    assert data[""agents""][0][""name""] == ""Agent 1""","Point(row=56, column=0)","Point(row=61, column=49)",,autogpt_platform/market/tests/test_agents.py
test_get_agent_details,function,,"async def test_get_agent_details(test_client, mock_get_agent_details):
    response = await test_client.get(""/agents/1"")
    assert response.status_code == 200
    data = response.json()
    assert data[""id""] == ""1""
    assert data[""name""] == ""Agent 1""
    assert ""graph"" in data","Point(row=65, column=0)","Point(row=71, column=26)",,autogpt_platform/market/tests/test_agents.py
test_get_nonexistent_agent,function,,"async def test_get_nonexistent_agent(test_client, mock_get_agent_details):
    mock_get_agent_details.side_effect = AgentQueryError(""Agent not found"")
    response = await test_client.get(""/agents/999"")
    assert response.status_code == 404","Point(row=75, column=0)","Point(row=78, column=38)",,autogpt_platform/market/tests/test_agents.py
AgentQueryError,class,Custom exception for agent query errors,"class AgentQueryError(Exception):
    """"""Custom exception for agent query errors""""""

    pass","Point(row=14, column=0)","Point(row=17, column=8)",,autogpt_platform/market/market/db.py
TopAgentsDBResponse,class,"
    Represents a response containing a list of top agents.

    Attributes:
        analytics (list[AgentResponse]): The list of top agents.
        total_count (int): The total count of agents.
        page (int): The current page number.
        page_size (int): The number of agents per page.
        total_pages (int): The total number of pages.
","class TopAgentsDBResponse(pydantic.BaseModel):
    """"""
    Represents a response containing a list of top agents.

    Attributes:
        analytics (list[AgentResponse]): The list of top agents.
        total_count (int): The total count of agents.
        page (int): The current page number.
        page_size (int): The number of agents per page.
        total_pages (int): The total number of pages.
    """"""

    analytics: list[prisma.models.AnalyticsTracker]
    total_count: int
    page: int
    page_size: int
    total_pages: int","Point(row=20, column=0)","Point(row=36, column=20)",,autogpt_platform/market/market/db.py
FeaturedAgentResponse,class,"
    Represents a response containing a list of featured agents.

    Attributes:
        featured_agents (list[FeaturedAgent]): The list of featured agents.
        total_count (int): The total count of featured agents.
        page (int): The current page number.
        page_size (int): The number of agents per page.
        total_pages (int): The total number of pages.
","class FeaturedAgentResponse(pydantic.BaseModel):
    """"""
    Represents a response containing a list of featured agents.

    Attributes:
        featured_agents (list[FeaturedAgent]): The list of featured agents.
        total_count (int): The total count of featured agents.
        page (int): The current page number.
        page_size (int): The number of agents per page.
        total_pages (int): The total number of pages.
    """"""

    featured_agents: list[prisma.models.FeaturedAgent]
    total_count: int
    page: int
    page_size: int
    total_pages: int","Point(row=39, column=0)","Point(row=55, column=20)",,autogpt_platform/market/market/db.py
delete_agent,function,"
    Delete an agent from the database.

    Args:
        agent_id (str): The ID of the agent to delete.

    Returns:
        prisma.models.Agents | None: The deleted agent if found, None otherwise.

    Raises:
        AgentQueryError: If there is an error deleting the agent from the database.
","async def delete_agent(agent_id: str) -> prisma.models.Agents | None:
    """"""
    Delete an agent from the database.

    Args:
        agent_id (str): The ID of the agent to delete.

    Returns:
        prisma.models.Agents | None: The deleted agent if found, None otherwise.

    Raises:
        AgentQueryError: If there is an error deleting the agent from the database.
    """"""
    try:
        deleted_agent = await prisma.models.Agents.prisma().delete(
            where={""id"": agent_id}
        )
        return deleted_agent
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")","Point(row=58, column=0)","Point(row=79, column=69)",,autogpt_platform/market/market/db.py
create_agent_entry,function,"
    Create a new agent entry in the database.

    Args:
        name (str): The name of the agent.
        description (str): The description of the agent.
        author (str): The author of the agent.
        keywords (List[str]): The keywords associated with the agent.
        categories (List[str]): The categories associated with the agent.
        graph (dict): The graph data of the agent.

    Returns:
        dict: The newly created agent entry.

    Raises:
        AgentQueryError: If there is an error creating the agent entry.
","async def create_agent_entry(
    name: str,
    description: str,
    author: str,
    keywords: typing.List[str],
    categories: typing.List[str],
    graph: prisma.Json,
    submission_state: prisma.enums.SubmissionStatus = prisma.enums.SubmissionStatus.PENDING,
):
    """"""
    Create a new agent entry in the database.

    Args:
        name (str): The name of the agent.
        description (str): The description of the agent.
        author (str): The author of the agent.
        keywords (List[str]): The keywords associated with the agent.
        categories (List[str]): The categories associated with the agent.
        graph (dict): The graph data of the agent.

    Returns:
        dict: The newly created agent entry.

    Raises:
        AgentQueryError: If there is an error creating the agent entry.
    """"""
    try:
        agent = await prisma.models.Agents.prisma().create(
            data={
                ""name"": name,
                ""description"": description,
                ""author"": author,
                ""keywords"": keywords,
                ""categories"": categories,
                ""graph"": graph,
                ""AnalyticsTracker"": {""create"": {""downloads"": 0, ""views"": 0}},
                ""submissionStatus"": submission_state,
            }
        )

        return agent

    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")","Point(row=82, column=0)","Point(row=127, column=69)",,autogpt_platform/market/market/db.py
update_agent_entry,function,"
    Update an existing agent entry in the database.

    Args:
        agent_id (str): The ID of the agent.
        version (int): The version of the agent.
        submission_state (prisma.enums.SubmissionStatus): The submission state of the agent.
","async def update_agent_entry(
    agent_id: str,
    version: int,
    submission_state: prisma.enums.SubmissionStatus,
    comments: str | None = None,
) -> prisma.models.Agents | None:
    """"""
    Update an existing agent entry in the database.

    Args:
        agent_id (str): The ID of the agent.
        version (int): The version of the agent.
        submission_state (prisma.enums.SubmissionStatus): The submission state of the agent.
    """"""

    try:
        agent = await prisma.models.Agents.prisma().update(
            where={""id"": agent_id},
            data={
                ""version"": version,
                ""submissionStatus"": submission_state,
                ""submissionReviewDate"": datetime.datetime.now(datetime.timezone.utc),
                ""submissionReviewComments"": comments,
            },
        )

        return agent
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Agent Update Failed Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")","Point(row=130, column=0)","Point(row=160, column=69)",,autogpt_platform/market/market/db.py
get_agents,function,"
    Retrieve a list of agents from the database based on the provided filters and pagination parameters.

    Args:
        page (int, optional): The page number to retrieve. Defaults to 1.
        page_size (int, optional): The number of agents per page. Defaults to 10.
        name (str, optional): Filter agents by name. Defaults to None.
        keyword (str, optional): Filter agents by keyword. Defaults to None.
        category (str, optional): Filter agents by category. Defaults to None.
        description (str, optional): Filter agents by description. Defaults to None.
        description_threshold (int, optional): The minimum fuzzy search threshold for the description. Defaults to 60.
        sort_by (str, optional): The field to sort the agents by. Defaults to ""createdAt"".
        sort_order (str, optional): The sort order (""asc"" or ""desc""). Defaults to ""desc"".

    Returns:
        dict: A dictionary containing the list of agents, total count, current page number, page size, and total number of pages.
","async def get_agents(
    page: int = 1,
    page_size: int = 10,
    name: str | None = None,
    keyword: str | None = None,
    category: str | None = None,
    description: str | None = None,
    description_threshold: int = 60,
    submission_status: prisma.enums.SubmissionStatus = prisma.enums.SubmissionStatus.APPROVED,
    sort_by: str = ""createdAt"",
    sort_order: typing.Literal[""desc""] | typing.Literal[""asc""] = ""desc"",
):
    """"""
    Retrieve a list of agents from the database based on the provided filters and pagination parameters.

    Args:
        page (int, optional): The page number to retrieve. Defaults to 1.
        page_size (int, optional): The number of agents per page. Defaults to 10.
        name (str, optional): Filter agents by name. Defaults to None.
        keyword (str, optional): Filter agents by keyword. Defaults to None.
        category (str, optional): Filter agents by category. Defaults to None.
        description (str, optional): Filter agents by description. Defaults to None.
        description_threshold (int, optional): The minimum fuzzy search threshold for the description. Defaults to 60.
        sort_by (str, optional): The field to sort the agents by. Defaults to ""createdAt"".
        sort_order (str, optional): The sort order (""asc"" or ""desc""). Defaults to ""desc"".

    Returns:
        dict: A dictionary containing the list of agents, total count, current page number, page size, and total number of pages.
    """"""
    try:
        # Define the base query
        query = {}

        # Add optional filters
        if name:
            query[""name""] = {""contains"": name, ""mode"": ""insensitive""}
        if keyword:
            query[""keywords""] = {""has"": keyword}
        if category:
            query[""categories""] = {""has"": category}

        query[""submissionStatus""] = submission_status

        # Define sorting
        order = {sort_by: sort_order}

        # Calculate pagination
        skip = (page - 1) * page_size

        # Execute the query
        try:
            agents = await prisma.models.Agents.prisma().find_many(
                where=query,  # type: ignore
                order=order,  # type: ignore
                skip=skip,
                take=page_size,
            )
        except prisma.errors.PrismaError as e:
            raise AgentQueryError(f""Database query failed: {str(e)}"")

        # Apply fuzzy search on description if provided
        if description:
            try:
                filtered_agents = []
                for agent in agents:
                    if (
                        agent.description
                        and fuzzywuzzy.fuzz.partial_ratio(
                            description.lower(), agent.description.lower()
                        )
                        >= description_threshold
                    ):
                        filtered_agents.append(agent)
                agents = filtered_agents
            except AttributeError as e:
                raise AgentQueryError(f""Error during fuzzy search: {str(e)}"")

        # Get total count for pagination info
        total_count = len(agents)

        return {
            ""agents"": agents,
            ""total_count"": total_count,
            ""page"": page,
            ""page_size"": page_size,
            ""total_pages"": (total_count + page_size - 1) // page_size,
        }

    except AgentQueryError as e:
        # Log the error or handle it as needed
        raise e
    except ValueError as e:
        raise AgentQueryError(f""Invalid input parameter: {str(e)}"")
    except Exception as e:
        # Catch any other unexpected exceptions
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")","Point(row=163, column=0)","Point(row=258, column=69)",,autogpt_platform/market/market/db.py
get_agent_details,function,"
    Retrieve agent details from the database.

    Args:
        agent_id (str): The ID of the agent.
        version (int | None, optional): The version of the agent. Defaults to None.

    Returns:
        dict: The agent details.

    Raises:
        AgentQueryError: If the agent is not found or if there is an error querying the database.
","async def get_agent_details(agent_id: str, version: int | None = None):
    """"""
    Retrieve agent details from the database.

    Args:
        agent_id (str): The ID of the agent.
        version (int | None, optional): The version of the agent. Defaults to None.

    Returns:
        dict: The agent details.

    Raises:
        AgentQueryError: If the agent is not found or if there is an error querying the database.
    """"""
    try:
        query = {""id"": agent_id}
        if version is not None:
            query[""version""] = version  # type: ignore

        agent = await prisma.models.Agents.prisma().find_first(where=query)  # type: ignore

        if not agent:
            raise AgentQueryError(""Agent not found"")

        return agent

    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")","Point(row=261, column=0)","Point(row=290, column=69)",,autogpt_platform/market/market/db.py
search_db,function,"Perform a search for agents based on the provided query string.

    Args:
        query (str): the search string
        page (int, optional): page for searching. Defaults to 1.
        page_size (int, optional): the number of results to return. Defaults to 10.
        categories (List[str] | None, optional): list of category filters. Defaults to None.
        description_threshold (int, optional): number of characters to return. Defaults to 60.
        sort_by (str, optional): sort by option. Defaults to ""rank"".
        sort_order (""asc"" | ""desc"", optional): the sort order. Defaults to ""desc"".

    Raises:
        AgentQueryError: Raises an error if the query fails.
        AgentQueryError: Raises if an unexpected error occurs.

    Returns:
        List[AgentsWithRank]: List of agents matching the search criteria.
","async def search_db(
    query: str,
    page: int = 1,
    page_size: int = 10,
    categories: typing.List[str] | None = None,
    description_threshold: int = 60,
    sort_by: str = ""rank"",
    sort_order: typing.Literal[""desc""] | typing.Literal[""asc""] = ""desc"",
    submission_status: prisma.enums.SubmissionStatus = prisma.enums.SubmissionStatus.APPROVED,
) -> market.model.ListResponse[market.utils.extension_types.AgentsWithRank]:
    """"""Perform a search for agents based on the provided query string.

    Args:
        query (str): the search string
        page (int, optional): page for searching. Defaults to 1.
        page_size (int, optional): the number of results to return. Defaults to 10.
        categories (List[str] | None, optional): list of category filters. Defaults to None.
        description_threshold (int, optional): number of characters to return. Defaults to 60.
        sort_by (str, optional): sort by option. Defaults to ""rank"".
        sort_order (""asc"" | ""desc"", optional): the sort order. Defaults to ""desc"".

    Raises:
        AgentQueryError: Raises an error if the query fails.
        AgentQueryError: Raises if an unexpected error occurs.

    Returns:
        List[AgentsWithRank]: List of agents matching the search criteria.
    """"""
    try:
        offset = (page - 1) * page_size

        category_filter = ""1=1""
        if categories:
            category_conditions = [f""'{cat}' = ANY(categories)"" for cat in categories]
            category_filter = ""AND ("" + "" OR "".join(category_conditions) + "")""

        # Construct the ORDER BY clause based on the sort_by parameter
        if sort_by in [""createdAt"", ""updatedAt""]:
            order_by_clause = f'""{sort_by}"" {sort_order.upper()}, rank DESC'
        elif sort_by == ""name"":
            order_by_clause = f""name {sort_order.upper()}, rank DESC""
        else:
            order_by_clause = 'rank DESC, ""createdAt"" DESC'

        submission_status_filter = f""""""""submissionStatus"" = '{submission_status}'""""""

        sql_query = f""""""
        WITH query AS (
            SELECT to_tsquery(string_agg(lexeme || ':*', ' & ' ORDER BY positions)) AS q 
            FROM unnest(to_tsvector('{query}'))
        )
        SELECT 
            id, 
            ""createdAt"", 
            ""updatedAt"", 
            version, 
            name, 
            LEFT(description, {description_threshold}) AS description, 
            author, 
            keywords, 
            categories, 
            graph,
            ""submissionStatus"",
            ""submissionDate"",
            CASE 
                WHEN query.q::text = '' THEN 1.0
                ELSE COALESCE(ts_rank(CAST(search AS tsvector), query.q), 0.0)
            END AS rank
        FROM market.""Agents"", query
        WHERE 
            (query.q::text = '' OR search @@ query.q)
            AND {category_filter} 
            AND {submission_status_filter}
        ORDER BY {order_by_clause}
        LIMIT {page_size}
        OFFSET {offset};
        """"""

        results = await prisma.client.get_client().query_raw(
            query=sql_query,
            model=market.utils.extension_types.AgentsWithRank,
        )

        class CountResponse(pydantic.BaseModel):
            count: int

        count_query = f""""""
        WITH query AS (
            SELECT to_tsquery(string_agg(lexeme || ':*', ' & ' ORDER BY positions)) AS q 
            FROM unnest(to_tsvector('{query}'))
        )
        SELECT COUNT(*)
        FROM market.""Agents"", query
        WHERE (search @@ query.q OR query.q = '') AND {category_filter} AND {submission_status_filter};
        """"""

        total_count = await prisma.client.get_client().query_first(
            query=count_query,
            model=CountResponse,
        )
        total_count = total_count.count if total_count else 0

        return market.model.ListResponse(
            items=results,
            total_count=total_count,
            page=page,
            page_size=page_size,
            total_pages=(total_count + page_size - 1) // page_size,
        )

    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")","Point(row=293, column=0)","Point(row=406, column=69)",,autogpt_platform/market/market/db.py
CountResponse,class,,"class CountResponse(pydantic.BaseModel):
            count: int","Point(row=376, column=8)","Point(row=377, column=22)",,autogpt_platform/market/market/db.py
get_top_agents_by_downloads,function,"Retrieve the top agents by download count.

    Args:
        page (int, optional): The page number. Defaults to 1.
        page_size (int, optional): The number of agents per page. Defaults to 10.

    Returns:
        dict: A dictionary containing the list of agents, total count, current page number, page size, and total number of pages.
","async def get_top_agents_by_downloads(
    page: int = 1,
    page_size: int = 10,
    submission_status: prisma.enums.SubmissionStatus = prisma.enums.SubmissionStatus.APPROVED,
) -> market.model.ListResponse[prisma.models.AnalyticsTracker]:
    """"""Retrieve the top agents by download count.

    Args:
        page (int, optional): The page number. Defaults to 1.
        page_size (int, optional): The number of agents per page. Defaults to 10.

    Returns:
        dict: A dictionary containing the list of agents, total count, current page number, page size, and total number of pages.
    """"""
    try:
        # Calculate pagination
        skip = (page - 1) * page_size

        # Execute the query
        try:
            # Agents with no downloads will not be included in the results... is this the desired behavior?
            analytics = await prisma.models.AnalyticsTracker.prisma().find_many(
                include={""agent"": True},
                order={""downloads"": ""desc""},
                where={""agent"": {""is"": {""submissionStatus"": submission_status}}},
                skip=skip,
                take=page_size,
            )
        except prisma.errors.PrismaError as e:
            raise AgentQueryError(f""Database query failed: {str(e)}"")

        try:
            total_count = await prisma.models.AnalyticsTracker.prisma().count(
                where={""agent"": {""is"": {""submissionStatus"": submission_status}}},
            )
        except prisma.errors.PrismaError as e:
            raise AgentQueryError(f""Database query failed: {str(e)}"")

        return market.model.ListResponse(
            items=analytics,
            total_count=total_count,
            page=page,
            page_size=page_size,
            total_pages=(total_count + page_size - 1) // page_size,
        )

    except AgentQueryError as e:
        # Log the error or handle it as needed
        raise e from e
    except ValueError as e:
        raise AgentQueryError(f""Invalid input parameter: {str(e)}"") from e
    except Exception as e:
        # Catch any other unexpected exceptions
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"") from e","Point(row=409, column=0)","Point(row=462, column=76)",,autogpt_platform/market/market/db.py
set_agent_featured,function,"Set an agent as featured in the database.

    Args:
        agent_id (str): The ID of the agent.
        category (str, optional): The category to set the agent as featured. Defaults to ""featured"".

    Raises:
        AgentQueryError: If there is an error setting the agent as featured.
","async def set_agent_featured(
    agent_id: str, is_active: bool = True, featured_categories: list[str] = [""featured""]
) -> prisma.models.FeaturedAgent:
    """"""Set an agent as featured in the database.

    Args:
        agent_id (str): The ID of the agent.
        category (str, optional): The category to set the agent as featured. Defaults to ""featured"".

    Raises:
        AgentQueryError: If there is an error setting the agent as featured.
    """"""
    try:
        agent = await prisma.models.Agents.prisma().find_unique(where={""id"": agent_id})
        if not agent:
            raise AgentQueryError(f""Agent with ID {agent_id} not found."")

        featured = await prisma.models.FeaturedAgent.prisma().upsert(
            where={""agentId"": agent_id},
            data={
                ""update"": {
                    ""featuredCategories"": featured_categories,
                    ""isActive"": is_active,
                },
                ""create"": {
                    ""featuredCategories"": featured_categories,
                    ""isActive"": is_active,
                    ""agent"": {""connect"": {""id"": agent_id}},
                },
            },
        )
        return featured

    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")","Point(row=465, column=0)","Point(row=501, column=69)",,autogpt_platform/market/market/db.py
get_featured_agents,function,"Retrieve a list of featured agents from the database based on the provided category.

    Args:
        category (str, optional): The category of featured agents to retrieve. Defaults to ""featured"".
        page (int, optional): The page number to retrieve. Defaults to 1.
        page_size (int, optional): The number of agents per page. Defaults to 10.

    Returns:
        dict: A dictionary containing the list of featured agents, total count, current page number, page size, and total number of pages.
","async def get_featured_agents(
    category: str = ""featured"",
    page: int = 1,
    page_size: int = 10,
    submission_status: prisma.enums.SubmissionStatus = prisma.enums.SubmissionStatus.APPROVED,
) -> FeaturedAgentResponse:
    """"""Retrieve a list of featured agents from the database based on the provided category.

    Args:
        category (str, optional): The category of featured agents to retrieve. Defaults to ""featured"".
        page (int, optional): The page number to retrieve. Defaults to 1.
        page_size (int, optional): The number of agents per page. Defaults to 10.

    Returns:
        dict: A dictionary containing the list of featured agents, total count, current page number, page size, and total number of pages.
    """"""
    try:
        # Calculate pagination
        skip = (page - 1) * page_size

        # Execute the query
        try:
            featured_agents = await prisma.models.FeaturedAgent.prisma().find_many(
                where={
                    ""featuredCategories"": {""has"": category},
                    ""isActive"": True,
                    ""agent"": {""is"": {""submissionStatus"": submission_status}},
                },
                include={""agent"": {""include"": {""AnalyticsTracker"": True}}},
                skip=skip,
                take=page_size,
            )
        except prisma.errors.PrismaError as e:
            raise AgentQueryError(f""Database query failed: {str(e)}"")

        # Get total count for pagination info
        total_count = len(featured_agents)

        return FeaturedAgentResponse(
            featured_agents=featured_agents,
            total_count=total_count,
            page=page,
            page_size=page_size,
            total_pages=(total_count + page_size - 1) // page_size,
        )

    except AgentQueryError as e:
        # Log the error or handle it as needed
        raise e from e
    except ValueError as e:
        raise AgentQueryError(f""Invalid input parameter: {str(e)}"") from e
    except Exception as e:
        # Catch any other unexpected exceptions
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"") from e","Point(row=504, column=0)","Point(row=557, column=76)",,autogpt_platform/market/market/db.py
remove_featured_category,function,"Adds a featured category to an agent.

    Args:
        agent_id (str): The ID of the agent.
        category (str): The category to add to the agent.

    Returns:
        FeaturedAgentResponse: The updated list of featured agents.
","async def remove_featured_category(
    agent_id: str, category: str
) -> prisma.models.FeaturedAgent | None:
    """"""Adds a featured category to an agent.

    Args:
        agent_id (str): The ID of the agent.
        category (str): The category to add to the agent.

    Returns:
        FeaturedAgentResponse: The updated list of featured agents.
    """"""
    try:
        # get the existing categories
        featured_agent = await prisma.models.FeaturedAgent.prisma().find_unique(
            where={""agentId"": agent_id},
            include={""agent"": True},
        )

        if not featured_agent:
            raise AgentQueryError(f""Agent with ID {agent_id} not found."")

        # remove the category from the list
        featured_agent.featuredCategories.remove(category)

        featured_agent = await prisma.models.FeaturedAgent.prisma().update(
            where={""agentId"": agent_id},
            data={""featuredCategories"": featured_agent.featuredCategories},
        )

        return featured_agent

    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")","Point(row=560, column=0)","Point(row=595, column=69)",,autogpt_platform/market/market/db.py
add_featured_category,function,"Removes a featured category from an agent.

    Args:
        agent_id (str): The ID of the agent.
        category (str): The category to remove from the agent.

    Returns:
        FeaturedAgentResponse: The updated list of featured agents.
","async def add_featured_category(
    agent_id: str, category: str
) -> prisma.models.FeaturedAgent | None:
    """"""Removes a featured category from an agent.

    Args:
        agent_id (str): The ID of the agent.
        category (str): The category to remove from the agent.

    Returns:
        FeaturedAgentResponse: The updated list of featured agents.
    """"""
    try:
        featured_agent = await prisma.models.FeaturedAgent.prisma().update(
            where={""agentId"": agent_id},
            data={""featuredCategories"": {""push"": [category]}},
        )

        return featured_agent

    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")","Point(row=598, column=0)","Point(row=621, column=69)",,autogpt_platform/market/market/db.py
get_agent_featured,function,"Retrieve an agent's featured categories from the database.

    Args:
        agent_id (str): The ID of the agent.

    Returns:
        FeaturedAgentResponse: The list of featured agents.
","async def get_agent_featured(agent_id: str) -> prisma.models.FeaturedAgent | None:
    """"""Retrieve an agent's featured categories from the database.

    Args:
        agent_id (str): The ID of the agent.

    Returns:
        FeaturedAgentResponse: The list of featured agents.
    """"""
    try:
        featured_agent = await prisma.models.FeaturedAgent.prisma().find_unique(
            where={""agentId"": agent_id},
        )
        return featured_agent
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")","Point(row=624, column=0)","Point(row=641, column=69)",,autogpt_platform/market/market/db.py
get_not_featured_agents,function,"
    Retrieve a list of not featured agents from the database.
","async def get_not_featured_agents(
    page: int = 1, page_size: int = 10
) -> typing.List[prisma.models.Agents]:
    """"""
    Retrieve a list of not featured agents from the database.
    """"""
    try:
        agents = await prisma.client.get_client().query_raw(
            query=f""""""
            SELECT 
                ""market"".""Agents"".id, 
                ""market"".""Agents"".""createdAt"", 
                ""market"".""Agents"".""updatedAt"", 
                ""market"".""Agents"".version, 
                ""market"".""Agents"".name, 
                LEFT(""market"".""Agents"".description, 500) AS description, 
                ""market"".""Agents"".author, 
                ""market"".""Agents"".keywords, 
                ""market"".""Agents"".categories, 
                ""market"".""Agents"".graph,
                ""market"".""Agents"".""submissionStatus"",
                ""market"".""Agents"".""submissionDate"",
                ""market"".""Agents"".search::text AS search
            FROM ""market"".""Agents""
            LEFT JOIN ""market"".""FeaturedAgent"" ON ""market"".""Agents"".""id"" = ""market"".""FeaturedAgent"".""agentId""
            WHERE (""market"".""FeaturedAgent"".""agentId"" IS NULL OR ""market"".""FeaturedAgent"".""featuredCategories"" = '{{}}')
                AND ""market"".""Agents"".""submissionStatus"" = 'APPROVED'
            ORDER BY ""market"".""Agents"".""createdAt"" DESC
            LIMIT {page_size} OFFSET {page_size * (page - 1)}
            """""",
            model=prisma.models.Agents,
        )
        return agents
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")","Point(row=644, column=0)","Point(row=680, column=69)",,autogpt_platform/market/market/db.py
get_all_categories,function,"
    Retrieve all unique categories from the database.

    Returns:
        CategoriesResponse: A list of unique categories.
","async def get_all_categories() -> market.model.CategoriesResponse:
    """"""
    Retrieve all unique categories from the database.

    Returns:
        CategoriesResponse: A list of unique categories.
    """"""
    try:
        agents = await prisma.models.Agents.prisma().find_many(distinct=[""categories""])

        # Aggregate categories on the Python side
        all_categories = set()
        for agent in agents:
            all_categories.update(agent.categories)

        unique_categories = sorted(list(all_categories))

        return market.model.CategoriesResponse(unique_categories=unique_categories)
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception:
        # Return an empty list of categories in case of unexpected errors
        return market.model.CategoriesResponse(unique_categories=[])","Point(row=683, column=0)","Point(row=705, column=68)",,autogpt_platform/market/market/db.py
create_agent_installed_event,function,,"async def create_agent_installed_event(
    event_data: market.model.AgentInstalledFromMarketplaceEventData,
):
    try:
        await prisma.models.InstallTracker.prisma().create(
            data={
                ""installedAgentId"": event_data.installed_agent_id,
                ""marketplaceAgentId"": event_data.marketplace_agent_id,
                ""installationLocation"": prisma.enums.InstallationLocation(
                    event_data.installation_location.name
                ),
            }
        )
    except prisma.errors.PrismaError as e:
        raise AgentQueryError(f""Database query failed: {str(e)}"")
    except Exception as e:
        raise AgentQueryError(f""Unexpected error occurred: {str(e)}"")","Point(row=708, column=0)","Point(row=724, column=69)",,autogpt_platform/market/market/db.py
LogConfig,class,Logging configuration to be set for the server,"class LogConfig(BaseModel):
    """"""Logging configuration to be set for the server""""""

    LOGGER_NAME: str = ""marketplace""
    LOG_FORMAT: str = ""%(levelprefix)s | %(asctime)s | %(message)s""
    LOG_LEVEL: str = ""DEBUG""

    # Logging config
    version: int = 1
    disable_existing_loggers: bool = False
    formatters: dict = {
        ""default"": {
            ""()"": ""uvicorn.logging.DefaultFormatter"",
            ""fmt"": LOG_FORMAT,
            ""datefmt"": ""%Y-%m-%d %H:%M:%S"",
        },
    }
    handlers: dict = {
        ""default"": {
            ""formatter"": ""default"",
            ""class"": ""logging.StreamHandler"",
            ""stream"": ""ext://sys.stderr"",
        },
    }
    loggers: dict = {
        LOGGER_NAME: {""handlers"": [""default""], ""level"": LOG_LEVEL},
    }","Point(row=3, column=0)","Point(row=29, column=5)",,autogpt_platform/market/market/config.py
InstallationLocation,class,,"class InstallationLocation(str, Enum):
    LOCAL = ""local""
    CLOUD = ""cloud""","Point(row=9, column=0)","Point(row=11, column=19)",,autogpt_platform/market/market/model.py
AgentInstalledFromMarketplaceEventData,class,,"class AgentInstalledFromMarketplaceEventData(pydantic.BaseModel):
    marketplace_agent_id: str
    installed_agent_id: str
    installation_location: InstallationLocation","Point(row=14, column=0)","Point(row=17, column=47)",,autogpt_platform/market/market/model.py
AgentInstalledFromTemplateEventData,class,,"class AgentInstalledFromTemplateEventData(pydantic.BaseModel):
    template_id: str
    installed_agent_id: str
    installation_location: InstallationLocation","Point(row=20, column=0)","Point(row=23, column=47)",,autogpt_platform/market/market/model.py
AgentInstalledFromMarketplaceEvent,class,,"class AgentInstalledFromMarketplaceEvent(pydantic.BaseModel):
    event_name: Literal[""agent_installed_from_marketplace""]
    event_data: AgentInstalledFromMarketplaceEventData","Point(row=26, column=0)","Point(row=28, column=54)",,autogpt_platform/market/market/model.py
AgentInstalledFromTemplateEvent,class,,"class AgentInstalledFromTemplateEvent(pydantic.BaseModel):
    event_name: Literal[""agent_installed_from_template""]
    event_data: AgentInstalledFromTemplateEventData","Point(row=31, column=0)","Point(row=33, column=51)",,autogpt_platform/market/market/model.py
AnalyticsRequest,class,,"class AnalyticsRequest(pydantic.BaseModel):
    event: AnalyticsEvent","Point(row=41, column=0)","Point(row=42, column=25)",,autogpt_platform/market/market/model.py
AddAgentRequest,class,,"class AddAgentRequest(pydantic.BaseModel):
    graph: dict[str, typing.Any]
    author: str
    keywords: list[str]
    categories: list[str]","Point(row=45, column=0)","Point(row=49, column=25)",,autogpt_platform/market/market/model.py
SubmissionReviewRequest,class,,"class SubmissionReviewRequest(pydantic.BaseModel):
    agent_id: str
    version: int
    status: prisma.enums.SubmissionStatus
    comments: str | None","Point(row=52, column=0)","Point(row=56, column=24)",,autogpt_platform/market/market/model.py
AgentResponse,class,"
    Represents a response from an agent.

    Attributes:
        id (str): The ID of the agent.
        name (str, optional): The name of the agent.
        description (str, optional): The description of the agent.
        author (str, optional): The author of the agent.
        keywords (list[str]): The keywords associated with the agent.
        categories (list[str]): The categories the agent belongs to.
        version (int): The version of the agent.
        createdAt (str): The creation date of the agent.
        updatedAt (str): The last update date of the agent.
","class AgentResponse(pydantic.BaseModel):
    """"""
    Represents a response from an agent.

    Attributes:
        id (str): The ID of the agent.
        name (str, optional): The name of the agent.
        description (str, optional): The description of the agent.
        author (str, optional): The author of the agent.
        keywords (list[str]): The keywords associated with the agent.
        categories (list[str]): The categories the agent belongs to.
        version (int): The version of the agent.
        createdAt (str): The creation date of the agent.
        updatedAt (str): The last update date of the agent.
    """"""

    id: str
    name: typing.Optional[str]
    description: typing.Optional[str]
    author: typing.Optional[str]
    keywords: list[str]
    categories: list[str]
    version: int
    createdAt: datetime.datetime
    updatedAt: datetime.datetime
    submissionStatus: str
    views: int = 0
    downloads: int = 0","Point(row=59, column=0)","Point(row=86, column=22)",,autogpt_platform/market/market/model.py
AgentDetailResponse,class,"
    Represents the response data for an agent detail.

    Attributes:
        id (str): The ID of the agent.
        name (Optional[str]): The name of the agent.
        description (Optional[str]): The description of the agent.
        author (Optional[str]): The author of the agent.
        keywords (List[str]): The keywords associated with the agent.
        categories (List[str]): The categories the agent belongs to.
        version (int): The version of the agent.
        createdAt (str): The creation date of the agent.
        updatedAt (str): The last update date of the agent.
        graph (Dict[str, Any]): The graph data of the agent.
","class AgentDetailResponse(pydantic.BaseModel):
    """"""
    Represents the response data for an agent detail.

    Attributes:
        id (str): The ID of the agent.
        name (Optional[str]): The name of the agent.
        description (Optional[str]): The description of the agent.
        author (Optional[str]): The author of the agent.
        keywords (List[str]): The keywords associated with the agent.
        categories (List[str]): The categories the agent belongs to.
        version (int): The version of the agent.
        createdAt (str): The creation date of the agent.
        updatedAt (str): The last update date of the agent.
        graph (Dict[str, Any]): The graph data of the agent.
    """"""

    id: str
    name: typing.Optional[str]
    description: typing.Optional[str]
    author: typing.Optional[str]
    keywords: list[str]
    categories: list[str]
    version: int
    createdAt: datetime.datetime
    updatedAt: datetime.datetime
    graph: dict[str, typing.Any]","Point(row=89, column=0)","Point(row=115, column=32)",,autogpt_platform/market/market/model.py
FeaturedAgentResponse,class,"
    Represents the response data for an agent detail.
","class FeaturedAgentResponse(pydantic.BaseModel):
    """"""
    Represents the response data for an agent detail.
    """"""

    agentId: str
    featuredCategories: list[str]
    createdAt: datetime.datetime
    updatedAt: datetime.datetime
    isActive: bool","Point(row=118, column=0)","Point(row=127, column=18)",,autogpt_platform/market/market/model.py
CategoriesResponse,class,"
    Represents the response data for a list of categories.

    Attributes:
        unique_categories (list[str]): The list of unique categories.
","class CategoriesResponse(pydantic.BaseModel):
    """"""
    Represents the response data for a list of categories.

    Attributes:
        unique_categories (list[str]): The list of unique categories.
    """"""

    unique_categories: list[str]","Point(row=130, column=0)","Point(row=138, column=32)",,autogpt_platform/market/market/model.py
ListResponse,class,"
    Represents a list response.

    Attributes:
        items (list[T]): The list of items.
        total_count (int): The total count of items.
        page (int): The current page number.
        page_size (int): The number of items per page.
        total_pages (int): The total number of pages.
","class ListResponse(pydantic.BaseModel, Generic[T]):
    """"""
    Represents a list response.

    Attributes:
        items (list[T]): The list of items.
        total_count (int): The total count of items.
        page (int): The current page number.
        page_size (int): The number of items per page.
        total_pages (int): The total number of pages.
    """"""

    items: list[T]
    total_count: int
    page: int
    page_size: int
    total_pages: int","Point(row=144, column=0)","Point(row=160, column=20)",,autogpt_platform/market/market/model.py
lifespan,function,,"async def lifespan(app: fastapi.FastAPI):
    await db_client.connect()
    yield
    await db_client.disconnect()","Point(row=46, column=0)","Point(row=49, column=32)",,autogpt_platform/market/market/app.py
health,function,,"def health():
    return fastapi.responses.HTMLResponse(
        content=""<h1>Marketplace API</h1>"", status_code=200
    )","Point(row=83, column=0)","Point(row=86, column=5)",,autogpt_platform/market/market/app.py
default,function,,"def default():
    return fastapi.responses.HTMLResponse(
        content=""<h1>Marketplace API</h1>"", status_code=200
    )","Point(row=90, column=0)","Point(row=93, column=5)",,autogpt_platform/market/market/app.py
AgentsWithRank,class,,"class AgentsWithRank(prisma.models.Agents):
    rank: float","Point(row=3, column=0)","Point(row=4, column=15)",,autogpt_platform/market/market/utils/extension_types.py
track_download,function,"
    Track the download event in the database.

    Args:
        agent_id (str): The ID of the agent.
        version (int | None, optional): The version of the agent. Defaults to None.

    Raises:
        Exception: If there is an error tracking the download event.
","async def track_download(agent_id: str):
    """"""
    Track the download event in the database.

    Args:
        agent_id (str): The ID of the agent.
        version (int | None, optional): The version of the agent. Defaults to None.

    Raises:
        Exception: If there is an error tracking the download event.
    """"""
    try:
        await prisma.models.AnalyticsTracker.prisma().upsert(
            where={""agentId"": agent_id},
            data={
                ""update"": {""downloads"": {""increment"": 1}},
                ""create"": {""agentId"": agent_id, ""downloads"": 1, ""views"": 0},
            },
        )
    except Exception as e:
        raise Exception(f""Error tracking download event: {str(e)}"")","Point(row=3, column=0)","Point(row=23, column=67)",,autogpt_platform/market/market/utils/analytics.py
track_view,function,"
    Track the view event in the database.

    Args:
        agent_id (str): The ID of the agent.
        version (int | None, optional): The version of the agent. Defaults to None.

    Raises:
        Exception: If there is an error tracking the view event.
","async def track_view(agent_id: str):
    """"""
    Track the view event in the database.

    Args:
        agent_id (str): The ID of the agent.
        version (int | None, optional): The version of the agent. Defaults to None.

    Raises:
        Exception: If there is an error tracking the view event.
    """"""
    try:
        await prisma.models.AnalyticsTracker.prisma().upsert(
            where={""agentId"": agent_id},
            data={
                ""update"": {""views"": {""increment"": 1}},
                ""create"": {""agentId"": agent_id, ""downloads"": 0, ""views"": 1},
            },
        )
    except Exception as e:
        raise Exception(f""Error tracking view event: {str(e)}"")","Point(row=26, column=0)","Point(row=46, column=63)",,autogpt_platform/market/market/utils/analytics.py
list_agents,function,"
    Retrieve a list of agents based on the provided filters.

    Args:
        page (int): Page number (default: 1).
        page_size (int): Number of items per page (default: 10, min: 1, max: 100).
        name (str, optional): Filter by agent name.
        keyword (str, optional): Filter by keyword.
        category (str, optional): Filter by category.
        description (str, optional): Fuzzy search in description.
        description_threshold (int): Fuzzy search threshold (default: 60, min: 0, max: 100).
        sort_by (str): Field to sort by (default: ""createdAt"").
        sort_order (str): Sort order (asc or desc) (default: ""desc"").
        submission_status (str): Filter by submission status (default: ""APPROVED"").

    Returns:
        market.model.ListResponse[market.model.AgentResponse]: A response containing the list of agents and pagination information.

    Raises:
        HTTPException: If there is a client error (status code 400) or an unexpected error (status code 500).
","async def list_agents(
    page: int = fastapi.Query(1, ge=1, description=""Page number""),
    page_size: int = fastapi.Query(
        10, ge=1, le=100, description=""Number of items per page""
    ),
    name: typing.Optional[str] = fastapi.Query(
        None, description=""Filter by agent name""
    ),
    keyword: typing.Optional[str] = fastapi.Query(
        None, description=""Filter by keyword""
    ),
    category: typing.Optional[str] = fastapi.Query(
        None, description=""Filter by category""
    ),
    description: typing.Optional[str] = fastapi.Query(
        None, description=""Fuzzy search in description""
    ),
    description_threshold: int = fastapi.Query(
        60, ge=0, le=100, description=""Fuzzy search threshold""
    ),
    sort_by: str = fastapi.Query(""createdAt"", description=""Field to sort by""),
    sort_order: typing.Literal[""asc"", ""desc""] = fastapi.Query(
        ""desc"", description=""Sort order (asc or desc)""
    ),
    submission_status: prisma.enums.SubmissionStatus = fastapi.Query(
        default=prisma.enums.SubmissionStatus.APPROVED,
        description=""Filter by submission status"",
    ),
):
    """"""
    Retrieve a list of agents based on the provided filters.

    Args:
        page (int): Page number (default: 1).
        page_size (int): Number of items per page (default: 10, min: 1, max: 100).
        name (str, optional): Filter by agent name.
        keyword (str, optional): Filter by keyword.
        category (str, optional): Filter by category.
        description (str, optional): Fuzzy search in description.
        description_threshold (int): Fuzzy search threshold (default: 60, min: 0, max: 100).
        sort_by (str): Field to sort by (default: ""createdAt"").
        sort_order (str): Sort order (asc or desc) (default: ""desc"").
        submission_status (str): Filter by submission status (default: ""APPROVED"").

    Returns:
        market.model.ListResponse[market.model.AgentResponse]: A response containing the list of agents and pagination information.

    Raises:
        HTTPException: If there is a client error (status code 400) or an unexpected error (status code 500).
    """"""
    try:
        result = await market.db.get_agents(
            page=page,
            page_size=page_size,
            name=name,
            keyword=keyword,
            category=category,
            description=description,
            description_threshold=description_threshold,
            sort_by=sort_by,
            sort_order=sort_order,
            submission_status=submission_status,
        )

        agents = [
            market.model.AgentResponse(**agent.dict()) for agent in result[""agents""]
        ]

        return market.model.ListResponse(
            items=agents,
            total_count=result[""total_count""],
            page=result[""page""],
            page_size=result[""page_size""],
            total_pages=result[""total_pages""],
        )

    except market.db.AgentQueryError as e:
        raise fastapi.HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise fastapi.HTTPException(
            status_code=500, detail=f""An unexpected error occurred: {e}""
        )","Point(row=19, column=0)","Point(row=100, column=9)",,autogpt_platform/market/market/routes/agents.py
get_agent_details_endpoint,function,"
    Retrieve details of a specific agent.

    Args:
        agent_id (str): The ID of the agent to retrieve.
        version (Optional[int]): Specific version of the agent (default: None).

    Returns:
        market.model.AgentDetailResponse: The response containing the agent details.

    Raises:
        HTTPException: If the agent is not found or an unexpected error occurs.
","async def get_agent_details_endpoint(
    background_tasks: fastapi.BackgroundTasks,
    agent_id: str = fastapi.Path(..., description=""The ID of the agent to retrieve""),
    version: typing.Optional[int] = fastapi.Query(
        None, description=""Specific version of the agent""
    ),
):
    """"""
    Retrieve details of a specific agent.

    Args:
        agent_id (str): The ID of the agent to retrieve.
        version (Optional[int]): Specific version of the agent (default: None).

    Returns:
        market.model.AgentDetailResponse: The response containing the agent details.

    Raises:
        HTTPException: If the agent is not found or an unexpected error occurs.
    """"""
    try:
        agent = await market.db.get_agent_details(agent_id, version)
        background_tasks.add_task(market.utils.analytics.track_view, agent_id)
        return market.model.AgentDetailResponse(**agent.model_dump())

    except market.db.AgentQueryError as e:
        raise fastapi.HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise fastapi.HTTPException(
            status_code=500, detail=f""An unexpected error occurred: {str(e)}""
        )","Point(row=104, column=0)","Point(row=134, column=9)",,autogpt_platform/market/market/routes/agents.py
download_agent,function,"
    Download details of a specific agent.

    NOTE: This is the same as agent details, however it also triggers
    the ""download"" tracking. We don't actually want to download a file though

    Args:
        agent_id (str): The ID of the agent to retrieve.
        version (Optional[int]): Specific version of the agent (default: None).

    Returns:
        market.model.AgentDetailResponse: The response containing the agent details.

    Raises:
        HTTPException: If the agent is not found or an unexpected error occurs.
","async def download_agent(
    background_tasks: fastapi.BackgroundTasks,
    agent_id: str = fastapi.Path(..., description=""The ID of the agent to retrieve""),
    version: typing.Optional[int] = fastapi.Query(
        None, description=""Specific version of the agent""
    ),
):
    """"""
    Download details of a specific agent.

    NOTE: This is the same as agent details, however it also triggers
    the ""download"" tracking. We don't actually want to download a file though

    Args:
        agent_id (str): The ID of the agent to retrieve.
        version (Optional[int]): Specific version of the agent (default: None).

    Returns:
        market.model.AgentDetailResponse: The response containing the agent details.

    Raises:
        HTTPException: If the agent is not found or an unexpected error occurs.
    """"""
    try:
        agent = await market.db.get_agent_details(agent_id, version)
        background_tasks.add_task(market.utils.analytics.track_download, agent_id)
        return market.model.AgentDetailResponse(**agent.model_dump())

    except market.db.AgentQueryError as e:
        raise fastapi.HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise fastapi.HTTPException(
            status_code=500, detail=f""An unexpected error occurred: {str(e)}""
        )","Point(row=138, column=0)","Point(row=171, column=9)",,autogpt_platform/market/market/routes/agents.py
download_agent_file,function,"
    Download the agent file by streaming its content.

    Args:
        agent_id (str): The ID of the agent to download.
        version (Optional[int]): Specific version of the agent to download.

    Returns:
        StreamingResponse: A streaming response containing the agent's graph data.

    Raises:
        HTTPException: If the agent is not found or an unexpected error occurs.
","async def download_agent_file(
    background_tasks: fastapi.BackgroundTasks,
    agent_id: str = fastapi.Path(..., description=""The ID of the agent to download""),
    version: typing.Optional[int] = fastapi.Query(
        None, description=""Specific version of the agent""
    ),
) -> fastapi.responses.FileResponse:
    """"""
    Download the agent file by streaming its content.

    Args:
        agent_id (str): The ID of the agent to download.
        version (Optional[int]): Specific version of the agent to download.

    Returns:
        StreamingResponse: A streaming response containing the agent's graph data.

    Raises:
        HTTPException: If the agent is not found or an unexpected error occurs.
    """"""
    agent = await market.db.get_agent_details(agent_id, version)

    graph_data: prisma.Json = agent.graph

    background_tasks.add_task(market.utils.analytics.track_download, agent_id)

    file_name = f""agent_{agent_id}_v{version or 'latest'}.json""

    with tempfile.NamedTemporaryFile(
        mode=""w"", suffix="".json"", delete=False
    ) as tmp_file:
        tmp_file.write(json.dumps(graph_data))
        tmp_file.flush()

        return fastapi.responses.FileResponse(
            tmp_file.name, filename=file_name, media_type=""application/json""
        )","Point(row=175, column=0)","Point(row=211, column=9)",,autogpt_platform/market/market/routes/agents.py
top_agents_by_downloads,function,"
    Retrieve a list of top agents based on the number of downloads.

    Args:
        page (int): Page number (default: 1).
        page_size (int): Number of items per page (default: 10, min: 1, max: 100).
        submission_status (str): Filter by submission status (default: ""APPROVED"").

    Returns:
        market.model.ListResponse[market.model.AgentResponse]: A response containing the list of top agents and pagination information.

    Raises:
        HTTPException: If there is a client error (status code 400) or an unexpected error (status code 500).
","async def top_agents_by_downloads(
    page: int = fastapi.Query(1, ge=1, description=""Page number""),
    page_size: int = fastapi.Query(
        10, ge=1, le=100, description=""Number of items per page""
    ),
    submission_status: prisma.enums.SubmissionStatus = fastapi.Query(
        default=prisma.enums.SubmissionStatus.APPROVED,
        description=""Filter by submission status"",
    ),
) -> market.model.ListResponse[market.model.AgentResponse]:
    """"""
    Retrieve a list of top agents based on the number of downloads.

    Args:
        page (int): Page number (default: 1).
        page_size (int): Number of items per page (default: 10, min: 1, max: 100).
        submission_status (str): Filter by submission status (default: ""APPROVED"").

    Returns:
        market.model.ListResponse[market.model.AgentResponse]: A response containing the list of top agents and pagination information.

    Raises:
        HTTPException: If there is a client error (status code 400) or an unexpected error (status code 500).
    """"""
    try:
        result = await market.db.get_top_agents_by_downloads(
            page=page,
            page_size=page_size,
            submission_status=submission_status,
        )

        ret = market.model.ListResponse(
            total_count=result.total_count,
            page=result.page,
            page_size=result.page_size,
            total_pages=result.total_pages,
            items=[
                market.model.AgentResponse(
                    id=item.agent.id,
                    name=item.agent.name,
                    description=item.agent.description,
                    author=item.agent.author,
                    keywords=item.agent.keywords,
                    categories=item.agent.categories,
                    version=item.agent.version,
                    createdAt=item.agent.createdAt,
                    updatedAt=item.agent.updatedAt,
                    views=item.views,
                    downloads=item.downloads,
                    submissionStatus=item.agent.submissionStatus,
                )
                for item in result.items
                if item.agent is not None
            ],
        )

        return ret

    except market.db.AgentQueryError as e:
        raise fastapi.HTTPException(status_code=400, detail=str(e)) from e
    except Exception as e:
        raise fastapi.HTTPException(
            status_code=500, detail=f""An unexpected error occurred: {e}""
        ) from e","Point(row=219, column=0)","Point(row=282, column=16)",,autogpt_platform/market/market/routes/agents.py
get_featured_agents,function,"
    Retrieve a list of featured agents based on the provided category.

    Args:
        category (str): Category of featured agents (default: ""featured"").
        page (int): Page number (default: 1).
        page_size (int): Number of items per page (default: 10, min: 1, max: 100).
        submission_status (str): Filter by submission status (default: ""APPROVED"").

    Returns:
        market.model.ListResponse[market.model.AgentResponse]: A response containing the list of featured agents and pagination information.

    Raises:
        HTTPException: If there is a client error (status code 400) or an unexpected error (status code 500).
","async def get_featured_agents(
    category: str = fastapi.Query(
        ""featured"", description=""Category of featured agents""
    ),
    page: int = fastapi.Query(1, ge=1, description=""Page number""),
    page_size: int = fastapi.Query(
        10, ge=1, le=100, description=""Number of items per page""
    ),
    submission_status: prisma.enums.SubmissionStatus = fastapi.Query(
        default=prisma.enums.SubmissionStatus.APPROVED,
        description=""Filter by submission status"",
    ),
):
    """"""
    Retrieve a list of featured agents based on the provided category.

    Args:
        category (str): Category of featured agents (default: ""featured"").
        page (int): Page number (default: 1).
        page_size (int): Number of items per page (default: 10, min: 1, max: 100).
        submission_status (str): Filter by submission status (default: ""APPROVED"").

    Returns:
        market.model.ListResponse[market.model.AgentResponse]: A response containing the list of featured agents and pagination information.

    Raises:
        HTTPException: If there is a client error (status code 400) or an unexpected error (status code 500).
    """"""
    try:
        result = await market.db.get_featured_agents(
            category=category,
            page=page,
            page_size=page_size,
            submission_status=submission_status,
        )

        ret = market.model.ListResponse(
            total_count=result.total_count,
            page=result.page,
            page_size=result.page_size,
            total_pages=result.total_pages,
            items=[
                market.model.AgentResponse(
                    id=item.agent.id,
                    name=item.agent.name,
                    description=item.agent.description,
                    author=item.agent.author,
                    keywords=item.agent.keywords,
                    categories=item.agent.categories,
                    version=item.agent.version,
                    createdAt=item.agent.createdAt,
                    updatedAt=item.agent.updatedAt,
                    views=(
                        item.agent.AnalyticsTracker[0].views
                        if item.agent.AnalyticsTracker
                        and len(item.agent.AnalyticsTracker) > 0
                        else 0
                    ),
                    downloads=(
                        item.agent.AnalyticsTracker[0].downloads
                        if item.agent.AnalyticsTracker
                        and len(item.agent.AnalyticsTracker) > 0
                        else 0
                    ),
                    submissionStatus=item.agent.submissionStatus,
                )
                for item in result.featured_agents
                if item.agent is not None
            ],
        )

        return ret

    except market.db.AgentQueryError as e:
        raise fastapi.HTTPException(status_code=400, detail=str(e)) from e
    except Exception as e:
        raise fastapi.HTTPException(
            status_code=500, detail=f""An unexpected error occurred: {e}""
        ) from e","Point(row=289, column=0)","Point(row=367, column=16)",,autogpt_platform/market/market/routes/agents.py
delete_agent,function,"
    Delete an agent and all related records from the database.

    Args:
        agent_id (str): The ID of the agent to delete.

    Returns:
        market.model.AgentResponse: The deleted agent's data.

    Raises:
        fastapi.HTTPException: If the agent is not found or if there's an error during deletion.
","async def delete_agent(
    agent_id: str,
    user: autogpt_libs.auth.User = fastapi.Depends(
        autogpt_libs.auth.requires_admin_user
    ),
):
    """"""
    Delete an agent and all related records from the database.

    Args:
        agent_id (str): The ID of the agent to delete.

    Returns:
        market.model.AgentResponse: The deleted agent's data.

    Raises:
        fastapi.HTTPException: If the agent is not found or if there's an error during deletion.
    """"""
    try:
        deleted_agent = await market.db.delete_agent(agent_id)
        if deleted_agent:
            return market.model.AgentResponse(**deleted_agent.dict())
        else:
            raise fastapi.HTTPException(status_code=404, detail=""Agent not found"")
    except market.db.AgentQueryError as e:
        logger.error(f""Error deleting agent: {e}"")
        raise fastapi.HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        logger.error(f""Unexpected error deleting agent: {e}"")
        raise fastapi.HTTPException(
            status_code=500, detail=""An unexpected error occurred""
        )","Point(row=18, column=0)","Point(row=49, column=9)",,autogpt_platform/market/market/routes/admin.py
create_agent_entry,function,"
    A basic endpoint to create a new agent entry in the database.

","async def create_agent_entry(
    request: market.model.AddAgentRequest,
    user: autogpt_libs.auth.User = fastapi.Depends(
        autogpt_libs.auth.requires_admin_user
    ),
):
    """"""
    A basic endpoint to create a new agent entry in the database.

    """"""
    try:
        agent = await market.db.create_agent_entry(
            request.graph[""name""],
            request.graph[""description""],
            request.author,
            request.keywords,
            request.categories,
            prisma.Json(request.graph),
        )

        return fastapi.responses.PlainTextResponse(agent.model_dump_json())
    except market.db.AgentQueryError as e:
        raise fastapi.HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        raise fastapi.HTTPException(status_code=500, detail=str(e))","Point(row=53, column=0)","Point(row=77, column=67)",,autogpt_platform/market/market/routes/admin.py
set_agent_featured,function,"
    A basic endpoint to set an agent as featured in the database.
","async def set_agent_featured(
    agent_id: str,
    categories: list[str] = fastapi.Query(
        default=[""featured""],
        description=""The categories to set the agent as featured in"",
    ),
    user: autogpt_libs.auth.User = fastapi.Depends(
        autogpt_libs.auth.requires_admin_user
    ),
) -> market.model.FeaturedAgentResponse:
    """"""
    A basic endpoint to set an agent as featured in the database.
    """"""
    try:
        agent = await market.db.set_agent_featured(
            agent_id, is_active=True, featured_categories=categories
        )
        return market.model.FeaturedAgentResponse(**agent.model_dump())
    except market.db.AgentQueryError as e:
        raise fastapi.HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        raise fastapi.HTTPException(status_code=500, detail=str(e))","Point(row=81, column=0)","Point(row=102, column=67)",,autogpt_platform/market/market/routes/admin.py
get_agent_featured,function,"
    A basic endpoint to get an agent as featured in the database.
","async def get_agent_featured(
    agent_id: str,
    user: autogpt_libs.auth.User = fastapi.Depends(
        autogpt_libs.auth.requires_admin_user
    ),
) -> market.model.FeaturedAgentResponse | None:
    """"""
    A basic endpoint to get an agent as featured in the database.
    """"""
    try:
        agent = await market.db.get_agent_featured(agent_id)
        if agent:
            return market.model.FeaturedAgentResponse(**agent.model_dump())
        else:
            return None
    except market.db.AgentQueryError as e:
        raise fastapi.HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        raise fastapi.HTTPException(status_code=500, detail=str(e))","Point(row=106, column=0)","Point(row=124, column=67)",,autogpt_platform/market/market/routes/admin.py
unset_agent_featured,function,"
    A basic endpoint to unset an agent as featured in the database.
","async def unset_agent_featured(
    agent_id: str,
    category: str = ""featured"",
    user: autogpt_libs.auth.User = fastapi.Depends(
        autogpt_libs.auth.requires_admin_user
    ),
) -> market.model.FeaturedAgentResponse | None:
    """"""
    A basic endpoint to unset an agent as featured in the database.
    """"""
    try:
        featured = await market.db.remove_featured_category(agent_id, category=category)
        if featured:
            return market.model.FeaturedAgentResponse(**featured.model_dump())
        else:
            return None
    except market.db.AgentQueryError as e:
        raise fastapi.HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        raise fastapi.HTTPException(status_code=500, detail=str(e))","Point(row=128, column=0)","Point(row=147, column=67)",,autogpt_platform/market/market/routes/admin.py
get_not_featured_agents,function,"
    A basic endpoint to get all not featured agents in the database.
","async def get_not_featured_agents(
    page: int = fastapi.Query(1, ge=1, description=""Page number""),
    page_size: int = fastapi.Query(
        10, ge=1, le=100, description=""Number of items per page""
    ),
    user: autogpt_libs.auth.User = fastapi.Depends(
        autogpt_libs.auth.requires_admin_user
    ),
) -> market.model.ListResponse[market.model.AgentResponse]:
    """"""
    A basic endpoint to get all not featured agents in the database.
    """"""
    try:
        agents = await market.db.get_not_featured_agents(page=page, page_size=page_size)
        return market.model.ListResponse(
            items=[
                market.model.AgentResponse(**agent.model_dump()) for agent in agents
            ],
            total_count=len(agents),
            page=page,
            page_size=page_size,
            total_pages=999,
        )
    except market.db.AgentQueryError as e:
        raise fastapi.HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        raise fastapi.HTTPException(status_code=500, detail=str(e))","Point(row=151, column=0)","Point(row=177, column=67)",,autogpt_platform/market/market/routes/admin.py
get_agent_submissions,function,,"async def get_agent_submissions(
    page: int = fastapi.Query(1, ge=1, description=""Page number""),
    page_size: int = fastapi.Query(
        10, ge=1, le=100, description=""Number of items per page""
    ),
    name: typing.Optional[str] = fastapi.Query(
        None, description=""Filter by agent name""
    ),
    keyword: typing.Optional[str] = fastapi.Query(
        None, description=""Filter by keyword""
    ),
    category: typing.Optional[str] = fastapi.Query(
        None, description=""Filter by category""
    ),
    description: typing.Optional[str] = fastapi.Query(
        None, description=""Fuzzy search in description""
    ),
    description_threshold: int = fastapi.Query(
        60, ge=0, le=100, description=""Fuzzy search threshold""
    ),
    sort_by: str = fastapi.Query(""createdAt"", description=""Field to sort by""),
    sort_order: typing.Literal[""asc"", ""desc""] = fastapi.Query(
        ""desc"", description=""Sort order (asc or desc)""
    ),
    user: autogpt_libs.auth.User = fastapi.Depends(
        autogpt_libs.auth.requires_admin_user
    ),
) -> market.model.ListResponse[market.model.AgentResponse]:
    logger.info(""Getting agent submissions"")
    try:
        result = await market.db.get_agents(
            page=page,
            page_size=page_size,
            name=name,
            keyword=keyword,
            category=category,
            description=description,
            description_threshold=description_threshold,
            sort_by=sort_by,
            sort_order=sort_order,
            submission_status=prisma.enums.SubmissionStatus.PENDING,
        )

        agents = [
            market.model.AgentResponse(**agent.dict()) for agent in result[""agents""]
        ]

        return market.model.ListResponse(
            items=agents,
            total_count=result[""total_count""],
            page=result[""page""],
            page_size=result[""page_size""],
            total_pages=result[""total_pages""],
        )

    except market.db.AgentQueryError as e:
        logger.error(f""Error getting agent submissions: {e}"")
        raise fastapi.HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f""Error getting agent submissions: {e}"")
        raise fastapi.HTTPException(
            status_code=500, detail=f""An unexpected error occurred: {e}""
        )","Point(row=184, column=0)","Point(row=246, column=9)",,autogpt_platform/market/market/routes/admin.py
review_submission,function,"
    A basic endpoint to review a submission in the database.
","async def review_submission(
    review_request: market.model.SubmissionReviewRequest,
    user: autogpt_libs.auth.User = fastapi.Depends(
        autogpt_libs.auth.requires_admin_user
    ),
) -> prisma.models.Agents | None:
    """"""
    A basic endpoint to review a submission in the database.
    """"""
    logger.info(
        f""Reviewing submission: {review_request.agent_id}, {review_request.version}""
    )
    try:
        agent = await market.db.update_agent_entry(
            agent_id=review_request.agent_id,
            version=review_request.version,
            submission_state=review_request.status,
            comments=review_request.comments,
        )
        return agent
    except market.db.AgentQueryError as e:
        raise fastapi.HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        raise fastapi.HTTPException(status_code=500, detail=str(e))","Point(row=250, column=0)","Point(row=273, column=67)",,autogpt_platform/market/market/routes/admin.py
get_categories,function,"
    A basic endpoint to get all available categories.
","async def get_categories() -> market.model.CategoriesResponse:
    """"""
    A basic endpoint to get all available categories.
    """"""
    try:
        categories = await market.db.get_all_categories()
        return categories
    except Exception as e:
        raise fastapi.HTTPException(status_code=500, detail=str(e))","Point(row=277, column=0)","Point(row=285, column=67)",,autogpt_platform/market/market/routes/admin.py
override_auth_middleware,function,,"async def override_auth_middleware(request: fastapi.Request):
    return {""sub"": ""3e53486c-cf57-477e-ba2a-cb02dc828e1a"", ""role"": ""admin""}","Point(row=14, column=0)","Point(row=15, column=75)",,autogpt_platform/market/market/routes/admin_tests.py
test_get_submissions,function,,"def test_get_submissions():
    with mock.patch(""market.db.get_agents"") as mock_get_agents:
        mock_get_agents.return_value = {
            ""agents"": [],
            ""total_count"": 0,
            ""page"": 1,
            ""page_size"": 10,
            ""total_pages"": 0,
        }
        response = client.get(
            ""/api/v1/market/admin/agent/submissions?page=1&page_size=10&description_threshold=60&sort_by=createdAt&sort_order=desc"",
            headers={""Bearer"": """"},
        )
        assert response.status_code == 200
        assert response.json() == {
            ""agents"": [],
            ""total_count"": 0,
            ""page"": 1,
            ""page_size"": 10,
            ""total_pages"": 0,
        }","Point(row=23, column=0)","Point(row=43, column=9)",,autogpt_platform/market/market/routes/admin_tests.py
test_review_submission,function,,"def test_review_submission():
    with mock.patch(""market.db.update_agent_entry"") as mock_update_agent_entry:
        mock_update_agent_entry.return_value = prisma.models.Agents(
            id=""aaa-bbb-ccc"",
            version=1,
            createdAt=datetime.datetime.fromisoformat(""2021-10-01T00:00:00+00:00""),
            updatedAt=datetime.datetime.fromisoformat(""2021-10-01T00:00:00+00:00""),
            submissionStatus=prisma.enums.SubmissionStatus.APPROVED,
            submissionDate=datetime.datetime.fromisoformat(""2021-10-01T00:00:00+00:00""),
            submissionReviewComments=""Looks good"",
            submissionReviewDate=datetime.datetime.fromisoformat(
                ""2021-10-01T00:00:00+00:00""
            ),
            keywords=[""test""],
            categories=[""test""],
            graph='{""name"": ""test"", ""description"": ""test""}',  # type: ignore
        )
        response = client.post(
            ""/api/v1/market/admin/agent/submissions"",
            headers={
                ""Authorization"": ""Bearer token""
            },  # Assuming you need an authorization token
            json={
                ""agent_id"": ""aaa-bbb-ccc"",
                ""version"": 1,
                ""status"": ""APPROVED"",
                ""comments"": ""Looks good"",
            },
        )
        assert response.status_code == 200","Point(row=46, column=0)","Point(row=75, column=42)",,autogpt_platform/market/market/routes/admin_tests.py
search,function,"searches endpoint for agents

    Args:
        query (str): the search query
        page (int, optional): the pagination page to start on. Defaults to 1.
        page_size (int, optional): the number of items to return per page. Defaults to 10.
        category (str | None, optional): the agent category to filter by. None is no filter. Defaults to None.
        description_threshold (int, optional): the number of characters to return from the description. Defaults to 60.
        sort_by (str, optional): Sorting by column. Defaults to ""rank"".
        sort_order ('asc' | 'desc', optional): the sort order based on sort_by. Defaults to ""desc"".
","async def search(
    query: str,
    page: int = fastapi.Query(1, description=""The pagination page to start on""),
    page_size: int = fastapi.Query(
        10, description=""The number of items to return per page""
    ),
    categories: typing.List[str] = fastapi.Query(
        None, description=""The categories to filter by""
    ),
    description_threshold: int = fastapi.Query(
        60, description=""The number of characters to return from the description""
    ),
    sort_by: str = fastapi.Query(""rank"", description=""Sorting by column""),
    sort_order: typing.Literal[""desc"", ""asc""] = fastapi.Query(
        ""desc"", description=""The sort order based on sort_by""
    ),
    submission_status: prisma.enums.SubmissionStatus = fastapi.Query(
        prisma.enums.SubmissionStatus.APPROVED,
        description=""The submission status to filter by"",
    ),
) -> market.model.ListResponse[market.utils.extension_types.AgentsWithRank]:
    """"""searches endpoint for agents

    Args:
        query (str): the search query
        page (int, optional): the pagination page to start on. Defaults to 1.
        page_size (int, optional): the number of items to return per page. Defaults to 10.
        category (str | None, optional): the agent category to filter by. None is no filter. Defaults to None.
        description_threshold (int, optional): the number of characters to return from the description. Defaults to 60.
        sort_by (str, optional): Sorting by column. Defaults to ""rank"".
        sort_order ('asc' | 'desc', optional): the sort order based on sort_by. Defaults to ""desc"".
    """"""
    agents = await market.db.search_db(
        query=query,
        page=page,
        page_size=page_size,
        categories=categories,
        description_threshold=description_threshold,
        sort_by=sort_by,
        sort_order=sort_order,
        submission_status=submission_status,
    )
    return agents","Point(row=13, column=0)","Point(row=55, column=17)",,autogpt_platform/market/market/routes/search.py
agent_installed_endpoint,function,"
    Endpoint to track agent installation events from the marketplace.

    Args:
        event_data (market.model.AgentInstalledFromMarketplaceEventData): The event data.
","async def agent_installed_endpoint(
    event_data: market.model.AgentInstalledFromMarketplaceEventData,
):
    """"""
    Endpoint to track agent installation events from the marketplace.

    Args:
        event_data (market.model.AgentInstalledFromMarketplaceEventData): The event data.
    """"""
    try:
        await market.db.create_agent_installed_event(event_data)
    except market.db.AgentQueryError as e:
        raise fastapi.HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise fastapi.HTTPException(
            status_code=500, detail=f""An unexpected error occurred: {e}""
        )","Point(row=9, column=0)","Point(row=25, column=9)",,autogpt_platform/market/market/routes/analytics.py
submit_agent,function,"
    A basic endpoint to create a new agent entry in the database.
","async def submit_agent(
    request: market.model.AddAgentRequest,
    user: autogpt_libs.auth.User = fastapi.Depends(autogpt_libs.auth.requires_user),
):
    """"""
    A basic endpoint to create a new agent entry in the database.
    """"""
    try:
        agent = await market.db.create_agent_entry(
            request.graph[""name""],
            request.graph[""description""],
            request.author,
            request.keywords,
            request.categories,
            prisma.Json(request.graph),
        )

        return fastapi.responses.PlainTextResponse(agent.model_dump_json())
    except market.db.AgentQueryError as e:
        raise fastapi.HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        raise fastapi.HTTPException(status_code=500, detail=str(e))","Point(row=13, column=0)","Point(row=34, column=67)",,autogpt_platform/market/market/routes/submissions.py
